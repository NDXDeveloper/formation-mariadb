ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 15.11 Sharding et distribution horizontale

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 4-5 heures  
> **PrÃ©requis** : 
> - Sections 15.1-15.10 (Performance, Partitionnement, Gestion avancÃ©e)
> - ExpÃ©rience avec rÃ©plication MariaDB
> - Architecture distribuÃ©e et concepts de scaling
> - ComprÃ©hension des trade-offs CAP theorem

---

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- **Comprendre le sharding** et le distinguer du partitionnement
- **Identifier les besoins** justifiant une architecture sharded
- **Choisir une stratÃ©gie** de sharding appropriÃ©e
- **ImplÃ©menter le sharding** avec Spider Storage Engine
- **Configurer ProxySQL** pour le routing intelligent
- **GÃ©rer les migrations** vers architecture sharded
- **RÃ©soudre les dÃ©fis** des jointures cross-shard
- **Monitorer et maintenir** un cluster sharded
- **Appliquer les best practices** de production
- **Planifier la croissance** et le rebalancing

---

## Introduction

Le **sharding** (ou **distribution horizontale**) est une technique qui distribue les donnÃ©es d'une table sur **plusieurs serveurs MariaDB indÃ©pendants**, chacun gÃ©rant un sous-ensemble des donnÃ©es.

### Partitionnement vs Sharding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARTITIONNEMENT (Vertical Scaling)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Un serveur unique avec partitions                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Server 1 (500 GB RAM, 100 cores)            â”‚          â”‚
â”‚  â”‚  â”œâ”€ orders p2020 (100 GB)                    â”‚          â”‚
â”‚  â”‚  â”œâ”€ orders p2021 (100 GB)                    â”‚          â”‚
â”‚  â”‚  â”œâ”€ orders p2022 (100 GB)                    â”‚          â”‚
â”‚  â”‚  â””â”€ orders p2023 (100 GB)                    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                            â”‚
â”‚  Limites :                                                 â”‚
â”‚  â€¢ RAM : ~4 TB maximum pratique                            â”‚
â”‚  â€¢ CPU : ~200 cores maximum                                â”‚
â”‚  â€¢ I/O : LimitÃ© par un seul serveur                        â”‚
â”‚  â€¢ CoÃ»t : Serveur unique trÃ¨s cher                         â”‚
â”‚  â€¢ SPOF : Single Point of Failure                          â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARDING (Horizontal Scaling)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Plusieurs serveurs indÃ©pendants                           â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Shard 1           â”‚  â”‚  Shard 2           â”‚            â”‚
â”‚  â”‚  (64 GB RAM)       â”‚  â”‚  (64 GB RAM)       â”‚            â”‚
â”‚  â”‚  orders 2020-2021  â”‚  â”‚  orders 2022-2023  â”‚            â”‚
â”‚  â”‚  (200 GB data)     â”‚  â”‚  (200 GB data)     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Shard 3           â”‚  â”‚  Shard 4           â”‚            â”‚
â”‚  â”‚  (64 GB RAM)       â”‚  â”‚  (64 GB RAM)       â”‚            â”‚
â”‚  â”‚  orders region EU  â”‚  â”‚  orders region US  â”‚            â”‚
â”‚  â”‚  (150 GB data)     â”‚  â”‚  (250 GB data)     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                            â”‚
â”‚  Avantages :                                               â”‚
â”‚  âœ… Scaling quasi-illimitÃ© (ajout serveurs)                â”‚
â”‚  âœ… CoÃ»t optimisÃ© (serveurs standards)                     â”‚
â”‚  âœ… ParallÃ©lisation native                                 â”‚
â”‚  âœ… Isolation des pannes                                   â”‚
â”‚  âœ… Localisation gÃ©ographique possible                     â”‚
â”‚                                                            â”‚
â”‚  DÃ©fis :                                                   â”‚
â”‚  âš ï¸ ComplexitÃ© architecture                                â”‚
â”‚  âš ï¸ Jointures cross-shard difficiles                       â”‚
â”‚  âš ï¸ Transactions distribuÃ©es complexes                     â”‚
â”‚  âš ï¸ Rebalancing nÃ©cessaire                                 â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quand utiliser le sharding ?

### ScÃ©narios appropriÃ©s âœ…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SIGNAUX QUE LE SHARDING EST NÃ‰CESSAIRE            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  1. LIMITES MATÃ‰RIELLES ATTEINTES                  â”‚
â”‚     â€¢ Table > 10 TB impossible Ã  gÃ©rer             â”‚
â”‚     â€¢ RAM requise > 2 TB (limite pratique)         â”‚
â”‚     â€¢ Single server trop coÃ»teux                   â”‚
â”‚                                                    â”‚
â”‚  2. CROISSANCE EXPONENTIELLE                       â”‚
â”‚     â€¢ +100% donnÃ©es/an                             â”‚
â”‚     â€¢ Vertical scaling non viable long terme       â”‚
â”‚     â€¢ Budget contraint                             â”‚
â”‚                                                    â”‚
â”‚  3. WORKLOAD ISOLABLE                              â”‚
â”‚     â€¢ DonnÃ©es naturellement partitionnables        â”‚
â”‚     â€¢ Peu de requÃªtes cross-partition              â”‚
â”‚     â€¢ Exemples : Multi-tenant, gÃ©ographie          â”‚
â”‚                                                    â”‚
â”‚  4. HAUTE DISPONIBILITÃ‰ REQUISE                    â”‚
â”‚     â€¢ TolÃ©rance aux pannes partielles              â”‚
â”‚     â€¢ 99.99%+ uptime SLA                           â”‚
â”‚     â€¢ Isolation des incidents                      â”‚
â”‚                                                    â”‚
â”‚  5. LOCALISATION GÃ‰OGRAPHIQUE                      â”‚
â”‚     â€¢ ConformitÃ© rÃ©glementaire (GDPR)              â”‚
â”‚     â€¢ Latence optimisÃ©e par rÃ©gion                 â”‚
â”‚     â€¢ DonnÃ©es EU vs US vs APAC                     â”‚
â”‚                                                    â”‚
â”‚  6. WRITE THROUGHPUT INSUFFISANT                   â”‚
â”‚     â€¢ Single server saturÃ© en writes               â”‚
â”‚     â€¢ Besoin > 100k writes/sec                     â”‚
â”‚     â€¢ Distribution pour parallÃ©lisme               â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quand NE PAS sharder âŒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ALTERNATIVES AU SHARDING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Si problÃ¨me = READ performance :                  â”‚
â”‚  â†’ Read replicas + ProxySQL                        â”‚
â”‚  â†’ Beaucoup plus simple que sharding               â”‚
â”‚                                                    â”‚
â”‚  Si table < 5 TB :                                 â”‚
â”‚  â†’ Partitionnement + serveur plus gros             â”‚
â”‚  â†’ Optimisation requÃªtes + index                   â”‚
â”‚                                                    â”‚
â”‚  Si peu de croissance :                            â”‚
â”‚  â†’ Archivage des anciennes donnÃ©es                 â”‚
â”‚  â†’ Pas besoin de sharding                          â”‚
â”‚                                                    â”‚
â”‚  Si beaucoup de jointures complexes :              â”‚
â”‚  â†’ Sharding rendra queries impossibles             â”‚
â”‚  â†’ Rester sur architecture centralisÃ©e             â”‚
â”‚                                                    â”‚
â”‚  Si Ã©quipe petite/inexpÃ©rimentÃ©e :                 â”‚
â”‚  â†’ ComplexitÃ© opÃ©rationnelle trop Ã©levÃ©e           â”‚
â”‚  â†’ Risque d'erreurs critiques                      â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## StratÃ©gies de sharding

### 1. Sharding par clÃ© (Key-based / Hash)

**Principe** : Distribuer selon hash d'une clÃ©.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARDING PAR CLÃ‰ (customer_id)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Fonction : shard_id = HASH(customer_id) MOD 4      â”‚
â”‚                                                     â”‚
â”‚  customer_id = 12345 â†’ HASH â†’ 2 â†’ Shard 2           â”‚
â”‚  customer_id = 67890 â†’ HASH â†’ 0 â†’ Shard 0           â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Shard 0 â”‚  â”‚ Shard 1 â”‚  â”‚ Shard 2 â”‚  â”‚ Shard 3 â”‚ â”‚
â”‚  â”‚ 25% dataâ”‚  â”‚ 25% dataâ”‚  â”‚ 25% dataâ”‚  â”‚ 25% dataâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  Avantages :                                        â”‚
â”‚  âœ… Distribution uniforme                           â”‚
â”‚  âœ… Routing simple (calcul hash)                    â”‚
â”‚  âœ… Scaling prÃ©visible                              â”‚
â”‚                                                     â”‚
â”‚  InconvÃ©nients :                                    â”‚
â”‚  âŒ Rebalancing complexe (ajout shard)              â”‚
â”‚  âŒ RequÃªtes multi-client = multi-shard             â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplÃ©mentation** :

```sql
-- Fonction de sharding
CREATE FUNCTION get_shard_id(p_customer_id BIGINT)
RETURNS INT
DETERMINISTIC
RETURN MOD(p_customer_id, 4);  -- 4 shards

-- Application dÃ©termine shard
SET @shard_id = get_shard_id(12345);  -- 1

-- Connect to shard 1 et exÃ©cuter requÃªte
SELECT * FROM orders WHERE customer_id = 12345;
```

### 2. Sharding par plage (Range-based)

**Principe** : Distribuer selon plages de valeurs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARDING PAR PLAGE (order_date)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Shard 0 : 2020-2021                               â”‚
â”‚  Shard 1 : 2022-2023                               â”‚
â”‚  Shard 2 : 2024+                                   â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Shard 0    â”‚  â”‚  Shard 1    â”‚  â”‚  Shard 2    â”‚ â”‚
â”‚  â”‚  2020-2021  â”‚  â”‚  2022-2023  â”‚  â”‚  2024+      â”‚ â”‚
â”‚  â”‚  (archive)  â”‚  â”‚  (archive)  â”‚  â”‚  (actif)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚
â”‚  Avantages :                                       â”‚
â”‚  âœ… RequÃªtes par plage efficaces                   â”‚
â”‚  âœ… Archivage naturel (shard complet)              â”‚
â”‚  âœ… Ajout shard simple                             â”‚
â”‚                                                    â”‚
â”‚  InconvÃ©nients :                                   â”‚
â”‚  âŒ Distribution potentiellement inÃ©gale           â”‚
â”‚  âŒ Hot shard (shard actif surchargÃ©)              â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Sharding gÃ©ographique

**Principe** : Distribuer selon localisation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARDING GÃ‰OGRAPHIQUE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Shard EU    â”‚  â”‚  Shard US    â”‚  â”‚  Shard APAC  â”‚  â”‚
â”‚  â”‚  Frankfurt   â”‚  â”‚  Virginia    â”‚  â”‚  Singapore   â”‚  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â”‚  FR, DE, IT  â”‚  â”‚  US, CA, BR  â”‚  â”‚  SG, JP, AU  â”‚  â”‚
â”‚  â”‚  ES, UK      â”‚  â”‚  MX          â”‚  â”‚  IN, KR      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â”‚  Avantages :                                           â”‚
â”‚  âœ… Latence minimale (proximitÃ© users)                 â”‚
â”‚  âœ… ConformitÃ© GDPR (donnÃ©es EU en EU)                 â”‚
â”‚  âœ… Isolation des pannes gÃ©ographiques                 â”‚
â”‚                                                        â”‚
â”‚  Cas d'usage :                                         â”‚
â”‚  â€¢ SaaS global                                         â”‚
â”‚  â€¢ E-commerce international                            â”‚
â”‚  â€¢ RÃ©glementation stricte                              â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Sharding par tenant (Multi-tenancy)

**Principe** : Un ou plusieurs tenants par shard.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARDING PAR TENANT (SaaS)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Shard 0 : Tenants 1-100 (petits clients)          â”‚
â”‚  Shard 1 : Tenant 101 (gros client exclusif)       â”‚
â”‚  Shard 2 : Tenants 102-200                         â”‚
â”‚  Shard 3 : Tenant 201 (gros client exclusif)       â”‚
â”‚                                                    â”‚
â”‚  Avantages :                                       â”‚
â”‚  âœ… Isolation complÃ¨te des donnÃ©es                 â”‚
â”‚  âœ… Performance prÃ©dictible par tenant             â”‚
â”‚  âœ… Migration tenant facile                        â”‚
â”‚  âœ… Facturation prÃ©cise                            â”‚
â”‚                                                    â”‚
â”‚  Use case :                                        â”‚
â”‚  â€¢ Plateforme SaaS B2B                             â”‚
â”‚  â€¢ Clients avec besoins diffÃ©rents                 â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ImplÃ©mentation avec Spider Storage Engine

### Introduction Ã  Spider

**Spider** est un storage engine MariaDB qui permet de crÃ©er des tables fÃ©dÃ©rÃ©es distribuÃ©es sur plusieurs serveurs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCHITECTURE SPIDER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Application                                       â”‚
â”‚       â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  MariaDB Spider Node (Proxy)         â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚
â”‚  â”‚  â”‚ Table "orders" (ENGINE=Spider) â”‚  â”‚          â”‚
â”‚  â”‚  â”‚ - Metadata seulement           â”‚  â”‚          â”‚
â”‚  â”‚  â”‚ - Routing logic                â”‚  â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚                                  â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚        â–¼        â–¼        â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Shard 0 â”‚ â”‚ Shard 1 â”‚ â”‚ Shard 2 â”‚               â”‚
â”‚  â”‚ orders  â”‚ â”‚ orders  â”‚ â”‚ orders  â”‚               â”‚
â”‚  â”‚ (data)  â”‚ â”‚ (data)  â”‚ â”‚ (data)  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration Spider

```sql
-- Sur Spider Node (proxy)

-- 1. Installer Spider engine
INSTALL SONAME 'ha_spider';

-- 2. CrÃ©er serveurs backend (shards)
CREATE SERVER shard0
FOREIGN DATA WRAPPER mysql
OPTIONS (
    HOST '192.168.1.10',
    DATABASE 'orders_db',
    USER 'spider_user',
    PASSWORD 'spider_pass',
    PORT 3306
);

CREATE SERVER shard1
FOREIGN DATA WRAPPER mysql
OPTIONS (
    HOST '192.168.1.11',
    DATABASE 'orders_db',
    USER 'spider_user',
    PASSWORD 'spider_pass',
    PORT 3306
);

CREATE SERVER shard2
FOREIGN DATA WRAPPER mysql
OPTIONS (
    HOST '192.168.1.12',
    DATABASE 'orders_db',
    USER 'spider_user',
    PASSWORD 'spider_pass',
    PORT 3306
);

-- 3. CrÃ©er table Spider avec sharding par hash
CREATE TABLE orders (
    id BIGINT NOT NULL AUTO_INCREMENT,
    customer_id BIGINT NOT NULL,
    order_date DATE NOT NULL,
    amount DECIMAL(10,2),
    PRIMARY KEY (id, customer_id)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "orders"'
PARTITION BY HASH (customer_id) (
    PARTITION pt0 COMMENT='srv "shard0"',
    PARTITION pt1 COMMENT='srv "shard1"',
    PARTITION pt2 COMMENT='srv "shard2"'
);

-- 4. CrÃ©er tables backend sur chaque shard
-- Sur shard0, shard1, shard2 :
CREATE TABLE orders (
    id BIGINT NOT NULL AUTO_INCREMENT,
    customer_id BIGINT NOT NULL,
    order_date DATE NOT NULL,
    amount DECIMAL(10,2),
    PRIMARY KEY (id, customer_id)
) ENGINE=InnoDB;
```

### RequÃªtes via Spider

```sql
-- Application se connecte au Spider Node

-- INSERT : Spider route automatiquement vers bon shard
INSERT INTO orders (customer_id, order_date, amount)
VALUES (12345, '2024-01-15', 99.99);
-- Automatiquement vers shard HASH(12345) MOD 3 = shard 1

-- SELECT avec sharding key : Query un seul shard
SELECT * FROM orders WHERE customer_id = 12345;
-- Automatiquement routÃ© vers shard 1

-- SELECT sans sharding key : Query tous shards
SELECT COUNT(*) FROM orders WHERE order_date = '2024-01-15';
-- Spider query shard0, shard1, shard2 en parallÃ¨le
-- AgrÃ¨ge rÃ©sultats

-- JOIN : Spider tente d'optimiser
SELECT o.*, c.name
FROM orders o
JOIN customers c ON c.id = o.customer_id
WHERE o.customer_id = 12345;
-- Si customers aussi sharded par customer_id : local join
-- Sinon : Cross-shard join (lent)
```

### Sharding par plage avec Spider

```sql
-- Sharding par annÃ©e
CREATE TABLE orders_by_year (
    id BIGINT NOT NULL AUTO_INCREMENT,
    customer_id BIGINT NOT NULL,
    order_date DATE NOT NULL,
    amount DECIMAL(10,2),
    PRIMARY KEY (id, order_date)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "orders"'
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2022 VALUES LESS THAN (2023) 
        COMMENT='srv "shard_2022"',
    PARTITION p2023 VALUES LESS THAN (2024) 
        COMMENT='srv "shard_2023"',
    PARTITION p2024 VALUES LESS THAN (2025) 
        COMMENT='srv "shard_2024"'
);
```

---

## ProxySQL pour routing intelligent

### Architecture ProxySQL

ProxySQL est un proxy SQL haute performance qui peut router intelligemment les requÃªtes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCHITECTURE PROXYSQL SHARDING                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Applications                                      â”‚
â”‚       â†“ (Port 6033)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚         ProxySQL                     â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚
â”‚  â”‚  â”‚ Query Rules :                  â”‚  â”‚          â”‚
â”‚  â”‚  â”‚ - Parse SQL                    â”‚  â”‚          â”‚
â”‚  â”‚  â”‚ - Extract shard key            â”‚  â”‚          â”‚
â”‚  â”‚  â”‚ - Route to shard               â”‚  â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚                                  â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚        â–¼        â–¼        â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Shard 0 â”‚ â”‚ Shard 1 â”‚ â”‚ Shard 2 â”‚               â”‚
â”‚  â”‚ Host 0  â”‚ â”‚ Host 1  â”‚ â”‚ Host 2  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration ProxySQL

```sql
-- Configuration ProxySQL admin interface

-- 1. DÃ©finir les shards comme backend servers
INSERT INTO mysql_servers (hostgroup_id, hostname, port)
VALUES
(10, '192.168.1.10', 3306),  -- Shard 0
(11, '192.168.1.11', 3306),  -- Shard 1
(12, '192.168.1.12', 3306);  -- Shard 2

-- 2. DÃ©finir utilisateurs
INSERT INTO mysql_users (username, password, default_hostgroup)
VALUES ('app_user', 'app_pass', 10);

-- 3. CrÃ©er query rules pour routing
-- Rule : Route queries avec customer_id vers shard appropriÃ©

-- Utiliser hint SQL pour indiquer shard
INSERT INTO mysql_query_rules (rule_id, active, match_pattern, destination_hostgroup, apply)
VALUES
(1, 1, '.*/*+ SHARD0 */.*', 10, 1),
(2, 1, '.*/*+ SHARD1 */.*', 11, 1),
(3, 1, '.*/*+ SHARD2 */.*', 12, 1);

-- 4. Load config
LOAD MYSQL SERVERS TO RUNTIME;
LOAD MYSQL USERS TO RUNTIME;
LOAD MYSQL QUERY RULES TO RUNTIME;

-- 5. Persist
SAVE MYSQL SERVERS TO DISK;
SAVE MYSQL USERS TO DISK;
SAVE MYSQL QUERY RULES TO DISK;
```

### Application avec ProxySQL

```python
# Application Python avec routing manuel

import mysql.connector
import hashlib

def get_shard_id(customer_id, num_shards=3):
    """Calculer shard basÃ© sur customer_id"""
    return int(hashlib.md5(str(customer_id).encode()).hexdigest(), 16) % num_shards

def execute_query(customer_id, query):
    """ExÃ©cuter requÃªte avec hint shard"""
    shard_id = get_shard_id(customer_id)
    
    # Connexion ProxySQL
    conn = mysql.connector.connect(
        host='proxysql-host',
        port=6033,
        user='app_user',
        password='app_pass',
        database='orders_db'
    )
    
    cursor = conn.cursor()
    
    # Ajouter hint pour routing
    query_with_hint = f"/*+ SHARD{shard_id} */ {query}"
    
    cursor.execute(query_with_hint)
    results = cursor.fetchall()
    
    cursor.close()
    conn.close()
    
    return results

# Utilisation
customer_id = 12345
results = execute_query(
    customer_id,
    f"SELECT * FROM orders WHERE customer_id = {customer_id}"
)
```

---

## Gestion des jointures cross-shard

### Le problÃ¨me

```sql
-- RequÃªte problÃ©matique : JOIN entre tables sharded diffÃ©remment
SELECT o.*, c.name, p.product_name
FROM orders o
JOIN customers c ON c.id = o.customer_id
JOIN products p ON p.id = o.product_id
WHERE o.order_date >= '2024-01-01';

-- Si :
-- orders sharded par customer_id
-- customers sharded par customer_id (OK, colocated)
-- products PAS sharded (table de rÃ©fÃ©rence)
-- â†’ JOIN cross-shard nÃ©cessaire
```

### Solutions

#### 1. Denormalisation

```sql
-- Dupliquer donnÃ©es pour Ã©viter JOIN
CREATE TABLE orders_denormalized (
    id BIGINT,
    customer_id BIGINT,
    customer_name VARCHAR(100),      -- DÃ©normalisÃ©
    customer_email VARCHAR(100),     -- DÃ©normalisÃ©
    product_id INT,
    product_name VARCHAR(100),       -- DÃ©normalisÃ©
    amount DECIMAL(10,2),
    order_date DATE
);

-- Maintenant : Single table query, pas de JOIN
SELECT * FROM orders_denormalized
WHERE customer_id = 12345;
```

#### 2. Tables de rÃ©fÃ©rence (broadcast)

```sql
-- Petites tables : RÃ©pliquer sur tous shards

-- Sur chaque shard : Copie complÃ¨te de products
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    price DECIMAL(10,2)
) ENGINE=InnoDB;

-- Sync automatique via rÃ©plication ou script
-- Maintenant JOIN local possible sur chaque shard
```

#### 3. Application-level JOIN

```python
# JOIN au niveau application

def get_order_with_details(customer_id):
    # 1. Query orders (shard customer_id)
    orders = query_shard(
        get_shard(customer_id),
        f"SELECT * FROM orders WHERE customer_id = {customer_id}"
    )
    
    # 2. Extract product_ids
    product_ids = [o['product_id'] for o in orders]
    
    # 3. Query products (table centralisÃ©e ou cache)
    products = query_products(product_ids)
    products_map = {p['id']: p for p in products}
    
    # 4. JOIN en Python
    for order in orders:
        order['product'] = products_map.get(order['product_id'])
    
    return orders
```

#### 4. AgrÃ©gation distribuÃ©e

```sql
-- Pour requÃªtes analytiques : Map-Reduce pattern

-- Map phase : Query chaque shard
-- Shard 0 : SELECT region, SUM(amount) FROM orders GROUP BY region
-- Shard 1 : SELECT region, SUM(amount) FROM orders GROUP BY region
-- Shard 2 : SELECT region, SUM(amount) FROM orders GROUP BY region

-- Reduce phase : Application agrÃ¨ge rÃ©sultats
-- Total = SUM(rÃ©sultats de tous shards)
```

---

## Migration vers architecture sharded

### Approche progressive

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MIGRATION PROGRESSIVE (Zero Downtime)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Phase 1 : PrÃ©paration (1-2 semaines)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  â€¢ Provisionner serveurs shards                    â”‚
â”‚  â€¢ Installer MariaDB + rÃ©plication                 â”‚
â”‚  â€¢ Configurer Spider ou ProxySQL                   â”‚
â”‚  â€¢ Tests intensifs                                 â”‚
â”‚                                                    â”‚
â”‚  Phase 2 : Migration donnÃ©es (2-4 semaines)        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  â€¢ Copie initiale par batch                        â”‚
â”‚  â€¢ Binlog replication pour sync                    â”‚
â”‚  â€¢ VÃ©rification intÃ©gritÃ©                          â”‚
â”‚                                                    â”‚
â”‚  Phase 3 : Dual-write (1 semaine)                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚  â€¢ Application Ã©crit sur old + new                 â”‚
â”‚  â€¢ Lectures encore sur old                         â”‚
â”‚  â€¢ Monitoring diffÃ©rences                          â”‚
â”‚                                                    â”‚
â”‚  Phase 4 : Basculement lectures (1 jour)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  â€¢ Lectures progressivement sur shards             â”‚
â”‚  â€¢ Canary deployment                               â”‚
â”‚  â€¢ Rollback possible                               â”‚
â”‚                                                    â”‚
â”‚  Phase 5 : Cleanup (1 semaine)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  â€¢ ArrÃªter dual-write                              â”‚
â”‚  â€¢ Archiver ancien systÃ¨me                         â”‚
â”‚  â€¢ Documentation                                   â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Script de migration

```python
# Script de migration batch avec vÃ©rification

import mysql.connector
from datetime import datetime

class ShardMigrator:
    def __init__(self, source_db, shard_configs):
        self.source = source_db
        self.shards = shard_configs
        self.batch_size = 10000
        
    def get_shard_for_row(self, customer_id):
        """DÃ©terminer shard destination"""
        return customer_id % len(self.shards)
    
    def migrate_batch(self, table, start_id, end_id):
        """Migrer un batch de donnÃ©es"""
        # 1. Lire depuis source
        source_conn = mysql.connector.connect(**self.source)
        cursor = source_conn.cursor(dictionary=True)
        
        query = f"""
            SELECT * FROM {table}
            WHERE id >= {start_id} AND id < {end_id}
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        
        # 2. Grouper par shard
        shard_rows = {}
        for row in rows:
            shard_id = self.get_shard_for_row(row['customer_id'])
            if shard_id not in shard_rows:
                shard_rows[shard_id] = []
            shard_rows[shard_id].append(row)
        
        # 3. Insert dans chaque shard
        for shard_id, rows in shard_rows.items():
            shard_conn = mysql.connector.connect(**self.shards[shard_id])
            shard_cursor = shard_conn.cursor()
            
            # Bulk insert
            values = []
            for row in rows:
                values.append(tuple(row.values()))
            
            insert_query = f"""
                INSERT INTO {table} 
                VALUES ({','.join(['%s'] * len(row))})
                ON DUPLICATE KEY UPDATE id=id
            """
            shard_cursor.executemany(insert_query, values)
            shard_conn.commit()
            
            shard_cursor.close()
            shard_conn.close()
        
        cursor.close()
        source_conn.close()
        
        return len(rows)
    
    def verify_migration(self, table):
        """VÃ©rifier intÃ©gritÃ© aprÃ¨s migration"""
        # Count source
        source_conn = mysql.connector.connect(**self.source)
        cursor = source_conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table}")
        source_count = cursor.fetchone()[0]
        
        # Count shards
        total_shard_count = 0
        for shard_config in self.shards:
            shard_conn = mysql.connector.connect(**shard_config)
            shard_cursor = shard_conn.cursor()
            shard_cursor.execute(f"SELECT COUNT(*) FROM {table}")
            total_shard_count += shard_cursor.fetchone()[0]
        
        # VÃ©rifier
        if source_count == total_shard_count:
            print(f"âœ“ Migration OK : {source_count} lignes")
            return True
        else:
            print(f"âœ— Erreur : Source={source_count}, Shards={total_shard_count}")
            return False

# Utilisation
migrator = ShardMigrator(
    source_db={'host': 'old-server', 'database': 'orders_db'},
    shard_configs=[
        {'host': 'shard0', 'database': 'orders_db'},
        {'host': 'shard1', 'database': 'orders_db'},
        {'host': 'shard2', 'database': 'orders_db'}
    ]
)

# Migrer par batches
max_id = 10000000  # 10M lignes
for start in range(0, max_id, 10000):
    count = migrator.migrate_batch('orders', start, start + 10000)
    print(f"Migrated batch {start}-{start+10000}: {count} rows")

# VÃ©rifier
migrator.verify_migration('orders')
```

---

## Monitoring et maintenance

### MÃ©triques critiques

```sql
-- Dashboard sharding

CREATE TABLE shard_health (
    shard_id INT,
    hostname VARCHAR(100),
    checked_at TIMESTAMP,
    status ENUM('OK', 'WARNING', 'CRITICAL'),
    row_count BIGINT,
    data_size_gb DECIMAL(10,2),
    cpu_usage DECIMAL(5,2),
    memory_usage DECIMAL(5,2),
    active_connections INT,
    slow_queries_count INT
);

-- Script de monitoring (exÃ©cutÃ© pÃ©riodiquement)
DELIMITER //
CREATE OR REPLACE PROCEDURE monitor_shards()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE v_shard_id INT;
    DECLARE v_hostname VARCHAR(100);
    
    DECLARE shard_cursor CURSOR FOR
        SELECT id, hostname FROM shard_registry;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN shard_cursor;
    
    read_loop: LOOP
        FETCH shard_cursor INTO v_shard_id, v_hostname;
        
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        -- Connect to shard et collecter mÃ©triques
        -- (Via FEDERATED ou API externe)
        
        -- Exemple simplifiÃ©
        INSERT INTO shard_health 
        VALUES (v_shard_id, v_hostname, NOW(), 'OK', 
                12500000, 45.2, 35.5, 62.1, 50, 2);
        
    END LOOP;
    
    CLOSE shard_cursor;
    
END //
DELIMITER ;

-- ExÃ©cuter toutes les 5 minutes
CREATE EVENT monitor_shards_event
ON SCHEDULE EVERY 5 MINUTE
DO CALL monitor_shards();
```

### Alertes

```sql
-- DÃ©tecter dÃ©sÃ©quilibre entre shards
CREATE OR REPLACE VIEW v_shard_imbalance AS
SELECT 
    shard_id,
    row_count,
    AVG(row_count) OVER() as avg_rows,
    row_count - AVG(row_count) OVER() as diff_from_avg,
    ROUND((row_count - AVG(row_count) OVER()) * 100.0 / 
          NULLIF(AVG(row_count) OVER(), 0), 2) as imbalance_pct
FROM (
    SELECT shard_id, MAX(row_count) as row_count
    FROM shard_health
    WHERE checked_at > DATE_SUB(NOW(), INTERVAL 1 HOUR)
    GROUP BY shard_id
) latest;

-- Alerte si dÃ©sÃ©quilibre > 20%
SELECT * FROM v_shard_imbalance
WHERE ABS(imbalance_pct) > 20;
```

---

## Rebalancing

### Quand rebalancer ?

```
Indicateurs de besoin de rebalancing :
â€¢ DÃ©sÃ©quilibre > 30% entre shards
â€¢ Un shard saturÃ© (CPU > 80%, I/O > 90%)
â€¢ Croissance inÃ©gale des donnÃ©es
â€¢ Ajout de nouveaux shards
```

### StratÃ©gie de rebalancing

```python
# Rebalancing progressif

class ShardRebalancer:
    def __init__(self, shards):
        self.shards = shards
        
    def analyze_imbalance(self):
        """Calculer dÃ©sÃ©quilibre actuel"""
        counts = {}
        for shard_id, config in enumerate(self.shards):
            conn = mysql.connector.connect(**config)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM orders")
            counts[shard_id] = cursor.fetchone()[0]
        
        avg = sum(counts.values()) / len(counts)
        imbalance = {
            sid: ((count - avg) / avg * 100)
            for sid, count in counts.items()
        }
        
        return imbalance
    
    def move_data(self, from_shard, to_shard, customer_ids):
        """DÃ©placer donnÃ©es entre shards"""
        # 1. Copier vers nouveau shard
        source = mysql.connector.connect(**self.shards[from_shard])
        dest = mysql.connector.connect(**self.shards[to_shard])
        
        source_cursor = source.cursor(dictionary=True)
        dest_cursor = dest.cursor()
        
        # Copy en batch
        for customer_id in customer_ids:
            source_cursor.execute(
                "SELECT * FROM orders WHERE customer_id = %s",
                (customer_id,)
            )
            rows = source_cursor.fetchall()
            
            if rows:
                # Insert dans destination
                for row in rows:
                    dest_cursor.execute(
                        "INSERT INTO orders VALUES (...)",
                        tuple(row.values())
                    )
                dest.commit()
                
                # Delete de source (aprÃ¨s vÃ©rification)
                source_cursor.execute(
                    "DELETE FROM orders WHERE customer_id = %s",
                    (customer_id,)
                )
                source.commit()
    
    def rebalance(self, max_move_pct=10):
        """Rebalancer progressivement"""
        imbalance = self.analyze_imbalance()
        
        # Identifier shard surcharge et sous-charge
        overloaded = [(sid, pct) for sid, pct in imbalance.items() if pct > max_move_pct]
        underloaded = [(sid, pct) for sid, pct in imbalance.items() if pct < -max_move_pct]
        
        # DÃ©placer donnÃ©es
        for over_sid, over_pct in overloaded:
            for under_sid, under_pct in underloaded:
                # Calculer combien dÃ©placer
                # Move customer_ids...
                pass
```

---

## Best Practices

### 1. Choix de la clÃ© de sharding

```
âœ… BONNES clÃ©s de sharding :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â€¢ customer_id / user_id (multi-tenant)
  â†’ Isolement parfait
  â†’ Queries locales au shard

â€¢ tenant_id (SaaS)
  â†’ Un tenant = un shard possible
  â†’ Migration tenant facile

â€¢ GÃ©ographie (country_code)
  â†’ ConformitÃ© rÃ©glementaire
  â†’ Latence optimisÃ©e

âŒ MAUVAISES clÃ©s :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â€¢ Timestamp / date
  â†’ Hot shard (toutes Ã©critures sur shard actuel)

â€¢ Auto-increment ID
  â†’ Distribution alÃ©atoire mais queries difficiles

â€¢ Colonnes frÃ©quemment modifiÃ©es
  â†’ Reclassification difficile
```

### 2. Nombre de shards

```
Recommandations :
â€¢ Commencer petit : 2-4 shards
â€¢ Doubler si nÃ©cessaire : 4 â†’ 8 â†’ 16
â€¢ Ã‰viter nombres premiers (rebalancing difficile)
â€¢ Maximum pratique : ~100 shards
â€¢ Au-delÃ  : ConsidÃ©rer autre architecture
```

### 3. Haute disponibilitÃ©

```sql
-- Chaque shard avec rÃ©plication

Shard 0 :
â”œâ”€ Primary (writes)
â””â”€ Replicas (reads, failover)
   â”œâ”€ Replica 1
   â””â”€ Replica 2

Shard 1 :
â”œâ”€ Primary
â””â”€ Replicas
   â”œâ”€ Replica 1
   â””â”€ Replica 2
```

---

## âœ… Points clÃ©s Ã  retenir

- ğŸ”€ **Sharding = horizontal scaling** : Distribution donnÃ©es sur plusieurs serveurs
- ğŸ¯ **ClÃ© de sharding critique** : DÃ©termine distribution et performance
- ğŸ•·ï¸ **Spider Engine** : Solution native MariaDB pour sharding
- ğŸ”Œ **ProxySQL** : Routing intelligent et load balancing
- âš ï¸ **Jointures cross-shard = dÃ©fi** : Denormalisation ou application-level
- ğŸ“Š **Monitoring essentiel** : DÃ©sÃ©quilibre, performance par shard
- ğŸ”„ **Rebalancing nÃ©cessaire** : Croissance inÃ©gale des shards
- ğŸš€ **Migration progressive** : Zero downtime avec dual-write
- ğŸ“ **Sharding gÃ©ographique** : ConformitÃ© GDPR, latence
- ğŸ¢ **Multi-tenancy** : Pattern idÃ©al pour SaaS

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation MariaDB

- [ğŸ“– Spider Storage Engine](https://mariadb.com/kb/en/spider-storage-engine-overview/)
- [ğŸ“– Spider Partitioning](https://mariadb.com/kb/en/spider-storage-engine-partitioning/)

### ProxySQL

- [ğŸ“– ProxySQL Documentation](https://proxysql.com/documentation/)
- [ğŸ“– ProxySQL Query Rules](https://proxysql.com/documentation/query-rules/)

### Architectures distribuÃ©es

- [Sharding Best Practices](https://www.percona.com/blog/sharding-best-practices/)
- [Database Sharding Explained](https://www.digitalocean.com/community/tutorials/understanding-database-sharding)

---

*Le sharding est une technique puissante mais complexe. Ã€ n'utiliser que lorsque vÃ©ritablement nÃ©cessaire, aprÃ¨s avoir Ã©puisÃ© les options de vertical scaling et d'optimisation.*

â­ï¸ [Benchmarking](/15-performance-tuning/12-benchmarking.md)
