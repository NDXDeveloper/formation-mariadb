ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 15.7.2 pt-query-digest (Percona Toolkit)

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : Section 15.7.1 (Slow query log), ComprÃ©hension des statistiques et agrÃ©gation de donnÃ©es

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- Installer et configurer pt-query-digest sur vos environnements
- GÃ©nÃ©rer et interprÃ©ter des rapports d'analyse de performance complets
- Utiliser les fonctionnalitÃ©s avancÃ©es (filtrage, tri, comparaison)
- Identifier rapidement les requÃªtes Ã  optimiser en prioritÃ©
- Automatiser l'analyse des slow query logs
- Comparer les performances avant/aprÃ¨s optimisation
- IntÃ©grer pt-query-digest dans des workflows DevOps
- Analyser des sources multiples (slow log, processlist, tcpdump)

---

## Introduction

**pt-query-digest** est l'outil de rÃ©fÃ©rence pour l'analyse des slow query logs. DÃ©veloppÃ© par Percona, il transforme des gigaoctets de logs bruts en rapports structurÃ©s et actionnables.

### Pourquoi pt-query-digest est indispensable

```
Analyse manuelle du slow query log:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10 GB de logs texte bruts            â”‚
â”‚ 100,000+ requÃªtes individuelles      â”‚
â”‚ RÃ©pÃ©titions, variations...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Analyse manuelle ?
    - Grep/awk = fastidieux
    - ImpossibilitÃ© d'identifier patterns
    - Pas de priorisation
    - Perte de temps = heures/jours

Avec pt-query-digest:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10 GB de logs texte bruts            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    pt-query-digest (30 secondes)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rapport structurÃ©:                   â”‚
â”‚ - Top 10 requÃªtes par temps total    â”‚
â”‚ - AgrÃ©gation par pattern             â”‚
â”‚ - Statistiques dÃ©taillÃ©es            â”‚
â”‚ - RequÃªte example pour chaque patternâ”‚
â”‚ - EXPLAIN automatique                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Optimisation ciblÃ©e (minutes)
```

**Avantages clÃ©s** :
- âœ… **AgrÃ©gation intelligente** : Groupe les requÃªtes similaires (query fingerprint)
- âœ… **Priorisation automatique** : Top N par temps total, nombre d'exÃ©cutions, etc.
- âœ… **Statistiques complÃ¨tes** : Min, Max, Avg, 95th percentile, stddev
- âœ… **Analyse EXPLAIN** : GÃ©nÃ¨re automatiquement les EXPLAIN
- âœ… **Multiples sources** : Slow log, processlist, tcpdump, binlog
- âœ… **Comparaison temporelle** : Avant/aprÃ¨s optimisation
- âœ… **Automatisation** : IntÃ©gration CI/CD, cron, monitoring

---

## Installation

### Installation via package manager

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install percona-toolkit

# RHEL/CentOS/Rocky
sudo yum install percona-toolkit

# VÃ©rifier l'installation
pt-query-digest --version
pt-query-digest 3.5.7
```

### Installation depuis les sources

```bash
# TÃ©lÃ©charger depuis GitHub
cd /tmp
wget https://github.com/percona/percona-toolkit/archive/refs/tags/v3.5.7.tar.gz
tar -xzf v3.5.7.tar.gz
cd percona-toolkit-3.5.7

# Installer
perl Makefile.PL
make
sudo make install

# VÃ©rifier
which pt-query-digest
/usr/local/bin/pt-query-digest
```

### DÃ©pendances Perl

```bash
# Modules Perl requis
sudo cpan install DBI
sudo cpan install DBD::mysql
sudo cpan install Term::ReadKey

# Ou via apt (Ubuntu/Debian)
sudo apt-get install \
    libdbi-perl \
    libdbd-mysql-perl \
    libterm-readkey-perl
```

---

## Utilisation basique

### Analyse simple d'un slow query log

```bash
# Analyse basique
pt-query-digest /var/log/mysql/slow-query.log

# Sortie vers fichier
pt-query-digest /var/log/mysql/slow-query.log > slow-analysis.txt

# Analyse avec connexion Ã  la DB (pour EXPLAIN)
pt-query-digest /var/log/mysql/slow-query.log \
    --host=localhost \
    --user=root \
    --password=secret \
    --database=production
```

### Structure du rapport

Le rapport pt-query-digest contient 3 sections principales :

```
# pt-query-digest output structure

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. OVERALL SUMMARY                             â”‚
â”‚    - Total queries analyzed                    â”‚
â”‚    - Time range                                â”‚
â”‚    - Total execution time                      â”‚
â”‚    - Unique queries                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PROFILE (Top N queries)                     â”‚
â”‚    - Ranked by total time                      â”‚
â”‚    - Rank, Query ID, Response time, Calls      â”‚
â”‚    - Quick overview for prioritization         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. QUERY DETAILS                               â”‚
â”‚    For each query:                             â”‚
â”‚    - Statistics (min, max, avg, 95th...)       â”‚
â”‚    - Example query                             â”‚
â”‚    - EXPLAIN plan (if --host provided)         â”‚
â”‚    - Tables involved                           â”‚
â”‚    - Recommendations                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## InterprÃ©tation du rapport

### Section 1 : Overall Summary

```
# Overall: 12.5k total, 234 unique, 1.04 QPS, 0.45x concurrency ____________
# Time range: 2025-12-14T08:00:00 to 2025-12-14T11:00:00
# Attribute          total     min     max     avg     95%  stddev  median
# ============     ======= ======= ======= ======= ======= ======= =======
# Exec time         5432s     1ms    45s     434ms     2s      1s    250ms
# Lock time          234s     0us   500ms    19ms   100ms    45ms     5ms
# Rows sent         1.2M       0   5000     96.8   500.0   234.5    12.0
# Rows examine      45.6M      0   8.9M   3.6k   15.2k    8.7k   987.0
# Query size        2.3M      10    45k    195     1k     456      89
```

**MÃ©triques clÃ©s** :

| MÃ©trique | Valeur exemple | Signification |
|----------|----------------|---------------|
| **Total queries** | 12,500 | Nombre total de requÃªtes analysÃ©es |
| **Unique queries** | 234 | Nombre de patterns distincts |
| **QPS** | 1.04 | Queries per second (charge) |
| **Concurrency** | 0.45x | RequÃªtes simultanÃ©es en moyenne |
| **Exec time total** | 5432s | Temps total d'exÃ©cution cumulÃ© |
| **95th percentile** | 2s | 95% des requÃªtes < 2s |

ğŸ’¡ **Conseil** : Le **95th percentile** est plus important que la moyenne pour identifier les outliers.

### Section 2 : Profile

```
# Profile
# Rank Query ID                            Response time   Calls  R/Call  V/M
# ==== =================================== =============== ====== ======= ====
#    1 0x8B5E0E4F4B72A0E8C9D6F3A1B2C4D5E6  2134.5s  39.3%   1234  1.7297  0.89 SELECT orders
#    2 0x1A2B3C4D5E6F7G8H9I0J1K2L3M4N5O6P   987.2s  18.2%    567  1.7410  1.23 SELECT customers JOIN
#    3 0x9Z8Y7X6W5V4U3T2S1R0Q9P8O7N6M5L4K   654.3s  12.0%   8901  0.0735  0.45 UPDATE products
#    4 0xABCDEF1234567890ABCDEF1234567890   432.1s   8.0%    234  1.8465  2.34 SELECT inventory
# MISC 0xMISC                               1224.4s  22.5%   1564  0.7826  1.00 <230 ITEMS>
```

**Colonnes importantes** :

| Colonne | Description | Utilisation |
|---------|-------------|-------------|
| **Rank** | Position par temps total | Priorisation |
| **Query ID** | Hash MD5 du fingerprint | Identification unique |
| **Response time** | Temps total (% du total) | Impact global |
| **Calls** | Nombre d'exÃ©cutions | FrÃ©quence |
| **R/Call** | Response time moyen par appel | Performance unitaire |
| **V/M** | Variance/Mean ratio | StabilitÃ© (faible = stable) |

**StratÃ©gie d'optimisation** :

```
Priorisation par impact:
1. Rank #1: 39.3% du temps total
   â†’ Optimiser cette requÃªte = gain maximum

2. Rank #2 + #3: 30.2% du temps
   â†’ DeuxiÃ¨me prioritÃ©

3. MISC: 22.5% mais 230 requÃªtes diffÃ©rentes
   â†’ Analyse individuelle si nÃ©cessaire
```

### Section 3 : Query Details

Pour chaque requÃªte dans le profile :

```
# Query 1: 0.41 QPS, 0.71x concurrency, ID 0x8B5E0E4F4B72A0E8C9D6F3A1B2C4D5E6
# This item is included in the report because it matches --limit.
# Scores: V/M = 0.89
# Time range: 2025-12-14T08:15:23 to 2025-12-14T11:42:56
# Attribute    pct   total     min     max     avg     95%  stddev  median
# ============ === ======= ======= ======= ======= ======= ======= =======
# Count         10    1234
# Exec time     39  2135s     1ms    15s      1s      3s      2s    800ms
# Lock time     15    35s     0us   200ms    28ms   100ms    45ms     5ms
# Rows sent      5  61.7k       1     500      50     100      87      23
# Rows examine  42  19.2M      1k    8.9M    15k    45k     23k     8k
# Query size     8   195k      89     456     162     234     89     123
# String:
# Databases    ecommerce
# Hosts        webapp01.example.com
# Users        appuser
# Query_time distribution
#   1us
#  10us
# 100us
#   1ms
#  10ms  ####
# 100ms  ################################################################
#    1s  ####################################################
#  10s+  ##
# Tables
#    SHOW TABLE STATUS FROM `ecommerce` LIKE 'orders'\G
#    SHOW CREATE TABLE `ecommerce`.`orders`\G
# EXPLAIN /*!50100 PARTITIONS*/
SELECT o.order_id, o.total, c.customer_name, c.email
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
WHERE o.order_date >= '2025-01-01'
  AND o.status = 'pending'
ORDER BY o.order_date DESC\G
```

**Analyse dÃ©taillÃ©e** :

1. **Distribution Query_time** : Visualisation ASCII de la latence
   - MajoritÃ© autour de 100ms-1s
   - Quelques outliers Ã  10s+

2. **Rows examine = 19.2M** pour **Rows sent = 61.7k**
   - Ratio : 311:1 â†’ **TrÃ¨s inefficace** âŒ
   - Action : CrÃ©er index appropriÃ©

3. **EXPLAIN inclus** : Plan d'exÃ©cution pour analyse

---

## Options et fonctionnalitÃ©s avancÃ©es

### Filtrage par critÃ¨res

```bash
# Top 10 requÃªtes seulement
pt-query-digest --limit 10 /var/log/mysql/slow-query.log

# RequÃªtes avec temps d'exÃ©cution > 5s
pt-query-digest \
    --filter '($event->{Query_time} || 0) > 5' \
    /var/log/mysql/slow-query.log

# RequÃªtes sur une table spÃ©cifique
pt-query-digest \
    --filter '$event->{arg} =~ m/FROM orders/i' \
    /var/log/mysql/slow-query.log

# Exclure les requÃªtes SELECT
pt-query-digest \
    --filter '$event->{arg} !~ m/^SELECT/i' \
    /var/log/mysql/slow-query.log

# RequÃªtes avec ratio Rows_examined/Rows_sent > 100
pt-query-digest \
    --filter '($event->{Rows_examined} || 0) / ($event->{Rows_sent} || 1) > 100' \
    /var/log/mysql/slow-query.log
```

### Tri personnalisÃ©

```bash
# Par nombre d'appels (au lieu de temps total)
pt-query-digest --order-by Query_time:cnt /var/log/mysql/slow-query.log

# Par temps maximum (pire cas)
pt-query-digest --order-by Query_time:max /var/log/mysql/slow-query.log

# Par variance (requÃªtes instables)
pt-query-digest --order-by Query_time:stddev /var/log/mysql/slow-query.log

# Par 95th percentile
pt-query-digest --order-by Query_time:95 /var/log/mysql/slow-query.log
```

**MÃ©triques de tri disponibles** :

| MÃ©trique | Description | Use Case |
|----------|-------------|----------|
| `Query_time:sum` | Temps total (dÃ©faut) | Impact global |
| `Query_time:cnt` | Nombre d'appels | RequÃªtes frÃ©quentes |
| `Query_time:max` | Temps maximum | Outliers |
| `Query_time:95` | 95th percentile | Performance typique |
| `Query_time:stddev` | Ã‰cart-type | StabilitÃ© |
| `Rows_examined:sum` | Rows totales examinÃ©es | I/O disque |

### Analyse comparative (avant/aprÃ¨s)

```bash
# Analyser log "avant optimisation"
pt-query-digest /var/log/mysql/slow-before.log \
    --limit 10 \
    --output json \
    > before.json

# Analyser log "aprÃ¨s optimisation"
pt-query-digest /var/log/mysql/slow-after.log \
    --limit 10 \
    --output json \
    > after.json

# Comparer (nÃ©cessite jq ou parsing manuel)
# Ou utiliser --review pour stocker en DB et comparer via SQL
```

**MÃ©thode alternative avec stockage DB** :

```bash
# CrÃ©er schÃ©ma de review
pt-query-digest --create-review-table \
    --host=localhost \
    --user=root \
    --password=secret \
    --database=percona

# Analyser et stocker "avant"
pt-query-digest /var/log/mysql/slow-before.log \
    --review h=localhost,D=percona,t=query_review \
    --no-report

# Analyser et stocker "aprÃ¨s"
pt-query-digest /var/log/mysql/slow-after.log \
    --review h=localhost,D=percona,t=query_review \
    --no-report

# Comparer via SQL
SELECT 
    checksum,
    fingerprint,
    SUM(ts_cnt) AS total_queries,
    SUM(Query_time_sum) AS total_time
FROM percona.query_review
WHERE ts_min BETWEEN '2025-12-14 08:00:00' AND '2025-12-14 09:00:00'
GROUP BY checksum
ORDER BY total_time DESC
LIMIT 10;
```

### GÃ©nÃ©ration automatique d'EXPLAIN

```bash
# EXPLAIN automatique pour chaque requÃªte
pt-query-digest /var/log/mysql/slow-query.log \
    --host=localhost \
    --user=readonly_user \
    --password=secret \
    --database=production \
    --explain h=localhost,u=readonly_user,p=secret,D=production

# Output inclura:
# EXPLAIN /*!50100 PARTITIONS*/
# <requÃªte>\G
# *************************** 1. row ***************************
#            id: 1
#   select_type: SIMPLE
#         table: orders
#          type: ALL     â† âŒ Table scan !
# possible_keys: NULL
#           key: NULL    â† âŒ Pas d'index utilisÃ© !
#       key_len: NULL
#           ref: NULL
#          rows: 8901234 â† Ã‰norme !
#         Extra: Using where; Using filesort
```

### Analyse de processlist en temps rÃ©el

```bash
# Capturer processlist en continu
pt-query-digest \
    --processlist h=localhost,u=root,p=secret \
    --run-time 3600 \
    --interval 1

# Capture toutes les requÃªtes en cours pendant 1h
# Intervalle de 1 seconde entre captures
# Utile pour workload sans slow query log
```

### Analyse de tcpdump (traffic rÃ©seau)

```bash
# Capturer trafic MySQL (port 3306)
sudo tcpdump -i eth0 -s 65535 -x -n -q -tttt \
    'port 3306' > mysql-traffic.txt

# Analyser le trafic capturÃ©
pt-query-digest --type tcpdump mysql-traffic.txt

# Use case: Identifier requÃªtes sans accÃ¨s aux logs serveur
# (environnement cloud restrictif, RDS, etc.)
```

---

## Cas d'usage pratiques

### Cas 1 : Identification d'une requÃªte N+1

**ProblÃ¨me** : Application lente, suspicion de requÃªtes N+1.

```bash
# Analyser par nombre d'appels
pt-query-digest --order-by Query_time:cnt /var/log/mysql/slow-query.log

# Profile
# Rank Query ID                Response time  Calls   R/Call
# ==== ======================= ============== ======= =======
#    1 0xABC...                45.2s   8.3%   45678   0.001s  SELECT users WHERE id=?
#    2 0xDEF...               123.4s  22.7%    1234   0.100s  SELECT orders WHERE user_id=?
```

**Diagnostic** :
- RequÃªte #1 : 45,678 appels en 45s â†’ **N+1 problem** classique
- R/Call trÃ¨s faible (1ms) mais volume Ã©norme

**Solution** :
- ImplÃ©menter eager loading dans l'ORM
- Remplacer N requÃªtes par 1 requÃªte avec JOIN

**VÃ©rification post-fix** :

```bash
# AprÃ¨s optimisation
pt-query-digest /var/log/mysql/slow-after.log --order-by Query_time:cnt

# RequÃªte #1 devrait avoir disparu ou drastiquement rÃ©duite
```

### Cas 2 : RequÃªte avec variance Ã©levÃ©e

**ProblÃ¨me** : Performance imprÃ©visible.

```bash
# Trier par variance
pt-query-digest --order-by Query_time:stddev /var/log/mysql/slow-query.log

# Query 1: ID 0x123...
# Attribute    min     max     avg     stddev
# Exec time    10ms    45s     2s      8s      â† Variance Ã©norme !
# Rows exam    100     8.9M    234k    2.1M
```

**Diagnostic** :
- Temps varie de 10ms Ã  45s selon les paramÃ¨tres
- Rows_examined varie Ã©normÃ©ment

**Analyse dÃ©taillÃ©e** :

```bash
# Extraire exemples de cette requÃªte
pt-query-digest /var/log/mysql/slow-query.log \
    --filter '$event->{fingerprint} =~ /0x123.../' \
    --output slowlog \
    > query-0x123-examples.log

# Examiner les diffÃ©rents paramÃ¨tres
grep "WHERE" query-0x123-examples.log | sort | uniq -c
```

**Solution potentielle** :
- Index composite manquant
- Statistiques obsolÃ¨tes (ANALYZE TABLE)
- Plan d'exÃ©cution instable â†’ Forcer index avec hints

### Cas 3 : Audit complet de performance

**Objectif** : Rapport mensuel de performance.

```bash
#!/bin/bash
# monthly-perf-report.sh

MONTH=$(date -d "last month" +%Y-%m)
LOGDIR="/var/log/mysql/archive"
REPORTDIR="/var/reports/mysql"

# Analyser tous les logs du mois
cat $LOGDIR/slow-query-$MONTH-*.log.gz | gunzip | \
pt-query-digest \
    --limit 20 \
    --output slowlog \
    --filter '$event->{Query_time} > 1' \
    > $REPORTDIR/top20-$MONTH.txt

# GÃ©nÃ©rer rapport HTML
pt-query-digest \
    --limit 20 \
    --output html \
    $REPORTDIR/top20-$MONTH.txt \
    > $REPORTDIR/perf-report-$MONTH.html

# Envoyer par email
mail -s "MySQL Performance Report - $MONTH" \
     -a $REPORTDIR/perf-report-$MONTH.html \
     dba@example.com < /dev/null
```

### Cas 4 : Comparaison avant/aprÃ¨s migration

**Contexte** : Migration MySQL â†’ MariaDB 11.8.

```bash
# Avant migration (MySQL 8.0)
pt-query-digest /var/log/mysql/slow-before-migration.log \
    --limit 100 \
    --output json \
    > before-migration.json

# AprÃ¨s migration (MariaDB 11.8)
pt-query-digest /var/log/mysql/slow-after-migration.log \
    --limit 100 \
    --output json \
    > after-migration.json

# Script Python pour comparer les JSON
python3 << 'EOF'
import json

with open('before-migration.json') as f:
    before = json.load(f)
with open('after-migration.json') as f:
    after = json.load(f)

# Comparer temps total
print(f"Total time before: {before['global']['Query_time']['sum']}s")
print(f"Total time after:  {after['global']['Query_time']['sum']}s")
print(f"Improvement: {((before['global']['Query_time']['sum'] - after['global']['Query_time']['sum']) / before['global']['Query_time']['sum'] * 100):.1f}%")
EOF
```

---

## Automatisation et intÃ©gration

### Analyse automatique quotidienne

```bash
#!/bin/bash
# /usr/local/bin/daily-slow-analysis.sh

DATE=$(date +%Y-%m-%d)
LOGFILE="/var/log/mysql/slow-query.log"
REPORTDIR="/var/reports/mysql"
THRESHOLD_QUERIES=100

# Rotation du slow log
mysql -e "FLUSH SLOW LOGS;"

# Analyser le log prÃ©cÃ©dent
YESTERDAY_LOG="/var/log/mysql/slow-query.log.1"

if [ ! -f "$YESTERDAY_LOG" ]; then
    echo "No slow log to analyze"
    exit 0
fi

# GÃ©nÃ©rer rapport
pt-query-digest "$YESTERDAY_LOG" \
    --limit 20 \
    --output slowlog \
    > "$REPORTDIR/slow-report-$DATE.txt"

# Compter les slow queries
SLOW_COUNT=$(grep -c "^# Query_time:" "$REPORTDIR/slow-report-$DATE.txt")

# Alerter si seuil dÃ©passÃ©
if [ $SLOW_COUNT -gt $THRESHOLD_QUERIES ]; then
    SUBJECT="ALERT: $SLOW_COUNT slow queries on $DATE"
    
    {
        echo "Slow query threshold exceeded on $(hostname)"
        echo "Count: $SLOW_COUNT queries (threshold: $THRESHOLD_QUERIES)"
        echo ""
        echo "Top 5 queries:"
        head -n 100 "$REPORTDIR/slow-report-$DATE.txt" | \
            grep "^# Query [1-5]:" -A 10
    } | mail -s "$SUBJECT" dba@example.com
fi

# Compresser et archiver
gzip "$REPORTDIR/slow-report-$DATE.txt"
```

**Cron quotidien** :

```bash
# /etc/cron.d/mysql-slow-analysis
0 1 * * * root /usr/local/bin/daily-slow-analysis.sh
```

### IntÃ©gration CI/CD

```yaml
# .gitlab-ci.yml
performance_check:
  stage: test
  script:
    # ExÃ©cuter tests de charge
    - sysbench oltp_read_write --mysql-db=testdb run
    
    # Analyser slow query log gÃ©nÃ©rÃ©
    - pt-query-digest /var/log/mysql/slow-query.log --limit 10 > perf-report.txt
    
    # VÃ©rifier qu'aucune requÃªte > 5s
    - |
      MAX_TIME=$(grep "Exec time" perf-report.txt | \
                 awk '{print $4}' | \
                 sort -rn | \
                 head -1 | \
                 sed 's/s//')
      if (( $(echo "$MAX_TIME > 5" | bc -l) )); then
        echo "FAILED: Query > 5s detected!"
        cat perf-report.txt
        exit 1
      fi
  artifacts:
    paths:
      - perf-report.txt
    expire_in: 30 days
```

### Stockage historique et trending

```bash
# CrÃ©er base de donnÃ©es percona
mysql << 'EOF'
CREATE DATABASE IF NOT EXISTS percona;
EOF

# CrÃ©er tables de review
pt-query-digest --create-review-table \
    h=localhost,D=percona,t=query_review

pt-query-digest --create-history-table \
    h=localhost,D=percona,t=query_review_history

# Analyser et stocker quotidiennement
pt-query-digest /var/log/mysql/slow-query.log \
    --review h=localhost,D=percona,t=query_review \
    --history h=localhost,D=percona,t=query_review_history \
    --no-report

# RequÃªte pour trending (requÃªtes qui empirent)
SELECT 
    qr.fingerprint,
    qr.sample,
    DATE(qrh.ts_min) AS date,
    SUM(qrh.ts_cnt) AS queries,
    ROUND(SUM(qrh.Query_time_sum), 2) AS total_time,
    ROUND(AVG(qrh.Query_time_sum / qrh.ts_cnt), 4) AS avg_time
FROM percona.query_review qr
JOIN percona.query_review_history qrh ON qr.checksum = qrh.checksum
WHERE qrh.ts_min >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY qr.checksum, DATE(qrh.ts_min)
HAVING queries > 100
ORDER BY checksum, date;
```

---

## Options de sortie et formats

### Format texte (dÃ©faut)

```bash
pt-query-digest /var/log/mysql/slow-query.log
# Output: Rapport texte lisible humain
```

### Format JSON

```bash
pt-query-digest --output json /var/log/mysql/slow-query.log > report.json

# Exploiter avec jq
cat report.json | jq '.classes[0].example.query'
cat report.json | jq '.classes[] | {id: .checksum, time: .metrics.Query_time.sum}'
```

### Format JSON-anon (donnÃ©es anonymisÃ©es)

```bash
# Anonymiser les donnÃ©es sensibles
pt-query-digest --output json-anon /var/log/mysql/slow-query.log > report-anon.json

# ParamÃ¨tres, valeurs littÃ©rales remplacÃ©s par ?
# Utile pour partager avec support externe
```

### GÃ©nÃ©ration de slow log filtrÃ©

```bash
# Extraire seulement les requÃªtes SELECT
pt-query-digest \
    --filter '$event->{arg} =~ m/^SELECT/i' \
    --output slowlog \
    /var/log/mysql/slow-query.log \
    > select-queries-only.log

# RÃ©-analyser le log filtrÃ©
pt-query-digest select-queries-only.log
```

---

## Optimisations et bonnes pratiques

### Performance de pt-query-digest

Pour de **trÃ¨s gros logs** (> 10 GB) :

```bash
# DÃ©sactiver EXPLAIN (plus rapide)
pt-query-digest --no-report \
    --review h=localhost,D=percona,t=query_review \
    /var/log/mysql/huge-slow.log

# Limiter le nombre de requÃªtes traitÃ©es
pt-query-digest --limit 100 \
    --iterations 100000 \
    /var/log/mysql/huge-slow.log

# Analyser en streaming (sans charger tout en mÃ©moire)
cat /var/log/mysql/huge-slow.log | pt-query-digest
```

### Filtres utiles prÃ©dÃ©finis

```bash
# RequÃªtes avec full table scan
pt-query-digest \
    --filter '($event->{Full_scan} || "") eq "Yes"' \
    /var/log/mysql/slow-query.log

# RequÃªtes avec tmp table sur disque
pt-query-digest \
    --filter '($event->{Tmp_table_on_disk} || "") eq "Yes"' \
    /var/log/mysql/slow-query.log

# RequÃªtes de plus d'1 seconde avec lock > 100ms
pt-query-digest \
    --filter '($event->{Query_time} || 0) > 1 && ($event->{Lock_time} || 0) > 0.1' \
    /var/log/mysql/slow-query.log

# RequÃªtes d'un utilisateur spÃ©cifique
pt-query-digest \
    --filter '($event->{user} || "") eq "appuser"' \
    /var/log/mysql/slow-query.log
```

### IntÃ©gration avec Grafana/Prometheus

```bash
#!/bin/bash
# export-slow-metrics.sh
# Exporte mÃ©triques pour Prometheus

METRICS_FILE="/var/lib/node_exporter/textfile_collector/slow_queries.prom"
LOGFILE="/var/log/mysql/slow-query.log"

# Analyser derniÃ¨re heure
pt-query-digest "$LOGFILE" \
    --since "1h ago" \
    --output json \
    > /tmp/slow-1h.json

# Extraire mÃ©triques
TOTAL_QUERIES=$(jq '.global.ts_cnt' /tmp/slow-1h.json)
TOTAL_TIME=$(jq '.global.Query_time.sum' /tmp/slow-1h.json)
UNIQUE_QUERIES=$(jq '.classes | length' /tmp/slow-1h.json)

# Ã‰crire format Prometheus
cat > "$METRICS_FILE" << EOF
# HELP mysql_slow_queries_1h Total slow queries in last hour
# TYPE mysql_slow_queries_1h gauge
mysql_slow_queries_1h $TOTAL_QUERIES

# HELP mysql_slow_query_time_1h Total query time in last hour (seconds)
# TYPE mysql_slow_query_time_1h gauge
mysql_slow_query_time_1h $TOTAL_TIME

# HELP mysql_slow_unique_queries_1h Unique query patterns in last hour
# TYPE mysql_slow_unique_queries_1h gauge
mysql_slow_unique_queries_1h $UNIQUE_QUERIES
EOF
```

---

## Comparaison avec d'autres outils

| Outil | Avantages | InconvÃ©nients | Use Case |
|-------|-----------|---------------|----------|
| **pt-query-digest** | AgrÃ©gation intelligente, multiples sources, automatisation | NÃ©cessite Perl | **RecommandÃ© pour production** |
| **mysqldumpslow** | Inclus avec MySQL/MariaDB | FonctionnalitÃ©s limitÃ©es, pas de JSON | Analyse rapide basique |
| **Performance Schema** | Temps rÃ©el, zÃ©ro overhead fichier | Complexe, nÃ©cessite configuration | Monitoring temps rÃ©el |
| **Query Analytics (PMM)** | Interface graphique, graphes | Infrastructure lourde | Entreprise, Ã©quipes larges |

ğŸ’¡ **Recommandation** : **pt-query-digest** pour l'analyse batch quotidienne/hebdomadaire, **Performance Schema** pour le monitoring temps rÃ©el.

---

## Troubleshooting

### ProblÃ¨me 1 : "Can't locate DBI.pm"

```bash
# Erreur
Can't locate DBI.pm in @INC

# Solution
sudo apt-get install libdbi-perl libdbd-mysql-perl
# OU
sudo cpan install DBI DBD::mysql
```

### ProblÃ¨me 2 : EXPLAIN Ã©choue

```bash
# Erreur dans le rapport
# EXPLAIN: failed: Access denied for user 'readonly'@'localhost' to database 'production'

# Solution: Donner privilÃ¨ges SELECT
GRANT SELECT ON production.* TO 'readonly'@'localhost';
FLUSH PRIVILEGES;
```

### ProblÃ¨me 3 : MÃ©moire insuffisante (gros logs)

```bash
# Erreur
Out of memory!

# Solution: Analyser en streaming
cat /var/log/mysql/huge-slow.log | pt-query-digest

# Ou limiter le nombre d'itÃ©rations
pt-query-digest --iterations 100000 /var/log/mysql/huge-slow.log
```

---

## âœ… Points clÃ©s Ã  retenir

- **pt-query-digest transforme des GB de logs en rapports actionnables** : AgrÃ©gation par fingerprint, statistiques complÃ¨tes, priorisation automatique.

- **Utiliser --order-by pour prioriser diffÃ©remment** : Par dÃ©faut (temps total), mais aussi par nombre d'appels (N+1), variance (instabilitÃ©), ou 95th percentile.

- **Filtres puissants** : Isoler requÃªtes spÃ©cifiques (`--filter`), full scans, tmp tables sur disque, utilisateurs, etc.

- **EXPLAIN automatique** : GÃ©nÃ©rÃ© si `--host` fourni. Essentiel pour diagnostiquer les plans d'exÃ©cution sans accÃ¨s manuel.

- **Analyse comparative** : Stocker dans DB avec `--review` et `--history` pour comparer avant/aprÃ¨s optimisation et identifier trending.

- **Automatisation indispensable** : Cron quotidien + alerting si seuil dÃ©passÃ©. IntÃ©gration CI/CD pour dÃ©tecter rÃ©gressions.

- **Multiples sources** : Slow log (standard), processlist (temps rÃ©el), tcpdump (environnements sans accÃ¨s logs), binlog.

- **Performance** : Sur gros logs (> 10 GB), utiliser streaming (`cat | pt-query-digest`) ou limiter iterations (`--iterations`).

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle
- [ğŸ“– pt-query-digest Documentation](https://www.percona.com/doc/percona-toolkit/LATEST/pt-query-digest.html)
- [ğŸ“– Percona Toolkit Documentation](https://www.percona.com/doc/percona-toolkit/LATEST/index.html)
- [ğŸ“– GitHub - Percona Toolkit](https://github.com/percona/percona-toolkit)

### Articles et guides
- [Percona Blog - Query Digest Best Practices](https://www.percona.com/blog/)
- [Percona Blog - Query Analysis](https://www.percona.com/blog/)

### Outils complÃ©mentaires
- [PMM - Percona Monitoring and Management](https://www.percona.com/software/database-tools/percona-monitoring-and-management)
- [Performance Schema](https://mariadb.com/kb/en/performance-schema/)

### Tutoriels vidÃ©o
- [Percona University - Query Optimization](https://www.percona.com/resources/videos)

---

## â¡ï¸ Section suivante

**15.8 Performance Schema et sys schema** : Nous explorerons le monitoring temps rÃ©el avec Performance Schema, les vues du sys schema pour l'analyse simplifiÃ©e, et l'instrumentation avancÃ©e des requÃªtes et ressources MariaDB.

â­ï¸ [Performance Schema et sys schema](/15-performance-tuning/08-performance-schema-sys.md)
