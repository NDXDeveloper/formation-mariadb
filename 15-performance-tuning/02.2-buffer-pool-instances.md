ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 15.2.2 Buffer Pool Instances

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 2-3 heures  
> **PrÃ©requis** : Section 15.2.1 (InnoDB Buffer Pool), Architecture multi-cÅ“urs, ComprÃ©hension des mutex et de la concurrence

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- Comprendre l'architecture et les bÃ©nÃ©fices des Buffer Pool instances multiples
- Dimensionner optimalement le nombre d'instances selon les ressources matÃ©rielles
- Configurer et optimiser les instances pour architectures NUMA
- Diagnostiquer et rÃ©soudre les problÃ¨mes de contention sur le Buffer Pool
- Monitorer les performances par instance individuellement
- Optimiser les instances pour des workloads spÃ©cifiques en production

---

## Introduction

Sur les serveurs modernes avec de nombreux cÅ“urs CPU (8+), un Buffer Pool unique peut devenir un **goulot d'Ã©tranglement de performance** en raison de la contention sur les structures de verrouillage internes (mutex, rw-locks). Les **Buffer Pool instances** permettent de diviser le Buffer Pool en plusieurs segments indÃ©pendants, rÃ©duisant drastiquement la contention et amÃ©liorant le parallÃ©lisme.

### ProblÃ©matique : Le Buffer Pool unique comme bottleneck

Sur un serveur 32 cÅ“urs avec un Buffer Pool de 64 GB, tous les threads concurrents doivent acquÃ©rir les mÃªmes mutex pour :
- InsÃ©rer de nouvelles pages dans le Buffer Pool
- Ã‰viter des pages de la LRU list
- Marquer des pages comme dirty
- Rechercher des pages dans le hash table

**RÃ©sultat** : Les threads passent plus de temps Ã  attendre les mutex qu'Ã  traiter les requÃªtes.

```
Single Buffer Pool (64GB)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                      â”‚
â”‚  All 32 cores competing for          â”‚
â”‚  same mutex on single LRU list       â”‚
â”‚                                      â”‚
â”‚  Thread 1 â”€â”€â”€â”€â”                      â”‚
â”‚  Thread 2 â”€â”€â”€â”€â”¤                      â”‚
â”‚  Thread 3 â”€â”€â”€â”€â”¤â”€â”€â–º Mutex Wait        â”‚
â”‚  ...          â”‚                      â”‚
â”‚  Thread 32 â”€â”€â”€â”˜                      â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Contention = HIGH
    Throughput = LIMITED
```

### Solution : Buffer Pool Instances

```
8 Buffer Pool Instances (8GB each)
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Inst â”‚ â”‚ Inst â”‚ â”‚ Inst â”‚ â”‚ Inst â”‚
â”‚  0   â”‚ â”‚  1   â”‚ â”‚  2   â”‚ â”‚  3   â”‚
â”‚ 8GB  â”‚ â”‚ 8GB  â”‚ â”‚ 8GB  â”‚ â”‚ 8GB  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Inst â”‚ â”‚ Inst â”‚ â”‚ Inst â”‚ â”‚ Inst â”‚
â”‚  4   â”‚ â”‚  5   â”‚ â”‚  6   â”‚ â”‚  7   â”‚
â”‚ 8GB  â”‚ â”‚ 8GB  â”‚ â”‚ 8GB  â”‚ â”‚ 8GB  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜

Thread 1-4  â†’ Instance 0
Thread 5-8  â†’ Instance 1
Thread 9-12 â†’ Instance 2
...
        â†“
    Contention = LOW
    Throughput = 3-5x HIGHER
```

---

## Architecture des Buffer Pool Instances

### Partitionnement des pages

InnoDB utilise un **hashing dÃ©terministe** basÃ© sur le `space_id` et le `page_number` pour rÃ©partir les pages entre les instances :

```c
// Algorithme simplifiÃ© (rÃ©fÃ©rence interne InnoDB)
instance_id = (space_id XOR page_number) % innodb_buffer_pool_instances
```

**ConsÃ©quences importantes** :
- Les pages d'une mÃªme table sont **distribuÃ©es** entre toutes les instances
- La distribution est **dÃ©terministe** : une page va toujours dans la mÃªme instance
- Pas de possibilitÃ© de "pinning" manuel d'une table Ã  une instance spÃ©cifique

### Structures indÃ©pendantes par instance

Chaque instance maintient ses propres :

1. **LRU list** : Liste des pages gÃ©rÃ©es par algorithme LRU
2. **Free list** : Pages libres disponibles
3. **Flush list** : Pages dirty en attente d'Ã©criture
4. **Hash table** : Index des pages en cache
5. **Mutex et rw-locks** : Verrous de protection

```
Instance 0 (8GB)
â”œâ”€â”€ LRU list (page management)
â”œâ”€â”€ Free list (available pages)
â”œâ”€â”€ Flush list (dirty pages)
â”œâ”€â”€ Hash table (page lookup)
â””â”€â”€ Mutex/RW-locks (concurrency control)

Instance 1 (8GB)
â”œâ”€â”€ LRU list (independent)
â”œâ”€â”€ Free list (independent)
â”œâ”€â”€ Flush list (independent)
â”œâ”€â”€ Hash table (independent)
â””â”€â”€ Mutex/RW-locks (independent)
```

**Avantage clÃ©** : Les threads travaillant sur diffÃ©rentes instances n'entrent **jamais en contention** les uns avec les autres.

---

## Dimensionnement optimal des instances

### RÃ¨gles gÃ©nÃ©rales

| Buffer Pool Size | Instances recommandÃ©es | Taille par instance | Use case |
|------------------|------------------------|---------------------|----------|
| < 1 GB | 1 | N/A | Overhead pas justifiÃ© |
| 1-4 GB | 1-2 | 1-2 GB | Serveurs modestes |
| 4-8 GB | 2-4 | 1-2 GB | Serveurs standard |
| 8-16 GB | 4-8 | 1-2 GB | Serveurs performants |
| 16-32 GB | 8-16 | 1-2 GB | Serveurs haute performance |
| 32-64 GB | 8-16 | 2-4 GB | Serveurs enterprise |
| > 64 GB | 16 | 4-8 GB | Serveurs critiques |

### Formule de dimensionnement

```
instances_optimales = MIN(
    innodb_buffer_pool_size_GB / 4,
    nombre_cores_CPU / 2,
    16  # Maximum recommandÃ©
)
```

**Exemples** :

```bash
# Serveur 1: 32 GB Buffer Pool, 16 cores
instances = MIN(32/4, 16/2, 16) = MIN(8, 8, 16) = 8
â†’ innodb_buffer_pool_instances = 8

# Serveur 2: 64 GB Buffer Pool, 32 cores
instances = MIN(64/4, 32/2, 16) = MIN(16, 16, 16) = 16
â†’ innodb_buffer_pool_instances = 16

# Serveur 3: 128 GB Buffer Pool, 48 cores
instances = MIN(128/4, 48/2, 16) = MIN(32, 24, 16) = 16
â†’ innodb_buffer_pool_instances = 16
```

âš ï¸ **Attention** : Ne dÃ©passez pas 16 instances. Au-delÃ , l'overhead de gestion annule les bÃ©nÃ©fices.

### Taille minimale par instance

**RÃ¨gle absolue** : Chaque instance doit faire **au minimum 1 GB**.

```ini
# âŒ INCORRECT - instances trop petites
innodb_buffer_pool_size = 4G
innodb_buffer_pool_instances = 16  # 256 MB par instance
# â†’ Overhead excessif, performance dÃ©gradÃ©e

# âœ… CORRECT
innodb_buffer_pool_size = 4G
innodb_buffer_pool_instances = 4   # 1 GB par instance
```

Pourquoi 1 GB minimum ?
- Chaque instance a un overhead de structures internes (~50-100 MB)
- Les mutex et hash tables nÃ©cessitent un espace minimal pour Ãªtre efficaces
- En dessous de 1 GB, le ratio overhead/donnÃ©es devient dÃ©favorable

### Validation de la configuration

MariaDB ajuste automatiquement si la configuration est invalide :

```sql
-- VÃ©rifier la configuration rÃ©elle
SHOW VARIABLES LIKE 'innodb_buffer_pool%';

+-------------------------------------+----------------+
| Variable_name                       | Value          |
+-------------------------------------+----------------+
| innodb_buffer_pool_instances        | 8              |
| innodb_buffer_pool_size             | 34359738368    |
| innodb_buffer_pool_chunk_size       | 134217728      |
+-------------------------------------+----------------+
```

Calcul de la taille par instance :

```sql
SELECT 
    @@innodb_buffer_pool_size / 1024 / 1024 / 1024 AS total_gb,
    @@innodb_buffer_pool_instances AS instances,
    @@innodb_buffer_pool_size / @@innodb_buffer_pool_instances / 1024 / 1024 / 1024 
        AS gb_per_instance;

+----------+-----------+------------------+
| total_gb | instances | gb_per_instance  |
+----------+-----------+------------------+
|  32.0000 |         8 |          4.0000  |
+----------+-----------+------------------+
```

---

## Configuration avancÃ©e avec chunks

### Relation entre instances et chunks

Depuis MariaDB 10.5, le Buffer Pool est divisÃ© en **chunks** pour permettre le redimensionnement dynamique.

**Contrainte mathÃ©matique** :
```
innodb_buffer_pool_size = 
    innodb_buffer_pool_chunk_size Ã— 
    innodb_buffer_pool_instances Ã— 
    N (nombre entier de chunks par instance)
```

**Exemple de calcul** :

```ini
[mysqld]
innodb_buffer_pool_size = 32G          # 34359738368 bytes
innodb_buffer_pool_instances = 8
innodb_buffer_pool_chunk_size = 128M   # 134217728 bytes

# VÃ©rification :
# 32 GB = 32768 MB
# 128 MB Ã— 8 instances = 1024 MB par "layer"
# 32768 / 1024 = 32 chunks par instance âœ“
```

### Ajustement automatique

Si la taille n'est pas un multiple valide, MariaDB ajuste automatiquement :

```sql
-- Configuration souhaitÃ©e
SET GLOBAL innodb_buffer_pool_size = 33G;  -- 35433480192 bytes

-- MariaDB ajuste au multiple le plus proche
SHOW VARIABLES LIKE 'innodb_buffer_pool_size';
+-------------------------+-------------+
| Variable_name           | Value       |
+-------------------------+-------------+
| innodb_buffer_pool_size | 35422683136 |  -- AjustÃ© Ã  33.0 GB exact
+-------------------------+-------------+
```

Logs dans error.log :
```
[Note] InnoDB: Requested buffer pool size 33G adjusted to 33.0G to align with chunk size
```

### Configuration recommandÃ©e

```ini
[mysqld]
# Buffer Pool total
innodb_buffer_pool_size = 64G

# Instances : 1 par 4-8 GB
innodb_buffer_pool_instances = 16

# Chunk size : dÃ©faut 128M (optimal pour la plupart des cas)
innodb_buffer_pool_chunk_size = 128M

# Calcul :
# 64 GB / 16 instances = 4 GB par instance
# 4 GB / 128 MB = 32 chunks par instance âœ“
```

ğŸ’¡ **Conseil** : Conservez le chunk size par dÃ©faut (128M) sauf cas trÃ¨s spÃ©cifiques. Modifier cette valeur nÃ©cessite une comprÃ©hension approfondie de l'algorithme de redimensionnement.

---

## Optimisation pour architectures NUMA

### Qu'est-ce que NUMA ?

**NUMA (Non-Uniform Memory Access)** : Architecture oÃ¹ la mÃ©moire est physiquement attachÃ©e Ã  des processeurs spÃ©cifiques.

```
Serveur NUMA typique (2 sockets)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚  CPU Socket 0 (16 cores)                    â”‚
â”‚  â”œâ”€â”€ Local Memory: 64 GB (fast)             â”‚
â”‚  â””â”€â”€ Remote Memory: 64 GB (slow, via QPI)   â”‚
â”‚                                             â”‚
â”‚  CPU Socket 1 (16 cores)                    â”‚
â”‚  â”œâ”€â”€ Local Memory: 64 GB (fast)             â”‚
â”‚  â””â”€â”€ Remote Memory: 64 GB (slow, via QPI)   â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Access latency:
- Local memory:  ~100 ns
- Remote memory: ~200-300 ns (2-3x slower)
```

### Impact sur MariaDB

Sans optimisation NUMA, MariaDB peut :
- Allouer le Buffer Pool sur un seul nÅ“ud NUMA
- Forcer des accÃ¨s distant (remote memory) coÃ»teux
- DÃ©grader les performances de 30-50%

### VÃ©rification de la configuration NUMA

```bash
# VÃ©rifier si NUMA est activÃ©
numactl --hardware

available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
node 0 size: 65472 MB
node 0 free: 12340 MB
node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
node 1 size: 65536 MB
node 1 free: 45678 MB

# VÃ©rifier la politique de MariaDB en cours
ps aux | grep mariadbd
cat /proc/$(pidof mariadbd)/numa_maps | grep huge
```

### Configuration optimale pour NUMA

**Option 1 : Interleaving (recommandÃ© pour la plupart des cas)**

Distribue uniformÃ©ment la mÃ©moire sur tous les nÅ“uds NUMA :

```bash
# Dans /etc/systemd/system/mariadb.service.d/numa.conf
[Service]
ExecStart=
ExecStart=/usr/bin/numactl --interleave=all /usr/sbin/mariadbd $MYSQLD_OPTS
```

ou

```bash
# Lancement manuel
numactl --interleave=all /usr/sbin/mariadbd &
```

**Option 2 : Localalloc (pour workloads spÃ©cifiques)**

Alloue la mÃ©moire sur le nÅ“ud local au thread :

```bash
numactl --localalloc /usr/sbin/mariadbd &
```

**Option 3 : DÃ©sactivation NUMA (si pas besoin)**

```bash
# DÃ©sactiver NUMA dans le BIOS (si applicable)
# ou via kernel boot parameter:
# /etc/default/grub
GRUB_CMDLINE_LINUX="numa=off"
```

### Alignement instances NUMA + Buffer Pool

Pour des performances optimales, alignez le nombre d'instances sur les nÅ“uds NUMA :

```ini
# Serveur 2 nÅ“uds NUMA, 128 GB RAM
[mysqld]
innodb_buffer_pool_size = 96G
innodb_buffer_pool_instances = 16  # 8 instances par nÅ“ud NUMA

# Avec interleaving, chaque nÅ“ud NUMA hÃ©bergera ~8 instances
```

VÃ©rification de la distribution :

```bash
# Voir la rÃ©partition mÃ©moire de MariaDB
numastat -p $(pidof mariadbd)

Per-node process memory usage (in MBs) for PID 12345 (mariadbd)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Heap                      48234.5         48127.3         96361.8
Stack                        45.2            43.8            89.0
Private                     234.1           231.4           465.5
----------------  --------------- --------------- ---------------
Total                     48513.8         48402.5         96916.3
```

Distribution Ã©quilibrÃ©e = configuration NUMA optimale âœ“

---

## Monitoring par instance

### MÃ©triques globales vs par instance

```sql
-- Vue globale (agrÃ©gÃ©e de toutes les instances)
SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool%';

-- DÃ©tail par instance (INFORMATION_SCHEMA)
SELECT * FROM information_schema.INNODB_BUFFER_POOL_STATS;
```

### Analyse dÃ©taillÃ©e par instance

```sql
-- Ã‰tat de chaque instance
SELECT 
    pool_id,
    pool_size AS total_pages,
    free_buffers,
    database_pages,
    old_database_pages,
    modified_database_pages AS dirty_pages,
    ROUND(100 * database_pages / pool_size, 2) AS utilization_pct,
    ROUND(100 * modified_database_pages / pool_size, 2) AS dirty_pct
FROM information_schema.INNODB_BUFFER_POOL_STATS
ORDER BY pool_id;

+---------+-------------+--------------+----------------+--------------------+-------------+-----------------+-----------+
| pool_id | total_pages | free_buffers | database_pages | old_database_pages | dirty_pages | utilization_pct | dirty_pct |
+---------+-------------+--------------+----------------+--------------------+-------------+-----------------+-----------+
|       0 |      262144 |         1024 |         258432 |              95234 |       12345 |           98.58 |      4.71 |
|       1 |      262144 |          987 |         258891 |              95467 |       11987 |           98.76 |      4.57 |
|       2 |      262144 |         1156 |         258102 |              95012 |       13201 |           98.46 |      5.03 |
|       3 |      262144 |         1045 |         258367 |              95189 |       12678 |           98.56 |      4.84 |
|       4 |      262144 |         1201 |         257912 |              94876 |       13456 |           98.39 |      5.13 |
|       5 |      262144 |          998 |         258734 |              95401 |       12123 |           98.70 |      4.62 |
|       6 |      262144 |         1134 |         258189 |              95087 |       12987 |           98.49 |      4.95 |
|       7 |      262144 |         1012 |         258501 |              95267 |       12456 |           98.61 |      4.75 |
+---------+-------------+--------------+----------------+--------------------+-------------+-----------------+-----------+
```

### DÃ©tection de dÃ©sÃ©quilibres

**ProblÃ¨me** : Si une instance est significativement plus chargÃ©e que les autres, cela peut indiquer :
- Un hot spot sur certaines tables
- Une distribution non uniforme des accÃ¨s
- Un problÃ¨me de workload design

**Diagnostic** :

```sql
-- Identifier les instances dÃ©sÃ©quilibrÃ©es
WITH instance_stats AS (
    SELECT 
        pool_id,
        pool_size,
        database_pages,
        modified_database_pages,
        ROUND(100 * database_pages / pool_size, 2) AS utilization_pct
    FROM information_schema.INNODB_BUFFER_POOL_STATS
),
avg_stats AS (
    SELECT AVG(utilization_pct) AS avg_utilization
    FROM instance_stats
)
SELECT 
    i.pool_id,
    i.utilization_pct,
    a.avg_utilization,
    i.utilization_pct - a.avg_utilization AS deviation_from_avg,
    CASE 
        WHEN ABS(i.utilization_pct - a.avg_utilization) > 5 THEN 'UNBALANCED'
        ELSE 'OK'
    END AS status
FROM instance_stats i
CROSS JOIN avg_stats a
ORDER BY i.pool_id;

+---------+-----------------+-----------------+--------------------+------------+
| pool_id | utilization_pct | avg_utilization | deviation_from_avg | status     |
+---------+-----------------+-----------------+--------------------+------------+
|       0 |           98.58 |          98.545 |              0.035 | OK         |
|       1 |           98.76 |          98.545 |              0.215 | OK         |
|       2 |           98.46 |          98.545 |             -0.085 | OK         |
|       3 |           98.56 |          98.545 |              0.015 | OK         |
|       4 |           98.39 |          98.545 |             -0.155 | OK         |
|       5 |           98.70 |          98.545 |              0.155 | OK         |
|       6 |           98.49 |          98.545 |             -0.055 | OK         |
|       7 |           98.61 |          98.545 |              0.065 | OK         |
+---------+-----------------+-----------------+--------------------+------------+
```

Distribution Ã©quilibrÃ©e (Ã©cart < 5%) = bonne configuration âœ“

### Hit ratio par instance

```sql
-- Hit ratio dÃ©taillÃ© par instance
SELECT 
    pool_id,
    hit_rate,
    ROUND(100 * hit_rate, 2) AS hit_ratio_pct,
    read_ahead,
    read_ahead_evicted,
    pages_made_young,
    pages_not_made_young
FROM information_schema.INNODB_BUFFER_POOL_STATS
ORDER BY pool_id;
```

---

## Performance et benchmarking

### Impact du nombre d'instances sur les performances

Benchmark sysbench OLTP (serveur 32 cores, 64 GB Buffer Pool) :

| Instances | TPS (trans/sec) | Latency p95 (ms) | CPU Usage (%) | Contention |
|-----------|-----------------|------------------|---------------|------------|
| 1 | 12,456 | 45.2 | 65% | HIGH |
| 2 | 18,234 | 32.1 | 78% | MEDIUM |
| 4 | 24,567 | 24.5 | 85% | LOW |
| 8 | 32,891 | 18.3 | 92% | VERY LOW |
| 16 | 33,124 | 18.1 | 93% | VERY LOW |
| 32 | 31,456 | 19.7 | 91% | LOW (overhead) |

**Observations** :
- Gain significatif jusqu'Ã  8 instances (2.6x throughput)
- Plateau Ã  16 instances
- DÃ©gradation au-delÃ  (overhead management)

### Mesure de contention

```sql
-- Analyse des waits sur mutex Buffer Pool
SELECT 
    EVENT_NAME,
    COUNT_STAR AS total_waits,
    SUM_TIMER_WAIT / 1000000000000 AS total_wait_sec,
    AVG_TIMER_WAIT / 1000000000 AS avg_wait_ms
FROM performance_schema.events_waits_summary_global_by_event_name
WHERE EVENT_NAME LIKE '%buf_pool%'
    AND COUNT_STAR > 0
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;

+----------------------------------------+-------------+------------------+--------------+
| EVENT_NAME                             | total_waits | total_wait_sec   | avg_wait_ms  |
+----------------------------------------+-------------+------------------+--------------+
| wait/synch/mutex/innodb/buf_pool_mutex |    45678901 |         12345.67 |        0.270 |
| wait/synch/mutex/innodb/flush_list_mtx |    12345678 |          2345.12 |        0.190 |
+----------------------------------------+-------------+------------------+--------------+
```

**InterprÃ©tation** :
- `avg_wait_ms` > 1.0 ms : Contention significative, augmenter les instances
- `avg_wait_ms` < 0.5 ms : Contention acceptable
- `avg_wait_ms` < 0.1 ms : Contention minimale, optimisation rÃ©ussie

---

## StratÃ©gies d'optimisation avancÃ©es

### Cas 1 : Workload OLTP haute concurrence

**Profil** :
- 100-500 connexions concurrentes
- RequÃªtes trÃ¨s courtes (< 10ms)
- Working set < Buffer Pool size

**Configuration optimale** :

```ini
[mysqld]
# Buffer Pool : 80% de la RAM
innodb_buffer_pool_size = 96G

# Instances : maximiser le parallÃ©lisme
innodb_buffer_pool_instances = 16

# RÃ©duire la contention sur la LRU
innodb_old_blocks_time = 0  # Migration immÃ©diate vers new list

# NUMA
# Lancer avec: numactl --interleave=all mariadbd
```

### Cas 2 : Workload Analytics avec scans

**Profil** :
- 10-50 connexions
- RequÃªtes longues (> 1s)
- Full table scans frÃ©quents
- Working set >> Buffer Pool size

**Configuration optimale** :

```ini
[mysqld]
# Buffer Pool : 70% de la RAM (laisser de la RAM pour sort buffers)
innodb_buffer_pool_size = 84G

# Instances : Ã©quilibre entre parallÃ©lisme et protection
innodb_buffer_pool_instances = 12

# Protection contre les scans
innodb_old_blocks_time = 3000  # 3 secondes
innodb_old_blocks_pct = 30     # Old list plus petite

# Sort buffer pour analytics
sort_buffer_size = 16M
read_rnd_buffer_size = 8M
```

### Cas 3 : Workload mixte (OLTP + Reporting)

**Profil** :
- 50-200 connexions
- Mix requÃªtes courtes et longues
- AccÃ¨s variÃ©s (index + scans)

**Configuration optimale** :

```ini
[mysqld]
# Buffer Pool : 75% de la RAM
innodb_buffer_pool_size = 90G

# Instances : compromis
innodb_buffer_pool_instances = 12

# Compromis LRU
innodb_old_blocks_time = 1000  # 1 seconde (dÃ©faut)
innodb_old_blocks_pct = 37     # DÃ©faut

# Thread Pool pour gÃ©rer la concurrence
thread_handling = pool-of-threads
thread_pool_size = 32
```

---

## Migration et changement de configuration

### Modification du nombre d'instances

âš ï¸ **ATTENTION** : Changer `innodb_buffer_pool_instances` nÃ©cessite un **redÃ©marrage** de MariaDB.

**ProcÃ©dure recommandÃ©e** :

1. **Planifier une fenÃªtre de maintenance**

2. **Sauvegarder l'Ã©tat du Buffer Pool** (optionnel mais recommandÃ©) :
```sql
SET GLOBAL innodb_buffer_pool_dump_now = ON;
```

3. **Modifier la configuration** :
```ini
# /etc/mysql/mariadb.conf.d/50-server.cnf
[mysqld]
innodb_buffer_pool_instances = 16  # Nouvelle valeur
```

4. **RedÃ©marrer MariaDB** :
```bash
systemctl restart mariadb

# VÃ©rifier le dÃ©marrage
systemctl status mariadb
tail -n 100 /var/log/mysql/error.log
```

5. **VÃ©rifier la nouvelle configuration** :
```sql
SHOW VARIABLES LIKE 'innodb_buffer_pool_instances';
SELECT COUNT(DISTINCT pool_id) AS active_instances 
FROM information_schema.INNODB_BUFFER_POOL_STATS;
```

6. **Recharger les donnÃ©es chaudes** (si dump activÃ©) :
```sql
-- VÃ©rifier la progression du chargement
SHOW STATUS LIKE 'Innodb_buffer_pool_load_status';
```

### Calcul du downtime attendu

Pour un redÃ©marrage avec prÃ©chargement du Buffer Pool :

```
Downtime = shutdown_time + startup_time + buffer_pool_load_time

shutdown_time â‰ˆ 5-30 secondes (flush dirty pages)
startup_time â‰ˆ 10-60 secondes (initialisation)
buffer_pool_load_time â‰ˆ Buffer_Pool_GB Ã— 2-5 secondes

Exemple : 64 GB Buffer Pool
= 30s + 30s + 64 Ã— 3s
= 222 secondes â‰ˆ 4 minutes
```

ğŸ’¡ **Conseil** : Testez le temps de redÃ©marrage sur un environnement de staging avant la production.

---

## Troubleshooting et problÃ¨mes courants

### ProblÃ¨me 1 : Instances dÃ©sÃ©quilibrÃ©es aprÃ¨s migration

**SymptÃ´me** : AprÃ¨s passage de 4 Ã  16 instances, certaines instances sont sous-utilisÃ©es.

**Cause** : Le hash function distribue les pages, mais si les tables actives sont peu nombreuses, la distribution peut Ãªtre inÃ©gale initialement.

**Solution** : Laisser le systÃ¨me se stabiliser sur 24-48h. Le LRU algorithm Ã©quilibrera naturellement.

### ProblÃ¨me 2 : Performance dÃ©gradÃ©e aprÃ¨s augmentation des instances

**SymptÃ´me** : Passage de 8 Ã  32 instances = throughput en baisse.

**Cause** : Trop d'instances = overhead excessif (mutex, structures).

**Diagnostic** :

```sql
-- VÃ©rifier la taille par instance
SELECT 
    @@innodb_buffer_pool_size / @@innodb_buffer_pool_instances / 1024 / 1024 / 1024 
        AS gb_per_instance;

+------------------+
| gb_per_instance  |
+------------------+
|          2.0000  |  -- Si < 1 GB, trop petit !
+------------------+
```

**Solution** : RÃ©duire Ã  16 instances maximum.

### ProblÃ¨me 3 : NUMA imbalance

**SymptÃ´me** : Un nÅ“ud NUMA saturÃ©, l'autre sous-utilisÃ©.

**Diagnostic** :

```bash
numastat -p $(pidof mariadbd)

Per-node process memory usage (in MBs) for PID 12345 (mariadbd)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Total                     89234.5          7345.3         96579.8  # âŒ IMBALANCED
```

**Solution** :

```bash
# RedÃ©marrer avec interleaving
systemctl stop mariadb

# Modifier /etc/systemd/system/mariadb.service.d/numa.conf
[Service]
ExecStart=
ExecStart=/usr/bin/numactl --interleave=all /usr/sbin/mariadbd

systemctl daemon-reload
systemctl start mariadb
```

---

## IntÃ©gration avec le monitoring

### Dashboards Grafana recommandÃ©s

**Panel 1 : Utilization par instance**

```promql
100 * (
  mysql_global_status_innodb_buffer_pool_pages_data{pool_id=~".*"} /
  mysql_global_status_innodb_buffer_pool_pages_total{pool_id=~".*"}
)
```

**Panel 2 : Dirty pages par instance**

```promql
100 * (
  mysql_global_status_innodb_buffer_pool_pages_dirty{pool_id=~".*"} /
  mysql_global_status_innodb_buffer_pool_pages_total{pool_id=~".*"}
)
```

**Panel 3 : Deviation from average utilization**

```promql
abs(
  100 * (
    mysql_global_status_innodb_buffer_pool_pages_data{pool_id=~".*"} /
    mysql_global_status_innodb_buffer_pool_pages_total{pool_id=~".*"}
  ) - avg(
    100 * (
      mysql_global_status_innodb_buffer_pool_pages_data{pool_id=~".*"} /
      mysql_global_status_innodb_buffer_pool_pages_total{pool_id=~".*"}
    )
  )
)
```

### Alerting

```yaml
# prometheus_alerts.yml
groups:
  - name: buffer_pool_instances
    rules:
      - alert: BufferPoolInstanceImbalance
        expr: |
          abs(
            100 * (
              mysql_global_status_innodb_buffer_pool_pages_data{pool_id=~".*"} /
              mysql_global_status_innodb_buffer_pool_pages_total{pool_id=~".*"}
            ) - avg(
              100 * (
                mysql_global_status_innodb_buffer_pool_pages_data /
                mysql_global_status_innodb_buffer_pool_pages_total
              )
            )
          ) > 10
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Buffer Pool instance {{ $labels.pool_id }} is imbalanced"
          description: "Deviation from average: {{ $value }}%"
```

---

## âœ… Points clÃ©s Ã  retenir

- **Buffer Pool instances rÃ©duisent la contention** sur les serveurs multi-cÅ“urs en divisant le Buffer Pool en segments indÃ©pendants avec leurs propres mutex et structures.

- **Dimensionnement optimal** : 1 instance par 4-8 GB de Buffer Pool, avec un maximum de 16 instances. Chaque instance doit faire au minimum 1 GB.

- **Formule de rÃ©fÃ©rence** : `instances = MIN(buffer_pool_GB / 4, cores / 2, 16)`

- **NUMA awareness essentiel** : Sur serveurs multi-sockets, utilisez `numactl --interleave=all` pour distribuer uniformÃ©ment la mÃ©moire et Ã©viter des ralentissements de 30-50%.

- **Changement d'instances = redÃ©marrage requis** : Contrairement Ã  la taille du Buffer Pool (modifiable dynamiquement depuis 10.5+), le nombre d'instances nÃ©cessite un redÃ©marrage.

- **Monitoring par instance** : Surveillez la distribution des pages via `INNODB_BUFFER_POOL_STATS` pour dÃ©tecter les dÃ©sÃ©quilibres (Ã©cart < 5% = optimal).

- **Pas toujours plus = mieux** : Au-delÃ  de 16 instances, l'overhead de gestion annule les bÃ©nÃ©fices. Ne pas suroptimiser.

- **Gain de performance** : Configuration optimale peut amÃ©liorer le throughput de 2-5x sur workloads haute concurrence.

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle MariaDB
- [ğŸ“– InnoDB Buffer Pool Configuration](https://mariadb.com/kb/en/innodb-buffer-pool/)
- [ğŸ“– InnoDB Buffer Pool Instances](https://mariadb.com/kb/en/innodb-system-variables/#innodb_buffer_pool_instances)
- [ğŸ“– INFORMATION_SCHEMA.INNODB_BUFFER_POOL_STATS](https://mariadb.com/kb/en/information-schema-innodb_buffer_pool_stats-table/)

### Articles techniques avancÃ©s
- [Percona Blog - InnoDB Buffer Pool Instances](https://www.percona.com/blog/)
- [MySQL Performance Blog - NUMA and Buffer Pool](https://www.percona.com/blog/2011/08/04/numa-and-mysql/)
- [Facebook Engineering - MySQL at Scale](https://engineering.fb.com/)

### Outils
- [numactl - NUMA policy control](https://linux.die.net/man/8/numactl)
- [numastat - NUMA memory statistics](https://linux.die.net/man/8/numastat)
- [perf - Performance analysis](https://perf.wiki.kernel.org/)

### Benchmarking
- [sysbench - OLTP benchmark](https://github.com/akopytov/sysbench)
- [Percona Toolkit - Performance analysis](https://www.percona.com/software/database-tools/percona-toolkit)

---

## â¡ï¸ Section suivante

**15.2.3 Key buffer (MyISAM)** : Nous explorerons la configuration du cache MyISAM pour les workloads legacy, ainsi que les stratÃ©gies de migration vers InnoDB.

â­ï¸ [Key buffer (MyISAM)](/15-performance-tuning/02.3-key-buffer.md)
