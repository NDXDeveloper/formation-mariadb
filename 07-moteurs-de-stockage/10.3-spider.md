üîù Retour au [Sommaire](/SOMMAIRE.md)

# 7.10.3 Spider : Sharding Distribu√©

> **Niveau** : Avanc√©
> **Dur√©e estim√©e** : 3 heures

> **Pr√©requis** :
> - Architecture InnoDB (Section 7.2)
> - R√©plication MariaDB (Section 13)
> - Haute disponibilit√© (Section 14)
> - Compr√©hension du sharding et partitionnement

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre l'architecture et le fonctionnement du moteur Spider
- Concevoir des architectures de sharding horizontal avec Spider
- Configurer des clusters Spider multi-serveurs
- Impl√©menter diff√©rentes strat√©gies de partitionnement (hash, range, list)
- Ma√Ætriser le routage des requ√™tes et l'agr√©gation distribu√©e
- Mettre en place la haute disponibilit√© avec Spider
- Diagnostiquer et optimiser les performances distribu√©es
- Comparer Spider avec les alternatives (ProxySQL, Vitess, Galera)
- Migrer des architectures monolithiques vers du sharding

---

## Introduction

**Spider** est un moteur de stockage unique qui agit comme un **proxy de sharding** permettant de distribuer les donn√©es horizontalement sur plusieurs serveurs MariaDB. Il transforme MariaDB en une base de donn√©es distribu√©e sans n√©cessiter de modifications applicatives.

**Caract√©ristiques principales** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MOTEUR SPIDER                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                       ‚îÇ
‚îÇ  ‚úì Sharding horizontal transparent                    ‚îÇ
‚îÇ  ‚úì Partitionnement automatique                        ‚îÇ
‚îÇ  ‚úì Agr√©gation distribu√©e (JOIN, GROUP BY)             ‚îÇ
‚îÇ  ‚úì Scale-out horizontal                               ‚îÇ
‚îÇ  ‚úì High availability int√©gr√©                          ‚îÇ
‚îÇ  ‚úì Compatible MySQL/MariaDB                           ‚îÇ
‚îÇ  ‚úó Overhead r√©seau (latence)                          ‚îÇ
‚îÇ  ‚úó Complexit√© op√©rationnelle                          ‚îÇ
‚îÇ                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

üí° **Principe fondamental** : Spider ne stocke pas de donn√©es localement. Il route les requ√™tes vers des serveurs backend ("data nodes") et agr√®ge les r√©sultats.

### Qu'est-ce que le Sharding ?

**Sharding** (ou partitionnement horizontal) : Division des donn√©es d'une table en plusieurs sous-ensembles (shards) distribu√©s sur diff√©rents serveurs.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           MONOLITHIC vs SHARDED                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ  MONOLITHIC (Serveur unique)                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  users (100M rows, 500 GB)                   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ user_id: 1-100M                          ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Single server bottleneck                 ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  SHARDED (4 serveurs Spider)                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ Shard 1      ‚îÇ Shard 2      ‚îÇ Shard 3      ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (25M rows)   ‚îÇ (25M rows)   ‚îÇ (25M rows)   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ user_id:     ‚îÇ user_id:     ‚îÇ user_id:     ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ 1-25M        ‚îÇ 25M-50M      ‚îÇ 50M-75M      ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ 125 GB       ‚îÇ 125 GB       ‚îÇ 125 GB       ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ  ‚îÇ Shard 4      ‚îÇ                                       ‚îÇ
‚îÇ  ‚îÇ (25M rows)   ‚îÇ                                       ‚îÇ
‚îÇ  ‚îÇ user_id:     ‚îÇ                                       ‚îÇ
‚îÇ  ‚îÇ 75M-100M     ‚îÇ                                       ‚îÇ
‚îÇ  ‚îÇ 125 GB       ‚îÇ                                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Avantages :                                            ‚îÇ
‚îÇ  - Scale-out lin√©aire                                   ‚îÇ
‚îÇ  - Isolation des charges                                ‚îÇ
‚îÇ  - R√©silience (panne d'un shard ‚â† panne totale)         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Quand Utiliser Spider ?

**Cas d'usage appropri√©s** :
- Tables > 1 TB n√©cessitant scale-out
- Charge d√©passant capacit√© d'un serveur unique
- Besoin d'isoler certaines donn√©es g√©ographiquement
- Architecture multi-tenant avec isolation par tenant
- Haute disponibilit√© avec distribution g√©ographique
- Croissance continue n√©cessitant ajout de capacit√©

**Quand NE PAS utiliser Spider** :
- Tables < 100 GB (partitionnement local suffit)
- Requ√™tes massivement cross-shard (JOINs complexes)
- Latence critique (overhead r√©seau)
- √âquipe sans expertise en syst√®mes distribu√©s
- Infrastructure ne supportant pas multi-serveurs

---

## Architecture du Moteur Spider

### Architecture Globale

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SPIDER ARCHITECTURE                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                    ‚îÇ
‚îÇ  APPLICATION                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  SELECT * FROM users WHERE user_id = 12345   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                      ‚îÇ                             ‚îÇ
‚îÇ                      ‚Üì                             ‚îÇ
‚îÇ  SPIDER NODE (Routing/Aggregation)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Spider Engine                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Query Parser                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Shard Routing (hash/range/list)          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Parallel Execution                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Result Aggregation                       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ          ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ
‚îÇ          ‚Üì             ‚Üì             ‚Üì             ‚îÇ
‚îÇ  DATA NODES (Backend Storage)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ Shard 1   ‚îÇ  ‚îÇ Shard 2   ‚îÇ  ‚îÇ Shard 3   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ (InnoDB)  ‚îÇ  ‚îÇ (InnoDB)  ‚îÇ  ‚îÇ (InnoDB)  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ user_id:  ‚îÇ  ‚îÇ user_id:  ‚îÇ  ‚îÇ user_id:  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ 1-33M     ‚îÇ  ‚îÇ 33M-66M   ‚îÇ  ‚îÇ 66M-100M  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Composants** :

1. **Spider Node** : Serveur MariaDB avec moteur Spider
   - Stocke uniquement m√©tadonn√©es (d√©finitions tables)
   - Route requ√™tes vers data nodes appropri√©s
   - Agr√®ge r√©sultats

2. **Data Nodes** : Serveurs backend MariaDB
   - Stockent les donn√©es r√©elles (shards)
   - Moteur InnoDB (ou autre)
   - Peuvent √™tre r√©pliqu√©s (HA)

3. **Server Link** : Connexions configur√©es dans Spider
   - D√©finissent comment joindre data nodes
   - Credentials, host, port, database

### Types de D√©ploiement

#### **1. Single Spider Node (Simple)**

```
Application
     ‚Üì
Spider Node ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Data Node 1
     ‚îÇ      ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Data Node 2
     ‚îÇ      ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Data Node 3
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Data Node 4
```

- Simple √† mettre en place
- SPOF (Single Point of Failure)
- Convient pour dev/test

#### **2. Multiple Spider Nodes (HA)**

```
Application
     ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Load Balancer‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üì       ‚Üì        ‚Üì
Spider 1  Spider 2  Spider 3
     ‚îÇ       ‚îÇ        ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Data Node 1
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Data Node 2
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Data Node 3
```

- Haute disponibilit√©
- Load balancing (ProxySQL, HAProxy)
- Scalabilit√© des lectures

#### **3. Spider avec R√©plication (Full HA)**

```
Spider Nodes (HA)         Data Nodes (Replicated)

Spider 1                  Shard 1 Primary
Spider 2                    ‚îú‚îÄ Shard 1 Replica 1
Spider 3                    ‚îî‚îÄ Shard 1 Replica 2

     ‚Üì                    Shard 2 Primary
     ‚Üì                      ‚îú‚îÄ Shard 2 Replica 1
     ‚Üì                      ‚îî‚îÄ Shard 2 Replica 2
```

- Haute disponibilit√© compl√®te
- Failover automatique
- S√©paration lecture/√©criture par shard

---

## Configuration de Base

### Installation

```bash
# Spider est inclus dans MariaDB
# Activer le plugin

mysql -u root -p
```

```sql
-- Installer le plugin Spider
INSTALL SONAME 'ha_spider';

-- V√©rifier installation
SHOW ENGINES;
-- Spider | YES | Spider storage engine | YES | NO | NO

-- Tables syst√®me Spider
SHOW TABLES FROM mysql LIKE 'spider%';
-- mysql.spider_link_failed_log
-- mysql.spider_link_mon_servers
-- mysql.spider_tables
-- mysql.spider_xa
-- mysql.spider_xa_failed_log
-- mysql.spider_xa_member
```

### Configuration Serveur Backend (Data Node)

```ini
# /etc/mysql/conf.d/datanode.cnf
[mysqld]
server_id = 101                    # Unique par data node

# Binlog (si r√©plication)
log_bin = /var/log/mysql/mysql-bin
binlog_format = ROW

# InnoDB standard
innodb_buffer_pool_size = 16G
innodb_log_file_size = 2G

# Utilisateur Spider
# CREATE USER 'spider'@'%' IDENTIFIED BY 'spider_password';
# GRANT ALL ON *.* TO 'spider'@'%';
```

```sql
-- Sur chaque Data Node : cr√©er base et tables
CREATE DATABASE sharddb;

USE sharddb;

CREATE TABLE users (
    user_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    shard_key INT,  -- Optionnel: indicateur du shard
    INDEX idx_username (username)
) ENGINE=InnoDB;
```

### Configuration Spider Node

```sql
-- 1. D√©finir les server links (connexions vers data nodes)
CREATE SERVER shard1
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.101',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'spider_password',
        PORT 3306
    );

CREATE SERVER shard2
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.102',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'spider_password',
        PORT 3306
    );

CREATE SERVER shard3
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.103',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'spider_password',
        PORT 3306
    );

-- 2. Cr√©er la table Spider (proxy)
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    shard_key INT,
    INDEX idx_username (username)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "users"'
PARTITION BY HASH(user_id) PARTITIONS 3 (
    PARTITION pt1 COMMENT = 'srv "shard1"',
    PARTITION pt2 COMMENT = 'srv "shard2"',
    PARTITION pt3 COMMENT = 'srv "shard3"'
);
```

### V√©rification

```sql
-- V√©rifier la configuration Spider
SELECT * FROM mysql.spider_tables;

-- Tester insertion
INSERT INTO users (username, email)
VALUES ('alice', 'alice@example.com');

-- V√©rifier distribution
-- Sur Spider Node
SELECT COUNT(*) FROM users;  -- 1

-- Sur chaque Data Node
USE sharddb;
SELECT COUNT(*) FROM users;
-- Shard 1: 1 (si user_id a √©t√© rout√© vers shard1)
-- Shard 2: 0
-- Shard 3: 0
```

---

## Strat√©gies de Partitionnement

### 1. HASH Partitioning (Distribution Uniforme)

```sql
-- Distribution bas√©e sur hash de la cl√©
CREATE TABLE orders (
    order_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    customer_id BIGINT,
    order_date DATE,
    amount DECIMAL(10,2),
    INDEX idx_customer (customer_id)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "orders"'
PARTITION BY HASH(customer_id) PARTITIONS 4 (
    PARTITION p0 COMMENT = 'srv "shard1"',
    PARTITION p1 COMMENT = 'srv "shard2"',
    PARTITION p2 COMMENT = 'srv "shard3"',
    PARTITION p3 COMMENT = 'srv "shard4"'
);

-- Distribution automatique :
-- customer_id = 12345 ‚Üí HASH(12345) % 4 ‚Üí Partition 1 (shard2)
-- customer_id = 67890 ‚Üí HASH(67890) % 4 ‚Üí Partition 2 (shard3)
```

**Avantages** :
- Distribution uniforme automatique
- Aucune hotspot (√©quilibrage parfait)
- Simple √† mettre en place

**Inconv√©nients** :
- Requ√™tes par range inefficaces
- Difficile d'ajouter/retirer shards (redistribution)

**Cas d'usage** :
- Donn√©es sans pattern temporel ou g√©ographique
- Distribution √©gale requise
- Croissance stable

### 2. RANGE Partitioning (Distribution Temporelle/S√©quentielle)

```sql
-- Distribution par plages de valeurs
CREATE TABLE logs (
    log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    log_date DATE,
    user_id BIGINT,
    message TEXT,
    INDEX idx_date (log_date)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "logs"'
PARTITION BY RANGE (YEAR(log_date) * 10000 + MONTH(log_date) * 100 + DAY(log_date)) (
    PARTITION p_2024_q1 VALUES LESS THAN (20240401) COMMENT = 'srv "shard1"',
    PARTITION p_2024_q2 VALUES LESS THAN (20240701) COMMENT = 'srv "shard2"',
    PARTITION p_2024_q3 VALUES LESS THAN (20241001) COMMENT = 'srv "shard3"',
    PARTITION p_2024_q4 VALUES LESS THAN (20250101) COMMENT = 'srv "shard4"',
    PARTITION p_future VALUES LESS THAN MAXVALUE COMMENT = 'srv "shard4"'
);

-- Distribution :
-- log_date = '2024-02-15' ‚Üí Partition p_2024_q1 (shard1)
-- log_date = '2024-08-20' ‚Üí Partition p_2024_q3 (shard3)
```

**Avantages** :
- Requ√™tes par plage tr√®s efficaces (partition pruning)
- Archivage simple (DROP PARTITION)
- Croissance pr√©visible

**Inconv√©nients** :
- Distribution potentiellement d√©s√©quilibr√©e
- Hotspot sur partition la plus r√©cente

**Cas d'usage** :
- Donn√©es temporelles (logs, transactions)
- S√©quences croissantes (IDs, dates)
- Besoin d'archivage p√©riodique

### 3. LIST Partitioning (Distribution G√©ographique/Cat√©gorielle)

```sql
-- Distribution par valeurs explicites
CREATE TABLE products (
    product_id BIGINT PRIMARY KEY,
    region VARCHAR(10),
    name VARCHAR(100),
    price DECIMAL(10,2),
    INDEX idx_region (region)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "products"'
PARTITION BY LIST COLUMNS(region) (
    PARTITION p_us VALUES IN ('US', 'CA', 'MX') COMMENT = 'srv "shard_americas"',
    PARTITION p_eu VALUES IN ('UK', 'FR', 'DE', 'IT') COMMENT = 'srv "shard_europe"',
    PARTITION p_asia VALUES IN ('JP', 'CN', 'IN', 'KR') COMMENT = 'srv "shard_asia"',
    PARTITION p_other VALUES IN ('AU', 'BR', 'ZA') COMMENT = 'srv "shard_other"'
);

-- Distribution :
-- region = 'FR' ‚Üí Partition p_eu (shard_europe)
-- region = 'JP' ‚Üí Partition p_asia (shard_asia)
```

**Avantages** :
- Isolation logique parfaite
- Requ√™tes filtr√©es par cat√©gorie tr√®s rapides
- Conformit√© r√©glementaire (GDPR, data residency)

**Inconv√©nients** :
- Distribution manuelle requise
- Croissance d√©s√©quilibr√©e possible

**Cas d'usage** :
- Multi-tenant (tenant_id)
- G√©olocalisation (region, country)
- Cat√©gories distinctes (product_type, department)

---

## Op√©rations et Requ√™tes

### INSERT

```sql
-- INSERT simple (rout√© vers shard appropri√©)
INSERT INTO users (username, email)
VALUES ('bob', 'bob@example.com');

-- Spider calcule HASH(user_id) et route vers le bon shard

-- INSERT multiple (distribution automatique)
INSERT INTO users (username, email) VALUES
    ('charlie', 'charlie@example.com'),  -- Shard X
    ('david', 'david@example.com'),      -- Shard Y
    ('eve', 'eve@example.com');          -- Shard Z

-- Chaque ligne est rout√©e vers son shard
```

### SELECT

#### **SELECT par Cl√© (Single Shard Query)**

```sql
-- Requ√™te sur un seul shard (optimal)
SELECT * FROM users WHERE user_id = 12345;

-- Spider :
-- 1. Calcule HASH(12345) ‚Üí shard2
-- 2. Envoie requ√™te uniquement √† shard2
-- 3. Retourne r√©sultat
-- Temps : ~1ms (latence r√©seau)
```

#### **SELECT Multi-Shard (Agr√©gation)**

```sql
-- Requ√™te sur tous les shards (scan complet)
SELECT COUNT(*) FROM users;

-- Spider :
-- 1. Envoie "SELECT COUNT(*)" √† tous les shards en parall√®le
-- 2. Re√ßoit 4 r√©sultats : [250k, 250k, 250k, 250k]
-- 3. Agr√®ge : SUM = 1 million
-- Temps : ~50ms (parall√®le)

-- SELECT avec ORDER BY LIMIT (agr√©gation complexe)
SELECT * FROM users ORDER BY created_at DESC LIMIT 10;

-- Spider :
-- 1. Envoie "SELECT * ORDER BY created_at DESC LIMIT 10" √† tous
-- 2. Re√ßoit 4 √ó 10 lignes = 40 lignes
-- 3. Trie les 40 lignes localement
-- 4. Retourne TOP 10
```

#### **JOIN Cross-Shard**

```sql
-- JOIN entre tables shard√©es (co√ªteux)
SELECT u.username, COUNT(o.order_id)
FROM users u
JOIN orders o ON u.user_id = o.customer_id
GROUP BY u.username;

-- Spider :
-- 1. Scan complet de 'users' (tous shards)
-- 2. Pour chaque user, query 'orders' (potentiellement tous shards)
-- 3. Agr√©gation locale
-- Tr√®s lent : O(N√óM) requ√™tes r√©seau

-- Optimisation : co-location (m√™me cl√© de sharding)
-- Si users et orders sharded par user_id ‚Üí m√™me shard
-- JOIN local sur chaque shard ‚Üí parall√®le
```

### UPDATE et DELETE

```sql
-- UPDATE par cl√© (single shard)
UPDATE users SET email = 'newemail@example.com'
WHERE user_id = 12345;
-- Rout√© vers shard appropri√©

-- UPDATE multi-shard (broadcast)
UPDATE users SET status = 'inactive'
WHERE created_at < '2020-01-01';
-- Envoy√© √† tous les shards

-- DELETE par cl√© (single shard)
DELETE FROM users WHERE user_id = 12345;

-- DELETE multi-shard (broadcast)
DELETE FROM users WHERE created_at < '2015-01-01';
```

‚ö†Ô∏è **Attention** : Op√©rations multi-shard sont lentes et peuvent causer des timeouts.

### Transactions Distribu√©es

```sql
-- Spider supporte transactions distribu√©es (XA)
START TRANSACTION;

INSERT INTO users (username, email) VALUES ('frank', 'frank@example.com');
INSERT INTO orders (customer_id, amount) VALUES (LAST_INSERT_ID(), 100.00);

-- Spider coordonne 2PC (Two-Phase Commit) entre shards
COMMIT;

-- En cas d'erreur sur un shard
ROLLBACK;  -- Annule sur tous les shards
```

üí° **Limitation** : Transactions distribu√©es ont un overhead significatif. Privil√©gier transactions single-shard quand possible.

---

## Haute Disponibilit√© avec Spider

### Architecture HA Compl√®te

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        SPIDER HIGH AVAILABILITY ARCHITECTURE            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ  Application                                            ‚îÇ
‚îÇ       ‚Üì                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ  ‚îÇ ProxySQL/HAProxy‚îÇ  (Connection pooling + LB)         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ           ‚îÇ                                             ‚îÇ
‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ      ‚Üì         ‚Üì         ‚Üì                              ‚îÇ
‚îÇ  Spider 1  Spider 2  Spider 3  (HA Spider nodes)        ‚îÇ
‚îÇ      ‚îÇ         ‚îÇ         ‚îÇ                              ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Shard 1 Primary           ‚îÇ
‚îÇ                             ‚îú‚îÄ Shard 1 Replica 1        ‚îÇ
‚îÇ                             ‚îî‚îÄ Shard 1 Replica 2        ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Shard 2 Primary           ‚îÇ
‚îÇ                             ‚îú‚îÄ Shard 2 Replica 1        ‚îÇ
‚îÇ                             ‚îî‚îÄ Shard 2 Replica 2        ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Shard 3 Primary           ‚îÇ
‚îÇ                             ‚îú‚îÄ Shard 3 Replica 1        ‚îÇ
‚îÇ                             ‚îî‚îÄ Shard 3 Replica 2        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Configuration R√©plication par Shard

```sql
-- Sur Spider Node : configurer primary + replicas
CREATE SERVER shard1_primary
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.101',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'password',
        PORT 3306
    );

CREATE SERVER shard1_replica1
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.111',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'password',
        PORT 3306
    );

CREATE SERVER shard1_replica2
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.112',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'password',
        PORT 3306
    );

-- Table Spider avec read/write split
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50),
    email VARCHAR(100)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "users", srv "shard1_primary shard1_replica1 shard1_replica2"'
PARTITION BY HASH(user_id) PARTITIONS 3 (
    PARTITION p0 COMMENT = 'srv "shard1_primary shard1_replica1 shard1_replica2"',
    PARTITION p1 COMMENT = 'srv "shard2_primary shard2_replica1 shard2_replica2"',
    PARTITION p2 COMMENT = 'srv "shard3_primary shard3_replica1 shard3_replica2"'
);
```

### Monitoring de Sant√©

```sql
-- Table de monitoring Spider
CREATE TABLE spider_link_mon_servers (
    db_name VARCHAR(64),
    table_name VARCHAR(64),
    link_id CHAR(5),
    sid INT,
    server VARCHAR(64),
    scheme VARCHAR(64),
    host VARCHAR(64),
    port CHAR(5),
    socket TEXT,
    username VARCHAR(64),
    password VARCHAR(64),
    tgt_db_name VARCHAR(64),
    tgt_table_name VARCHAR(64),
    link_status TINYINT,
    PRIMARY KEY (db_name, table_name, link_id)
) ENGINE=Spider;

-- V√©rifier statut des liens
SELECT
    db_name,
    table_name,
    server,
    host,
    link_status,  -- 1: OK, 0: Failed
    CASE
        WHEN link_status = 1 THEN 'ONLINE'
        ELSE 'OFFLINE'
    END AS status
FROM mysql.spider_link_mon_servers;

-- Logs d'√©chec
SELECT * FROM mysql.spider_link_failed_log
ORDER BY failed_time DESC
LIMIT 10;
```

### Failover Automatique

```sql
-- Configuration auto-failover
SET GLOBAL spider_link_mon_enable = 1;
SET GLOBAL spider_link_mon_interval = 5;  -- Check toutes les 5 secondes

-- Spider d√©tecte automatiquement data node down et bascule sur replica

-- Exemple de notification
CREATE EVENT spider_failover_alert
ON SCHEDULE EVERY 1 MINUTE
DO
BEGIN
    DECLARE v_failed_count INT;

    SELECT COUNT(*) INTO v_failed_count
    FROM mysql.spider_link_failed_log
    WHERE failed_time > DATE_SUB(NOW(), INTERVAL 5 MINUTE);

    IF v_failed_count > 0 THEN
        -- Envoyer alerte (email, Slack, etc.)
        INSERT INTO alerts (message, severity, created_at)
        VALUES (CONCAT('Spider failover: ', v_failed_count, ' links failed'), 'WARNING', NOW());
    END IF;
END;
```

---

## Performance et Optimisation

### 1. Co-Location (M√™me Cl√© de Sharding)

```sql
-- ‚ùå Mauvais : cl√©s diff√©rentes
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY
) ENGINE=Spider
PARTITION BY HASH(user_id);  -- Sharded par user_id

CREATE TABLE orders (
    order_id BIGINT PRIMARY KEY,
    customer_id BIGINT
) ENGINE=Spider
PARTITION BY HASH(order_id);  -- Sharded par order_id ‚â† user_id

-- JOIN users-orders = cross-shard (lent)

-- ‚úÖ Bon : m√™me cl√©
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY
) ENGINE=Spider
PARTITION BY HASH(user_id);

CREATE TABLE orders (
    order_id BIGINT PRIMARY KEY,
    customer_id BIGINT
) ENGINE=Spider
PARTITION BY HASH(customer_id);  -- M√™me cl√© = user_id/customer_id

-- JOIN users-orders = local sur chaque shard (rapide)
```

### 2. Partition Pruning

```sql
-- Requ√™te avec filtre sur cl√© de sharding (optimal)
SELECT * FROM users WHERE user_id = 12345;
-- Spider scan : 1 shard uniquement

-- Requ√™te sans filtre sur cl√© (scan complet)
SELECT * FROM users WHERE email = 'user@example.com';
-- Spider scan : tous les shards (lent)

-- Solution : index secondaire local sur chaque shard
-- Mais reste multi-shard query
```

### 3. Agr√©gations Distribu√©es

```sql
-- Agr√©gation simple (parall√®le efficace)
SELECT COUNT(*), AVG(amount) FROM orders;
-- Spider : parall√©lise sur tous shards, agr√®ge localement

-- Agr√©gation avec GROUP BY (potentiellement lente)
SELECT customer_id, COUNT(*), SUM(amount)
FROM orders
GROUP BY customer_id
ORDER BY SUM(amount) DESC
LIMIT 10;

-- Spider :
-- 1. GROUP BY sur chaque shard
-- 2. Transfert r√©sultats partiels
-- 3. Re-agr√©gation locale
-- 4. Sort + LIMIT local
```

### 4. Bulk Operations

```sql
-- INSERT en masse (batch)
-- ‚ùå Lent : INSERT individuel
INSERT INTO users VALUES (1, 'user1');
INSERT INTO users VALUES (2, 'user2');
-- 1000 requ√™tes r√©seau

-- ‚úÖ Rapide : INSERT multiple
INSERT INTO users VALUES
    (1, 'user1'),
    (2, 'user2'),
    -- ...
    (1000, 'user1000');
-- 1 requ√™te, Spider distribue en batch √† chaque shard

-- LOAD DATA (optimal)
LOAD DATA LOCAL INFILE '/tmp/users.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
-- Spider streame vers shards appropri√©s
```

### Configuration Spider pour Performance

```sql
-- Variables Spider importantes
SET GLOBAL spider_conn_recycle_mode = 0;       -- 0: disable, 1: enable
SET GLOBAL spider_conn_recycle_strict = 0;
SET GLOBAL spider_max_connections = 100;       -- Connexions max par data node
SET GLOBAL spider_net_read_timeout = 600;      -- Timeout lecture (secondes)
SET GLOBAL spider_net_write_timeout = 600;     -- Timeout √©criture
SET GLOBAL spider_quick_mode = 3;              -- Mode rapide (0-3)
SET GLOBAL spider_quick_page_size = 100;       -- Taille page pour quick mode
SET GLOBAL spider_use_all_conns_snapshot = 0;
SET GLOBAL spider_remote_access_charset = 'utf8mb4';

-- Connection pooling
SET GLOBAL spider_conn_pool_enable = 1;
SET GLOBAL spider_conn_pool_size = 10;
```

---

## Migration et Conversion

### Migration Monolithic ‚Üí Spider

```sql
-- 1. Pr√©paration : Table existante InnoDB
CREATE TABLE users_monolithic (
    user_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50),
    email VARCHAR(100),
    created_at DATETIME
) ENGINE=InnoDB;

-- Donn√©es : 100 millions de lignes

-- 2. Setup data nodes
-- Sur shard1, shard2, shard3 : cr√©er structure
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    created_at DATETIME
) ENGINE=InnoDB;

-- 3. Cr√©er table Spider
CREATE TABLE users_spider (
    user_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50),
    email VARCHAR(100),
    created_at DATETIME
) ENGINE=Spider
COMMENT='wrapper "mysql", table "users"'
PARTITION BY HASH(user_id) PARTITIONS 3 (
    PARTITION p0 COMMENT = 'srv "shard1"',
    PARTITION p1 COMMENT = 'srv "shard2"',
    PARTITION p2 COMMENT = 'srv "shard3"'
);

-- 4. Migration par lots (√©viter downtime)
DELIMITER //
CREATE PROCEDURE migrate_to_spider()
BEGIN
    DECLARE v_batch_size INT DEFAULT 100000;
    DECLARE v_offset BIGINT DEFAULT 0;
    DECLARE v_rows INT DEFAULT 1;

    WHILE v_rows > 0 DO
        -- Copier un lot
        INSERT INTO users_spider
        SELECT * FROM users_monolithic
        ORDER BY user_id
        LIMIT v_offset, v_batch_size;

        SET v_rows = ROW_COUNT();
        SET v_offset = v_offset + v_batch_size;

        -- Log progression
        SELECT CONCAT('Migrated ', v_offset, ' rows') AS status;

        -- Pause pour ne pas surcharger
        DO SLEEP(1);
    END WHILE;
END//
DELIMITER ;

-- Ex√©cution
CALL migrate_to_spider();

-- 5. V√©rification
SELECT COUNT(*) FROM users_monolithic;  -- 100M
SELECT COUNT(*) FROM users_spider;      -- 100M

-- 6. Cutover (switch application)
RENAME TABLE
    users_monolithic TO users_old,
    users_spider TO users;

-- 7. Validation et cleanup
-- Tester application
-- DROP TABLE users_old;
```

### Ajout de Shards (Scale-out)

```sql
-- Configuration initiale : 3 shards
-- Besoin : Passer √† 4 shards

-- 1. Ajouter nouveau data node
CREATE SERVER shard4
    FOREIGN DATA WRAPPER mysql
    OPTIONS (
        HOST '192.168.1.104',
        DATABASE 'sharddb',
        USER 'spider',
        PASSWORD 'password',
        PORT 3306
    );

-- 2. Restructurer partitionnement
-- ‚ö†Ô∏è ATTENTION : Requiert redistribution des donn√©es

-- Option A : Recr√©er table (downtime)
DROP TABLE users;
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100)
) ENGINE=Spider
COMMENT='wrapper "mysql", table "users"'
PARTITION BY HASH(user_id) PARTITIONS 4 (  -- 3 ‚Üí 4
    PARTITION p0 COMMENT = 'srv "shard1"',
    PARTITION p1 COMMENT = 'srv "shard2"',
    PARTITION p2 COMMENT = 'srv "shard3"',
    PARTITION p3 COMMENT = 'srv "shard4"'
);

-- Recharger donn√©es (redistribution automatique)
INSERT INTO users SELECT * FROM users_backup;

-- Option B : REORGANIZE PARTITION (moins de downtime)
ALTER TABLE users REORGANIZE PARTITION;
-- MariaDB redistribue automatiquement

-- Option C : Consistent Hashing (avanc√©)
-- Utiliser consistent hashing pour minimiser redistribution
-- N√©cessite planification architecturale
```

---

## Comparaison avec Alternatives

### Spider vs ProxySQL

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SPIDER vs PROXYSQL                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Spider (Storage Engine)                                 ‚îÇ
‚îÇ  - Int√©gr√© MariaDB                                       ‚îÇ
‚îÇ  - Sharding natif                                        ‚îÇ
‚îÇ  - Agr√©gations distribu√©es                               ‚îÇ
‚îÇ  - Configuration via SQL                                 ‚îÇ
‚îÇ  - Transactions distribu√©es (XA)                         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ProxySQL (Proxy externe)                                ‚îÇ
‚îÇ  - Application agnostique                                ‚îÇ
‚îÇ  - Connection pooling avanc√©                             ‚îÇ
‚îÇ  - Query routing (read/write split)                      ‚îÇ
‚îÇ  - Pas d'agr√©gation automatique                          ‚îÇ
‚îÇ  - Monitoring et m√©triques riches                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Recommandation** :
- **Spider** : Sharding automatique, agr√©gations complexes
- **ProxySQL** : Read/write split, connection pooling, HA simpler

### Spider vs Vitess (MySQL)

| Aspect | Spider | Vitess |
|--------|--------|--------|
| Int√©gration | Moteur MariaDB | Middleware externe |
| Sharding | Automatique | Orchestr√© |
| Topologie | Flexible | Opinionated |
| Overhead | Faible | Moyen (VTGate) |
| Kubernetes | Basique | Natif (Operator) |
| Complexit√© | Moyenne | √âlev√©e |
| √âcosyst√®me | MariaDB | YouTube/Cloud Native |

**Recommandation** :
- **Spider** : Infrastructure MariaDB existante, simplicit√©
- **Vitess** : Kubernetes natif, scaling massif (>1000 shards)

### Spider vs Galera Cluster

| Aspect | Spider (Sharding) | Galera (Multi-Master) |
|--------|------------------|----------------------|
| Architecture | Horizontal partitioning | Synchronous replication |
| Distribution | Donn√©es divis√©es | Donn√©es r√©pliqu√©es |
| Scalabilit√© | Lin√©aire (scale-out) | Limit√©e (3-5 nodes) |
| Writes | Distribu√©es | Toutes r√©pliqu√©es |
| Reads | Distribu√©es | Toutes locales |
| Cas d'usage | Tr√®s grandes bases | HA + load balancing |

üí° **Combinaison** : Spider + Galera = Sharding avec HA par shard
- Chaque shard = Galera cluster 3 nodes
- Haute disponibilit√© + scalabilit√© massive

---

## Monitoring et Maintenance

### M√©triques Essentielles

```sql
-- Dashboard Spider
CREATE VIEW spider_health AS
SELECT
    'Total Shards' AS metric,
    COUNT(DISTINCT server) AS value
FROM mysql.spider_link_mon_servers

UNION ALL

SELECT
    'Failed Links',
    COUNT(*)
FROM mysql.spider_link_mon_servers
WHERE link_status = 0

UNION ALL

SELECT
    'Active Transactions',
    COUNT(*)
FROM mysql.spider_xa

UNION ALL

SELECT
    'Failed Transactions (last hour)',
    COUNT(*)
FROM mysql.spider_xa_failed_log
WHERE failed_time > DATE_SUB(NOW(), INTERVAL 1 HOUR);

-- Utilisation
SELECT * FROM spider_health;
```

### Logging et Troubleshooting

```sql
-- Activer debug logging Spider
SET GLOBAL spider_log_result_errors = 3;  -- 0-4 (4=verbose)
SET GLOBAL spider_log_result_error_with_sql = 3;

-- Analyser erreurs
SELECT
    failed_time,
    db_name,
    table_name,
    link_id,
    failed_link_id
FROM mysql.spider_link_failed_log
ORDER BY failed_time DESC
LIMIT 20;

-- Transactions √©chou√©es
SELECT
    xa.xid,
    xa.status,
    xf.failed_time,
    xf.error_no,
    xf.error_msg
FROM mysql.spider_xa xa
LEFT JOIN mysql.spider_xa_failed_log xf ON xa.xid = xf.xid
WHERE xa.status != 'COMMITTED'
ORDER BY xf.failed_time DESC;
```

---

## Best Practices

### ‚úÖ √Ä Faire

1. **Co-localiser donn√©es li√©es** : M√™me cl√© de sharding pour tables avec JOINs
2. **Monitoring proactif** : Alertes sur links down, latence √©lev√©e
3. **Tester failover** : Simuler pannes r√©guli√®rement
4. **Documenter topologie** : Sch√©mas √† jour des shards
5. **Backup par shard** : Strat√©gie de backup distribu√©e
6. **Commencer petit** : 3-4 shards initialement, scale progressivement
7. **Utiliser ProxySQL** : Connection pooling devant Spider nodes

```sql
-- Exemple de monitoring automatis√©
CREATE EVENT spider_health_check
ON SCHEDULE EVERY 1 MINUTE
DO
BEGIN
    -- V√©rifier liens down
    IF (SELECT COUNT(*) FROM mysql.spider_link_mon_servers WHERE link_status = 0) > 0 THEN
        INSERT INTO alerts (message, severity)
        VALUES ('Spider links down detected', 'CRITICAL');
    END IF;

    -- V√©rifier latence
    -- (Custom implementation avec query performance)
END;
```

### ‚ùå √Ä √âviter

1. **Trop de shards** : Overhead r√©seau > gains
2. **Cl√©s de sharding diff√©rentes** : JOINs cross-shard
3. **Transactions distribu√©es longues** : Risque deadlock/timeout
4. **N√©gliger r√©plication** : SPOF sur chaque shard
5. **Pas de plan de migration** : Ajout de shards complexe
6. **Monitoring insuffisant** : Pannes non d√©tect√©es
7. **Over-engineering** : Spider pour petites bases (<500GB)

```sql
-- ‚ùå Anti-pattern : mauvaise cl√© de sharding
CREATE TABLE sessions (
    session_id VARCHAR(32) PRIMARY KEY,
    user_id BIGINT,
    data TEXT
) ENGINE=Spider
PARTITION BY HASH(session_id);  -- Random ‚Üí pas de co-location

-- JOINs users-sessions seront cross-shard

-- ‚úÖ Bon pattern
PARTITION BY HASH(user_id);  -- Co-location avec users
```

---

## ‚úÖ Points cl√©s √† retenir

- **Spider = Sharding transparent** : Moteur de stockage proxy pour distribution horizontale
- **Scale-out lin√©aire** : Ajout de shards augmente capacit√© proportionnellement
- **Pas de donn√©es locales** : Spider route requ√™tes vers data nodes backend
- **3 strat√©gies** : HASH (uniforme), RANGE (temporel), LIST (cat√©goriel)
- **Overhead r√©seau** : Latence additionnelle (~1-5ms par shard)
- **HA requise** : R√©plication par shard + multiple Spider nodes
- **Co-location critique** : M√™me cl√© de sharding pour tables li√©es (JOINs)
- **Agr√©gations distribu√©es** : GROUP BY, ORDER BY, JOIN possibles mais co√ªteux si cross-shard
- **Transactions XA** : Support distribu√© mais avec overhead
- **Monitoring essentiel** : Sant√© des liens, failover, latence
- **Cas d'usage** : Tables > 1TB, scale-out requis, isolation g√©ographique

---

## üîó Ressources et r√©f√©rences

- [üìñ Spider Storage Engine](https://mariadb.com/kb/en/spider-storage-engine/)
- [üìñ Spider Installation](https://mariadb.com/kb/en/spider-installation/)
- [üìñ Spider Server System Variables](https://mariadb.com/kb/en/spider-server-system-variables/)
- [üìñ Sharding with Spider](https://mariadb.com/kb/en/sharding-with-spider/)
- [üìñ Spider High Availability](https://mariadb.com/kb/en/spider-high-availability/)

**Articles techniques** :
- "Spider Storage Engine: MySQL Sharding" - MariaDB Blog
- "Horizontal Sharding with MariaDB Spider" - Percona Blog
- "Spider vs ProxySQL: When to Use Which" - Planet MySQL
- "Building Scalable Architectures with Spider" - MariaDB Corporation

**Pr√©sentations** :
- "Spider: The Sharding Engine for MariaDB" - MariaDB.org
- "Scale-out MySQL with Spider" - Percona Live

**Outils** :
- ProxySQL - Connection pooling devant Spider
- HAProxy - Load balancing Spider nodes
- PMM - Monitoring distribu√©

---

## ‚û°Ô∏è Section suivante

**[7.10.4 CONNECT : Acc√®s donn√©es externes](/07-moteurs-de-stockage/10.4-connect.md)** : Moteur f√©d√©r√© pour acc√©der √† des sources de donn√©es h√©t√©rog√®nes (CSV, XML, ODBC, REST API), cas d'usage d'int√©gration, et ETL avec MariaDB.

‚è≠Ô∏è [CONNECT : Acc√®s donn√©es externes](/07-moteurs-de-stockage/10.4-connect.md)
