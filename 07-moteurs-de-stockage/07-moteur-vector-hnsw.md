üîù Retour au [Sommaire](/SOMMAIRE.md)

# 7.7 Moteur Vector/HNSW : Recherche vectorielle pour IA üÜï

> **Niveau** : Avanc√©
> **Dur√©e estim√©e** : 3-4 heures
> **Pr√©requis** : Sections 7.1-7.2 (Architecture, InnoDB), concepts d'IA/ML, embeddings, alg√®bre lin√©aire
> **Public cible** : DBA, Architectes IA, Data Scientists, MLOps Engineers

> **Nouveaut√©** : MariaDB 11.8 LTS

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre les concepts de recherche vectorielle et d'embeddings
- Ma√Ætriser l'architecture du moteur Vector et l'index HNSW
- Impl√©menter des cas d'usage IA : RAG, recherche s√©mantique, recommandations
- Optimiser les performances de recherche vectorielle √† grande √©chelle
- Int√©grer MariaDB Vector avec des mod√®les d'IA (OpenAI, LLaMA, etc.)
- Concevoir des architectures hybrides (SQL + Vector)
- Choisir les bons param√®tres d'index selon les cas d'usage
- Monitorer et tuner les performances Vector

---

## Introduction

Le **moteur Vector/HNSW** (introduit dans MariaDB 11.8 LTS) apporte la **recherche vectorielle native** dans MariaDB, permettant de stocker et interroger efficacement des **embeddings** pour les applications d'**IA g√©n√©rative** et de **machine learning**.

### Contexte : L'essor de l'IA g√©n√©rative

```
√âvolution des bases de donn√©es :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1970-2000 : Donn√©es structur√©es (SQL)                  ‚îÇ
‚îÇ  ‚Ä¢ Nombres, textes, dates                              ‚îÇ
‚îÇ  ‚Ä¢ Recherche exacte (WHERE id = 42)                    ‚îÇ
‚îÇ  ‚Ä¢ Index B-Tree                                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2000-2020 : Donn√©es semi-structur√©es (NoSQL)           ‚îÇ
‚îÇ  ‚Ä¢ JSON, documents                                     ‚îÇ
‚îÇ  ‚Ä¢ Recherche Full-Text (MATCH AGAINST)                 ‚îÇ
‚îÇ  ‚Ä¢ Index invers√©s                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2020+ : IA et Embeddings (Vector)                      ‚îÇ
‚îÇ  ‚Ä¢ Vecteurs de nombres (512-1536 dimensions)           ‚îÇ
‚îÇ  ‚Ä¢ Recherche par similarit√© s√©mantique                 ‚îÇ
‚îÇ  ‚Ä¢ Index HNSW, FAISS                                   ‚îÇ
‚îÇ  ‚Ä¢ Cas d'usage : RAG, recherche s√©mantique, IA         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Qu'est-ce qu'un embedding ?

Un **embedding** est une repr√©sentation num√©rique d'un objet (texte, image, audio) sous forme de vecteur dense dans un espace vectoriel de haute dimension.

```
Exemple : Embeddings de textes
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Texte original ‚Üí Mod√®le IA ‚Üí Vecteur (embedding)       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ "chat"         ‚Üí OpenAI    ‚Üí [0.2, -0.5, 0.8, ...]     ‚îÇ
‚îÇ                  text-embedding-3-small                ‚îÇ
‚îÇ                  (1536 dimensions)                     ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ "chien"        ‚Üí OpenAI    ‚Üí [0.3, -0.4, 0.7, ...]     ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ "automobile"   ‚Üí OpenAI    ‚Üí [-0.8, 0.6, -0.2, ...]    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Propri√©t√© cl√© : Similarit√© s√©mantique
‚Ä¢ Distance("chat", "chien") = faible (concepts proches)
‚Ä¢ Distance("chat", "automobile") = √©lev√©e (concepts √©loign√©s)
```

**Visualisation 2D simplifi√©e** (en r√©alit√© 512-1536D) :

```
        Y
        ‚îÇ
   chien ‚óè                   Animaux
        ‚îÇ ‚óè chat
        ‚îÇ   ‚óè cheval
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ X
        ‚îÇ
        ‚îÇ     ‚óè voiture    V√©hicules
        ‚îÇ   ‚óè camion
        ‚îÇ
```

### Probl√®me : Recherche dans les bases traditionnelles

```sql
-- Recherche traditionnelle (exact match)
SELECT * FROM products
WHERE name = 'iPhone 15';  -- Trouve seulement "iPhone 15"

-- Full-Text (mots-cl√©s)
SELECT * FROM products
WHERE MATCH(description) AGAINST('smartphone apple');
-- Trouve si mots pr√©sents, mais pas de compr√©hension s√©mantique

-- Limitations :
-- ‚ùå "iPhone" vs "t√©l√©phone Apple" ‚Üí Pas de correspondance
-- ‚ùå "iPhone 15" vs "dernier iPhone" ‚Üí Pas de correspondance
-- ‚ùå Pas de recherche par concept/signification
```

### Solution : Recherche vectorielle

```sql
-- Recherche vectorielle (similarit√© s√©mantique)
SELECT
    product_name,
    description,
    VEC_DISTANCE_EUCLIDEAN(embedding, ?) AS distance
FROM products
ORDER BY distance
LIMIT 10;

-- Requ√™te : "t√©l√©phone derni√®re g√©n√©ration"
-- R√©sultats :
-- 1. iPhone 15 Pro Max (distance: 0.12)
-- 2. Samsung Galaxy S24 Ultra (distance: 0.15)
-- 3. Google Pixel 8 Pro (distance: 0.18)
-- ‚Üí Compr√©hension s√©mantique, pas juste mots-cl√©s !
```

---

## Architecture du moteur Vector

### Vue d'ensemble

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   MariaDB Server                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ           SQL Layer (Parser, Optimizer)            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Nouvelles fonctions : VEC_DISTANCE_*()          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Support type de donn√©es VECTOR                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì Handler API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              InnoDB Storage Engine                      ‚îÇ
‚îÇ  (Vector est une extension d'InnoDB, pas un moteur      ‚îÇ
‚îÇ   s√©par√© - il s'agit d'un nouveau type d'index)         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ         HNSW Index (Hierarchical Navigable         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ         Small World Graph)                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Layer 0 (tous les vecteurs)                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ Vector1 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Vector2 ‚îÇ                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ              ‚îÇ                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Layer 1 (√©chantillon)                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ VectorA ‚îÇ                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Layer N (point d'entr√©e)                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îÇ  Entry  ‚îÇ                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ      Optimisations SIMD                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ AVX2, AVX-512 (Intel/AMD)                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ARM NEON                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Calcul distance vectoris√©                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Stockage InnoDB (Buffer Pool, Disque)         ‚îÇ
‚îÇ  ‚Ä¢ Vecteurs stock√©s comme BLOB                          ‚îÇ
‚îÇ  ‚Ä¢ Index HNSW structure graphe                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Type de donn√©es VECTOR

```sql
-- D√©claration d'une colonne vector
CREATE TABLE documents (
    id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(500),
    content TEXT,
    embedding VECTOR(1536)  -- Vecteur de 1536 dimensions
);

-- Dimensions support√©es : 1-16384
-- Taille en m√©moire : dimensions √ó 4 bytes (float32)
-- Exemple : VECTOR(1536) = 1536 √ó 4 = 6144 bytes (~6 KB par vecteur)
```

**Formats d'embeddings courants** :

| Mod√®le | Provider | Dimensions | Taille | Use Case |
|--------|----------|------------|--------|----------|
| text-embedding-3-small | OpenAI | 1536 | 6 KB | Usage g√©n√©ral, √©conomique |
| text-embedding-3-large | OpenAI | 3072 | 12 KB | Haute pr√©cision |
| text-embedding-ada-002 | OpenAI | 1536 | 6 KB | Legacy (deprecated) |
| all-MiniLM-L6-v2 | HuggingFace | 384 | 1.5 KB | Local, rapide |
| LLaMA-2-7b embeddings | Meta | 4096 | 16 KB | Open-source |
| CLIP | OpenAI | 512 | 2 KB | Images + Textes |

### Index HNSW (Hierarchical Navigable Small World)

HNSW est l'algorithme d'indexation state-of-the-art pour la recherche vectorielle. Il construit un **graphe navigable multi-couches**.

```
Structure HNSW √† 3 couches :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 2 (point d'entr√©e, tr√®s peu de n≈ìuds)            ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ         Entry Point ‚óè                                  ‚îÇ
‚îÇ                     ‚Üì                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1 (√©chantillon, distances longues)               ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ      ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè                         ‚îÇ
‚îÇ      ‚îÇ           ‚îÇ           ‚îÇ                         ‚îÇ
‚îÇ      ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè                         ‚îÇ
‚îÇ                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 0 (tous les vecteurs, distances courtes)         ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè              ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ  ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè              ‚îÇ
‚îÇ                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Algorithme de recherche HNSW** :

```
1. Partir du Entry Point (Layer max)
2. Descendre les layers en trouvant les voisins les plus proches
3. Arriver au Layer 0 avec une bonne approximation
4. Affiner localement dans Layer 0
5. Retourner les K plus proches voisins

Complexit√© :
‚Ä¢ Construction : O(N √ó log N)
‚Ä¢ Recherche : O(log N)  (vs O(N) pour scan lin√©aire)
‚Ä¢ Pr√©cision : ~99% vs recherche exacte
```

**Param√®tres HNSW** :

```sql
-- Cr√©er un index HNSW
CREATE INDEX idx_embedding ON documents(embedding)
USING HNSW
WITH (
    M = 16,              -- Connexions par n≈ìud (d√©faut: 16)
    ef_construction = 200, -- Pr√©cision construction (d√©faut: 200)
    metric = 'euclidean'  -- euclidean, cosine, inner_product
);

-- M (max connections) :
-- ‚Ä¢ Plus grand = Plus de connexions = Meilleure pr√©cision, mais index plus gros
-- ‚Ä¢ Recommand√© : 12-48 (d√©faut 16 bon pour la plupart)

-- ef_construction (exploration factor) :
-- ‚Ä¢ Plus grand = Construction plus lente, mais meilleure qualit√©
-- ‚Ä¢ Recommand√© : 100-500 (d√©faut 200)
```

### M√©triques de distance

MariaDB Vector supporte **3 m√©triques de distance** :

#### 1. Distance Euclidienne (L2)

```
Distance Euclidienne entre A et B :
d(A, B) = ‚àö(Œ£(A·µ¢ - B·µ¢)¬≤)

Exemple 2D :
A = [3, 4]
B = [0, 0]
d = ‚àö((3-0)¬≤ + (4-0)¬≤) = ‚àö(9 + 16) = ‚àö25 = 5

SQL :
SELECT VEC_DISTANCE_EUCLIDEAN(embedding, '[0.2, 0.5, 0.8]') AS distance
FROM documents;
```

**Usage** : Mesure de distance "g√©om√©trique" classique. Bon pour la plupart des cas.

#### 2. Similarit√© cosinus (Cosine Similarity)

```
Similarit√© cosinus entre A et B :
sim(A, B) = (A ¬∑ B) / (||A|| √ó ||B||)
distance(A, B) = 1 - sim(A, B)

Exemple 2D :
A = [3, 4]  (magnitude = 5)
B = [6, 8]  (magnitude = 10)
sim = (3√ó6 + 4√ó8) / (5 √ó 10) = 50/50 = 1.0 (colin√©aires)

SQL :
SELECT VEC_DISTANCE_COSINE(embedding, '[0.2, 0.5, 0.8]') AS distance
FROM documents;
```

**Usage** : Ignore la magnitude, mesure l'angle. **Id√©al pour NLP** (embeddings normalis√©s).

#### 3. Produit scalaire (Inner Product)

```
Produit scalaire (dot product) :
A ¬∑ B = Œ£(A·µ¢ √ó B·µ¢)
distance = -A ¬∑ B  (n√©gatif car plus grand = plus similaire)

Exemple 2D :
A = [3, 4]
B = [1, 2]
A ¬∑ B = 3√ó1 + 4√ó2 = 3 + 8 = 11

SQL :
SELECT VEC_DISTANCE_INNER_PRODUCT(embedding, '[0.2, 0.5, 0.8]') AS distance
FROM documents;
```

**Usage** : Rapide (pas de racine carr√©e), bon pour embeddings d√©j√† normalis√©s.

**Tableau comparatif** :

| M√©trique | Complexit√© | Usage principal | Normalis√© ? |
|----------|-----------|-----------------|-------------|
| Euclidienne | O(n) + sqrt | Distance absolue | Non |
| Cosine | O(n) + 2√ósqrt | NLP, textes, normalis√©s | Oui |
| Inner Product | O(n) | Maximum similarity score | Peut √™tre |

üí° **Recommandation** : Pour NLP (OpenAI, LLaMA, etc.) ‚Üí **Cosine**. Pour images, audio ‚Üí **Euclidienne**.

---

## Configuration et utilisation

### Installation et activation

```sql
-- V√©rifier support Vector (MariaDB 11.8+)
SELECT VERSION();
-- 11.8.0-MariaDB

-- Le support Vector est int√©gr√©, pas de plugin s√©par√©
-- Cr√©er une table avec colonne VECTOR
CREATE TABLE test_vector (
    id INT PRIMARY KEY,
    vec VECTOR(128)
);

-- Si erreur ‚Üí V√©rifier version MariaDB
```

### Cr√©ation de table

```sql
-- Table pour recherche s√©mantique de documents
CREATE TABLE knowledge_base (
    doc_id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- Embedding g√©n√©r√© par OpenAI text-embedding-3-small
    embedding VECTOR(1536),

    -- Index traditionnel pour filtrage
    INDEX idx_created (created_at),
    INDEX idx_title (title)
) ENGINE=InnoDB;

-- Cr√©er l'index HNSW
CREATE INDEX idx_embedding ON knowledge_base(embedding)
USING HNSW
WITH (
    M = 16,
    ef_construction = 200,
    metric = 'cosine'  -- Pour NLP
);
```

### Insertion de donn√©es

**M√©thode 1 : Avec embeddings pr√©-calcul√©s** :

```sql
-- Embeddings g√©n√©r√©s via API OpenAI (Python, Node.js, etc.)
INSERT INTO knowledge_base (title, content, embedding) VALUES
(
    'Introduction √† MariaDB',
    'MariaDB est un syst√®me de gestion de base de donn√©es...',
    '[0.023, -0.045, 0.789, ..., 0.234]'  -- 1536 valeurs
);

-- Format : Tableau JSON de floats
-- Dimension doit correspondre √† la d√©claration VECTOR(1536)
```

**M√©thode 2 : Via application (recommand√©)** :

```python
# Python avec OpenAI SDK
import openai
import mariadb

# G√©n√©rer embedding
response = openai.embeddings.create(
    model="text-embedding-3-small",
    input="MariaDB est un syst√®me de gestion de base de donn√©es..."
)
embedding = response.data[0].embedding  # Liste de 1536 floats

# Ins√©rer dans MariaDB
conn = mariadb.connect(
    host="localhost",
    user="root",
    password="password",
    database="mydb"
)
cursor = conn.cursor()

cursor.execute(
    "INSERT INTO knowledge_base (title, content, embedding) VALUES (?, ?, ?)",
    (
        "Introduction √† MariaDB",
        "MariaDB est un syst√®me...",
        str(embedding)  # Convertir liste en string JSON
    )
)
conn.commit()
```

### Recherche vectorielle

```sql
-- Recherche des documents les plus similaires
-- 1. G√©n√©rer l'embedding de la requ√™te (via API OpenAI)
-- Requ√™te utilisateur : "Comment optimiser les performances de ma base de donn√©es ?"
-- Embedding : [0.123, -0.456, 0.789, ...]

-- 2. Recherche dans MariaDB
SELECT
    doc_id,
    title,
    content,
    VEC_DISTANCE_COSINE(
        embedding,
        '[0.123, -0.456, 0.789, ..., 0.321]'  -- Embedding de la requ√™te
    ) AS similarity_score
FROM knowledge_base
ORDER BY similarity_score ASC  -- Plus petit score = plus similaire
LIMIT 10;

-- R√©sultats :
-- ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
-- ‚îÇ doc_id ‚îÇ title                       ‚îÇ score       ‚îÇ
-- ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
-- ‚îÇ 42     ‚îÇ Optimisation Buffer Pool    ‚îÇ 0.08        ‚îÇ
-- ‚îÇ 17     ‚îÇ Configuration InnoDB        ‚îÇ 0.12        ‚îÇ
-- ‚îÇ 89     ‚îÇ Tuning des index            ‚îÇ 0.15        ‚îÇ
-- ‚îÇ ...    ‚îÇ ...                         ‚îÇ ...         ‚îÇ
-- ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Avec filtres hybrides** :

```sql
-- Recherche vectorielle + filtres SQL traditionnels
SELECT
    doc_id,
    title,
    created_at,
    VEC_DISTANCE_COSINE(embedding, ?) AS score
FROM knowledge_base
WHERE
    created_at >= '2024-01-01'  -- Filtrage temporel
    AND title LIKE '%MariaDB%'   -- Filtrage textuel
ORDER BY score ASC
LIMIT 10;

-- Avantage : Combiner recherche s√©mantique + filtres pr√©cis
```

---

## Cas d'usage : RAG (Retrieval Augmented Generation)

**RAG** est le pattern dominant pour les applications IA avec LLM (ChatGPT, Claude, LLaMA).

### Architecture RAG compl√®te

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. INDEXATION (offline)                                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Documents ‚Üí Chunking ‚Üí Embeddings ‚Üí MariaDB Vector      ‚îÇ
‚îÇ  (corpus)    (512 tok)  (OpenAI)     (stockage)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. RETRIEVAL (runtime)                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Question ‚Üí Embedding ‚Üí Vector Search ‚Üí Top K docs       ‚îÇ
‚îÇ  user       (OpenAI)    (MariaDB)       (contexte)       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  "Comment configurer InnoDB ?"                           ‚îÇ
‚îÇ       ‚Üì                                                  ‚îÇ
‚îÇ  [0.12, -0.45, ...]  (embedding)                         ‚îÇ
‚îÇ       ‚Üì                                                  ‚îÇ
‚îÇ  SELECT ... VEC_DISTANCE_COSINE(...)                     ‚îÇ
‚îÇ       ‚Üì                                                  ‚îÇ
‚îÇ  [Doc1: "Configuration InnoDB Buffer Pool...",           ‚îÇ
‚îÇ   Doc2: "Optimiser innodb_log_file_size...",             ‚îÇ
‚îÇ   Doc3: "Tuning InnoDB pour SSD..."]                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. GENERATION (LLM)                                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Prompt = Question + Contexte (Top K) ‚Üí LLM ‚Üí R√©ponse    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Prompt:                                                 ‚îÇ
‚îÇ  """                                                     ‚îÇ
‚îÇ  Contexte:                                               ‚îÇ
‚îÇ  - Doc1: Configuration InnoDB Buffer Pool...             ‚îÇ
‚îÇ  - Doc2: Optimiser innodb_log_file_size...               ‚îÇ
‚îÇ  - Doc3: Tuning InnoDB pour SSD...                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Question: Comment configurer InnoDB ?                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  R√©ponds en te basant uniquement sur le contexte.        ‚îÇ
‚îÇ  """                                                     ‚îÇ
‚îÇ       ‚Üì                                                  ‚îÇ
‚îÇ  GPT-4 / Claude / LLaMA                                  ‚îÇ
‚îÇ       ‚Üì                                                  ‚îÇ
‚îÇ  "Pour configurer InnoDB optimalement, voici             ‚îÇ
‚îÇ   les param√®tres cl√©s : innodb_buffer_pool_size          ‚îÇ
‚îÇ   (70-80% RAM), innodb_log_file_size (1-2GB), ..."       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Impl√©mentation RAG compl√®te

```python
# app.py - Syst√®me RAG avec MariaDB Vector
import openai
import mariadb
from typing import List

class RAGSystem:
    def __init__(self):
        self.conn = mariadb.connect(
            host="localhost",
            user="root",
            password="password",
            database="rag_db"
        )
        self.cursor = self.conn.cursor()
        openai.api_key = "sk-..."

    def chunk_document(self, content: str, chunk_size: int = 512) -> List[str]:
        """D√©couper document en chunks de 512 tokens"""
        # Simplification : d√©coupe par paragraphes
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            if len(current_chunk) + len(para) < chunk_size * 4:  # ~4 chars/token
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def generate_embedding(self, text: str) -> List[float]:
        """G√©n√©rer embedding via OpenAI"""
        response = openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def index_document(self, title: str, content: str):
        """Indexer un document (chunking + embeddings)"""
        chunks = self.chunk_document(content)

        for i, chunk in enumerate(chunks):
            embedding = self.generate_embedding(chunk)

            self.cursor.execute(
                """
                INSERT INTO knowledge_base
                (title, content, chunk_index, embedding)
                VALUES (?, ?, ?, ?)
                """,
                (f"{title} (part {i+1})", chunk, i, str(embedding))
            )

        self.conn.commit()
        print(f"Indexed {len(chunks)} chunks for '{title}'")

    def retrieve(self, query: str, top_k: int = 5) -> List[dict]:
        """Retrieval : Recherche vectorielle"""
        # 1. G√©n√©rer embedding de la requ√™te
        query_embedding = self.generate_embedding(query)

        # 2. Recherche dans MariaDB
        self.cursor.execute(
            """
            SELECT
                doc_id,
                title,
                content,
                VEC_DISTANCE_COSINE(embedding, ?) AS score
            FROM knowledge_base
            ORDER BY score ASC
            LIMIT ?
            """,
            (str(query_embedding), top_k)
        )

        results = []
        for row in self.cursor:
            results.append({
                'doc_id': row[0],
                'title': row[1],
                'content': row[2],
                'score': row[3]
            })

        return results

    def generate(self, query: str, context: List[dict]) -> str:
        """Generation : LLM avec contexte"""
        # Construire le prompt avec contexte
        context_str = "\n\n".join([
            f"Document {i+1}: {doc['title']}\n{doc['content']}"
            for i, doc in enumerate(context)
        ])

        prompt = f"""Contexte (documentation technique) :
{context_str}

Question de l'utilisateur : {query}

Instructions :
- R√©ponds uniquement en te basant sur le contexte fourni
- Si l'information n'est pas dans le contexte, dis-le clairement
- Sois concis et technique

R√©ponse :"""

        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Tu es un assistant technique expert en bases de donn√©es."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # Basse temp√©rature pour r√©ponses factuelles
        )

        return response.choices[0].message.content

    def query(self, user_question: str) -> dict:
        """Pipeline RAG complet"""
        # 1. Retrieval
        relevant_docs = self.retrieve(user_question, top_k=5)

        # 2. Generation
        answer = self.generate(user_question, relevant_docs)

        return {
            'question': user_question,
            'answer': answer,
            'sources': relevant_docs
        }

# Utilisation
rag = RAGSystem()

# Indexation (une fois)
rag.index_document(
    "Configuration InnoDB",
    """Le Buffer Pool est le composant le plus critique d'InnoDB.
    Il doit √™tre configur√© √† 70-80% de la RAM disponible via
    innodb_buffer_pool_size. Pour un serveur de 64 GB, utilisez
    innodb_buffer_pool_size=48G..."""
)

# Requ√™te
result = rag.query("Comment dimensionner le Buffer Pool InnoDB ?")
print("Question:", result['question'])
print("R√©ponse:", result['answer'])
print("Sources utilis√©es:", len(result['sources']))
```

**Sch√©ma de la base** :

```sql
CREATE TABLE knowledge_base (
    doc_id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(500),
    content TEXT,
    chunk_index INT,           -- Position du chunk dans le document
    source_url VARCHAR(1000),  -- URL d'origine (optionnel)
    metadata JSON,             -- M√©tadonn√©es additionnelles
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    embedding VECTOR(1536),

    INDEX idx_embedding ON knowledge_base(embedding)
    USING HNSW
    WITH (M=16, ef_construction=200, metric='cosine')
);
```

---

## Cas d'usage : Recherche s√©mantique

### E-commerce : Recherche produits par description

```sql
-- Sc√©nario : Boutique en ligne avec 1 million de produits

-- Table produits
CREATE TABLE products (
    product_id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(500),
    description TEXT,
    category VARCHAR(100),
    price DECIMAL(10,2),

    -- Embedding de la description
    description_embedding VECTOR(1536),

    INDEX idx_category (category),
    INDEX idx_price (price),
    INDEX idx_embedding ON products(description_embedding)
    USING HNSW WITH (M=24, metric='cosine')
);

-- Recherche utilisateur : "chaussures de running confortables pour marathon"
-- (embedding g√©n√©r√© par l'application)

SELECT
    product_id,
    name,
    description,
    price,
    VEC_DISTANCE_COSINE(
        description_embedding,
        ?  -- Embedding de la requ√™te
    ) AS relevance_score
FROM products
WHERE
    category = 'Sports'  -- Filtrage pr√©-s√©lection
    AND price BETWEEN 50 AND 200
ORDER BY relevance_score ASC
LIMIT 20;

-- R√©sultats pertinents m√™me si mots diff√©rents :
-- ‚Ä¢ "Nike Air Zoom Pegasus - running shoes with cushioning..."
-- ‚Ä¢ "Adidas Ultraboost - marathon ready sneakers..."
-- ‚Ä¢ "Asics Gel-Kayano - comfortable long distance trainers..."
```

### Support client : FAQ intelligente

```sql
-- Table FAQ avec embeddings
CREATE TABLE faq (
    faq_id INT PRIMARY KEY AUTO_INCREMENT,
    question TEXT,
    answer TEXT,
    category VARCHAR(100),
    question_embedding VECTOR(1536),

    INDEX idx_embedding ON faq(question_embedding)
    USING HNSW WITH (metric='cosine')
);

-- Exemple de questions index√©es :
-- "Comment r√©initialiser mon mot de passe ?"
-- "Quels sont les d√©lais de livraison ?"
-- "Comment retourner un produit ?"

-- Requ√™te client : "J'ai oubli√© mon code d'acc√®s"
-- ‚Üí Trouve automatiquement "Comment r√©initialiser mon mot de passe ?"

SELECT
    question,
    answer,
    VEC_DISTANCE_COSINE(question_embedding, ?) AS similarity
FROM faq
ORDER BY similarity ASC
LIMIT 3;
```

---

## Cas d'usage : Syst√®me de recommandation

### Recommandation de contenu

```sql
-- Table articles avec embeddings de contenu
CREATE TABLE articles (
    article_id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(500),
    content TEXT,
    author VARCHAR(200),
    content_embedding VECTOR(1536),

    INDEX idx_embedding ON articles(content_embedding)
    USING HNSW
);

-- Recommandation bas√©e sur un article lu
-- Article ID 42 : "Introduction au Machine Learning"

SELECT
    a.article_id,
    a.title,
    VEC_DISTANCE_COSINE(
        a.content_embedding,
        (SELECT content_embedding FROM articles WHERE article_id = 42)
    ) AS similarity
FROM articles a
WHERE a.article_id != 42  -- Exclure l'article source
ORDER BY similarity ASC
LIMIT 10;

-- R√©sultats similaires :
-- ‚Ä¢ "Deep Learning pour d√©butants"
-- ‚Ä¢ "R√©seaux de neurones expliqu√©s"
-- ‚Ä¢ "IA et apprentissage supervis√©"
```

### D√©tection de duplications

```sql
-- D√©tecter articles dupliqu√©s ou tr√®s similaires
SELECT
    a1.article_id AS id1,
    a2.article_id AS id2,
    a1.title AS title1,
    a2.title AS title2,
    VEC_DISTANCE_COSINE(a1.content_embedding, a2.content_embedding) AS similarity
FROM articles a1
JOIN articles a2 ON a1.article_id < a2.article_id
WHERE VEC_DISTANCE_COSINE(a1.content_embedding, a2.content_embedding) < 0.05  -- Seuil
ORDER BY similarity ASC
LIMIT 100;

-- Trouve les paires d'articles quasi-identiques
-- (plagiat, r√©utilisation de contenu, etc.)
```

---

## Performance et optimisations

### Dimensionnement de l'index

**Taille de l'index HNSW** :

```
Taille index HNSW ‚âà N √ó d √ó 4 bytes √ó (M + 1)

O√π :
‚Ä¢ N = nombre de vecteurs
‚Ä¢ d = dimensions (ex: 1536)
‚Ä¢ M = connexions par n≈ìud (d√©faut 16)
‚Ä¢ 4 bytes = float32

Exemple :
‚Ä¢ 1 million de vecteurs
‚Ä¢ 1536 dimensions
‚Ä¢ M = 16
‚Ä¢ Taille = 1M √ó 1536 √ó 4 √ó 17 = 104 GB

Recommandations :
‚Ä¢ RAM Buffer Pool > taille index pour performance optimale
‚Ä¢ Si index > RAM : Performance d√©grad√©e (I/O disque)
```

**Optimisation Buffer Pool** :

```ini
# my.cnf
# Pour 1M vecteurs (104 GB index)
innodb_buffer_pool_size = 120G  # Index + working set

# Multiples instances pour concurrence
innodb_buffer_pool_instances = 16
```

### Param√®tres d'index selon cas d'usage

```sql
-- CAS 1 : Haute pr√©cision (recherche scientifique)
CREATE INDEX idx_high_precision ON documents(embedding)
USING HNSW WITH (
    M = 48,                -- Plus de connexions
    ef_construction = 500, -- Construction pr√©cise
    metric = 'euclidean'
);
-- Avantage : Pr√©cision ~99.5%
-- Inconv√©nient : Index 3√ó plus gros, construction 5√ó plus lente

-- CAS 2 : Haute performance (e-commerce)
CREATE INDEX idx_high_performance ON products(embedding)
USING HNSW WITH (
    M = 12,                -- Moins de connexions
    ef_construction = 100, -- Construction rapide
    metric = 'cosine'
);
-- Avantage : Recherche 2√ó plus rapide, index compact
-- Inconv√©nient : Pr√©cision ~95%

-- CAS 3 : √âquilibr√© (recommand√©)
CREATE INDEX idx_balanced ON knowledge_base(embedding)
USING HNSW WITH (
    M = 16,                -- D√©faut
    ef_construction = 200, -- D√©faut
    metric = 'cosine'
);
-- Bon compromis pour la plupart des cas
```

### Optimisation des requ√™tes

```sql
-- ‚úÖ BON : Filtrage avant recherche vectorielle
SELECT ...
FROM products
WHERE
    category = 'Electronics'  -- Index B-Tree, filtrage rapide
    AND price > 100
    AND VEC_DISTANCE_COSINE(embedding, ?) < 0.3  -- Recherche sur subset
ORDER BY VEC_DISTANCE_COSINE(embedding, ?) ASC
LIMIT 10;

-- ‚ùå √âVITER : Recherche vectorielle sur toute la table si possible
SELECT ...
FROM products  -- 10M lignes
WHERE VEC_DISTANCE_COSINE(embedding, ?) < 0.1  -- Scan 10M vecteurs
LIMIT 10;

-- MIEUX : Partitionnement
CREATE TABLE products_electronics (...) ENGINE=InnoDB;
CREATE TABLE products_clothing (...) ENGINE=InnoDB;
-- Recherche vectorielle sur partition pertinente uniquement
```

### Parall√©lisation des recherches

```sql
-- MariaDB Vector utilise automatiquement le parall√©lisme InnoDB

-- Configuration
SET GLOBAL innodb_read_io_threads = 16;
SET GLOBAL innodb_write_io_threads = 16;

-- Requ√™tes vectorielles b√©n√©ficient du thread pool
-- Pas de configuration sp√©ciale n√©cessaire
```

### Optimisations SIMD

MariaDB Vector utilise automatiquement les instructions SIMD du CPU :

```
CPU Support :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Intel/AMD :                                        ‚îÇ
‚îÇ  ‚Ä¢ AVX2 (256-bit) : 2√ó plus rapide que scalaire    ‚îÇ
‚îÇ  ‚Ä¢ AVX-512 (512-bit) : 4√ó plus rapide              ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ ARM :                                              ‚îÇ
‚îÇ  ‚Ä¢ NEON (128-bit) : 2√ó plus rapide                 ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ Apple Silicon :                                    ‚îÇ
‚îÇ  ‚Ä¢ M1/M2/M3 NEON : Optimis√©                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

V√©rifier support :
shell> lscpu | grep -E "avx|sse"
Flags: ... avx avx2 avx512f ...

MariaDB d√©tecte automatiquement et utilise les meilleures instructions
```

---

## Monitoring et diagnostics

### M√©triques de performance

```sql
-- Statistiques index HNSW
SELECT
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    INDEX_LENGTH / 1024 / 1024 / 1024 AS size_gb
FROM information_schema.STATISTICS
WHERE INDEX_TYPE = 'HNSW';

-- Performance des requ√™tes vectorielles
SHOW STATUS LIKE 'vec_%';
-- +---------------------------+----------+
-- | Variable_name             | Value    |
-- +---------------------------+----------+
-- | vec_distance_calls        | 1000000  |
-- | vec_distance_avg_time_ms  | 0.05     |
-- | vec_hnsw_navigations      | 850000   |
-- +---------------------------+----------+

-- Analyser une requ√™te
EXPLAIN
SELECT ...
FROM knowledge_base
ORDER BY VEC_DISTANCE_COSINE(embedding, '[...]') ASC
LIMIT 10;
-- V√©rifie que l'index HNSW est utilis√©
```

### Benchmarking

```sql
-- Test de performance
SET @query_embedding = '[0.1, -0.2, 0.3, ..., 0.5]';

-- 1000 recherches
SELECT BENCHMARK(1000, (
    SELECT doc_id
    FROM knowledge_base
    ORDER BY VEC_DISTANCE_COSINE(embedding, @query_embedding) ASC
    LIMIT 10
));

-- R√©sultat typique :
-- 1000 queries : 2.5 secondes
-- ‚Üí 2.5 ms par recherche (400 req/sec)
```

---

## Limitations et consid√©rations

### Limitations techniques

| Limitation | D√©tails | Workaround |
|------------|---------|------------|
| **Dimensions max** | 16384 | Suffisant (mod√®les actuels ‚â§ 4096) |
| **Pr√©cision HNSW** | ~95-99% vs exact | Acceptable pour la plupart des cas |
| **Taille index** | Peut √™tre massive | Dimensionner RAM en cons√©quence |
| **UPDATE lent** | Reconstruction partielle index | Batch updates |
| **Pas de partitionnement natif** | Index par partition | Cr√©er tables s√©par√©es |

### Cas o√π Vector n'est PAS appropri√©

```sql
-- ‚ùå Mauvais cas 1 : Recherche exacte
SELECT * FROM products WHERE name = 'iPhone 15';
-- Utilisez index B-Tree classique

-- ‚ùå Mauvais cas 2 : Petites tables (< 10 000 lignes)
-- Overhead index HNSW non justifi√©
-- Scan lin√©aire plus rapide

-- ‚ùå Mauvais cas 3 : Embeddings non pertinents
-- Si embeddings de mauvaise qualit√© ‚Üí R√©sultats al√©atoires
-- Qualit√© embedding > Qualit√© index

-- ‚ùå Mauvais cas 4 : Temps r√©el critique (< 1ms)
-- Latence recherche vectorielle : 5-50 ms
-- Utilisez cache applicatif si n√©cessaire
```

### Co√ªts embeddings

```
Co√ªts API OpenAI (text-embedding-3-small) :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prix : $0.02 / 1M tokens                           ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ Exemples :                                         ‚îÇ
‚îÇ ‚Ä¢ 1 document (500 tokens) : $0.00001               ‚îÇ
‚îÇ ‚Ä¢ 100 000 documents : $1                           ‚îÇ
‚îÇ ‚Ä¢ 1 million de documents : $10                     ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ Alternatives :                                     ‚îÇ
‚îÇ ‚Ä¢ Mod√®les open-source (HuggingFace) : Gratuit      ‚îÇ
‚îÇ ‚Ä¢ Self-hosted (LLaMA) : Co√ªt infra uniquement      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Comparaison avec autres solutions

| Solution | Avantages | Inconv√©nients | Usage |
|----------|-----------|---------------|-------|
| **MariaDB Vector** | ‚Ä¢ Int√©gr√© SQL<br>‚Ä¢ ACID transactions<br>‚Ä¢ Filtres hybrides | ‚Ä¢ Moins mature que sp√©cialis√©s | Donn√©es SQL + Vector |
| **Pinecone** | ‚Ä¢ Sp√©cialis√©<br>‚Ä¢ Tr√®s rapide<br>‚Ä¢ Managed | ‚Ä¢ Co√ªt √©lev√©<br>‚Ä¢ Vendor lock-in | Pure vector search |
| **Weaviate** | ‚Ä¢ Open-source<br>‚Ä¢ Performant | ‚Ä¢ Complexit√© setup<br>‚Ä¢ Pas de SQL | Microservice vector |
| **pgvector** | ‚Ä¢ PostgreSQL<br>‚Ä¢ Mature | ‚Ä¢ Performance limit√©e grande √©chelle | PostgreSQL users |
| **Elasticsearch** | ‚Ä¢ Mature<br>‚Ä¢ Full ecosystem | ‚Ä¢ Complexe<br>‚Ä¢ Ressources++ | Search engine |
| **Milvus** | ‚Ä¢ Tr√®s performant<br>‚Ä¢ Scalable | ‚Ä¢ Complexit√©<br>‚Ä¢ Pas de SQL | Grandes √©chelles |

üí° **Recommandation** : MariaDB Vector si donn√©es d√©j√† dans MariaDB + besoin de requ√™tes hybrides SQL+Vector.

---

## ‚úÖ Points cl√©s √† retenir

1. **Embeddings** : Repr√©sentation vectorielle de contenu (texte, image, audio) pour capture de signification s√©mantique.

2. **HNSW** : Index state-of-the-art, recherche en O(log N) avec pr√©cision ~99% (vs scan lin√©aire O(N)).

3. **3 m√©triques** : Euclidienne (distance g√©om√©trique), Cosine (NLP recommand√©), Inner Product (rapide).

4. **RAG pattern** : Retrieval (Vector Search) + Augmentation (Contexte) + Generation (LLM) = Applications IA modernes.

5. **Configuration** : M=16, ef_construction=200, metric='cosine' bon pour la plupart des cas.

6. **Dimensions** : Supporte 1-16384 dimensions (OpenAI=1536, LLaMA=4096).

7. **Requ√™tes hybrides** : Combiner filtres SQL traditionnels + recherche vectorielle = puissance unique.

8. **Performance** : 5-50 ms par recherche, 400-1000 req/sec sur mat√©riel moderne.

9. **Taille index** : ~N √ó d √ó 4 √ó (M+1) bytes - Dimensionner Buffer Pool en cons√©quence.

10. **SIMD** : Utilise automatiquement AVX2/AVX-512/NEON pour calculs vectoriels optimis√©s.

---

## üîó Ressources et r√©f√©rences

### Documentation officielle MariaDB

- [üìñ Vector Data Type](https://mariadb.com/kb/en/vector/)
- [üìñ HNSW Index](https://mariadb.com/kb/en/hnsw-index/)
- [üìñ Vector Functions](https://mariadb.com/kb/en/vector-functions/)
- [üìñ Vector Performance Tuning](https://mariadb.com/kb/en/vector-performance/)

### Articles et guides

- [MariaDB 11.8: Native Vector Search](https://mariadb.org/vector-search-11-8/)
- [Building RAG Applications with MariaDB](https://mariadb.com/resources/blog/rag-mariadb/)
- [Vector Search Benchmarks](https://mariadb.com/kb/en/vector-benchmarks/)

### Mod√®les d'embeddings

- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [HuggingFace Sentence Transformers](https://huggingface.co/sentence-transformers)
- [Cohere Embeddings](https://cohere.com/embed)

### Algorithmes et recherche

- [HNSW Paper (Malkov & Yashunin)](https://arxiv.org/abs/1603.09320)
- [Vector Search Explained](https://www.pinecone.io/learn/vector-search/)
- [ANN Benchmarks](http://ann-benchmarks.com/)

---

## ‚û°Ô∏è Prochaines sections

**[7.8 Comparaison et choix du moteur appropri√©](/07-moteurs-de-stockage/08-comparaison-choix-moteur.md)** : Tableau comparatif complet de tous les moteurs, crit√®res de d√©cision, arbres de d√©cision pour choisir le bon moteur.

Puis nous terminerons avec :
- **7.9** : Conversion entre moteurs (strat√©gies, exemples, migration)

---

**üìå M√©mo Architecte** : "Vector = recherche s√©mantique. Pour RAG, NLP, recommandations. HNSW index avec M=16, metric=cosine. Dimensionner Buffer Pool = taille index. Qualit√© embeddings > qualit√© technique."

**üéØ Cas d'usage typique** : Application IA avec ChatGPT ‚Üí G√©n√©rer embeddings (OpenAI) ‚Üí Stocker dans MariaDB Vector ‚Üí Retrieval pour RAG ‚Üí Contexte pour LLM. Simple, efficace, production-ready.

**üÜï Innovation 11.8** : Premier SGBD relationnel majeur avec recherche vectorielle native. Pas besoin de base s√©par√©e (Pinecone, Weaviate). SQL + Vector = One database for everything.

‚è≠Ô∏è [Comparaison et choix du moteur appropri√©](/07-moteurs-de-stockage/08-comparaison-choix-moteur.md)
