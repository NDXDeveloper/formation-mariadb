üîù Retour au [Sommaire](/SOMMAIRE.md)

# 7.9 Conversion entre moteurs (ALTER TABLE ENGINE)

> **Niveau** : Avanc√©
> **Dur√©e estim√©e** : 3-4 heures
> **Pr√©requis** : Sections 7.1-7.8 (tous les moteurs et crit√®res de choix)

> **Public cible** : DBA, Ing√©nieurs DevOps, Architectes de bases de donn√©es

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Ma√Ætriser les diff√©rentes m√©thodes de conversion entre moteurs
- Choisir la strat√©gie optimale selon la taille et la criticit√©
- Utiliser ALTER TABLE ENGINE et ses alternatives
- Impl√©menter des migrations zero-downtime avec gh-ost ou pt-online-schema-change
- G√©rer les cas particuliers (Foreign Keys, Full-Text, partitions)
- V√©rifier l'int√©grit√© apr√®s migration
- Planifier et ex√©cuter des rollbacks
- Automatiser les conversions avec scripts et proc√©dures

---

## Introduction

La **conversion entre moteurs** est une op√©ration courante mais critique qui n√©cessite une planification minutieuse. Les enjeux sont importants :

- **Downtime** : De quelques secondes √† plusieurs heures selon la m√©thode
- **Int√©grit√© des donn√©es** : Risque de perte si mal ex√©cut√©e
- **Performance** : Impact sur la production pendant la conversion
- **Espace disque** : Besoin temporaire de 2-3√ó la taille de la table

> "Une migration de moteur de stockage est une op√©ration √† haut risque. Une planification de 80% et une ex√©cution de 20% garantissent le succ√®s."

### Quand convertir un moteur ?

```
Raisons de conversion :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Migration legacy ‚Üí moderne                         ‚îÇ
‚îÇ    MyISAM ‚Üí InnoDB (URGENT en 2025)                   ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ 2. Optimisation performance                           ‚îÇ
‚îÇ    InnoDB ‚Üí ColumnStore (analytics massif)            ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ 3. R√©duction co√ªts                                    ‚îÇ
‚îÇ    InnoDB ‚Üí S3 (archivage donn√©es froides)            ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ 4. Nouveaux besoins fonctionnels                      ‚îÇ
‚îÇ    InnoDB ‚Üí Vector/HNSW (ajout recherche IA)          ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ 5. Correction erreur architecturale                   ‚îÇ
‚îÇ    ColumnStore ‚Üí InnoDB (mal dimensionn√©)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## M√©thodes de conversion

### Vue d'ensemble des m√©thodes

| M√©thode | Downtime | Complexit√© | Taille max | Rollback | Usage |
|---------|----------|------------|------------|----------|-------|
| **ALTER TABLE ENGINE** | ‚ö†Ô∏è Total | Faible | < 10 GB | Difficile | Tables petites, fen√™tre maintenance |
| **CREATE + INSERT SELECT** | ‚ö†Ô∏è Total | Faible | < 50 GB | Facile | Tables moyennes, plus de contr√¥le |
| **CREATE + RENAME** | ‚ö° Minimal | Moyenne | < 500 GB | Facile | Tables moyennes-grandes |
| **gh-ost** | ‚úÖ Aucun | Moyenne | Illimit√© | Facile | Tables critiques, production |
| **pt-online-schema-change** | ‚úÖ Aucun | Moyenne | Illimit√© | Facile | Alternative √† gh-ost |
| **Partition swap** | ‚ö° Minimal | √âlev√©e | Illimit√© | Facile | Tables partitionn√©es |

### M√©thode 1 : ALTER TABLE ENGINE (simple mais bloquant)

**Principe** : Reconstruction compl√®te de la table en place.

```sql
-- Conversion simple
ALTER TABLE orders ENGINE=InnoDB;

-- MariaDB :
-- 1. Lock table en EXCLUSIVE (lecture/√©criture bloqu√©es)
-- 2. Cr√©e table temporaire avec nouveau moteur
-- 3. Copie ligne par ligne
-- 4. Remplace table originale
-- 5. D√©verrouille
```

**Avantages** :
- ‚úÖ Simple (une seule commande)
- ‚úÖ Pas de script complexe
- ‚úÖ Gestion automatique des index

**Inconv√©nients** :
- ‚ùå Table bloqu√©e pendant toute la dur√©e
- ‚ùå Pas de progression visible
- ‚ùå Rollback difficile (CTRL+C = corruption possible)
- ‚ùå Espace disque : 2√ó la taille de la table

**Dur√©e estim√©e** :

```
Exemples de dur√©e ALTER TABLE ENGINE :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Taille table  ‚îÇ Hardware         ‚îÇ Dur√©e estim√©e      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 100 MB        ‚îÇ HDD              ‚îÇ 10-30 sec          ‚îÇ
‚îÇ 1 GB          ‚îÇ SSD SATA         ‚îÇ 1-3 min            ‚îÇ
‚îÇ 10 GB         ‚îÇ SSD NVMe         ‚îÇ 5-15 min           ‚îÇ
‚îÇ 100 GB        ‚îÇ SSD NVMe         ‚îÇ 30 min - 2 heures  ‚îÇ
‚îÇ 1 TB          ‚îÇ SSD NVMe RAID    ‚îÇ 5-20 heures        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Formule approximative :
Dur√©e (min) = Taille (GB) √ó (30 / Throughput MB/s)
Exemple : 100 GB sur SSD 500 MB/s ‚Üí 100 √ó (30/500) = 6 min
```

**Quand utiliser ALTER TABLE ENGINE** :
- ‚úÖ Tables < 10 GB
- ‚úÖ Fen√™tre de maintenance disponible
- ‚úÖ Environnement de d√©veloppement/test
- ‚ùå Production sans fen√™tre maintenance
- ‚ùå Tables > 50 GB

### M√©thode 2 : CREATE + INSERT SELECT (contr√¥le total)

**Principe** : Cr√©er nouvelle table, copier, puis remplacer.

```sql
-- √âtape 1 : Cr√©er nouvelle table avec moteur cible
CREATE TABLE orders_new LIKE orders;
ALTER TABLE orders_new ENGINE=InnoDB;

-- √âtape 2 : Copier les donn√©es
INSERT INTO orders_new SELECT * FROM orders;

-- √âtape 3 : V√©rifier
SELECT COUNT(*) FROM orders;      -- 10000000
SELECT COUNT(*) FROM orders_new;  -- 10000000

-- √âtape 4 : Swap atomique (downtime minimal)
RENAME TABLE
    orders TO orders_old,
    orders_new TO orders;

-- √âtape 5 : Valider puis supprimer ancienne table
-- ... tests applicatifs ...
DROP TABLE orders_old;
```

**Avantages** :
- ‚úÖ Contr√¥le total du processus
- ‚úÖ Progression visible (SELECT COUNT(*))
- ‚úÖ Rollback facile (garder ancienne table)
- ‚úÖ RENAME atomique (downtime < 1 seconde)
- ‚úÖ Possibilit√© de copie par batch

**Inconv√©nients** :
- ‚ö†Ô∏è Espace disque : 2√ó la taille
- ‚ö†Ô∏è Copie ne capture pas les modifications pendant l'INSERT
- ‚ö†Ô∏è N√©cessite script pour g√©rer les d√©pendances

**Copie par batch** :

```sql
-- Pour tr√®s grosses tables : Copie par batch
SET @batch_size = 10000;
SET @offset = 0;
SET @total = (SELECT COUNT(*) FROM orders);

WHILE @offset < @total DO
    INSERT INTO orders_new
    SELECT * FROM orders
    LIMIT @offset, @batch_size;

    SET @offset = @offset + @batch_size;

    -- Afficher progression
    SELECT CONCAT(
        'Progress: ',
        ROUND((@offset / @total) * 100, 2),
        '%'
    ) AS status;

    -- Petit d√©lai pour ne pas saturer
    DO SLEEP(0.1);
END WHILE;
```

### M√©thode 3 : gh-ost (zero-downtime, recommand√© production)

**gh-ost** (GitHub Online Schema Change) est l'outil state-of-the-art pour migrations zero-downtime.

**Architecture** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Table originale (orders)                ‚îÇ
‚îÇ  ‚Ä¢ Re√ßoit les √©critures de l'application                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
                    Binary Log
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      gh-ost                              ‚îÇ
‚îÇ  1. Cr√©e table fant√¥me (_orders_gho)                     ‚îÇ
‚îÇ  2. Copie donn√©es batch par batch                        ‚îÇ
‚îÇ  3. Applique changements via binlog                      ‚îÇ
‚îÇ  4. Swap atomique √† la fin                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Table fant√¥me (_orders_gho)                 ‚îÇ
‚îÇ  ‚Ä¢ Nouveau moteur (InnoDB)                               ‚îÇ
‚îÇ  ‚Ä¢ Synchronis√©e en temps r√©el                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Installation** :

```bash
# T√©l√©charger gh-ost
wget https://github.com/github/gh-ost/releases/download/v1.1.6/gh-ost-binary-linux-amd64-20231207144046.tar.gz
tar -xzf gh-ost-binary-linux-amd64-20231207144046.tar.gz
sudo mv gh-ost /usr/local/bin/
chmod +x /usr/local/bin/gh-ost

# V√©rifier
gh-ost --version
```

**Utilisation** :

```bash
# Conversion MyISAM ‚Üí InnoDB avec gh-ost
gh-ost \
  --host=localhost \
  --port=3306 \
  --user=root \
  --password=secret \
  --database=mydb \
  --table=orders \
  --alter="ENGINE=InnoDB" \
  --execute \
  --allow-on-master \
  --exact-rowcount \
  --concurrent-rowcount \
  --default-retries=120 \
  --cut-over=default \
  --chunk-size=1000 \
  --max-load="Threads_running=25,Threads_connected=100" \
  --critical-load="Threads_running=50,Threads_connected=200" \
  --nice-ratio=0.5

# Param√®tres cl√©s :
# --alter : Changement √† effectuer
# --chunk-size : Taille des batch (1000 lignes)
# --max-load : Seuil de throttling (ralentit si d√©pass√©)
# --critical-load : Seuil critique (pause si d√©pass√©)
# --nice-ratio : Temps d'attente entre batch (0.5 = 50%)
```

**Commandes pendant l'ex√©cution** :

```bash
# Surveiller la progression
# gh-ost cr√©e un fichier de contr√¥le
echo "status" > /tmp/gh-ost.orders.sock

# Throttler (ralentir)
echo "throttle" > /tmp/gh-ost.orders.sock

# Reprendre
echo "no-throttle" > /tmp/gh-ost.orders.sock

# Annuler proprement
echo "panic" > /tmp/gh-ost.orders.sock
```

**Avantages** :
- ‚úÖ Zero-downtime (< 1 seconde de lock final)
- ‚úÖ Pausable et reprisable
- ‚úÖ Throttling automatique
- ‚úÖ Rollback propre (DROP _gho table)
- ‚úÖ Progression visible en temps r√©el

**Inconv√©nients** :
- ‚ö†Ô∏è N√©cessite binlog (row-based)
- ‚ö†Ô∏è Espace disque : 2√ó la taille
- ‚ö†Ô∏è Surcharge CPU/I/O pendant la migration
- ‚ö†Ô∏è Configuration plus complexe

### M√©thode 4 : pt-online-schema-change (Percona Toolkit)

Alternative √† gh-ost, tr√®s similaire.

**Installation** :

```bash
# Debian/Ubuntu
sudo apt-get install percona-toolkit

# RHEL/CentOS
sudo yum install percona-toolkit

# V√©rifier
pt-online-schema-change --version
```

**Utilisation** :

```bash
pt-online-schema-change \
  --alter="ENGINE=InnoDB" \
  --execute \
  D=mydb,t=orders \
  --host=localhost \
  --user=root \
  --password=secret \
  --chunk-size=1000 \
  --chunk-size-limit=4.0 \
  --max-load="Threads_running:25" \
  --critical-load="Threads_running:50" \
  --progress=percentage,5 \
  --no-drop-old-table
```

**Diff√©rences gh-ost vs pt-online-schema-change** :

| Aspect | gh-ost | pt-osc |
|--------|--------|--------|
| M√©thode | Binlog streaming | Triggers |
| Overhead | Moyen (binlog) | √âlev√© (3 triggers) |
| Maintenance | Actif (GitHub) | Mature (Percona) |
| Flexibilit√© | Plus moderne | Plus √©prouv√© |
| Use case | Pr√©f√©r√© pour nouvelles migrations | Pr√©f√©r√© si probl√®me binlog |

**Recommandation** : gh-ost pour nouveaux projets, pt-osc si d√©j√† en place.

---

## Conversions courantes : Proc√©dures d√©taill√©es

### Conversion 1 : MyISAM ‚Üí InnoDB (migration legacy URGENTE)

**Contexte** : MyISAM est d√©pr√©ci√© et dangereux. Migration vers InnoDB imp√©rative.

#### √âtape 1 : Audit pr√©-migration

```sql
-- Identifier toutes les tables MyISAM
SELECT
    TABLE_SCHEMA,
    TABLE_NAME,
    ENGINE,
    TABLE_ROWS,
    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS size_mb,
    ROUND(DATA_FREE / 1024 / 1024, 2) AS fragmentation_mb
FROM information_schema.TABLES
WHERE ENGINE = 'MyISAM'
  AND TABLE_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema')
ORDER BY (DATA_LENGTH + INDEX_LENGTH) DESC;

-- R√©sultat exemple :
-- +-------------+----------------+--------+------------+---------+-----------------+
-- | TABLE_SCHEMA| TABLE_NAME     | ENGINE | TABLE_ROWS | size_mb | fragmentation_mb|
-- +-------------+----------------+--------+------------+---------+-----------------+
-- | mydb        | huge_table     | MyISAM | 50000000   | 12000   | 500             |
-- | mydb        | medium_table   | MyISAM | 1000000    | 500     | 20              |
-- | mydb        | small_table    | MyISAM | 10000      | 5       | 0               |
-- +-------------+----------------+--------+------------+---------+-----------------+
```

#### √âtape 2 : V√©rifier l'espace disque

```bash
# Espace disque n√©cessaire : 2√ó taille tables + 30% marge
# Exemple : 12.5 GB tables ‚Üí 32.5 GB espace libre requis

df -h /var/lib/mysql
# Filesystem      Size  Used Avail Use% Mounted on
# /dev/sda1       100G   60G   40G  60% /

# Si espace insuffisant ‚Üí Nettoyer ou ajouter disque
```

#### √âtape 3 : Backup complet

```bash
# Backup avant toute migration
mysqldump --single-transaction --routines --triggers \
    --master-data=2 --flush-logs \
    mydb > /backup/mydb_before_migration.sql

# V√©rifier le backup
ls -lh /backup/mydb_before_migration.sql
# -rw-r--r-- 1 root root 15G Dec 12 10:00 mydb_before_migration.sql

# Tester la restauration sur serveur secondaire
mysql -h test-server mydb_test < /backup/mydb_before_migration.sql
```

#### √âtape 4 : Migration par taille de table

**Tables < 1 GB : ALTER TABLE direct**

```sql
-- Fen√™tre de maintenance acceptable
ALTER TABLE small_table ENGINE=InnoDB;
-- Dur√©e : 5-30 secondes

-- V√©rifier
SHOW CREATE TABLE small_table\G
-- ENGINE=InnoDB
```

**Tables 1-100 GB : CREATE + RENAME**

```sql
-- Cr√©er nouvelle table InnoDB
CREATE TABLE medium_table_new LIKE medium_table;
ALTER TABLE medium_table_new ENGINE=InnoDB;

-- Copier donn√©es
INSERT INTO medium_table_new SELECT * FROM medium_table;
-- Dur√©e : 5-30 minutes selon hardware

-- V√©rifier int√©grit√©
SELECT COUNT(*),
       SUM(CRC32(CONCAT_WS(',', col1, col2, col3))) AS checksum
FROM medium_table;
-- Comparer avec medium_table_new

-- Swap atomique (< 1 sec downtime)
RENAME TABLE
    medium_table TO medium_table_old_myisam,
    medium_table_new TO medium_table;

-- Tester application
-- ... tests ...

-- Supprimer ancienne table
DROP TABLE medium_table_old_myisam;
```

**Tables > 100 GB : gh-ost**

```bash
# Migration zero-downtime
gh-ost \
  --host=localhost \
  --user=root \
  --password=secret \
  --database=mydb \
  --table=huge_table \
  --alter="ENGINE=InnoDB" \
  --execute \
  --allow-on-master \
  --exact-rowcount \
  --concurrent-rowcount \
  --chunk-size=1000 \
  --max-load="Threads_running=25" \
  --critical-load="Threads_running=50" \
  --nice-ratio=0.5 \
  --panic-flag-file=/tmp/gh-ost.panic \
  --postpone-cut-over-flag-file=/tmp/gh-ost.postpone

# Dur√©e estim√©e : 2-10 heures selon taille et charge
# Progression visible en temps r√©el
```

#### √âtape 5 : Cas particuliers MyISAM

**Full-Text indexes** :

```sql
-- MyISAM Full-Text ‚Üí InnoDB Full-Text (support√© depuis MariaDB 10.0)
CREATE TABLE articles_innodb LIKE articles_myisam;
ALTER TABLE articles_innodb ENGINE=InnoDB;

-- Copier
INSERT INTO articles_innodb SELECT * FROM articles_myisam;

-- V√©rifier Full-Text fonctionne
SELECT * FROM articles_innodb
WHERE MATCH(title, content) AGAINST('database');
-- Doit retourner r√©sultats
```

**AUTO_INCREMENT** :

```sql
-- Pr√©server AUTO_INCREMENT value
SELECT AUTO_INCREMENT
FROM information_schema.TABLES
WHERE TABLE_SCHEMA = 'mydb' AND TABLE_NAME = 'users_myisam';
-- 1234567

CREATE TABLE users_innodb LIKE users_myisam;
ALTER TABLE users_innodb ENGINE=InnoDB AUTO_INCREMENT=1234567;

INSERT INTO users_innodb SELECT * FROM users_myisam;
```

#### √âtape 6 : Post-migration

```sql
-- Analyser les nouvelles tables InnoDB
ANALYZE TABLE small_table, medium_table, huge_table;

-- V√©rifier les statistiques
SHOW TABLE STATUS LIKE 'huge_table'\G

-- Optimizer les tables (d√©fragmentation)
OPTIMIZE TABLE huge_table;

-- Surveiller les performances
SHOW ENGINE INNODB STATUS\G
```

### Conversion 2 : InnoDB ‚Üí ColumnStore (analytics)

**Contexte** : Table analytics avec requ√™tes d'agr√©gation lentes sur InnoDB.

#### Proc√©dure

```sql
-- Table InnoDB existante (analytics)
-- sales_fact : 500 millions de lignes, 200 GB

-- √âtape 1 : Cr√©er table ColumnStore
CREATE TABLE sales_fact_cs (
    sale_date DATE,
    customer_id INT,
    product_id INT,
    region VARCHAR(50),
    revenue DECIMAL(10,2),
    quantity INT
) ENGINE=ColumnStore;

-- √âtape 2 : Charger les donn√©es
-- Option A : INSERT SELECT (pour tables moyennes < 50 GB)
INSERT INTO sales_fact_cs SELECT * FROM sales_fact;

-- Option B : cpimport (pour tables massives > 50 GB)
-- 1. Exporter depuis InnoDB
SELECT * FROM sales_fact
INTO OUTFILE '/tmp/sales_fact.csv'
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n';

-- 2. Importer dans ColumnStore
cpimport mydb sales_fact_cs /tmp/sales_fact.csv -s ',' -E '"'
-- Jusqu'√† 10M lignes/sec

-- √âtape 3 : V√©rifier
SELECT COUNT(*) FROM sales_fact;     -- 500000000
SELECT COUNT(*) FROM sales_fact_cs;  -- 500000000

-- √âtape 4 : Tester performance
-- Avant (InnoDB)
SELECT region, SUM(revenue) FROM sales_fact GROUP BY region;
-- 45 secondes

-- Apr√®s (ColumnStore)
SELECT region, SUM(revenue) FROM sales_fact_cs GROUP BY region;
-- 2 secondes (22√ó plus rapide !)

-- √âtape 5 : Swap (si validation OK)
RENAME TABLE
    sales_fact TO sales_fact_innodb_old,
    sales_fact_cs TO sales_fact;

-- √âtape 6 : Cleanup apr√®s validation
DROP TABLE sales_fact_innodb_old;
```

### Conversion 3 : InnoDB ‚Üí S3 (archivage)

**Contexte** : Archives historiques rarement consult√©es.

#### Proc√©dure

```sql
-- Table InnoDB : orders_2020 (50 millions lignes, 20 GB)

-- √âtape 1 : Cr√©er table Aria interm√©diaire
CREATE TABLE orders_2020_aria ENGINE=Aria TRANSACTIONAL=0
SELECT * FROM orders_2020;

-- √âtape 2 : Convertir en S3
ALTER TABLE orders_2020_aria ENGINE=S3;
-- Upload vers s3://my-bucket/mydb/orders_2020_aria/
-- Dur√©e : 5-15 minutes selon connexion S3

-- √âtape 3 : V√©rifier accessibilit√©
SELECT COUNT(*) FROM orders_2020_aria;  -- 50000000
SELECT * FROM orders_2020_aria LIMIT 10;  -- Fonctionne

-- √âtape 4 : Renommer
RENAME TABLE
    orders_2020 TO orders_2020_innodb_backup,
    orders_2020_aria TO orders_2020;

-- √âtape 5 : Tester en production (quelques jours)

-- √âtape 6 : Supprimer backup InnoDB
DROP TABLE orders_2020_innodb_backup;

-- √âconomie : 20 GB SSD ‚Üí 4 GB S3 compress√©
-- Co√ªt : $2/mois ‚Üí $0.10/mois (√©conomie 95%)
```

### Conversion 4 : InnoDB ‚Üí Vector/HNSW (ajout IA)

**Contexte** : Ajouter recherche s√©mantique sur table existante.

#### Proc√©dure

```sql
-- Table InnoDB existante
CREATE TABLE documents (
    doc_id INT PRIMARY KEY,
    title VARCHAR(500),
    content TEXT
) ENGINE=InnoDB;

-- √âtape 1 : Ajouter colonne VECTOR
ALTER TABLE documents
ADD COLUMN embedding VECTOR(1536);
-- Rapide (ajout colonne vide)

-- √âtape 2 : G√©n√©rer embeddings (via application)
-- Python/Node.js script pour appeler OpenAI API et UPDATE

-- Pseudo-code Python :
-- for doc in documents:
--     embedding = openai.embed(doc.content)
--     UPDATE documents SET embedding = embedding WHERE doc_id = doc.id

-- √âtape 3 : Cr√©er index HNSW
CREATE INDEX idx_embedding ON documents(embedding)
USING HNSW WITH (M=16, ef_construction=200, metric='cosine');
-- Dur√©e : D√©pend du nombre de vecteurs (1M vecteurs ~ 10-30 min)

-- √âtape 4 : Tester recherche vectorielle
SELECT
    doc_id,
    title,
    VEC_DISTANCE_COSINE(embedding, '[0.1, -0.2, ...]') AS relevance
FROM documents
ORDER BY relevance ASC
LIMIT 10;
```

### Conversion 5 : ColumnStore ‚Üí InnoDB (correction erreur)

**Contexte** : Table mal plac√©e dans ColumnStore (trop petite, besoins OLTP).

#### Proc√©dure

```sql
-- Table ColumnStore : products (100K lignes, 50 MB)
-- Probl√®me : UPDATE fr√©quents (prix, stock) ‚Üí ColumnStore inadapt√©

-- √âtape 1 : Cr√©er table InnoDB
CREATE TABLE products_innodb (
    product_id INT PRIMARY KEY,
    name VARCHAR(200),
    price DECIMAL(10,2),
    stock INT,
    INDEX idx_name (name)
) ENGINE=InnoDB;

-- √âtape 2 : Copier depuis ColumnStore
INSERT INTO products_innodb SELECT * FROM products;

-- √âtape 3 : V√©rifier
SELECT COUNT(*) FROM products;         -- 100000
SELECT COUNT(*) FROM products_innodb;  -- 100000

-- √âtape 4 : Swap
RENAME TABLE
    products TO products_columnstore_old,
    products_innodb TO products;

-- √âtape 5 : Tester UPDATE
UPDATE products SET price = price * 1.05 WHERE product_id = 42;
-- Rapide avec InnoDB (0.1 ms vs 1 sec avec ColumnStore)

-- √âtape 6 : Cleanup
DROP TABLE products_columnstore_old;
```

---

## Gestion des d√©pendances

### Foreign Keys

Les Foreign Keys doivent √™tre g√©r√©es manuellement lors des conversions.

```sql
-- Sc√©nario : orders (InnoDB) avec FK vers customers
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(200)
) ENGINE=InnoDB;

CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    amount DECIMAL(10,2),
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
) ENGINE=InnoDB;

-- Conversion orders ‚Üí ColumnStore (FK non support√©e)

-- √âtape 1 : Lister les FK
SELECT
    CONSTRAINT_NAME,
    TABLE_NAME,
    REFERENCED_TABLE_NAME,
    REFERENCED_COLUMN_NAME
FROM information_schema.KEY_COLUMN_USAGE
WHERE TABLE_SCHEMA = 'mydb'
  AND TABLE_NAME = 'orders'
  AND REFERENCED_TABLE_NAME IS NOT NULL;

-- √âtape 2 : Supprimer FK
ALTER TABLE orders DROP FOREIGN KEY orders_ibfk_1;

-- √âtape 3 : Convertir
ALTER TABLE orders ENGINE=ColumnStore;

-- √âtape 4 : Documenter perte de FK
-- ‚ö†Ô∏è Int√©grit√© r√©f√©rentielle doit √™tre g√©r√©e par l'application
```

**Alternative pour pr√©server int√©grit√©** :

```sql
-- Garder tables relationnelles en InnoDB
-- Cr√©er table analytics d√©normalis√©e en ColumnStore

CREATE TABLE orders_analytics (
    order_id INT,
    order_date DATE,
    customer_name VARCHAR(200),  -- D√©normalis√©
    customer_country VARCHAR(50), -- D√©normalis√©
    amount DECIMAL(10,2)
) ENGINE=ColumnStore;

-- ETL quotidien
INSERT INTO orders_analytics
SELECT
    o.order_id,
    o.order_date,
    c.name,
    c.country,
    o.amount
FROM orders o  -- InnoDB (avec FK)
JOIN customers c ON o.customer_id = c.customer_id
WHERE o.order_date = CURDATE() - INTERVAL 1 DAY;
```

### Triggers

Les triggers restent attach√©s √† la table lors de la conversion.

```sql
-- Table avec trigger
CREATE TABLE audit_log (
    log_id INT PRIMARY KEY AUTO_INCREMENT,
    action VARCHAR(50),
    timestamp DATETIME
) ENGINE=InnoDB;

CREATE TRIGGER orders_after_insert
AFTER INSERT ON orders
FOR EACH ROW
    INSERT INTO audit_log (action, timestamp)
    VALUES ('INSERT', NOW());

-- Conversion : Trigger reste fonctionnel
ALTER TABLE orders ENGINE=ColumnStore;

-- V√©rifier triggers
SHOW TRIGGERS WHERE `Table` = 'orders'\G
```

### Vues

Les vues restent fonctionnelles mais peuvent devenir plus lentes.

```sql
-- Vue bas√©e sur table InnoDB
CREATE VIEW active_orders AS
SELECT * FROM orders WHERE status = 'active';

-- Apr√®s conversion orders ‚Üí ColumnStore
-- Vue fonctionne toujours mais :
-- ‚Ä¢ Plus lente si filtre non-optimal pour columnar
-- ‚Ä¢ B√©n√©ficie des agr√©gations si la vue les utilise

-- Recommandation : Revoir d√©finition vue apr√®s conversion
```

---

## V√©rifications post-migration

### Checklist compl√®te

```sql
-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
-- CHECKLIST POST-MIGRATION
-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

-- 1. V√©rifier nombre de lignes
SELECT 'Old table', COUNT(*) FROM table_old;
SELECT 'New table', COUNT(*) FROM table_new;
-- Doivent √™tre identiques

-- 2. V√©rifier checksum (d√©tection corruption)
SELECT
    SUM(CRC32(CONCAT_WS(',', col1, col2, col3))) AS checksum
FROM table_old;
-- Comparer avec table_new

-- 3. V√©rifier structure
SHOW CREATE TABLE table_old\G
SHOW CREATE TABLE table_new\G
-- Comparer ENGINE, colonnes, index

-- 4. V√©rifier index
SHOW INDEX FROM table_new;
-- Tous les index pr√©sents ?

-- 5. V√©rifier AUTO_INCREMENT
SHOW TABLE STATUS LIKE 'table_new'\G
-- Auto_increment doit √™tre coh√©rent

-- 6. V√©rifier statistiques
ANALYZE TABLE table_new;
SHOW TABLE STATUS LIKE 'table_new'\G
-- Rows, Avg_row_length, Data_length

-- 7. Test requ√™tes critiques
-- Ex√©cuter 10-20 requ√™tes SQL critiques de l'application
SELECT * FROM table_new WHERE id = 12345;
SELECT COUNT(*), AVG(amount) FROM table_new GROUP BY category;
-- etc.

-- 8. Test performance
-- Comparer temps d'ex√©cution avant/apr√®s
EXPLAIN SELECT ... FROM table_new;

-- 9. Test √©criture (si applicable)
INSERT INTO table_new VALUES (...);
UPDATE table_new SET ... WHERE ...;
DELETE FROM table_new WHERE ...;

-- 10. V√©rifier logs erreurs
-- shell> tail -100 /var/log/mysql/error.log
-- Pas d'erreurs li√©es √† la nouvelle table ?
```

### Script de v√©rification automatique

```bash
#!/bin/bash
# verify_migration.sh

DB="mydb"
TABLE_OLD="orders_old"
TABLE_NEW="orders"
USER="root"
PASS="secret"

echo "=== Migration Verification ==="
echo "Database: $DB"
echo "Old table: $TABLE_OLD"
echo "New table: $TABLE_NEW"
echo ""

# 1. Row count
echo "1. Checking row count..."
COUNT_OLD=$(mysql -u$USER -p$PASS -D$DB -sNe "SELECT COUNT(*) FROM $TABLE_OLD")
COUNT_NEW=$(mysql -u$USER -p$PASS -D$DB -sNe "SELECT COUNT(*) FROM $TABLE_NEW")

if [ "$COUNT_OLD" == "$COUNT_NEW" ]; then
    echo "‚úì Row count match: $COUNT_NEW"
else
    echo "‚úó Row count mismatch: $COUNT_OLD vs $COUNT_NEW"
    exit 1
fi

# 2. Checksum
echo "2. Checking checksum..."
CHECKSUM_OLD=$(mysql -u$USER -p$PASS -D$DB -sNe \
    "SELECT SUM(CRC32(CONCAT_WS(',', *))) FROM $TABLE_OLD")
CHECKSUM_NEW=$(mysql -u$USER -p$PASS -D$DB -sNe \
    "SELECT SUM(CRC32(CONCAT_WS(',', *))) FROM $TABLE_NEW")

if [ "$CHECKSUM_OLD" == "$CHECKSUM_NEW" ]; then
    echo "‚úì Checksum match"
else
    echo "‚úó Checksum mismatch"
    exit 1
fi

# 3. Engine verification
echo "3. Checking engine..."
ENGINE=$(mysql -u$USER -p$PASS -D$DB -sNe \
    "SELECT ENGINE FROM information_schema.TABLES
     WHERE TABLE_SCHEMA='$DB' AND TABLE_NAME='$TABLE_NEW'")
echo "‚úì Engine: $ENGINE"

# 4. Index count
echo "4. Checking indexes..."
INDEX_COUNT=$(mysql -u$USER -p$PASS -D$DB -sNe \
    "SELECT COUNT(DISTINCT INDEX_NAME) FROM information_schema.STATISTICS
     WHERE TABLE_SCHEMA='$DB' AND TABLE_NAME='$TABLE_NEW'")
echo "‚úì Index count: $INDEX_COUNT"

echo ""
echo "=== Verification Complete ==="
echo "All checks passed ‚úì"
```

---

## Rollback strategies

### Rollback m√©thode CREATE + RENAME

```sql
-- Si probl√®me d√©tect√© apr√®s swap

-- Situation actuelle :
-- orders (nouvelle table, probl√®me)
-- orders_old (ancienne table, sauvegard√©e)

-- Rollback (< 1 sec)
RENAME TABLE
    orders TO orders_problematic,
    orders_old TO orders;

-- Application revient sur ancienne table
-- Analyser le probl√®me sur orders_problematic
-- Puis DROP orders_problematic une fois r√©solu
```

### Rollback gh-ost

```bash
# Pendant la migration
echo "panic" > /tmp/gh-ost.orders.sock
# gh-ost arr√™te et DROP la table fant√¥me
# Table originale non modifi√©e

# Apr√®s le cut-over (si probl√®me imm√©diat)
# L'ancienne table existe encore : _orders_old
RENAME TABLE
    orders TO orders_new_problematic,
    _orders_old TO orders;
```

### Rollback via backup

```bash
# Dernier recours si corruption ou perte donn√©es

# 1. Arr√™ter MariaDB
systemctl stop mariadb

# 2. Supprimer table corrompue
rm /var/lib/mysql/mydb/orders.*

# 3. Restaurer depuis backup
mysql mydb < /backup/orders_backup.sql

# 4. Red√©marrer
systemctl start mariadb

# 5. V√©rifier
mysql -e "SELECT COUNT(*) FROM mydb.orders"
```

---

## Automatisation : Scripts et proc√©dures

### Proc√©dure stock√©e : Migration automatique

```sql
DELIMITER $$

CREATE PROCEDURE migrate_table_to_innodb(
    IN p_schema VARCHAR(64),
    IN p_table VARCHAR(64),
    IN p_method VARCHAR(20)  -- 'ALTER' ou 'CREATE_RENAME'
)
BEGIN
    DECLARE v_old_engine VARCHAR(64);
    DECLARE v_row_count BIGINT;
    DECLARE v_table_new VARCHAR(128);
    DECLARE v_table_old VARCHAR(128);

    -- V√©rifier moteur actuel
    SELECT ENGINE, TABLE_ROWS
    INTO v_old_engine, v_row_count
    FROM information_schema.TABLES
    WHERE TABLE_SCHEMA = p_schema
      AND TABLE_NAME = p_table;

    IF v_old_engine = 'InnoDB' THEN
        SELECT CONCAT('Table ', p_table, ' is already InnoDB') AS result;
        LEAVE;
    END IF;

    -- Logging
    INSERT INTO migration_log (schema_name, table_name, started_at, old_engine, row_count, method)
    VALUES (p_schema, p_table, NOW(), v_old_engine, v_row_count, p_method);

    IF p_method = 'ALTER' THEN
        -- M√©thode ALTER TABLE
        SET @sql = CONCAT('ALTER TABLE ', p_schema, '.', p_table, ' ENGINE=InnoDB');
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;

    ELSEIF p_method = 'CREATE_RENAME' THEN
        -- M√©thode CREATE + RENAME
        SET v_table_new = CONCAT(p_table, '_new');
        SET v_table_old = CONCAT(p_table, '_old');

        -- Cr√©er nouvelle table
        SET @sql = CONCAT('CREATE TABLE ', p_schema, '.', v_table_new,
                          ' LIKE ', p_schema, '.', p_table);
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;

        -- Changer moteur
        SET @sql = CONCAT('ALTER TABLE ', p_schema, '.', v_table_new, ' ENGINE=InnoDB');
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;

        -- Copier donn√©es
        SET @sql = CONCAT('INSERT INTO ', p_schema, '.', v_table_new,
                          ' SELECT * FROM ', p_schema, '.', p_table);
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;

        -- Swap
        SET @sql = CONCAT('RENAME TABLE ',
                          p_schema, '.', p_table, ' TO ', p_schema, '.', v_table_old, ',',
                          p_schema, '.', v_table_new, ' TO ', p_schema, '.', p_table);
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
    END IF;

    -- Log success
    UPDATE migration_log
    SET completed_at = NOW(), status = 'SUCCESS'
    WHERE schema_name = p_schema
      AND table_name = p_table
      AND completed_at IS NULL;

    SELECT CONCAT('Migration of ', p_table, ' to InnoDB completed') AS result;
END$$

DELIMITER ;

-- Table de log
CREATE TABLE migration_log (
    log_id INT PRIMARY KEY AUTO_INCREMENT,
    schema_name VARCHAR(64),
    table_name VARCHAR(64),
    started_at DATETIME,
    completed_at DATETIME,
    old_engine VARCHAR(64),
    row_count BIGINT,
    method VARCHAR(20),
    status VARCHAR(20),
    INDEX idx_table (schema_name, table_name)
) ENGINE=InnoDB;

-- Utilisation
CALL migrate_table_to_innodb('mydb', 'orders', 'CREATE_RENAME');
```

### Script Bash : Migration batch

```bash
#!/bin/bash
# migrate_all_myisam.sh
# Migre toutes les tables MyISAM vers InnoDB

DB_USER="root"
DB_PASS="secret"
DB_NAME="mydb"

# Lister tables MyISAM
TABLES=$(mysql -u$DB_USER -p$DB_PASS -D$DB_NAME -sNe \
    "SELECT TABLE_NAME FROM information_schema.TABLES
     WHERE TABLE_SCHEMA='$DB_NAME' AND ENGINE='MyISAM'")

echo "=== MyISAM ‚Üí InnoDB Migration ==="
echo "Database: $DB_NAME"
echo "Tables to migrate: $(echo "$TABLES" | wc -l)"
echo ""

for TABLE in $TABLES; do
    echo "Migrating $TABLE..."

    # Obtenir taille
    SIZE=$(mysql -u$DB_USER -p$DB_PASS -D$DB_NAME -sNe \
        "SELECT ROUND((DATA_LENGTH+INDEX_LENGTH)/1024/1024,2)
         FROM information_schema.TABLES
         WHERE TABLE_SCHEMA='$DB_NAME' AND TABLE_NAME='$TABLE'")

    echo "  Size: ${SIZE} MB"

    # Choisir m√©thode selon taille
    if (( $(echo "$SIZE < 1000" | bc -l) )); then
        echo "  Method: ALTER TABLE (direct)"
        mysql -u$DB_USER -p$DB_PASS -D$DB_NAME -e \
            "ALTER TABLE $TABLE ENGINE=InnoDB"
    else
        echo "  Method: gh-ost (large table)"
        gh-ost \
            --host=localhost \
            --user=$DB_USER \
            --password=$DB_PASS \
            --database=$DB_NAME \
            --table=$TABLE \
            --alter="ENGINE=InnoDB" \
            --execute \
            --allow-on-master \
            --exact-rowcount
    fi

    echo "  ‚úì Completed"
    echo ""
done

echo "=== Migration Complete ==="
```

---

## Cas particuliers et pi√®ges √† √©viter

### Pi√®ge 1 : Espace disque insuffisant

```bash
# Probl√®me : Conversion √©choue √† 90% par manque d'espace

# Pr√©vention : V√©rifier avant
TABLE_SIZE=$(mysql -uroot -psecret -sNe \
    "SELECT (DATA_LENGTH+INDEX_LENGTH)*2/1024/1024/1024
     FROM information_schema.TABLES
     WHERE TABLE_NAME='huge_table'")

DISK_FREE=$(df /var/lib/mysql | tail -1 | awk '{print $4/1024/1024}')

if (( $(echo "$TABLE_SIZE > $DISK_FREE" | bc -l) )); then
    echo "ERROR: Insufficient disk space"
    echo "Required: ${TABLE_SIZE} GB"
    echo "Available: ${DISK_FREE} GB"
    exit 1
fi
```

### Pi√®ge 2 : Conversion pendant haute charge

```bash
# Probl√®me : Conversion sature le serveur, ralentit production

# Solution : Throttler avec gh-ost
gh-ost \
    --max-load="Threads_running=25,Threads_connected=100" \
    --critical-load="Threads_running=50" \
    --nice-ratio=0.8  # 80% attente entre batch
    ...
```

### Pi√®ge 3 : Oublier de v√©rifier Foreign Keys

```sql
-- Probl√®me : Conversion casse les FK

-- Solution : Lister et documenter AVANT
SELECT
    CONSTRAINT_NAME,
    TABLE_NAME,
    REFERENCED_TABLE_NAME
FROM information_schema.KEY_COLUMN_USAGE
WHERE TABLE_SCHEMA = 'mydb'
  AND REFERENCED_TABLE_NAME IS NOT NULL;

-- Recr√©er apr√®s si n√©cessaire
ALTER TABLE orders
ADD CONSTRAINT fk_customer
FOREIGN KEY (customer_id) REFERENCES customers(customer_id);
```

### Pi√®ge 4 : N√©gliger les statistiques

```sql
-- Probl√®me : Performances d√©grad√©es apr√®s conversion

-- Solution : ANALYZE TABLE obligatoire
ALTER TABLE orders ENGINE=InnoDB;
ANALYZE TABLE orders;  -- ‚Üê OBLIGATOIRE

-- Puis surveiller plans d'ex√©cution
EXPLAIN SELECT * FROM orders WHERE customer_id = 42;
```

### Pi√®ge 5 : Conversion pendant backup

```bash
# Probl√®me : Corruption ou backup incoh√©rent

# Solution : Coordination backup/migration
# 1. Planifier migrations hors fen√™tre backup
# 2. Ou suspendre backup pendant migration
# 3. Ou utiliser gh-ost (pas d'impact backup)
```

---

## ‚úÖ Points cl√©s √† retenir

1. **Planification critique** : 80% planification, 20% ex√©cution pour succ√®s.

2. **Backup imp√©ratif** : Toujours backup complet avant conversion.

3. **M√©thode selon taille** : ALTER (< 10 GB), CREATE+RENAME (< 100 GB), gh-ost (> 100 GB).

4. **gh-ost recommand√© production** : Zero-downtime, pausable, throttling auto.

5. **V√©rifications post-migration** : COUNT, checksum, structure, performance.

6. **Espace disque** : Pr√©voir 2-3√ó taille table (conversion + marge).

7. **Foreign Keys** : G√©r√©es manuellement, documenter pertes.

8. **ANALYZE TABLE** : Obligatoire apr√®s conversion pour statistiques correctes.

9. **Rollback pr√©vu** : Garder ancienne table jusqu'√† validation compl√®te.

10. **MyISAM ‚Üí InnoDB URGENT** : Priorit√© absolue en 2025, MyISAM dangereux.

---

## üîó Ressources et r√©f√©rences

### Documentation MariaDB

- [üìñ ALTER TABLE](https://mariadb.com/kb/en/alter-table/)
- [üìñ Storage Engine Conversion](https://mariadb.com/kb/en/storage-engine-conversion/)
- [üìñ Online DDL](https://mariadb.com/kb/en/innodb-online-ddl/)

### Outils de migration

- [gh-ost (GitHub)](https://github.com/github/gh-ost)
- [Percona Toolkit](https://www.percona.com/software/database-tools/percona-toolkit)
- [mariabackup Documentation](https://mariadb.com/kb/en/mariabackup/)

### Guides pratiques

- [Migration Best Practices](https://mariadb.com/kb/en/migration-best-practices/)
- [MyISAM to InnoDB Migration Guide](https://mariadb.com/resources/blog/myisam-to-innodb/)
- [Zero-Downtime Migrations](https://mariadb.org/zero-downtime-migrations/)

---

## üéì Conclusion du chapitre 7

Vous avez maintenant une **ma√Ætrise compl√®te** des moteurs de stockage MariaDB :

1. ‚úÖ **Architecture Pluggable** (7.1) - Compr√©hension syst√®me
2. ‚úÖ **InnoDB** (7.2) - Moteur transactionnel ma√Ætre
3. ‚úÖ **MyISAM** (7.3) - Legacy √† migrer
4. ‚úÖ **Aria** (7.4) - Crash-safe simple
5. ‚úÖ **ColumnStore** (7.5) - Analytics OLAP
6. ‚úÖ **S3** (7.6) - Archivage √©conomique
7. ‚úÖ **Vector/HNSW** (7.7) - IA et recherche s√©mantique üÜï
8. ‚úÖ **Comparaison et choix** (7.8) - D√©cisions architecturales
9. ‚úÖ **Conversion entre moteurs** (7.9) - Migration pratique

**Comp√©tences acquises** :
- Choisir le moteur optimal selon le cas d'usage
- Concevoir des architectures hybrides multi-moteurs
- Migrer entre moteurs avec strat√©gies zero-downtime
- Optimiser configurations moteurs
- √âviter les anti-patterns courants

**Prochaines √©tapes** : Appliquer ces connaissances sur vos projets r√©els, documenter vos choix architecturaux, et r√©√©valuer r√©guli√®rement vos d√©cisions selon l'√©volution des besoins.

---

**üìå M√©mo DBA Final** :
```
Migration = Backup + Planification + V√©rification + Rollback pr√©vu
Jamais sur production sans tests complets en pr√©production
gh-ost = ami des grandes migrations
ALTER TABLE = petit tables uniquement
MyISAM ‚Üí InnoDB = URGENT 2025
```

**üéØ Checklist universelle migration** :
1. ‚úÖ Backup complet v√©rifi√©
2. ‚úÖ Espace disque suffisant (2-3√ó table)
3. ‚úÖ Fen√™tre maintenance ou gh-ost
4. ‚úÖ Plan rollback document√©
5. ‚úÖ V√©rifications post-migration (count, checksum, perf)
6. ‚úÖ Validation applicative compl√®te
7. ‚úÖ ANALYZE TABLE ex√©cut√©
8. ‚úÖ Monitoring 48h post-migration
9. ‚úÖ Documentation mise √† jour
10. ‚úÖ Cleanup apr√®s validation (DROP old tables)

‚è≠Ô∏è [Moteurs sp√©cialis√©s](/07-moteurs-de-stockage/10-moteurs-specialises.md)
