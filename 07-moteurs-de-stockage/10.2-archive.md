ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 7.10.2 Archive : Compression Maximale

> **Niveau** : AvancÃ©
> **DurÃ©e estimÃ©e** : 2 heures

> **PrÃ©requis** :
> - ComprÃ©hension des moteurs de stockage MariaDB
> - Architecture InnoDB (Section 7.2)
> - Concepts de data lifecycle management

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- Comprendre l'architecture et le fonctionnement du moteur Archive
- Identifier les cas d'usage optimaux pour l'archivage de donnÃ©es
- MaÃ®triser les limitations et contraintes du moteur Archive
- ImplÃ©menter des stratÃ©gies de data tiering (hot/warm/cold)
- Concevoir des architectures de partitionnement pour archivage efficace
- Comparer Archive avec les alternatives (S3, ColumnStore, compression InnoDB)
- Mettre en place des pipelines d'archivage automatisÃ©s
- Optimiser le stockage pour donnÃ©es historiques et logs

---

## Introduction

Le moteur **Archive** est un moteur de stockage spÃ©cialisÃ© conÃ§u pour stocker de **grandes quantitÃ©s de donnÃ©es historiques** avec une **compression maximale**. Il sacrifie les fonctionnalitÃ©s et la flexibilitÃ© au profit d'un taux de compression extrÃªme et d'un overhead minimal.

**CaractÃ©ristiques principales** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MOTEUR ARCHIVE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  âœ“ Compression zlib maximale (ratio 1:10)             â”‚
â”‚  âœ“ INSERT et SELECT uniquement                        â”‚
â”‚  âœ“ Row-level locking pour INSERT                      â”‚
â”‚  âœ“ Overhead minimal (mÃ©tadonnÃ©es)                     â”‚
â”‚  âœ— Pas d'UPDATE ni DELETE                             â”‚
â”‚  âœ— Pas d'index (sauf AUTO_INCREMENT)                  â”‚
â”‚  âœ— Performance lecture modÃ©rÃ©e (dÃ©compression)        â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ’¡ **Principe fondamental** : Archive est optimisÃ© pour **write-once, read-rarely** (WORM) - Ã©crire une fois, lire rarement.

### Quand Utiliser Archive ?

**Cas d'usage appropriÃ©s** :
- Logs applicatifs historiques
- DonnÃ©es d'audit et conformitÃ©
- Historiques de transactions
- DonnÃ©es de tÃ©lÃ©mÃ©trie/IoT
- Archives rÃ©glementaires (7-10 ans de rÃ©tention)
- DonnÃ©es de backup secondaire

**Quand NE PAS utiliser Archive** :
- DonnÃ©es frÃ©quemment mises Ã  jour
- RequÃªtes nÃ©cessitant index
- Besoin de DELETE rÃ©guliers
- DonnÃ©es nÃ©cessitant rÃ©ponse temps rÃ©el
- Tables de rÃ©fÃ©rence (lookup)

---

## Architecture du Moteur Archive

### Compression et Stockage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ARCHIVE ENGINE ARCHITECTURE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚   DISQUE                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  logs_archive.ARZ (donnÃ©es compressÃ©es)      â”‚     â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚   â”‚  â”‚ Row 1 (compressed)                     â”‚  â”‚     â”‚
â”‚   â”‚  â”‚ Row 2 (compressed)                     â”‚  â”‚     â”‚
â”‚   â”‚  â”‚ Row 3 (compressed)                     â”‚  â”‚     â”‚
â”‚   â”‚  â”‚ ...                                    â”‚  â”‚     â”‚
â”‚   â”‚  â”‚ Row N (compressed)                     â”‚  â”‚     â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚   â”‚  Compression : zlib level 6 (dÃ©faut)         â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  logs_archive.frm (structure)                â”‚     â”‚
â”‚   â”‚  logs_archive.ARM (metadata)                 â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                        â”‚
â”‚   MÃ‰MOIRE (Buffer)                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Write Buffer (avant flush)                  â”‚     â”‚
â”‚   â”‚  Read Buffer (dÃ©compression)                 â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fichiers crÃ©Ã©s** :
- `.ARZ` : DonnÃ©es compressÃ©es (zlib)
- `.ARM` : MÃ©tadonnÃ©es (nombre de lignes, statistiques)
- `.frm` : DÃ©finition de table (standard)

### Algorithme de Compression

Archive utilise **zlib** (DEFLATE) avec niveau de compression par dÃ©faut Ã  6 :

```
Taux de compression typiques :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DonnÃ©es texte (logs)      : 1:10 Ã  1:15
DonnÃ©es numÃ©riques        : 1:5 Ã  1:8
DonnÃ©es mixtes            : 1:7 Ã  1:10
DonnÃ©es dÃ©jÃ  compressÃ©es  : 1:1 (aucun gain)
```

**Exemple concret** :

```sql
-- Table de logs (100 millions de lignes)
CREATE TABLE access_logs_innodb (
    log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME,
    ip_address VARCHAR(45),
    user_agent VARCHAR(255),
    url VARCHAR(500),
    status_code SMALLINT,
    response_time INT
) ENGINE=InnoDB;

-- Taille InnoDB : 15 GB

-- MÃªme table avec Archive
CREATE TABLE access_logs_archive (
    log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME,
    ip_address VARCHAR(45),
    user_agent VARCHAR(255),
    url VARCHAR(500),
    status_code SMALLINT,
    response_time INT
) ENGINE=ARCHIVE;

-- Taille Archive : 1.5 GB (ratio 1:10)
-- Ã‰conomie : 13.5 GB (90%)
```

---

## OpÃ©rations SupportÃ©es

### INSERT : Seule OpÃ©ration d'Ã‰criture

```sql
-- CrÃ©ation de table
CREATE TABLE audit_trail (
    audit_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    event_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    user_id INT,
    action VARCHAR(50),
    table_name VARCHAR(100),
    record_id BIGINT,
    old_values TEXT,
    new_values TEXT,
    ip_address VARCHAR(45)
) ENGINE=ARCHIVE;

-- âœ… INSERT supportÃ©
INSERT INTO audit_trail (user_id, action, table_name, record_id, old_values, new_values)
VALUES (12345, 'UPDATE', 'orders', 98765, '{"status":"pending"}', '{"status":"completed"}');

-- âœ… INSERT multiple
INSERT INTO audit_trail (user_id, action, table_name, record_id)
VALUES
    (100, 'INSERT', 'products', 1001),
    (101, 'UPDATE', 'products', 1002),
    (102, 'DELETE', 'products', 1003);

-- âœ… INSERT ... SELECT (archivage depuis autre table)
INSERT INTO audit_trail
SELECT * FROM audit_trail_temp WHERE event_time < DATE_SUB(NOW(), INTERVAL 1 YEAR);

-- âŒ UPDATE non supportÃ©
UPDATE audit_trail SET action = 'MODIFIED' WHERE audit_id = 1;
-- ERROR 1031 (HY000): Table storage engine for 'audit_trail' doesn't have this option

-- âŒ DELETE non supportÃ©
DELETE FROM audit_trail WHERE event_time < '2020-01-01';
-- ERROR 1031 (HY000): Table storage engine for 'audit_trail' doesn't have this option

-- âš ï¸ TRUNCATE supportÃ© (supprime tout)
TRUNCATE TABLE audit_trail;  -- OK, vide la table
```

### SELECT : Lecture avec DÃ©compression

```sql
-- âœ… SELECT simple
SELECT * FROM audit_trail WHERE audit_id = 12345;

-- âœ… SELECT avec filtrage (mais lent sans index)
SELECT COUNT(*) FROM audit_trail
WHERE event_time BETWEEN '2024-01-01' AND '2024-12-31';

-- âœ… AgrÃ©gations
SELECT
    DATE(event_time) AS day,
    action,
    COUNT(*) AS event_count
FROM audit_trail
WHERE event_time >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)
GROUP BY DATE(event_time), action;

-- âš ï¸ Performance : Full table scan obligatoire (pas d'index)
-- Sur 100 millions de lignes : 30-60 secondes
```

**Impact de la dÃ©compression** :

```
OpÃ©ration           InnoDB          Archive
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ã‰criture INSERT     ~0.01 ms        ~0.02 ms
Lecture 1 row       ~0.05 ms        ~0.5 ms (dÃ©compression)
Scan 1M rows        ~5 s            ~30 s (dÃ©compression)
AgrÃ©gation          Index scan      Full table scan obligatoire
```

---

## Limitations et Contraintes

### 1. Pas d'Index (Sauf AUTO_INCREMENT)

```sql
-- âŒ Index secondaires interdits
CREATE TABLE logs_bad (
    log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME,
    user_id INT,
    INDEX idx_user (user_id),     -- ERROR
    INDEX idx_time (timestamp)    -- ERROR
) ENGINE=ARCHIVE;

-- âœ… Seul AUTO_INCREMENT PRIMARY KEY autorisÃ©
CREATE TABLE logs_good (
    log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME,
    user_id INT
) ENGINE=ARCHIVE;
```

ğŸ’¡ **Workaround** : Utiliser partitionnement pour optimiser les lectures.

### 2. Types de DonnÃ©es LimitÃ©s

```sql
CREATE TABLE archive_types_demo (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,

    -- âœ… Types numÃ©riques : OK
    int_col INT,
    bigint_col BIGINT,
    decimal_col DECIMAL(10,2),

    -- âœ… Types chaÃ®ne : OK
    char_col CHAR(50),
    varchar_col VARCHAR(255),
    text_col TEXT,

    -- âœ… Types temporels : OK
    date_col DATE,
    datetime_col DATETIME,
    timestamp_col TIMESTAMP,

    -- âœ… Types binaires : OK
    blob_col BLOB,

    -- âŒ Spatial : Non supportÃ©
    -- geo_col GEOMETRY,  -- ERROR

    -- âš ï¸ ENUM/SET : SupportÃ© mais dÃ©conseillÃ©
    status ENUM('active', 'archived')
) ENGINE=ARCHIVE;
```

### 3. Pas de Transactions ACID

```sql
-- Archive ne supporte pas les transactions
START TRANSACTION;
INSERT INTO audit_trail VALUES (...);
INSERT INTO audit_trail VALUES (...);
ROLLBACK;  -- N'annule PAS les insertions (dÃ©jÃ  Ã©crites)

-- Archive fonctionne en auto-commit permanent
```

### 4. Performance de Lecture

```sql
-- Benchmark : Table 100M lignes
-- InnoDB (avec index) :
SELECT * FROM logs_innodb WHERE user_id = 12345;
-- Temps : 0.05s (index seek)

-- Archive (sans index) :
SELECT * FROM logs_archive WHERE user_id = 12345;
-- Temps : 45s (full table scan + dÃ©compression)
```

âš ï¸ **ConsÃ©quence** : Archive n'est pas adaptÃ© pour requÃªtes interactives frÃ©quentes.

---

## StratÃ©gies de Data Tiering

### Architecture Hot/Warm/Cold

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA LIFECYCLE MANAGEMENT                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚   HOT DATA (Actuel - 30 jours)                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  ENGINE: InnoDB                              â”‚     â”‚
â”‚   â”‚  - Index complets                            â”‚     â”‚
â”‚   â”‚  - Performance maximale                      â”‚     â”‚
â”‚   â”‚  - UPDATE/DELETE autorisÃ©s                   â”‚     â”‚
â”‚   â”‚  Taille : 500 GB                             â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚            â†“ (aprÃ¨s 30 jours)                          â”‚
â”‚                                                        â”‚
â”‚   WARM DATA (RÃ©cent - 31-365 jours)                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  ENGINE: InnoDB Compressed                   â”‚     â”‚
â”‚   â”‚  - Index sÃ©lectifs                           â”‚     â”‚
â”‚   â”‚  - AccÃ¨s occasionnel                         â”‚     â”‚
â”‚   â”‚  - PartitionnÃ© par mois                      â”‚     â”‚
â”‚   â”‚  Taille : 2 TB â†’ 800 GB (compressed)         â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚            â†“ (aprÃ¨s 1 an)                              â”‚
â”‚                                                        â”‚
â”‚   COLD DATA (Archives - > 1 an)                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  ENGINE: Archive                             â”‚     â”‚
â”‚   â”‚  - Aucun index                               â”‚     â”‚
â”‚   â”‚  - Compression maximale                      â”‚     â”‚
â”‚   â”‚  - AccÃ¨s rare (compliance, audit)            â”‚     â”‚
â”‚   â”‚  Taille : 10 TB â†’ 1 TB (compressed 1:10)     â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation avec Partitionnement

```sql
-- 1. Table HOT (InnoDB - 30 derniers jours)
CREATE TABLE access_logs_hot (
    log_id BIGINT AUTO_INCREMENT,
    log_date DATE NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    user_id INT,
    ip_address VARCHAR(45),
    url VARCHAR(500),
    status_code SMALLINT,
    response_time INT,

    PRIMARY KEY (log_id, log_date),
    INDEX idx_user (user_id),
    INDEX idx_timestamp (timestamp),
    INDEX idx_status (status_code)
) ENGINE=InnoDB
PARTITION BY RANGE (TO_DAYS(log_date)) (
    PARTITION p_current VALUES LESS THAN (TO_DAYS(CURRENT_DATE)),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- 2. Table WARM (InnoDB Compressed - 1 an)
CREATE TABLE access_logs_warm (
    log_id BIGINT,
    log_date DATE NOT NULL,
    timestamp DATETIME,
    user_id INT,
    ip_address VARCHAR(45),
    url VARCHAR(500),
    status_code SMALLINT,
    response_time INT,

    PRIMARY KEY (log_id, log_date),
    INDEX idx_user (user_id),
    INDEX idx_timestamp (timestamp)
) ENGINE=InnoDB
ROW_FORMAT=COMPRESSED
KEY_BLOCK_SIZE=8
PARTITION BY RANGE (YEAR(log_date) * 100 + MONTH(log_date)) (
    PARTITION p_202401 VALUES LESS THAN (202402),
    PARTITION p_202402 VALUES LESS THAN (202403),
    -- ... 12 partitions par an
    PARTITION p_202412 VALUES LESS THAN (202501)
);

-- 3. Table COLD (Archive - > 1 an)
CREATE TABLE access_logs_cold (
    log_id BIGINT AUTO_INCREMENT,
    log_date DATE NOT NULL,
    timestamp DATETIME,
    user_id INT,
    ip_address VARCHAR(45),
    url VARCHAR(500),
    status_code SMALLINT,
    response_time INT,

    PRIMARY KEY (log_id, log_date)
) ENGINE=ARCHIVE
PARTITION BY RANGE (YEAR(log_date)) (
    PARTITION p_2020 VALUES LESS THAN (2021),
    PARTITION p_2021 VALUES LESS THAN (2022),
    PARTITION p_2022 VALUES LESS THAN (2023),
    PARTITION p_2023 VALUES LESS THAN (2024)
);
```

### ProcÃ©dure d'Archivage Automatique

```sql
DELIMITER //

-- ProcÃ©dure de migration HOT â†’ WARM
CREATE PROCEDURE archive_hot_to_warm()
BEGIN
    DECLARE v_cutoff_date DATE;

    SET v_cutoff_date = DATE_SUB(CURDATE(), INTERVAL 30 DAY);

    -- Copier vers warm
    INSERT INTO access_logs_warm
    SELECT * FROM access_logs_hot
    WHERE log_date < v_cutoff_date;

    -- Supprimer de hot
    DELETE FROM access_logs_hot
    WHERE log_date < v_cutoff_date;

    -- Log
    INSERT INTO archiving_log (operation, rows_affected, execution_time)
    VALUES ('hot_to_warm', ROW_COUNT(), NOW());
END//

-- ProcÃ©dure de migration WARM â†’ COLD
CREATE PROCEDURE archive_warm_to_cold()
BEGIN
    DECLARE v_cutoff_date DATE;

    SET v_cutoff_date = DATE_SUB(CURDATE(), INTERVAL 365 DAY);

    -- Copier vers cold (Archive)
    INSERT INTO access_logs_cold
    SELECT * FROM access_logs_warm
    WHERE log_date < v_cutoff_date;

    -- Supprimer partition warm
    -- (si partitionnÃ© par annÃ©e)
    ALTER TABLE access_logs_warm
    DROP PARTITION p_2023;  -- Exemple

    -- Log
    INSERT INTO archiving_log (operation, rows_affected, execution_time)
    VALUES ('warm_to_cold', ROW_COUNT(), NOW());
END//

DELIMITER ;

-- Event scheduler automatique
CREATE EVENT IF NOT EXISTS daily_archiving
ON SCHEDULE EVERY 1 DAY
STARTS '2025-01-01 02:00:00'
DO
BEGIN
    CALL archive_hot_to_warm();
    CALL archive_warm_to_cold();
END;

-- Activer
SET GLOBAL event_scheduler = ON;
```

---

## Comparaison avec Alternatives

### Archive vs InnoDB Compressed

```sql
-- InnoDB avec compression
CREATE TABLE logs_innodb_compressed (
    log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    timestamp DATETIME,
    data TEXT,
    INDEX idx_time (timestamp)
) ENGINE=InnoDB
ROW_FORMAT=COMPRESSED
KEY_BLOCK_SIZE=4;  -- 4K, 8K, ou 16K
```

**Comparaison** :

| Aspect | Archive | InnoDB Compressed |
|--------|---------|------------------|
| Taux compression | 1:10 Ã  1:15 | 1:2 Ã  1:4 |
| Index | âŒ Non | âœ… Oui |
| UPDATE/DELETE | âŒ Non | âœ… Oui |
| Performance lecture | Lente (dÃ©compression) | Moyenne |
| Transactions | âŒ Non | âœ… Oui |
| Overhead mÃ©moire | Minimal | Buffer Pool requis |
| Cas d'usage | Archives froides | DonnÃ©es semi-actives |

ğŸ’¡ **Recommandation** :
- **Archive** : DonnÃ©es > 2 ans, accÃ¨s < 1Ã—/mois
- **InnoDB Compressed** : DonnÃ©es 1-2 ans, accÃ¨s rÃ©guliers

### Archive vs Moteur S3

```sql
-- Moteur S3 (MariaDB 10.5+)
CREATE TABLE logs_s3 (
    log_id BIGINT,
    timestamp DATETIME,
    data TEXT
) ENGINE=S3
  s3_block_size=4M
  compression_algorithm='zlib';
```

**Comparaison** :

| Aspect | Archive | S3 Engine |
|--------|---------|-----------|
| Stockage | Local (disque) | AWS S3 / MinIO |
| CoÃ»t stockage | CoÃ»t disque local | TrÃ¨s faible (S3) |
| Latence lecture | ~100ms | ~500-1000ms (rÃ©seau) |
| DisponibilitÃ© | Serveur local | 99.99% (S3) |
| ScalabilitÃ© | LimitÃ©e par disque | IllimitÃ©e |
| Partage multi-serveur | âŒ Non | âœ… Oui |
| RÃ©silience | Backup requis | RÃ©plication S3 |

ğŸ’¡ **Recommandation** :
- **Archive** : DonnÃ©es locales, latence faible requise
- **S3** : Stockage cloud, coÃ»ts optimisÃ©s, multi-serveur

### Archive vs ColumnStore

```sql
-- ColumnStore (OLAP)
CREATE TABLE logs_columnstore (
    log_id BIGINT,
    timestamp DATETIME,
    data TEXT
) ENGINE=ColumnStore;
```

**Comparaison** :

| Aspect | Archive | ColumnStore |
|--------|---------|-------------|
| Compression | TrÃ¨s Ã©levÃ©e (1:10) | Ã‰levÃ©e (1:8) |
| RequÃªtes analytiques | Lente | TrÃ¨s rapide |
| AgrÃ©gations | Full scan | Columnar scan |
| INDEX | âŒ Aucun | Dictionnaire (auto) |
| UPDATE/DELETE | âŒ Non | âœ… Oui |
| Cas d'usage | Archivage froid | Analytics historiques |

ğŸ’¡ **Recommandation** :
- **Archive** : Compliance, audit (lectures rares)
- **ColumnStore** : Analytics sur donnÃ©es historiques (lectures frÃ©quentes)

---

## Patterns et Best Practices

### 1. Archivage par Lots (Batch)

```sql
-- Table source (InnoDB)
CREATE TABLE transactions (
    tx_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    tx_date DATE,
    amount DECIMAL(15,2),
    -- ... autres colonnes
) ENGINE=InnoDB;

-- Table archive (Archive)
CREATE TABLE transactions_archive (
    tx_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    tx_date DATE,
    amount DECIMAL(15,2),
    -- ... mÃªmes colonnes
) ENGINE=ARCHIVE;

-- ProcÃ©dure d'archivage par lots
DELIMITER //
CREATE PROCEDURE batch_archive_transactions()
BEGIN
    DECLARE v_batch_size INT DEFAULT 10000;
    DECLARE v_rows_affected INT DEFAULT 1;
    DECLARE v_cutoff_date DATE;

    SET v_cutoff_date = DATE_SUB(CURDATE(), INTERVAL 2 YEAR);

    -- Boucle par lots
    WHILE v_rows_affected > 0 DO
        -- Copier un lot vers Archive
        INSERT INTO transactions_archive
        SELECT * FROM transactions
        WHERE tx_date < v_cutoff_date
        LIMIT v_batch_size;

        SET v_rows_affected = ROW_COUNT();

        IF v_rows_affected > 0 THEN
            -- Supprimer le lot de la source
            DELETE FROM transactions
            WHERE tx_id IN (
                SELECT tx_id FROM transactions_archive
                ORDER BY tx_id DESC
                LIMIT v_batch_size
            );

            -- Pause pour ne pas saturer I/O
            DO SLEEP(1);
        END IF;
    END WHILE;

    -- Optimiser la table source
    OPTIMIZE TABLE transactions;
END//
DELIMITER ;

-- ExÃ©cution
CALL batch_archive_transactions();
```

### 2. Archivage avec Validation

```sql
DELIMITER //
CREATE PROCEDURE archive_with_verification(
    IN p_source_table VARCHAR(100),
    IN p_archive_table VARCHAR(100),
    IN p_cutoff_date DATE
)
BEGIN
    DECLARE v_source_count BIGINT;
    DECLARE v_archived_count BIGINT;
    DECLARE v_checksum_source VARCHAR(32);
    DECLARE v_checksum_archive VARCHAR(32);

    -- 1. Compter lignes source
    SET @sql = CONCAT('SELECT COUNT(*) INTO @v_source_count FROM ', p_source_table,
                      ' WHERE date_column < ?');
    PREPARE stmt FROM @sql;
    EXECUTE stmt USING p_cutoff_date;
    DEALLOCATE PREPARE stmt;
    SET v_source_count = @v_source_count;

    -- 2. Archiver
    SET @sql = CONCAT('INSERT INTO ', p_archive_table,
                      ' SELECT * FROM ', p_source_table,
                      ' WHERE date_column < ?');
    PREPARE stmt FROM @sql;
    EXECUTE stmt USING p_cutoff_date;
    DEALLOCATE PREPARE stmt;

    -- 3. VÃ©rifier nombre de lignes archivÃ©es
    SET @sql = CONCAT('SELECT COUNT(*) INTO @v_archived_count FROM ', p_archive_table,
                      ' WHERE date_column < ?');
    PREPARE stmt FROM @sql;
    EXECUTE stmt USING p_cutoff_date;
    DEALLOCATE PREPARE stmt;
    SET v_archived_count = @v_archived_count;

    -- 4. Validation
    IF v_source_count = v_archived_count THEN
        -- Supprimer de la source
        SET @sql = CONCAT('DELETE FROM ', p_source_table,
                          ' WHERE date_column < ?');
        PREPARE stmt FROM @sql;
        EXECUTE stmt USING p_cutoff_date;
        DEALLOCATE PREPARE stmt;

        -- Log succÃ¨s
        INSERT INTO archiving_log
        VALUES (NULL, NOW(), p_source_table, p_archive_table, v_archived_count, 'SUCCESS');
    ELSE
        -- Log erreur
        INSERT INTO archiving_log
        VALUES (NULL, NOW(), p_source_table, p_archive_table, 0,
                CONCAT('ERROR: Count mismatch - Source: ', v_source_count,
                       ', Archive: ', v_archived_count));

        SIGNAL SQLSTATE '45000'
        SET MESSAGE_TEXT = 'Archive verification failed';
    END IF;
END//
DELIMITER ;
```

### 3. Vue UnifiÃ©e (Hot + Cold)

```sql
-- Vue combinant donnÃ©es actives et archives
CREATE VIEW access_logs_unified AS
SELECT
    log_id,
    timestamp,
    user_id,
    url,
    status_code,
    'hot' AS data_tier
FROM access_logs_hot

UNION ALL

SELECT
    log_id,
    timestamp,
    user_id,
    url,
    status_code,
    'warm' AS data_tier
FROM access_logs_warm

UNION ALL

SELECT
    log_id,
    timestamp,
    user_id,
    url,
    status_code,
    'cold' AS data_tier
FROM access_logs_cold;

-- RequÃªte transparente
SELECT * FROM access_logs_unified
WHERE user_id = 12345
  AND timestamp BETWEEN '2020-01-01' AND '2025-12-31';

-- Optimisation : spÃ©cifier le tier si connu
SELECT * FROM access_logs_unified
WHERE data_tier = 'hot'  -- Force scan sur table hot uniquement
  AND user_id = 12345;
```

### 4. Compression Additionnelle avec gzip

```bash
#!/bin/bash
# Compression supplÃ©mentaire des fichiers Archive

# Variables
DATADIR="/var/lib/mysql/mydb"
ARCHIVE_TABLE="access_logs_cold"

# 1. Exporter vers fichier texte
mysql -u root -p -e "
    SELECT * FROM mydb.${ARCHIVE_TABLE}
    INTO OUTFILE '/tmp/${ARCHIVE_TABLE}.csv'
    FIELDS TERMINATED BY ','
    ENCLOSED BY '\"'
    LINES TERMINATED BY '\n'
"

# 2. Compresser avec gzip
gzip -9 /tmp/${ARCHIVE_TABLE}.csv
# RÃ©sultat : access_logs_cold.csv.gz (ratio additionnel 1:3)

# 3. Stocker sur S3 ou NAS
aws s3 cp /tmp/${ARCHIVE_TABLE}.csv.gz \
    s3://mybucket/archives/$(date +%Y/%m)/${ARCHIVE_TABLE}.csv.gz

# 4. Supprimer fichier local
rm /tmp/${ARCHIVE_TABLE}.csv.gz

# 5. Optionnel : Vider table Archive locale si backup S3 OK
mysql -u root -p -e "TRUNCATE TABLE mydb.${ARCHIVE_TABLE}"
```

---

## Migration et Conversion

### InnoDB â†’ Archive

```sql
-- Table source InnoDB
CREATE TABLE audit_innodb (
    audit_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    event_time DATETIME,
    user_id INT,
    action VARCHAR(50),
    details TEXT,
    INDEX idx_time (event_time),
    INDEX idx_user (user_id)
) ENGINE=InnoDB;

-- 1. CrÃ©er table Archive (sans index secondaires)
CREATE TABLE audit_archive LIKE audit_innodb;
ALTER TABLE audit_archive
    DROP INDEX idx_time,
    DROP INDEX idx_user,
    ENGINE=Archive;

-- 2. Copier donnÃ©es anciennes
INSERT INTO audit_archive
SELECT * FROM audit_innodb
WHERE event_time < DATE_SUB(NOW(), INTERVAL 1 YEAR);

-- 3. VÃ©rifier
SELECT COUNT(*) FROM audit_archive;

-- 4. Supprimer de source (optionnel)
DELETE FROM audit_innodb
WHERE event_time < DATE_SUB(NOW(), INTERVAL 1 YEAR);

-- 5. Comparer tailles
SELECT
    TABLE_NAME,
    ENGINE,
    TABLE_ROWS,
    ROUND(DATA_LENGTH / 1024 / 1024, 2) AS data_mb,
    ROUND(INDEX_LENGTH / 1024 / 1024, 2) AS index_mb,
    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS total_mb
FROM INFORMATION_SCHEMA.TABLES
WHERE TABLE_SCHEMA = 'mydb'
  AND TABLE_NAME IN ('audit_innodb', 'audit_archive');
```

### Archive â†’ InnoDB (Restauration)

```sql
-- Restaurer donnÃ©es archivÃ©es vers InnoDB pour analyse

-- 1. CrÃ©er table InnoDB temporaire
CREATE TABLE audit_restored (
    audit_id BIGINT PRIMARY KEY,
    event_time DATETIME,
    user_id INT,
    action VARCHAR(50),
    details TEXT,
    INDEX idx_time (event_time),
    INDEX idx_user (user_id)
) ENGINE=InnoDB;

-- 2. Copier depuis Archive (avec filtre si possible)
INSERT INTO audit_restored
SELECT * FROM audit_archive
WHERE event_time BETWEEN '2023-01-01' AND '2023-12-31';

-- 3. Analyser
SELECT
    user_id,
    COUNT(*) AS event_count,
    GROUP_CONCAT(DISTINCT action) AS actions
FROM audit_restored
GROUP BY user_id
ORDER BY event_count DESC
LIMIT 100;

-- 4. Nettoyer aprÃ¨s analyse
DROP TABLE audit_restored;
```

---

## Monitoring et Maintenance

### MÃ©triques de Suivi

```sql
-- Taille des tables Archive
SELECT
    TABLE_SCHEMA,
    TABLE_NAME,
    ENGINE,
    TABLE_ROWS,
    ROUND(DATA_LENGTH / 1024 / 1024, 2) AS data_mb,
    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS total_mb,
    ROUND(DATA_FREE / 1024 / 1024, 2) AS free_mb,
    CREATE_TIME,
    UPDATE_TIME
FROM INFORMATION_SCHEMA.TABLES
WHERE ENGINE = 'ARCHIVE'
  AND TABLE_SCHEMA NOT IN ('information_schema', 'mysql', 'performance_schema')
ORDER BY DATA_LENGTH DESC;

-- Statistiques d'archivage
CREATE TABLE archiving_stats (
    stat_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    stat_date DATE,
    source_table VARCHAR(100),
    archive_table VARCHAR(100),
    rows_archived BIGINT,
    source_size_mb DECIMAL(12,2),
    archive_size_mb DECIMAL(12,2),
    compression_ratio DECIMAL(5,2),
    execution_time_sec INT
) ENGINE=InnoDB;

-- Remplir aprÃ¨s chaque archivage
INSERT INTO archiving_stats
SELECT
    NULL,
    CURDATE(),
    'transactions',
    'transactions_archive',
    (SELECT COUNT(*) FROM transactions_archive),
    (SELECT ROUND(DATA_LENGTH / 1024 / 1024, 2)
     FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_NAME = 'transactions'),
    (SELECT ROUND(DATA_LENGTH / 1024 / 1024, 2)
     FROM INFORMATION_SCHEMA.TABLES
     WHERE TABLE_NAME = 'transactions_archive'),
    (SELECT ROUND(
        (SELECT DATA_LENGTH FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'transactions') /
        (SELECT DATA_LENGTH FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'transactions_archive'),
        2
    )),
    0;  -- Ã€ calculer via timer

-- Dashboard de monitoring
SELECT
    archive_table,
    DATE_FORMAT(MAX(stat_date), '%Y-%m') AS last_month,
    SUM(rows_archived) AS total_rows,
    ROUND(AVG(compression_ratio), 2) AS avg_compression,
    ROUND(SUM(archive_size_mb) / 1024, 2) AS total_size_gb
FROM archiving_stats
WHERE stat_date >= DATE_SUB(CURDATE(), INTERVAL 12 MONTH)
GROUP BY archive_table
ORDER BY total_size_gb DESC;
```

### VÃ©rification de l'IntÃ©gritÃ©

```sql
-- ProcÃ©dure de vÃ©rification
DELIMITER //
CREATE PROCEDURE verify_archive_integrity(
    IN p_table_name VARCHAR(100)
)
BEGIN
    DECLARE v_row_count BIGINT;
    DECLARE v_last_id BIGINT;
    DECLARE v_check_result VARCHAR(50);

    -- Compter lignes
    SET @sql = CONCAT('SELECT COUNT(*), MAX(',
                      (SELECT COLUMN_NAME
                       FROM INFORMATION_SCHEMA.COLUMNS
                       WHERE TABLE_NAME = p_table_name
                       AND COLUMN_KEY = 'PRI'
                       LIMIT 1),
                      ') INTO @v_row_count, @v_last_id FROM ', p_table_name);
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;

    -- CHECK TABLE
    SET @sql = CONCAT('CHECK TABLE ', p_table_name);
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;

    -- RÃ©sumÃ©
    SELECT
        p_table_name AS table_name,
        @v_row_count AS row_count,
        @v_last_id AS last_id,
        'OK' AS status;
END//
DELIMITER ;

-- ExÃ©cution
CALL verify_archive_integrity('audit_archive');
CALL verify_archive_integrity('transactions_archive');
```

---

## Configuration et Optimisation

### Variables SystÃ¨me

```ini
[mysqld]
# Archive engine n'a pas de variables spÃ©cifiques
# Mais optimiser l'environnement global

# Buffer pour lectures (dÃ©compression)
read_buffer_size = 8M
read_rnd_buffer_size = 8M

# Threads I/O (si nombreuses tables Archive)
innodb_read_io_threads = 8

# Tmp dir pour exports
tmpdir = /var/tmp

# Event scheduler (pour archivage automatique)
event_scheduler = ON
```

### Optimisation des Ã‰critures

```sql
-- 1. DÃ©sactiver binlog temporairement (si rÃ©plication pas critique)
SET sql_log_bin = 0;

INSERT INTO large_archive_table
SELECT * FROM source_table
WHERE date_col < '2020-01-01';

SET sql_log_bin = 1;

-- 2. Augmenter buffer de lecture pour INSERT ... SELECT
SET SESSION read_buffer_size = 16777216;  -- 16 MB

-- 3. Batch inserts (multi-values)
INSERT INTO archive_table VALUES
    (1, 'data1'),
    (2, 'data2'),
    -- ... 1000 lignes
    (1000, 'data1000');

-- Plus rapide que 1000 INSERT individuels
```

---

## Best Practices

### âœ… Ã€ Faire

1. **Partitionner par pÃ©riode** : Facilite suppression et maintenance
2. **Documenter le schÃ©ma d'archivage** : Politique de rÃ©tention claire
3. **Automatiser avec event scheduler** : Archivage rÃ©gulier sans intervention
4. **Valider aprÃ¨s archivage** : Compter lignes, vÃ©rifier checksums
5. **Monitorer la croissance** : Alertes si taille dÃ©passe seuils
6. **Tester la restauration** : Plan de recovery depuis archives
7. **Combiner avec tiering** : Hot â†’ Warm â†’ Cold

```sql
-- Exemple de documentation (table metadata)
CREATE TABLE archive_policy (
    table_name VARCHAR(100) PRIMARY KEY,
    hot_retention_days INT,
    warm_retention_days INT,
    cold_retention_years INT,
    archive_schedule VARCHAR(50),
    last_archived DATETIME,
    notes TEXT
) ENGINE=InnoDB;

INSERT INTO archive_policy VALUES
('access_logs', 30, 365, 7, 'DAILY 02:00', NOW(), 'Compliance requirement: 7 years'),
('transactions', 90, 730, 10, 'WEEKLY SUNDAY 03:00', NOW(), 'Financial data retention');
```

### âŒ Ã€ Ã‰viter

1. **Utiliser Archive pour donnÃ©es actives** : Performance lectures terrible
2. **Stocker sans stratÃ©gie d'accÃ¨s** : Archive â‰  poubelle
3. **Oublier les besoins de restauration** : Tester recovery
4. **Archiver sans compression prÃ©alable** : Source dÃ©jÃ  compressÃ©e = pas de gain
5. **NÃ©gliger la documentation** : Impossible de retrouver donnÃ©es archivÃ©es
6. **Pas de validation** : Risque perte de donnÃ©es
7. **Tables Archive sans partitionnement** : Full scan ultra-long

```sql
-- âŒ Anti-pattern
CREATE TABLE everything_archive (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    data TEXT,
    random_date DATETIME
) ENGINE=ARCHIVE;  -- Pas de structure, pas de partition

-- 100 millions de lignes
SELECT * FROM everything_archive
WHERE random_date BETWEEN '2020-01-01' AND '2020-12-31';
-- Temps : 5 minutes (full scan + dÃ©compression)

-- âœ… Bon usage
CREATE TABLE logs_archive (
    log_id BIGINT AUTO_INCREMENT,
    log_year INT,
    log_month TINYINT,
    timestamp DATETIME,
    data TEXT,
    PRIMARY KEY (log_id, log_year, log_month)
) ENGINE=ARCHIVE
PARTITION BY RANGE (log_year * 100 + log_month) (
    PARTITION p_202001 VALUES LESS THAN (202002),
    PARTITION p_202002 VALUES LESS THAN (202003),
    -- ... une partition par mois
);

-- MÃªme requÃªte : scan une seule partition
SELECT * FROM logs_archive
WHERE log_year = 2020 AND log_month BETWEEN 1 AND 12;
-- Temps : 10 secondes (partition pruning)
```

---

## âœ… Points clÃ©s Ã  retenir

- **Archive = Compression maximale** : Ratio 1:10 Ã  1:15, optimal pour donnÃ©es froides
- **INSERT et SELECT uniquement** : Pas d'UPDATE/DELETE, modÃ¨le WORM (write-once, read-rarely)
- **Aucun index secondaire** : Seul AUTO_INCREMENT supportÃ©, requÃªtes = full table scan obligatoire
- **Performance lecture modÃ©rÃ©e** : DÃ©compression zlib coÃ»teuse, ~10Ã— plus lent qu'InnoDB
- **Cas d'usage** : Logs historiques, audit, compliance, donnÃ©es > 1-2 ans, accÃ¨s < 1Ã—/mois
- **Data tiering essentiel** : Hot (InnoDB) â†’ Warm (InnoDB compressed) â†’ Cold (Archive)
- **Partitionnement recommandÃ©** : Par annÃ©e/mois pour Ã©viter full table scans
- **Alternatives** : InnoDB Compressed (donnÃ©es semi-actives), S3 (cloud, coÃ»ts), ColumnStore (analytics)
- **Automatisation requise** : Event scheduler pour migration automatique entre tiers
- **Validation critique** : Toujours vÃ©rifier intÃ©gritÃ© aprÃ¨s archivage (count, checksum)

---

## ğŸ”— Ressources et rÃ©fÃ©rences

- [ğŸ“– ARCHIVE Storage Engine](https://mariadb.com/kb/en/archive/)
- [ğŸ“– Partitioning](https://mariadb.com/kb/en/partitioning-types/)
- [ğŸ“– Data Lifecycle Management](https://mariadb.com/kb/en/data-lifecycle-management/)
- [ğŸ“– InnoDB Compression](https://mariadb.com/kb/en/innodb-page-compression/)
- [ğŸ“– S3 Storage Engine](https://mariadb.com/kb/en/s3-storage-engine/)

**Articles techniques** :
- "Implementing Data Tiering in MariaDB" - MariaDB Blog
- "Archive vs ColumnStore for Historical Data" - Percona Blog
- "Best Practices for Log Archival" - Planet MySQL

**Outils** :
- `pt-archiver` (Percona Toolkit) - Archivage automatisÃ©
- `mysqldump` - Export vers fichiers externes
- `mariadb-dump` - Backup logique avec compression

---

## â¡ï¸ Section suivante

**[7.10.3 Spider : Sharding distribuÃ©](/07-moteurs-de-stockage/10.3-spider.md)** : Moteur de partitionnement horizontal (sharding) distribuÃ©, architectures scale-out, configuration multi-serveurs, et cas d'usage pour trÃ¨s grandes bases de donnÃ©es.

â­ï¸ [Spider : Sharding distribuÃ©](/07-moteurs-de-stockage/10.3-spider.md)
