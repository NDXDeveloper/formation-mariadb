ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 19.8 Zero-downtime migrations

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 4-5 heures  
> **PrÃ©requis** : MaÃ®trise de la rÃ©plication MariaDB (chapitre 14), expÃ©rience avec les load balancers et proxies, connaissance des stratÃ©gies de rollback (section 19.7)

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- Concevoir et implÃ©menter des migrations sans interruption de service
- MaÃ®triser les architectures blue-green et canary pour les migrations de bases de donnÃ©es
- Orchestrer un cutover avec un temps d'indisponibilitÃ© minimal (secondes)
- Utiliser la rÃ©plication comme vecteur de migration zero-downtime
- PrÃ©venir et gÃ©rer les situations de split-brain
- Choisir les outils adaptÃ©s (pt-online-schema-change, gh-ost, ProxySQL)
- Appliquer ces techniques Ã  des scÃ©narios rÃ©els de migration MariaDB 11.8

---

## Introduction

Dans l'Ã©conomie numÃ©rique actuelle, chaque seconde d'indisponibilitÃ© a un coÃ»t. Pour un site e-commerce gÃ©nÃ©rant 100 000 â‚¬ par heure, une migration de 4 heures reprÃ©sente potentiellement 400 000 â‚¬ de pertes. Au-delÃ  de l'aspect financier, l'indisponibilitÃ© Ã©rode la confiance des utilisateurs et peut avoir des rÃ©percussions durables sur la rÃ©putation.

Les migrations zero-downtime rÃ©pondent Ã  cette exigence en permettant de faire Ã©voluer l'infrastructure de base de donnÃ©es tout en maintenant le service opÃ©rationnel. Ces techniques, longtemps rÃ©servÃ©es aux gÃ©ants du web, sont aujourd'hui accessibles grÃ¢ce Ã  des outils matures et des architectures bien documentÃ©es.

Cette section prÃ©sente les patterns, outils et mÃ©thodologies pour rÃ©aliser des migrations MariaDB sans interruption de service perceptible par les utilisateurs.

---

## DÃ©finition et objectifs

### Qu'est-ce qu'une migration zero-downtime ?

```
Spectrum du downtime de migration
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MIGRATION TRADITIONNELLE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVICE INDISPONIBLE                     â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚
â”‚  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2-8 heures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MIGRATION LOW-DOWNTIME
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service OK     â”‚ DOWN â”‚           Service OK                   â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â–‘â–‘â–‘â–‘ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚
â”‚                 â”‚â—€â”€â”€â”€â”€â–¶â”‚                                        â”‚
â”‚                  5-30 min                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MIGRATION ZERO-DOWNTIME
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service OK (source)  â”‚ Cutover â”‚   Service OK (cible)          â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–‘â–‘â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚                       â”‚â—€â–¶â”‚                                      â”‚
â”‚                      < 30 sec (souvent < 5 sec)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MIGRATION TRUE ZERO-DOWNTIME (avec dual-write)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SERVICE TOUJOURS DISPONIBLE                        â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚
â”‚  Aucune interruption perceptible par les utilisateurs           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Niveaux de zero-downtime

| Niveau | Downtime | Perception utilisateur | ComplexitÃ© |
|--------|----------|------------------------|------------|
| **Near-zero** | < 30 secondes | BrÃ¨ve interruption | ModÃ©rÃ©e |
| **Zero-downtime** | < 5 secondes | RequÃªtes en cours Ã©chouent | Ã‰levÃ©e |
| **True zero** | 0 seconde | Aucune interruption | TrÃ¨s Ã©levÃ©e |
| **Transparent** | 0 seconde + retry | Transparent total | Maximale |

---

## Architectures de migration zero-downtime

### Pattern 1 : Blue-Green Database

L'architecture blue-green maintient deux environnements complets et bascule le trafic instantanÃ©ment.

```
Architecture Blue-Green
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 1: Ã‰tat initial
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Load Balancer   â”‚
                    â”‚    (HAProxy)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ 100%
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      BLUE         â”‚
                    â”‚   MySQL 5.7       â”‚
                    â”‚    (Active)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      GREEN        â”‚
                    â”‚  MariaDB 11.8     â”‚
                    â”‚   (Standby)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PHASE 2: Synchronisation
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Load Balancer   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ 100%
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      BLUE         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   MySQL 5.7       â”‚ RÃ©plication
                    â”‚    (Active)       â”‚          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      GREEN        â”‚
                    â”‚  MariaDB 11.8     â”‚
                    â”‚   (Replica)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PHASE 3: Cutover
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Load Balancer   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ 100%
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      GREEN        â”‚
                    â”‚  MariaDB 11.8     â”‚
                    â”‚    (Active)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ RÃ©plication inverse
                              â–¼              (optionnel)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      BLUE         â”‚
                    â”‚   MySQL 5.7       â”‚
                    â”‚   (Standby)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages :**
- Rollback instantanÃ© (rebascule)
- Test complet avant cutover
- Isolation des environnements

**InconvÃ©nients :**
- Double infrastructure
- Synchronisation des donnÃ©es complexe
- CoÃ»t Ã©levÃ©

### Pattern 2 : Migration par rÃ©plication chaÃ®nÃ©e

La rÃ©plication permet une migration progressive avec synchronisation continue.

```
Migration par RÃ©plication
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              Applications               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              ProxySQL                   â”‚
                    â”‚         (Routage intelligent)           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                         â”‚                         â”‚
              â–¼                         â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MySQL Primary  â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ MariaDB Replica â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ MariaDB Replica â”‚
    â”‚    (Source)     â”‚ Binlogâ”‚    (Target 1)   â”‚ Binlogâ”‚    (Target 2)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                         â”‚                         â”‚
              â”‚ Writes                  â”‚ Reads (progressive)     â”‚
              â”‚                         â”‚                         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         Traffic progressif vers MariaDB
```

### Pattern 3 : Strangler Fig (Migration progressive)

InspirÃ© du pattern applicatif, cette approche migre les donnÃ©es table par table ou service par service.

```
Pattern Strangler Fig
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 1: Quelques tables migrÃ©es
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Application Layer                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Data Access Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  users_dao  â”‚  â”‚ orders_dao  â”‚  â”‚products_dao â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                â”‚
          â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MariaDB  â”‚    â”‚   MySQL   â”‚    â”‚   MySQL   â”‚
    â”‚  (migrÃ©)  â”‚    â”‚ (legacy)  â”‚    â”‚ (legacy)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 2: Plus de tables migrÃ©es
          â”‚                â”‚                â”‚
          â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MariaDB  â”‚    â”‚  MariaDB  â”‚    â”‚   MySQL   â”‚
    â”‚  (migrÃ©)  â”‚    â”‚  (migrÃ©)  â”‚    â”‚ (legacy)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE FINALE: Migration complÃ¨te
          â”‚                â”‚                â”‚
          â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              MariaDB 11.8 (complet)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mise en place de la rÃ©plication cross-version

### Configuration MySQL â†’ MariaDB

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CONFIGURATION RÃ‰PLICATION MySQL 5.7/8.0 â†’ MariaDB 11.8
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- SUR MYSQL (SOURCE)
-- -------------------

-- 1. VÃ©rifier que le binlog est activÃ©
SHOW VARIABLES LIKE 'log_bin';
SHOW VARIABLES LIKE 'binlog_format';  -- Doit Ãªtre ROW ou MIXED

-- 2. CrÃ©er l'utilisateur de rÃ©plication
CREATE USER 'repl_mariadb'@'%' IDENTIFIED BY 'SecureRepl!Pass123';
GRANT REPLICATION SLAVE ON *.* TO 'repl_mariadb'@'%';
FLUSH PRIVILEGES;

-- 3. Obtenir la position du binlog
FLUSH TABLES WITH READ LOCK;
SHOW MASTER STATUS;
-- Noter: File et Position
-- Exemple: mysql-bin.000042, 12345678

-- 4. Faire le dump initial (dans un autre terminal)
-- mysqldump --all-databases --single-transaction --master-data=2 \
--   --routines --triggers --events > full_dump.sql

-- 5. LibÃ©rer le lock
UNLOCK TABLES;
```

```sql
-- SUR MARIADB (CIBLE)
-- --------------------

-- 1. Importer le dump initial
-- mariadb < full_dump.sql

-- 2. Configurer la rÃ©plication
CHANGE MASTER TO
    MASTER_HOST = 'mysql-source.example.com',
    MASTER_PORT = 3306,
    MASTER_USER = 'repl_mariadb',
    MASTER_PASSWORD = 'SecureRepl!Pass123',
    MASTER_LOG_FILE = 'mysql-bin.000042',
    MASTER_LOG_POS = 12345678,
    MASTER_SSL = 1;  -- TLS recommandÃ©

-- 3. DÃ©marrer la rÃ©plication
START SLAVE;

-- 4. VÃ©rifier le statut
SHOW SLAVE STATUS\G

-- Points Ã  vÃ©rifier :
-- Slave_IO_Running: Yes
-- Slave_SQL_Running: Yes
-- Seconds_Behind_Master: 0 (ou proche de 0)
```

### Script de monitoring de la rÃ©plication

```bash
#!/bin/bash
# monitor_replication.sh
# Monitoring de la rÃ©plication pendant la migration

MARIADB_HOST="${1:-localhost}"
MARIADB_USER="${2:-root}"
MARIADB_PASS="${3:-}"
ALERT_LAG_SECONDS=60
CHECK_INTERVAL=5

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

check_replication() {
    local status=$(mariadb -h $MARIADB_HOST -u $MARIADB_USER -p$MARIADB_PASS -N -e "
        SHOW SLAVE STATUS\G" 2>/dev/null)
    
    local io_running=$(echo "$status" | grep "Slave_IO_Running:" | awk '{print $2}')
    local sql_running=$(echo "$status" | grep "Slave_SQL_Running:" | awk '{print $2}')
    local lag=$(echo "$status" | grep "Seconds_Behind_Master:" | awk '{print $2}')
    local last_error=$(echo "$status" | grep "Last_Error:" | cut -d: -f2-)
    
    # Statut
    if [ "$io_running" != "Yes" ] || [ "$sql_running" != "Yes" ]; then
        log "ğŸ”´ CRITICAL: RÃ©plication arrÃªtÃ©e!"
        log "   IO Running: $io_running"
        log "   SQL Running: $sql_running"
        log "   Erreur: $last_error"
        return 2
    fi
    
    # Lag
    if [ "$lag" == "NULL" ]; then
        log "âš ï¸ WARNING: Lag indÃ©terminÃ©"
        return 1
    elif [ "$lag" -gt "$ALERT_LAG_SECONDS" ]; then
        log "âš ï¸ WARNING: Lag Ã©levÃ©: ${lag}s"
        return 1
    else
        log "âœ… OK: Lag: ${lag}s"
        return 0
    fi
}

# Boucle de monitoring
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "   MONITORING RÃ‰PLICATION"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "Host: $MARIADB_HOST"
log "Seuil d'alerte: ${ALERT_LAG_SECONDS}s"
log "Intervalle: ${CHECK_INTERVAL}s"
log "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

while true; do
    check_replication
    sleep $CHECK_INTERVAL
done
```

---

## Outils de migration online

### pt-online-schema-change (Percona Toolkit)

`pt-online-schema-change` permet de modifier le schÃ©ma sans bloquer les Ã©critures.

```bash
#!/bin/bash
# Utilisation de pt-online-schema-change

# Installation
# apt install percona-toolkit  # Debian/Ubuntu
# dnf install percona-toolkit  # RHEL/CentOS

# Exemple : Ajouter une colonne sans downtime
pt-online-schema-change \
    --alter "ADD COLUMN status VARCHAR(50) DEFAULT 'active'" \
    --host=localhost \
    --user=root \
    --password=secret \
    --execute \
    --progress=time,30 \
    --max-lag=10 \
    --check-slave-lag=mariadb-replica \
    --chunk-size=1000 \
    --critical-load="Threads_running=100" \
    D=mydb,t=large_table

# Options importantes :
# --execute          : ExÃ©cuter (sans = dry-run)
# --max-lag          : Pause si lag rÃ©plication > N secondes
# --chunk-size       : Nombre de lignes par batch
# --critical-load    : ArrÃªt si charge critique atteinte
# --progress         : Affichage progression
```

**Fonctionnement interne :**

```
Fonctionnement pt-online-schema-change
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. CrÃ©ation table temporaire avec nouveau schÃ©ma
   original_table â”€â”€â–¶ _original_table_new (avec ALTER)

2. CrÃ©ation des triggers pour synchronisation
   INSERT â†’ INSERT dans _new
   UPDATE â†’ UPDATE dans _new  
   DELETE â†’ DELETE dans _new

3. Copie des donnÃ©es par chunks
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ SELECT * FROM original_table                        â”‚
   â”‚ WHERE id BETWEEN 1 AND 1000                         â”‚
   â”‚ INSERT INTO _original_table_new ...                 â”‚
   â”‚                                                     â”‚
   â”‚ ... rÃ©pÃ©tÃ© par chunks jusqu'Ã  la fin                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Ã‰change atomique des tables
   RENAME TABLE original_table TO _original_table_old,
                _original_table_new TO original_table;

5. Suppression de l'ancienne table
   DROP TABLE _original_table_old;
```

### gh-ost (GitHub Online Schema Transformation)

`gh-ost` est une alternative Ã  pt-osc, sans triggers, utilisant le binlog.

```bash
#!/bin/bash
# Utilisation de gh-ost

# Installation
# wget https://github.com/github/gh-ost/releases/latest/download/gh-ost-binary-linux-amd64.tar.gz
# tar xzf gh-ost-binary-linux-amd64.tar.gz
# mv gh-ost /usr/local/bin/

# Exemple : Migration de colonne
gh-ost \
    --host=localhost \
    --port=3306 \
    --user=root \
    --password=secret \
    --database=mydb \
    --table=large_table \
    --alter="ADD COLUMN new_field INT DEFAULT 0" \
    --execute \
    --allow-on-master \
    --chunk-size=1000 \
    --max-lag-millis=1500 \
    --throttle-control-replicas="mariadb-replica:3306" \
    --panic-flag-file=/tmp/gh-ost-panic \
    --postpone-cut-over-flag-file=/tmp/gh-ost-postpone

# Options clÃ©s :
# --execute              : ExÃ©cuter (sinon dry-run)
# --allow-on-master      : ExÃ©cuter sur le master directement
# --max-lag-millis       : Seuil de lag en ms
# --panic-flag-file      : CrÃ©er ce fichier pour arrÃªter immÃ©diatement
# --postpone-cut-over-flag-file : Bloquer le cutover tant que le fichier existe
```

**Avantages de gh-ost vs pt-osc :**

| Aspect | pt-online-schema-change | gh-ost |
|--------|------------------------|--------|
| **MÃ©thode** | Triggers | Binlog |
| **Impact sur source** | ModÃ©rÃ© (triggers) | Faible |
| **ContrÃ´le cutover** | Automatique | Manuel possible |
| **Pause/Reprise** | LimitÃ©e | ComplÃ¨te |
| **Rollback** | Difficile | Plus facile |
| **ComplexitÃ©** | ModÃ©rÃ©e | Plus Ã©levÃ©e |

### Script de migration online avec gh-ost

```bash
#!/bin/bash
# zero_downtime_alter.sh
# Migration de schÃ©ma zero-downtime avec gh-ost

set -e

# Configuration
DB_HOST="localhost"
DB_PORT="3306"
DB_USER="root"
DB_PASS="secret"
DATABASE="mydb"
TABLE="$1"
ALTER="$2"

PANIC_FILE="/tmp/gh-ost-panic-${TABLE}"
POSTPONE_FILE="/tmp/gh-ost-postpone-${TABLE}"
LOG_DIR="/var/log/gh-ost"

# Validation
if [ -z "$TABLE" ] || [ -z "$ALTER" ]; then
    echo "Usage: $0 <table> <alter_statement>"
    echo "Example: $0 users 'ADD COLUMN email_verified BOOLEAN DEFAULT FALSE'"
    exit 1
fi

mkdir -p $LOG_DIR

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "   MIGRATION ZERO-DOWNTIME AVEC GH-OST"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Table: ${DATABASE}.${TABLE}"
echo "Alter: $ALTER"
echo ""

# Nettoyage des fichiers de contrÃ´le prÃ©cÃ©dents
rm -f $PANIC_FILE $POSTPONE_FILE

# CrÃ©ation du fichier de postpone pour contrÃ´le manuel du cutover
touch $POSTPONE_FILE
echo "ğŸ“Œ Fichier de postpone crÃ©Ã©: $POSTPONE_FILE"
echo "   Supprimer ce fichier pour dÃ©clencher le cutover"
echo ""

# Dry-run d'abord
echo "[1/3] Dry-run..."
gh-ost \
    --host=$DB_HOST \
    --port=$DB_PORT \
    --user=$DB_USER \
    --password=$DB_PASS \
    --database=$DATABASE \
    --table=$TABLE \
    --alter="$ALTER" \
    --chunk-size=1000 \
    --max-lag-millis=1500 \
    --verbose 2>&1 | tee $LOG_DIR/gh-ost-dryrun-${TABLE}.log

echo ""
read -p "Dry-run OK. Lancer la migration rÃ©elle? (yes/no): " confirm
if [ "$confirm" != "yes" ]; then
    echo "Migration annulÃ©e"
    rm -f $POSTPONE_FILE
    exit 0
fi

# ExÃ©cution rÃ©elle
echo ""
echo "[2/3] Migration en cours..."
echo "      Pour arrÃªter d'urgence: touch $PANIC_FILE"
echo "      Pour dÃ©clencher le cutover: rm $POSTPONE_FILE"
echo ""

gh-ost \
    --host=$DB_HOST \
    --port=$DB_PORT \
    --user=$DB_USER \
    --password=$DB_PASS \
    --database=$DATABASE \
    --table=$TABLE \
    --alter="$ALTER" \
    --execute \
    --allow-on-master \
    --chunk-size=1000 \
    --max-lag-millis=1500 \
    --panic-flag-file=$PANIC_FILE \
    --postpone-cut-over-flag-file=$POSTPONE_FILE \
    --initially-drop-ghost-table \
    --initially-drop-old-table \
    --verbose 2>&1 | tee $LOG_DIR/gh-ost-exec-${TABLE}.log &

GHOST_PID=$!

echo "gh-ost PID: $GHOST_PID"
echo ""

# Attendre que la copie soit terminÃ©e
echo "En attente de la fin de la copie des donnÃ©es..."
echo "Supprimez $POSTPONE_FILE quand prÃªt pour le cutover"
echo ""

wait $GHOST_PID
EXIT_CODE=$?

echo ""
echo "[3/3] Migration terminÃ©e"

if [ $EXIT_CODE -eq 0 ]; then
    echo "âœ… Migration rÃ©ussie!"
else
    echo "âŒ Migration Ã©chouÃ©e (code: $EXIT_CODE)"
    echo "Consulter les logs: $LOG_DIR/gh-ost-exec-${TABLE}.log"
fi

# Nettoyage
rm -f $PANIC_FILE $POSTPONE_FILE

exit $EXIT_CODE
```

---

## Orchestration du cutover

### ProcÃ©dure de cutover avec ProxySQL

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- ORCHESTRATION CUTOVER ZERO-DOWNTIME AVEC PROXYSQL
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Ã‰TAT INITIAL
-- MySQL Source : hostgroup 10 (writer), hostgroup 20 (reader)
-- MariaDB Target : hostgroup 30 (replica, reader facultatif)

-- Phase 1: VÃ©rification prÃ©-cutover
-- ---------------------------------

-- VÃ©rifier la santÃ© des serveurs
SELECT * FROM mysql_servers;

-- VÃ©rifier le lag de rÃ©plication (doit Ãªtre 0)
SELECT hostgroup_id, hostname, port, status, 
       (SELECT variable_value FROM stats_mysql_global WHERE variable_name='Queries') as queries
FROM mysql_servers WHERE hostgroup_id IN (10, 30);


-- Phase 2: PrÃ©parer le cutover
-- ----------------------------

-- RÃ©duire le max_connections temporairement pour drainer
UPDATE mysql_servers SET max_connections = 1 WHERE hostname = 'mysql-source' AND hostgroup_id = 10;
LOAD MYSQL SERVERS TO RUNTIME;

-- Attendre que les connexions se drainent (quelques secondes)
SELECT * FROM stats_mysql_connection_pool WHERE hostgroup IN (10);


-- Phase 3: Cutover (< 5 secondes)
-- -------------------------------

-- 3.1 Mettre MySQL source offline
UPDATE mysql_servers SET status = 'OFFLINE_SOFT' WHERE hostname = 'mysql-source';
LOAD MYSQL SERVERS TO RUNTIME;

-- 3.2 Attendre la synchronisation finale (lag = 0)
-- (vÃ©rifier manuellement ou via script)

-- 3.3 Promouvoir MariaDB
UPDATE mysql_servers SET hostgroup_id = 10, status = 'ONLINE', max_connections = 100 
WHERE hostname = 'mariadb-target' AND hostgroup_id = 30;
LOAD MYSQL SERVERS TO RUNTIME;

-- 3.4 VÃ©rification immÃ©diate
SELECT * FROM mysql_servers WHERE hostgroup_id IN (10, 20);


-- Phase 4: Post-cutover
-- ---------------------

-- VÃ©rifier que le trafic passe bien
SELECT hostgroup, srv_host, Queries, Bytes_data_sent
FROM stats_mysql_connection_pool;

-- Ajuster les connexions
UPDATE mysql_servers SET max_connections = 200 WHERE hostname = 'mariadb-target';
LOAD MYSQL SERVERS TO RUNTIME;

-- Sauvegarder la configuration
SAVE MYSQL SERVERS TO DISK;
```

### Script d'orchestration automatique

```python
#!/usr/bin/env python3
# cutover_orchestrator.py
# Orchestration automatique du cutover zero-downtime

import time
import sys
from datetime import datetime
from typing import Dict, Optional
import mysql.connector

class CutoverOrchestrator:
    """Orchestre le cutover zero-downtime"""
    
    def __init__(self, config: Dict):
        self.source_config = config['source']
        self.target_config = config['target']
        self.proxy_config = config['proxy']
        self.max_lag_seconds = config.get('max_lag_seconds', 5)
        self.drain_timeout_seconds = config.get('drain_timeout', 30)
    
    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] [{level}] {message}")
    
    def get_replication_lag(self) -> Optional[int]:
        """Obtient le lag de rÃ©plication sur la cible"""
        conn = mysql.connector.connect(**self.target_config)
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SHOW SLAVE STATUS")
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if result:
            lag = result.get('Seconds_Behind_Master')
            return int(lag) if lag is not None else None
        return None
    
    def wait_for_sync(self, timeout: int = 300) -> bool:
        """Attend que la rÃ©plication soit synchronisÃ©e"""
        self.log(f"Attente synchronisation (timeout: {timeout}s)...")
        start = time.time()
        
        while time.time() - start < timeout:
            lag = self.get_replication_lag()
            
            if lag is None:
                self.log("Lag indÃ©terminÃ©, rÃ©plication peut-Ãªtre arrÃªtÃ©e", "WARNING")
                return False
            
            if lag <= self.max_lag_seconds:
                self.log(f"âœ“ SynchronisÃ© (lag: {lag}s)")
                return True
            
            self.log(f"Lag actuel: {lag}s, attente...")
            time.sleep(1)
        
        self.log(f"Timeout synchronisation aprÃ¨s {timeout}s", "ERROR")
        return False
    
    def execute_proxy_command(self, command: str):
        """ExÃ©cute une commande sur ProxySQL"""
        conn = mysql.connector.connect(**self.proxy_config)
        cursor = conn.cursor()
        cursor.execute(command)
        conn.commit()
        cursor.close()
        conn.close()
    
    def drain_source(self) -> bool:
        """Draine les connexions de la source"""
        self.log("Drainage des connexions source...")
        
        # RÃ©duire les connexions max
        self.execute_proxy_command("""
            UPDATE mysql_servers 
            SET max_connections = 1 
            WHERE hostname = '{}' AND hostgroup_id = 10
        """.format(self.source_config['host']))
        self.execute_proxy_command("LOAD MYSQL SERVERS TO RUNTIME")
        
        # Attendre le drainage
        start = time.time()
        while time.time() - start < self.drain_timeout_seconds:
            # VÃ©rifier les connexions actives
            conn = mysql.connector.connect(**self.proxy_config)
            cursor = conn.cursor(dictionary=True)
            cursor.execute("""
                SELECT ConnUsed FROM stats_mysql_connection_pool 
                WHERE srv_host = '{}'
            """.format(self.source_config['host']))
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if result and result['ConnUsed'] <= 1:
                self.log("âœ“ Connexions drainÃ©es")
                return True
            
            time.sleep(1)
        
        self.log("Timeout drainage", "WARNING")
        return True  # Continuer quand mÃªme
    
    def stop_source_writes(self):
        """ArrÃªte les Ã©critures sur la source"""
        self.log("ArrÃªt des Ã©critures sur la source...")
        
        # Mettre la source offline dans ProxySQL
        self.execute_proxy_command("""
            UPDATE mysql_servers 
            SET status = 'OFFLINE_SOFT' 
            WHERE hostname = '{}'
        """.format(self.source_config['host']))
        self.execute_proxy_command("LOAD MYSQL SERVERS TO RUNTIME")
        
        self.log("âœ“ Source offline")
    
    def stop_target_replication(self):
        """ArrÃªte la rÃ©plication sur la cible"""
        self.log("ArrÃªt de la rÃ©plication sur la cible...")
        
        conn = mysql.connector.connect(**self.target_config)
        cursor = conn.cursor()
        cursor.execute("STOP SLAVE")
        cursor.close()
        conn.close()
        
        self.log("âœ“ RÃ©plication arrÃªtÃ©e")
    
    def promote_target(self):
        """Promeut la cible en primary"""
        self.log("Promotion de la cible en primary...")
        
        # Activer la cible comme writer dans ProxySQL
        self.execute_proxy_command("""
            UPDATE mysql_servers 
            SET hostgroup_id = 10, status = 'ONLINE', max_connections = 200 
            WHERE hostname = '{}'
        """.format(self.target_config['host']))
        self.execute_proxy_command("LOAD MYSQL SERVERS TO RUNTIME")
        self.execute_proxy_command("SAVE MYSQL SERVERS TO DISK")
        
        self.log("âœ“ Cible promue")
    
    def verify_cutover(self) -> bool:
        """VÃ©rifie que le cutover est effectif"""
        self.log("VÃ©rification du cutover...")
        
        conn = mysql.connector.connect(**self.proxy_config)
        cursor = conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT srv_host, status, hostgroup 
            FROM stats_mysql_connection_pool 
            WHERE hostgroup = 10
        """)
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        for row in results:
            if row['srv_host'] == self.target_config['host']:
                self.log(f"âœ“ Trafic routÃ© vers {row['srv_host']}")
                return True
        
        self.log("Cutover non vÃ©rifiÃ©", "ERROR")
        return False
    
    def execute_cutover(self) -> bool:
        """ExÃ©cute le cutover complet"""
        self.log("â•" * 60)
        self.log("   DÃ‰BUT DU CUTOVER ZERO-DOWNTIME")
        self.log("â•" * 60)
        
        cutover_start = time.time()
        
        try:
            # Phase 1: PrÃ©-cutover
            self.log("\n[Phase 1] PrÃ©-cutover")
            if not self.wait_for_sync(timeout=60):
                raise Exception("Synchronisation impossible")
            
            # Phase 2: Drainage
            self.log("\n[Phase 2] Drainage")
            self.drain_source()
            
            # Phase 3: Cutover (critique - mesurer le temps)
            self.log("\n[Phase 3] CUTOVER")
            cutover_moment = time.time()
            
            self.stop_source_writes()
            
            # Attente synchronisation finale
            time.sleep(2)  # Petit dÃ©lai pour les derniÃ¨res transactions
            
            if not self.wait_for_sync(timeout=30):
                raise Exception("Synchronisation finale Ã©chouÃ©e")
            
            self.stop_target_replication()
            self.promote_target()
            
            cutover_duration = time.time() - cutover_moment
            self.log(f"â±ï¸ DurÃ©e du cutover: {cutover_duration:.2f}s")
            
            # Phase 4: VÃ©rification
            self.log("\n[Phase 4] VÃ©rification")
            if not self.verify_cutover():
                raise Exception("VÃ©rification Ã©chouÃ©e")
            
            total_duration = time.time() - cutover_start
            
            self.log("\n" + "â•" * 60)
            self.log(f"   âœ… CUTOVER RÃ‰USSI EN {total_duration:.2f}s")
            self.log(f"   â±ï¸ Downtime effectif: {cutover_duration:.2f}s")
            self.log("â•" * 60)
            
            return True
            
        except Exception as e:
            self.log(f"\nâŒ ERREUR: {e}", "ERROR")
            self.log("Rollback peut Ãªtre nÃ©cessaire", "ERROR")
            return False


# Utilisation
if __name__ == '__main__':
    config = {
        'source': {
            'host': 'mysql-source',
            'port': 3306,
            'user': 'admin',
            'password': 'password'
        },
        'target': {
            'host': 'mariadb-target',
            'port': 3306,
            'user': 'admin',
            'password': 'password'
        },
        'proxy': {
            'host': 'proxysql',
            'port': 6032,
            'user': 'admin',
            'password': 'admin'
        },
        'max_lag_seconds': 5,
        'drain_timeout': 30
    }
    
    orchestrator = CutoverOrchestrator(config)
    success = orchestrator.execute_cutover()
    sys.exit(0 if success else 1)
```

---

## Gestion du split-brain

### Comprendre le split-brain

```
Situation de Split-Brain
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ã‰TAT NORMAL
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Load Balancer    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Primary (seul)   â”‚
                    â”‚    Accepte writes   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SPLIT-BRAIN (DANGER!)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Applications     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                     â”‚
                    â–¼                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Primary A     â”‚   â”‚   Primary B     â”‚
         â”‚ (se croit seul) â”‚   â”‚ (se croit seul) â”‚
         â”‚                 â”‚   â”‚                 â”‚
         â”‚ Write: id=1     â”‚   â”‚ Write: id=1     â”‚
         â”‚ data='AAA'      â”‚   â”‚ data='BBB'      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         
         âš ï¸ CONFLIT! MÃªme ID, donnÃ©es diffÃ©rentes
         âš ï¸ DonnÃ©es divergentes, rÃ©conciliation complexe
```

### PrÃ©vention du split-brain

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- MÃ‰CANISMES DE PRÃ‰VENTION DU SPLIT-BRAIN
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- 1. FENCING : Garantir qu'un seul primary peut Ã©crire
-- --------------------------------------------------

-- Utiliser super_read_only sur tous les serveurs par dÃ©faut
-- Seul le primary actif a super_read_only = OFF

-- Sur tous les serveurs (configuration par dÃ©faut)
SET GLOBAL super_read_only = ON;
SET GLOBAL read_only = ON;

-- Sur le primary uniquement (aprÃ¨s Ã©lection)
SET GLOBAL super_read_only = OFF;
SET GLOBAL read_only = OFF;


-- 2. QUORUM : DÃ©cision basÃ©e sur la majoritÃ©
-- -----------------------------------------
-- Utiliser Galera Cluster ou Group Replication pour
-- garantir qu'un primary n'est Ã©lu que s'il a la majoritÃ©


-- 3. STONITH (Shoot The Other Node In The Head)
-- ---------------------------------------------
-- Si deux primaries sont dÃ©tectÃ©s, un mÃ©canisme externe
-- (Pacemaker, script) force l'arrÃªt de l'un d'eux
```

### Script de dÃ©tection et rÃ©solution

```python
#!/usr/bin/env python3
# splitbrain_detector.py
# DÃ©tection et rÃ©solution des situations de split-brain

import mysql.connector
from typing import Dict, List, Optional
from datetime import datetime
import subprocess
import sys

class SplitBrainDetector:
    """DÃ©tecte et rÃ©sout les situations de split-brain"""
    
    def __init__(self, servers: List[Dict]):
        self.servers = servers
        self.alerts = []
    
    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] [{level}] {message}")
    
    def check_server_mode(self, server: Dict) -> Dict:
        """VÃ©rifie le mode d'un serveur (read_only, super_read_only)"""
        try:
            conn = mysql.connector.connect(
                host=server['host'],
                port=server.get('port', 3306),
                user=server['user'],
                password=server['password']
            )
            cursor = conn.cursor(dictionary=True)
            
            cursor.execute("SELECT @@read_only as ro, @@super_read_only as sro, @@server_id as id")
            result = cursor.fetchone()
            
            cursor.close()
            conn.close()
            
            return {
                'host': server['host'],
                'reachable': True,
                'read_only': bool(result['ro']),
                'super_read_only': bool(result['sro']),
                'server_id': result['id'],
                'is_writable': not bool(result['ro']) and not bool(result['sro'])
            }
            
        except Exception as e:
            return {
                'host': server['host'],
                'reachable': False,
                'error': str(e)
            }
    
    def detect_split_brain(self) -> bool:
        """DÃ©tecte une situation de split-brain"""
        self.log("VÃ©rification des serveurs...")
        
        writable_servers = []
        
        for server in self.servers:
            status = self.check_server_mode(server)
            
            if not status['reachable']:
                self.log(f"âš ï¸ {status['host']}: Non joignable - {status.get('error', 'Unknown')}", "WARNING")
                continue
            
            mode = "WRITABLE" if status['is_writable'] else "READ-ONLY"
            self.log(f"  {status['host']}: {mode}")
            
            if status['is_writable']:
                writable_servers.append(status)
        
        if len(writable_servers) > 1:
            self.log("", "")
            self.log("ğŸš¨ SPLIT-BRAIN DÃ‰TECTÃ‰!", "CRITICAL")
            self.log(f"   {len(writable_servers)} serveurs en mode WRITABLE:", "CRITICAL")
            for srv in writable_servers:
                self.log(f"   - {srv['host']} (server_id: {srv['server_id']})", "CRITICAL")
            return True
        
        if len(writable_servers) == 0:
            self.log("âš ï¸ Aucun serveur writable dÃ©tectÃ©!", "WARNING")
        
        return False
    
    def resolve_split_brain(self, keep_server: str):
        """RÃ©sout le split-brain en gardant un seul primary"""
        self.log(f"RÃ©solution: Conservation de {keep_server} comme primary")
        
        for server in self.servers:
            status = self.check_server_mode(server)
            
            if not status['reachable']:
                continue
            
            if server['host'] == keep_server:
                # Ce serveur reste le primary
                if not status['is_writable']:
                    self.log(f"Activation des Ã©critures sur {server['host']}")
                    self._set_writable(server, True)
            else:
                # Les autres passent en read-only
                if status['is_writable']:
                    self.log(f"Passage en read-only de {server['host']}")
                    self._set_writable(server, False)
        
        self.log("âœ“ Split-brain rÃ©solu")
    
    def _set_writable(self, server: Dict, writable: bool):
        """Configure le mode writable d'un serveur"""
        conn = mysql.connector.connect(
            host=server['host'],
            port=server.get('port', 3306),
            user=server['user'],
            password=server['password']
        )
        cursor = conn.cursor()
        
        if writable:
            cursor.execute("SET GLOBAL super_read_only = OFF")
            cursor.execute("SET GLOBAL read_only = OFF")
        else:
            cursor.execute("SET GLOBAL read_only = ON")
            cursor.execute("SET GLOBAL super_read_only = ON")
        
        cursor.close()
        conn.close()
    
    def fence_server(self, server: Dict):
        """STONITH - ArrÃªt forcÃ© d'un serveur"""
        self.log(f"ğŸ”« STONITH: ArrÃªt forcÃ© de {server['host']}", "WARNING")
        
        # Option 1: ArrÃªt via SSH
        try:
            subprocess.run(
                ["ssh", server['host'], "systemctl", "stop", "mariadb"],
                timeout=10,
                check=True
            )
            self.log(f"âœ“ {server['host']} arrÃªtÃ© via SSH")
        except Exception as e:
            self.log(f"Ã‰chec SSH: {e}", "ERROR")
            
            # Option 2: ArrÃªt via IPMI/iLO
            # subprocess.run(["ipmitool", "-H", server['ipmi'], "power", "off"])
            
            # Option 3: DÃ©sactivation rÃ©seau
            # self._disable_network(server)
    
    def monitor_loop(self, interval: int = 10):
        """Boucle de monitoring continu"""
        self.log("DÃ©marrage du monitoring split-brain")
        
        import time
        while True:
            if self.detect_split_brain():
                # Alerte et attente d'intervention
                self.log("Intervention manuelle requise!", "CRITICAL")
                # En production: envoyer alerte PagerDuty, etc.
            
            time.sleep(interval)


# Utilisation
if __name__ == '__main__':
    servers = [
        {'host': 'mariadb-1', 'port': 3306, 'user': 'admin', 'password': 'password'},
        {'host': 'mariadb-2', 'port': 3306, 'user': 'admin', 'password': 'password'},
        {'host': 'mariadb-3', 'port': 3306, 'user': 'admin', 'password': 'password'},
    ]
    
    detector = SplitBrainDetector(servers)
    
    if len(sys.argv) > 1 and sys.argv[1] == '--monitor':
        detector.monitor_loop()
    else:
        if detector.detect_split_brain():
            print("\nPour rÃ©soudre, exÃ©cuter:")
            print(f"  python {sys.argv[0]} --resolve <hostname_a_garder>")
        
        if len(sys.argv) > 2 and sys.argv[1] == '--resolve':
            detector.resolve_split_brain(sys.argv[2])
```

---

## SpÃ©cificitÃ©s MariaDB 11.8 ğŸ†•

### Migration des System-Versioned Tables

MariaDB 11.8 modifie le format de stockage des timestamps dans les tables temporelles. Une migration zero-downtime de ces tables nÃ©cessite une attention particuliÃ¨re.

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- MIGRATION ZERO-DOWNTIME DES SYSTEM-VERSIONED TABLES
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Identification des tables system-versioned
SELECT 
    table_schema,
    table_name,
    create_options
FROM information_schema.tables
WHERE create_options LIKE '%VERSIONED%'
ORDER BY table_schema, table_name;

-- Exemple de sortie:
-- | app_db | contracts    | row_format=DYNAMIC with system versioning |
-- | app_db | audit_events | row_format=DYNAMIC with system versioning |

-- APPROCHE 1: Migration pendant la rÃ©plication
-- --------------------------------------------
-- Les tables sont automatiquement converties lors de la rÃ©plication
-- si la cible est en 11.8. VÃ©rifier aprÃ¨s migration:

SHOW CREATE TABLE contracts\G
-- VÃ©rifier que ROW_START et ROW_END utilisent le nouveau format

-- APPROCHE 2: Rebuild explicite post-migration
-- --------------------------------------------
-- Si nÃ©cessaire, reconstruire les tables pour le nouveau format

-- Option A: ALTER TABLE simple (prend un lock)
ALTER TABLE contracts ENGINE=InnoDB;

-- Option B: pt-online-schema-change (zero-downtime)
-- pt-online-schema-change --alter "ENGINE=InnoDB" D=app_db,t=contracts --execute

-- Option C: gh-ost
-- gh-ost --alter "ENGINE=InnoDB" --database=app_db --table=contracts --execute
```

### Gestion du TLS par dÃ©faut

```bash
#!/bin/bash
# prepare_tls_migration.sh
# PrÃ©pare la migration vers MariaDB 11.8 avec TLS par dÃ©faut

# VÃ©rifier les connexions actuelles sans TLS
mariadb -e "
SELECT 
    user, 
    host, 
    ssl_type,
    CASE WHEN ssl_type = '' THEN 'NO TLS' ELSE 'TLS' END as connection_security
FROM mysql.user 
WHERE user NOT IN ('mariadb.sys', 'mysql')
ORDER BY ssl_type, user;
"

# GÃ©nÃ©rer des certificats si nÃ©cessaire
if [ ! -f /etc/mysql/ssl/server-cert.pem ]; then
    echo "GÃ©nÃ©ration des certificats TLS..."
    
    mkdir -p /etc/mysql/ssl
    cd /etc/mysql/ssl
    
    # CA
    openssl genrsa 2048 > ca-key.pem
    openssl req -new -x509 -nodes -days 3650 -key ca-key.pem -out ca-cert.pem \
        -subj "/CN=MariaDB CA"
    
    # Server
    openssl req -newkey rsa:2048 -nodes -keyout server-key.pem -out server-req.pem \
        -subj "/CN=MariaDB Server"
    openssl x509 -req -in server-req.pem -days 3650 -CA ca-cert.pem -CAkey ca-key.pem \
        -set_serial 01 -out server-cert.pem
    
    # Client
    openssl req -newkey rsa:2048 -nodes -keyout client-key.pem -out client-req.pem \
        -subj "/CN=MariaDB Client"
    openssl x509 -req -in client-req.pem -days 3650 -CA ca-cert.pem -CAkey ca-key.pem \
        -set_serial 02 -out client-cert.pem
    
    chown mysql:mysql *.pem
    chmod 600 *-key.pem
    
    echo "âœ“ Certificats gÃ©nÃ©rÃ©s"
fi

# Configuration MariaDB pour TLS
cat >> /etc/mysql/mariadb.conf.d/99-tls.cnf << 'EOF'
[mariadbd]
ssl_cert = /etc/mysql/ssl/server-cert.pem
ssl_key = /etc/mysql/ssl/server-key.pem
ssl_ca = /etc/mysql/ssl/ca-cert.pem

# Pour 11.8, TLS est activÃ© par dÃ©faut
# DÃ©sactiver temporairement si nÃ©cessaire pendant la migration:
# require_secure_transport = OFF
EOF

echo "Configuration TLS ajoutÃ©e"
```

---

## ScÃ©narios rÃ©els

### ScÃ©nario 1 : E-commerce haute disponibilitÃ©

**Contexte :** Site e-commerce avec 50 000 utilisateurs actifs, 500 transactions/minute, SLA 99.95%.

```
Architecture E-commerce
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CDN + WAF                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Load Balancer (HAProxy)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â”‚   App x3    â”‚             â”‚   App x3    â”‚
       â”‚  (Zone A)   â”‚             â”‚  (Zone B)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”‚                           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         ProxySQL          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                  â”‚                  â”‚
         â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MySQL Primary  â”‚ â”‚ MySQL Replica 1 â”‚ â”‚ MySQL Replica 2 â”‚
â”‚   (Zone A)      â”‚ â”‚   (Zone B)      â”‚ â”‚   (Zone B)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**StratÃ©gie de migration :**

```yaml
# Plan de migration E-commerce
phases:
  1_preparation:
    duration: "2 semaines"
    tasks:
      - DÃ©ployer MariaDB 11.8 en replica
      - Configurer la rÃ©plication depuis MySQL
      - Tests de charge sur replica
      - Formation Ã©quipes
  
  2_canary:
    duration: "1 semaine"
    tasks:
      - Router 5% du trafic lecture vers MariaDB
      - Monitoring intensif
      - Validation mÃ©triques (latence, erreurs)
  
  3_progressive:
    duration: "1 semaine"
    tasks:
      - Augmenter progressivement: 10%, 25%, 50%
      - Validation Ã  chaque Ã©tape
  
  4_cutover:
    duration: "30 minutes (fenÃªtre)"
    tasks:
      - Cutover des Ã©critures vers MariaDB
      - Downtime cible: < 10 secondes
    
    cutover_procedure:
      - "00:00 - Annonce maintenance"
      - "00:05 - Drainage connexions MySQL"
      - "00:06 - SET read_only=ON sur MySQL"
      - "00:07 - Attente sync (lag=0)"
      - "00:08 - STOP SLAVE sur MariaDB"
      - "00:08 - Promotion MariaDB via ProxySQL"
      - "00:09 - VÃ©rification trafic"
      - "00:10 - Fin cutover"
  
  5_validation:
    duration: "24 heures"
    tasks:
      - Monitoring 24/7
      - MySQL en standby pour rollback
      - Validation business

rollback_trigger:
  - "Erreur rate > 1%"
  - "Latence P95 > 500ms"
  - "Transactions Ã©chouÃ©es > 10/min"
```

**RÃ©sultat obtenu :**
- Downtime effectif : 7 secondes
- Aucune transaction perdue
- Performance amÃ©liorÃ©e de 15% (optimizer MariaDB)

### ScÃ©nario 2 : SaaS multi-tenant

**Contexte :** Application SaaS B2B, 200 tenants, base de 2 TB, migrations de schÃ©ma frÃ©quentes.

```python
#!/usr/bin/env python3
# saas_migration_orchestrator.py
# Migration tenant par tenant pour SaaS

from typing import List, Dict
import time

class SaaSMigrationOrchestrator:
    """Orchestre la migration tenant par tenant"""
    
    def __init__(self, tenants: List[Dict], source_config: Dict, target_config: Dict):
        self.tenants = tenants
        self.source = source_config
        self.target = target_config
        self.migrated = []
        self.failed = []
    
    def migrate_tenant(self, tenant: Dict) -> bool:
        """Migre un tenant individuel"""
        tenant_id = tenant['id']
        database = tenant['database']
        
        print(f"\n[Tenant {tenant_id}] DÃ©but migration de {database}")
        
        try:
            # 1. CrÃ©er la rÃ©plication pour ce tenant
            self._setup_tenant_replication(tenant)
            
            # 2. Attendre synchronisation
            self._wait_tenant_sync(tenant)
            
            # 3. Cutover du tenant (< 5s)
            self._cutover_tenant(tenant)
            
            # 4. VÃ©rification
            if self._verify_tenant(tenant):
                self.migrated.append(tenant)
                print(f"[Tenant {tenant_id}] âœ“ Migration rÃ©ussie")
                return True
            else:
                raise Exception("VÃ©rification Ã©chouÃ©e")
                
        except Exception as e:
            print(f"[Tenant {tenant_id}] âœ— Erreur: {e}")
            self._rollback_tenant(tenant)
            self.failed.append({'tenant': tenant, 'error': str(e)})
            return False
    
    def migrate_all(self, batch_size: int = 5, pause_between_batches: int = 60):
        """Migre tous les tenants par batches"""
        
        print("=" * 60)
        print(f"Migration de {len(self.tenants)} tenants")
        print(f"Batch size: {batch_size}")
        print("=" * 60)
        
        for i in range(0, len(self.tenants), batch_size):
            batch = self.tenants[i:i+batch_size]
            print(f"\n--- Batch {i//batch_size + 1} ({len(batch)} tenants) ---")
            
            for tenant in batch:
                self.migrate_tenant(tenant)
            
            # Pause entre batches pour observation
            if i + batch_size < len(self.tenants):
                print(f"\nPause de {pause_between_batches}s avant prochain batch...")
                time.sleep(pause_between_batches)
        
        # Rapport final
        print("\n" + "=" * 60)
        print("RAPPORT DE MIGRATION")
        print("=" * 60)
        print(f"Total tenants: {len(self.tenants)}")
        print(f"MigrÃ©s avec succÃ¨s: {len(self.migrated)}")
        print(f"Ã‰checs: {len(self.failed)}")
        
        if self.failed:
            print("\nTenants en Ã©chec:")
            for f in self.failed:
                print(f"  - {f['tenant']['id']}: {f['error']}")
    
    def _setup_tenant_replication(self, tenant: Dict):
        """Configure la rÃ©plication pour un tenant"""
        # ImplÃ©mentation spÃ©cifique Ã  l'architecture
        pass
    
    def _wait_tenant_sync(self, tenant: Dict):
        """Attend la synchronisation du tenant"""
        pass
    
    def _cutover_tenant(self, tenant: Dict):
        """Cutover du tenant vers MariaDB"""
        pass
    
    def _verify_tenant(self, tenant: Dict) -> bool:
        """VÃ©rifie la migration du tenant"""
        return True
    
    def _rollback_tenant(self, tenant: Dict):
        """Rollback d'un tenant en cas d'erreur"""
        pass


# Utilisation
if __name__ == '__main__':
    tenants = [
        {'id': 'tenant_001', 'database': 'saas_tenant_001', 'tier': 'enterprise'},
        {'id': 'tenant_002', 'database': 'saas_tenant_002', 'tier': 'standard'},
        # ... 200 tenants
    ]
    
    # Trier par importance (enterprise en dernier pour plus de tests)
    tenants.sort(key=lambda t: 0 if t['tier'] == 'standard' else 1)
    
    orchestrator = SaaSMigrationOrchestrator(
        tenants=tenants,
        source_config={'host': 'mysql-primary'},
        target_config={'host': 'mariadb-primary'}
    )
    
    orchestrator.migrate_all(batch_size=10, pause_between_batches=120)
```

---

## Checklist migration zero-downtime

```markdown
## CHECKLIST MIGRATION ZERO-DOWNTIME

### PrÃ©-requis
- [ ] Architecture compatible (proxy, rÃ©plication)
- [ ] MariaDB 11.8 installÃ© et configurÃ© sur cible
- [ ] RÃ©plication source â†’ cible fonctionnelle
- [ ] Tests de performance validÃ©s sur cible
- [ ] Plan de rollback documentÃ© et testÃ©
- [ ] Ã‰quipe formÃ©e et disponible
- [ ] Communication planifiÃ©e

### J-1
- [ ] VÃ©rification finale de la rÃ©plication (lag = 0)
- [ ] Test du script de cutover en staging
- [ ] Backup complet de la source
- [ ] Notification aux Ã©quipes
- [ ] VÃ©rification des alertes et monitoring

### Jour J - PrÃ©-cutover
- [ ] RÃ©union kick-off avec toutes les Ã©quipes
- [ ] VÃ©rification santÃ© source et cible
- [ ] Lag de rÃ©plication < 1 seconde
- [ ] Connexions ProxySQL nominales
- [ ] War room ouvert

### Cutover
- [ ] Annonce dÃ©but cutover
- [ ] Drainage des connexions source
- [ ] Activation read_only sur source
- [ ] Attente synchronisation finale
- [ ] STOP SLAVE sur cible
- [ ] Promotion cible via proxy
- [ ] VÃ©rification trafic routÃ©
- [ ] Chrono du downtime effectif

### Post-cutover immÃ©diat
- [ ] VÃ©rification applications connectÃ©es
- [ ] MÃ©triques nominales (latence, erreurs)
- [ ] Test transactions critiques
- [ ] Communication "cutover rÃ©ussi"

### H+1 Ã  H+24
- [ ] Monitoring renforcÃ©
- [ ] Source disponible pour rollback
- [ ] Analyse des logs d'erreur
- [ ] Validation Ã©quipes mÃ©tier

### Post-migration
- [ ] Documentation mise Ã  jour
- [ ] DÃ©sactivation source (aprÃ¨s pÃ©riode de sÃ©curitÃ©)
- [ ] Retour d'expÃ©rience (post-mortem)
- [ ] Mise Ã  jour des runbooks
```

---

## âœ… Points clÃ©s Ã  retenir

- **Zero-downtime** signifie gÃ©nÃ©ralement < 30 secondes d'interruption, pas zÃ©ro absolu
- La **rÃ©plication** est le fondement de toute migration zero-downtime vers MariaDB
- **ProxySQL** ou Ã©quivalent est essentiel pour orchestrer le cutover instantanÃ©ment
- **pt-online-schema-change** et **gh-ost** permettent les modifications de schÃ©ma sans lock
- Le **split-brain** est le risque majeur Ã  prÃ©venir avec des mÃ©canismes de fencing
- MariaDB 11.8 nÃ©cessite une attention particuliÃ¨re pour les **System-Versioned Tables** et le **TLS par dÃ©faut**
- Toujours avoir un **plan de rollback testÃ©** mÃªme pour les migrations zero-downtime
- Le **monitoring continu** pendant et aprÃ¨s la migration est critique

---

## ğŸ”— Ressources et rÃ©fÃ©rences

- [ğŸ“– ProxySQL Documentation](https://proxysql.com/documentation/)
- [ğŸ“– pt-online-schema-change](https://docs.percona.com/percona-toolkit/pt-online-schema-change.html)
- [ğŸ“– gh-ost Documentation](https://github.com/github/gh-ost)
- [ğŸ“– MariaDB Replication](https://mariadb.com/kb/en/replication/)
- [ğŸ“– MariaDB Galera Cluster](https://mariadb.com/kb/en/galera-cluster/)
- [ğŸ“– Blue-Green Deployments](https://martinfowler.com/bliki/BlueGreenDeployment.html)

---

## â¡ï¸ Section suivante

**[19.9 Migration des System-Versioned Tables](./09-migration-system-versioned-tables.md)** : Nous approfondirons la migration des tables temporelles vers MariaDB 11.8, incluant le nouveau format de timestamp, les stratÃ©gies de conversion, et la prÃ©servation de l'historique.

â­ï¸ [Migration System-Versioned Tables (changement format timestamp 11.8)](/19-migration-compatibilite/09-migration-system-versioned-tables.md)
