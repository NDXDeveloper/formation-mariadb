ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 13.8 Failover et Switchover

> **Niveau** : AvancÃ©  
> **DurÃ©e estimÃ©e** : 2.5-3 heures  
> **PrÃ©requis** : 
> - Sections 13.1 Ã  13.7 (RÃ©plication complÃ¨te)
> - ComprÃ©hension des architectures haute disponibilitÃ©
> - ExpÃ©rience en gestion d'incidents production
> - Connaissance des SLA et mÃ©triques RTO/RPO

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- DiffÃ©rencier failover (non planifiÃ©) et switchover (planifiÃ©)
- ExÃ©cuter un failover manuel en cas de panne Primary
- Planifier et exÃ©cuter un switchover sans perte de donnÃ©es
- Utiliser GTID pour simplifier les opÃ©rations de basculement
- Automatiser le failover avec des outils spÃ©cialisÃ©s
- GÃ©rer la reconfiguration des applications et clients
- Tester rÃ©guliÃ¨rement les procÃ©dures de failover
- DÃ©finir et respecter des objectifs RTO/RPO

---

## Introduction

Le **failover** et le **switchover** sont deux opÃ©rations essentielles pour assurer la **continuitÃ© de service** d'une infrastructure MariaDB rÃ©pliquÃ©e.

### DÃ©finitions

**Failover (basculement d'urgence)** :
```
Situation : Primary tombe en panne (crash, hardware failure)
Action : Promotion d'urgence d'un Replica en nouveau Primary
Nature : NON PLANIFIÃ‰E, rÃ©active
Objectif : Minimiser downtime (RTO)
Risque : Potentielle perte de donnÃ©es (RPO > 0)
```

**Switchover (basculement planifiÃ©)** :
```
Situation : Maintenance planifiÃ©e, upgrade, migration
Action : Promotion ordonnÃ©e d'un Replica en nouveau Primary
Nature : PLANIFIÃ‰E, contrÃ´lÃ©e
Objectif : Zero downtime, zero data loss
Risque : Minimal (procÃ©dure testÃ©e)
```

### Comparaison

| Aspect | Failover | Switchover |
|--------|----------|------------|
| **DÃ©clencheur** | Panne inattendue | OpÃ©ration planifiÃ©e |
| **Urgence** | ğŸ”´ Critique | ğŸŸ¢ ContrÃ´lÃ©e |
| **Downtime** | Minimiser (1-5min) | Quasi-zero (< 30s) |
| **Perte donnÃ©es** | Possible (RPO > 0) | Zero (RPO = 0) |
| **ComplexitÃ©** | Ã‰levÃ©e (stress) | Moyenne (procÃ©dure) |
| **Testing** | Simulation difficile | Tests rÃ©guliers possibles |
| **Rollback** | Complexe | Possible |

### Architecture de rÃ©fÃ©rence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TOPOLOGIE HAUTE DISPONIBILITÃ‰                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                    â”‚   PRIMARY    â”‚                         â”‚
â”‚                    â”‚  (Active)    â”‚                         â”‚
â”‚                    â”‚              â”‚                         â”‚
â”‚                    â”‚  GTID: 0-1-  â”‚                         â”‚
â”‚                    â”‚  1000        â”‚                         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                           â”‚                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚            â”‚            â”‚                    â”‚
â”‚              â–¼            â–¼            â–¼                    â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚      â”‚ Replica 1  â”‚ â”‚ Replica 2  â”‚ â”‚ Replica 3  â”‚           â”‚
â”‚      â”‚ (Standby)  â”‚ â”‚ (Reads)    â”‚ â”‚ (Backup)   â”‚           â”‚
â”‚      â”‚            â”‚ â”‚            â”‚ â”‚            â”‚           â”‚
â”‚      â”‚ GTID: 0-1- â”‚ â”‚ GTID: 0-1- â”‚ â”‚ GTID: 0-1- â”‚           â”‚
â”‚      â”‚ 1000       â”‚ â”‚ 999        â”‚ â”‚ 998        â”‚           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                             â”‚
â”‚      Failover Scenario:                                     â”‚
â”‚      Primary CRASH ğŸ’¥                                       â”‚
â”‚      â†’ Promote Replica 1 (most up-to-date)                  â”‚
â”‚      â†’ Reconfigure Replica 2, 3 to new Primary              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## MÃ©triques Critiques

### RTO (Recovery Time Objective)

**DÃ©finition** : Temps maximum acceptable de downtime

```
RTO = Temps de dÃ©tection + Temps de dÃ©cision + Temps d'exÃ©cution

Exemple:
- DÃ©tection panne: 30 secondes (monitoring)
- DÃ©cision promotion: 30 secondes (manuel ou auto)
- ExÃ©cution failover: 2 minutes
- RTO total = 3 minutes
```

**Objectifs typiques** :

| CriticitÃ© | RTO Target | StratÃ©gie |
|-----------|------------|-----------|
| Critique (e-commerce) | < 1 minute | Failover automatique |
| Production standard | < 5 minutes | Failover manuel supervisÃ© |
| Non-critique | < 30 minutes | Failover manuel |

### RPO (Recovery Point Objective)

**DÃ©finition** : QuantitÃ© maximale acceptable de perte de donnÃ©es

```
RPO = Temps entre derniÃ¨re transaction confirmÃ©e et crash

Avec rÃ©plication asynchrone:
RPO = Lag de rÃ©plication au moment du crash
    = Secondes_Behind_Master

Avec rÃ©plication semi-synchrone:
RPO = 0 (zero data loss)
```

**Calcul** :

```sql
-- Estimer RPO actuel
SELECT 
  IFNULL(Seconds_Behind_Master, 'N/A') AS current_lag_sec,
  CASE 
    WHEN Seconds_Behind_Master IS NULL 
      THEN 'INFINITE (not replicating)'
    WHEN Seconds_Behind_Master = 0 
      THEN 'ZERO (synchronized)'
    WHEN Seconds_Behind_Master < 5 
      THEN 'LOW (< 5s data loss risk)'
    WHEN Seconds_Behind_Master < 60 
      THEN 'MEDIUM (< 1min data loss risk)'
    ELSE 'HIGH (> 1min data loss risk)'
  END AS rpo_risk
FROM information_schema.SLAVE_STATUS;
```

**Objectifs typiques** :

| CriticitÃ© | RPO Target | StratÃ©gie |
|-----------|------------|-----------|
| Financier | 0 (zero loss) | Semi-sync + validation |
| Critique | < 5 secondes | Semi-sync ou async faible lag |
| Standard | < 1 minute | Async avec monitoring |
| Non-critique | < 5 minutes | Async |

---

## Failover (Basculement d'Urgence)

### ScÃ©nario type

```
Timeline d'un failover:

T0: 14:35:00 - Primary fonctionne normalement
               Application Ã©crit/lit sur Primary
               Replicas rÃ©pliquent (lag: 2s)

T1: 14:35:15 - Primary CRASH ğŸ’¥
               (Cause: hardware failure, kernel panic, etc.)
               
T2: 14:35:30 - Monitoring dÃ©tecte Primary DOWN
               Alerte envoyÃ©e aux DBAs

T3: 14:35:45 - DÃ©cision: Promouvoir Replica1
               (Replica le plus Ã  jour)

T4: 14:36:00 - ExÃ©cution failover
               - STOP SLAVE sur Replica1
               - Promotion Replica1 â†’ New Primary
               - Reconfiguration Replica2, Replica3

T5: 14:37:00 - Applications basculÃ©es vers New Primary
               Service restaurÃ© âœ“

Downtime total: 2 minutes (T1 â†’ T5)
Data loss: ~2 secondes (lag au moment T1)
```

### ProcÃ©dure manuelle de failover

**Ã‰tape 1 : Identifier le Replica Ã  promouvoir**

```sql
-- Sur chaque Replica: VÃ©rifier la position GTID
SELECT 
  @@hostname AS replica,
  @@gtid_slave_pos AS gtid_position
FROM dual;

-- Output:
-- +----------+---------------+
-- | replica  | gtid_position |
-- +----------+---------------+
-- | replica1 | 0-1-1000      |  â† Plus Ã  jour
-- | replica2 | 0-1-999       |
-- | replica3 | 0-1-995       |
-- +----------+---------------+

-- Choisir replica1 (position la plus avancÃ©e)
```

**Sans GTID** : Comparer positions binlog

```sql
-- Sur chaque Replica
SELECT 
  @@hostname,
  Master_Log_File,
  Read_Master_Log_Pos
FROM information_schema.SLAVE_STATUS;

-- Choisir le Replica avec:
-- 1. Fichier binlog le plus rÃ©cent
-- 2. Si mÃªme fichier, position la plus Ã©levÃ©e
```

**Ã‰tape 2 : Promouvoir le Replica**

```sql
-- Sur Replica1 (futur nouveau Primary)

-- 1. ArrÃªter la rÃ©plication
STOP SLAVE;

-- 2. VÃ©rifier qu'aucune rÃ©plication ne tourne
SHOW SLAVE STATUS\G
-- Doit Ãªtre vide ou Slave_IO_Running: No

-- 3. RÃ©initialiser statut rÃ©plication
RESET SLAVE ALL;

-- 4. DÃ©sactiver read-only
SET GLOBAL read_only = OFF;
SET GLOBAL super_read_only = OFF;

-- 5. (Optionnel) Flush logs
FLUSH LOGS;

-- 6. Replica1 est maintenant le nouveau Primary âœ“
SELECT @@read_only;  -- Doit Ãªtre 0
```

**Ã‰tape 3 : Reconfigurer les autres Replicas**

```sql
-- Sur Replica2 et Replica3

-- 1. ArrÃªter rÃ©plication de l'ancien Primary
STOP SLAVE;

-- 2. Pointer vers le nouveau Primary (Replica1)
CHANGE MASTER TO
  MASTER_HOST = 'replica1.example.com',  -- Nouveau Primary
  MASTER_USER = 'repl_user',
  MASTER_PASSWORD = 'password',
  MASTER_USE_GTID = slave_pos;  -- Avec GTID: automatique !

-- Sans GTID, calculer position:
-- (Complexe, nÃ©cessite analyse binlog du nouveau Primary)

-- 3. RedÃ©marrer rÃ©plication
START SLAVE;

-- 4. VÃ©rifier
SHOW SLAVE STATUS\G
-- Slave_IO_Running: Yes
-- Slave_SQL_Running: Yes
```

**Ã‰tape 4 : Basculer les applications**

```bash
# MÃ©thode 1: DNS (lent, TTL)
# Changer A record: primary.example.com â†’ replica1_ip

# MÃ©thode 2: Virtual IP (rapide)
# DÃ©placer VIP vers replica1
ip addr add 192.168.1.100/24 dev eth0  # Sur replica1
arping -c 3 -S 192.168.1.100 -i eth0  # Annonce ARP

# MÃ©thode 3: HAProxy/ProxySQL (instantanÃ©)
# Reconfigurer backend
```

**Ã‰tape 5 : Validation**

```sql
-- Sur nouveau Primary
-- Tester Ã©criture
CREATE DATABASE failover_test;
USE failover_test;
CREATE TABLE test (id INT, ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP);
INSERT INTO test (id) VALUES (1);

-- Sur Replicas: VÃ©rifier rÃ©plication
SELECT * FROM failover_test.test;
-- Doit afficher la ligne
```

### ProcÃ©dure avec GTID

**Avantage dÃ©cisif** : Positions calculÃ©es automatiquement

```sql
-- Sur les Replicas (Replica2, Replica3)
-- Reconfiguration triviale avec GTID:

STOP SLAVE;

CHANGE MASTER TO
  MASTER_HOST = 'new-primary.example.com',
  MASTER_USE_GTID = slave_pos;  -- Magic ! âœ¨

START SLAVE;

-- MariaDB calcule automatiquement:
-- "Je suis Ã  0-1-999, je demande Ã  partir de 0-1-1000"
-- Zero calcul de position manuel !
```

**Sans GTID** : ProcÃ©dure complexe et error-prone

```bash
# 1. Sur nouveau Primary: Trouver position Ã©quivalente
# Analyser binlog, comparer Ã©vÃ©nements, calculer offset
# â†’ Complexe, long, risque d'erreur

# 2. Configuration manuelle
CHANGE MASTER TO
  MASTER_HOST = 'new-primary.example.com',
  MASTER_LOG_FILE = 'new-primary-bin.000001',  # Quel fichier ?
  MASTER_LOG_POS = 12345;  # Quelle position ? âš ï¸

# Risque: Mauvaise position â†’ Skip ou duplicate transactions
```

### Gestion du Primary en panne

```sql
-- Lorsque l'ancien Primary revient (aprÃ¨s rÃ©paration)

-- Option A: Le transformer en Replica
-- Sur ancien Primary (maintenant Replica)
SET GLOBAL read_only = ON;
SET GLOBAL super_read_only = ON;

CHANGE MASTER TO
  MASTER_HOST = 'new-primary.example.com',
  MASTER_USER = 'repl_user',
  MASTER_PASSWORD = 'password',
  MASTER_USE_GTID = slave_pos;

START SLAVE;

-- Option B: Le rÃ©initialiser complÃ¨tement
-- (Si donnÃ©es corrompues)
# Dump depuis nouveau Primary
mysqldump ... > full_dump.sql
# Restaurer sur ancien Primary
mariadb < full_dump.sql
# Puis configurer comme Replica (Option A)
```

---

## Switchover (Basculement PlanifiÃ©)

### ScÃ©nario type

```
Timeline d'un switchover:

T0: Planification
    - Objectif: Migrer vers nouveau serveur
    - FenÃªtre de maintenance: Dimanche 2h-4h

T1: 01:55 - PrÃ©paration
    - VÃ©rification Replicas synchronisÃ©s
    - Tests prÃ©-migration

T2: 02:00 - DÃ©but switchover
    - Applications en mode read-only
    - Drain Ã©critures

T3: 02:02 - Synchronisation finale
    - Attente lag = 0 sur tous Replicas

T4: 02:03 - Basculement
    - Promotion nouveau Primary
    - Reconfiguration

T5: 02:05 - Applications rebasculent
    - Mode read-write restaurÃ©
    - Validation

Downtime: ~5 minutes (lecture seule: 3 min, full down: 2 min)
Data loss: ZERO (planifiÃ©, synchronisation validÃ©e)
```

### ProcÃ©dure de switchover

**Ã‰tape 1 : PrÃ©-vÃ©rifications**

```sql
-- Sur le Primary actuel
-- VÃ©rifier santÃ© globale
SELECT 
  @@hostname AS primary,
  COUNT(*) AS active_connections
FROM information_schema.PROCESSLIST
WHERE command != 'Sleep';

-- VÃ©rifier binlog actif
SELECT @@log_bin, @@binlog_format;

-- Sur chaque Replica
-- VÃ©rifier rÃ©plication saine
SELECT 
  @@hostname AS replica,
  Slave_IO_Running,
  Slave_SQL_Running,
  Seconds_Behind_Master,
  Gtid_Slave_Pos
FROM information_schema.SLAVE_STATUS;

-- Tous doivent Ãªtre:
-- IO/SQL: Yes/Yes
-- Lag: 0 ou trÃ¨s faible (< 5s)
```

**Ã‰tape 2 : Mettre applications en read-only**

```sql
-- Sur Primary actuel
-- Bloquer nouvelles Ã©critures (sauf SUPER)
SET GLOBAL read_only = ON;

-- Attendre que transactions en cours se terminent
SELECT COUNT(*) FROM information_schema.PROCESSLIST 
WHERE command != 'Sleep' AND user != 'root';
-- Attendre que COUNT = 0

-- VÃ©rifier plus aucune Ã©criture
SHOW MASTER STATUS;
-- Noter la position finale
```

**Alternative application-level** :

```bash
# Mettre load balancer en mode maintenance
haproxy -sf $(cat /var/run/haproxy.pid)

# Ou via API application
curl -X POST https://app.example.com/api/maintenance/enable
```

**Ã‰tape 3 : Synchronisation finale**

```sql
-- Sur tous les Replicas
-- Attendre synchronisation complÃ¨te
SELECT 
  @@hostname,
  Seconds_Behind_Master
FROM information_schema.SLAVE_STATUS;

-- Attendre que TOUS affichent: 0

-- Script d'attente automatique
DELIMITER //
CREATE PROCEDURE wait_for_sync(max_wait_sec INT)
BEGIN
  DECLARE lag INT DEFAULT 999;
  DECLARE elapsed INT DEFAULT 0;
  
  WHILE lag > 0 AND elapsed < max_wait_sec DO
    SELECT IFNULL(Seconds_Behind_Master, 0) INTO lag
    FROM information_schema.SLAVE_STATUS;
    
    IF lag > 0 THEN
      DO SLEEP(1);
      SET elapsed = elapsed + 1;
    END IF;
  END WHILE;
  
  IF lag > 0 THEN
    SIGNAL SQLSTATE '45000' 
      SET MESSAGE_TEXT = 'Timeout waiting for replication sync';
  END IF;
END//
DELIMITER ;

-- Utilisation
CALL wait_for_sync(300);  -- Attendre max 5 minutes
```

**Ã‰tape 4 : VÃ©rification positions identiques**

```sql
-- Sur Primary actuel
SELECT 
  @@gtid_binlog_pos AS primary_gtid,
  @@gtid_current_pos AS primary_current;

-- Sur Replica cible (futur Primary)
SELECT 
  @@gtid_slave_pos AS replica_gtid,
  @@gtid_current_pos AS replica_current;

-- Les positions GTID doivent Ãªtre identiques !
-- Si diffÃ©rentes: ATTENDRE ou INVESTIGUER
```

**Ã‰tape 5 : Promotion du nouveau Primary**

```sql
-- Sur Replica cible (nouveau Primary)

-- 1. ArrÃªter rÃ©plication
STOP SLAVE;

-- 2. VÃ©rifier position finale
SELECT @@gtid_slave_pos;

-- 3. RÃ©initialiser statut esclave
RESET SLAVE ALL;

-- 4. Activer Ã©critures
SET GLOBAL read_only = OFF;
SET GLOBAL super_read_only = OFF;

-- 5. (Optionnel) Flush logs
FLUSH LOGS;

-- Nouveau Primary prÃªt âœ“
```

**Ã‰tape 6 : Reconfiguration topologie**

```sql
-- Sur ancien Primary (devient Replica)
SET GLOBAL read_only = ON;
SET GLOBAL super_read_only = ON;

CHANGE MASTER TO
  MASTER_HOST = 'new-primary.example.com',
  MASTER_USER = 'repl_user',
  MASTER_PASSWORD = 'password',
  MASTER_USE_GTID = slave_pos;

START SLAVE;

-- Sur autres Replicas
STOP SLAVE;

CHANGE MASTER TO
  MASTER_HOST = 'new-primary.example.com',
  MASTER_USE_GTID = slave_pos;

START SLAVE;
```

**Ã‰tape 7 : Basculement applications**

```bash
# Mise Ã  jour configuration application
# Option 1: Variable d'environnement
export DB_HOST=new-primary.example.com

# Option 2: Fichier config
sed -i 's/old-primary/new-primary/g' /etc/app/database.conf

# Option 3: DNS (avec TTL court)
# Changer A record primary.example.com

# Option 4: Virtual IP (instantanÃ©)
# DÃ©placer VIP vers nouveau Primary

# RedÃ©marrer applications ou reload config
systemctl reload application
```

**Ã‰tape 8 : Validation post-switchover**

```sql
-- Sur nouveau Primary
-- Test Ã©criture
INSERT INTO switchover_log (event, timestamp) 
VALUES ('Switchover completed', NOW());

-- VÃ©rifier connexions
SELECT COUNT(*) FROM information_schema.PROCESSLIST;

-- Sur Replicas (y compris ancien Primary)
-- VÃ©rifier rÃ©plication
SHOW SLAVE STATUS\G
-- Slave_IO_Running: Yes
-- Slave_SQL_Running: Yes
-- Seconds_Behind_Master: 0

-- VÃ©rifier donnÃ©e test rÃ©pliquÃ©e
SELECT * FROM switchover_log 
WHERE event = 'Switchover completed';
```

**Ã‰tape 9 : DÃ©sactiver mode maintenance**

```bash
# Applications
curl -X POST https://app.example.com/api/maintenance/disable

# Load balancer
# Remettre en production

# Monitoring
# VÃ©rifier mÃ©triques normales
```

---

## Automatisation du Failover

### Orchestrator

**Orchestrator** : Outil open-source de gestion de topologie et failover automatique

**Installation** :

```bash
# TÃ©lÃ©charger
wget https://github.com/openark/orchestrator/releases/download/v3.2.6/orchestrator-3.2.6-1.x86_64.rpm

# Installer
rpm -i orchestrator-3.2.6-1.x86_64.rpm

# CrÃ©er base de donnÃ©es
mysql -e "CREATE DATABASE IF NOT EXISTS orchestrator"
mysql orchestrator < /usr/share/orchestrator/orchestrator-schema.sql

# CrÃ©er utilisateur
mysql -e "
  CREATE USER 'orchestrator'@'%' IDENTIFIED BY 'OrchP@ss';
  GRANT SUPER, PROCESS, REPLICATION SLAVE, RELOAD 
    ON *.* TO 'orchestrator'@'%';
  GRANT ALL ON orchestrator.* TO 'orchestrator'@'%';
"
```

**Configuration** :

```json
// /etc/orchestrator.conf.json
{
  "Debug": false,
  "MySQLTopologyUser": "orchestrator",
  "MySQLTopologyPassword": "OrchP@ss",
  "MySQLOrchestratorHost": "localhost",
  "MySQLOrchestratorPort": 3306,
  "MySQLOrchestratorDatabase": "orchestrator",
  "MySQLOrchestratorUser": "orchestrator",
  "MySQLOrchestratorPassword": "OrchP@ss",
  
  "DiscoverByShowSlaveHosts": true,
  "InstancePollSeconds": 5,
  "ReadOnly": false,
  
  "RecoveryPeriodBlockSeconds": 300,
  "RecoverMasterClusterFilters": [
    ".*"
  ],
  "RecoverIntermediateMasterClusterFilters": [
    ".*"
  ],
  
  "OnFailureDetectionProcesses": [
    "echo 'Detected {failureType} on {failureCluster}' >> /tmp/orchestrator-failure.log"
  ],
  
  "PreFailoverProcesses": [
    "echo 'Will recover from {failureType} on {failureCluster}' >> /tmp/orchestrator-recovery.log"
  ],
  
  "PostFailoverProcesses": [
    "/usr/local/bin/notify-failover.sh {failureCluster} {newMaster}"
  ],
  
  "HTTPAdvertise": "http://orchestrator.example.com:3000"
}
```

**DÃ©marrage** :

```bash
systemctl start orchestrator
systemctl enable orchestrator

# Web UI
http://orchestrator.example.com:3000
```

**DÃ©couverte de topologie** :

```bash
# CLI
orchestrator-client -c discover -i primary.example.com:3306

# Ou via Web UI: Cluster â†’ Discover
```

**Failover automatique** :

```
Orchestrator dÃ©tecte Primary down:
1. Polling Ã©choue 3 fois consÃ©cutives
2. VÃ©rifie que Replicas confirment (quorum)
3. Identifie meilleur Replica (GTID position)
4. ExÃ©cute PreFailoverProcesses
5. Promote Replica
6. Reconfigure topologie
7. ExÃ©cute PostFailoverProcesses
8. Notifie (Slack, PagerDuty, etc.)

DurÃ©e totale: 30-60 secondes
```

### MHA (Master High Availability)

**MHA** : Alternative pour failover automatique

```bash
# Installation (Perl-based)
yum install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager

wget https://github.com/yoshinorim/mha4mysql-manager/releases/download/v0.58/mha4mysql-manager-0.58-0.el7.noarch.rpm
rpm -i mha4mysql-manager-0.58-0.el7.noarch.rpm

# Configuration
cat > /etc/mha/app1.cnf <<EOF
[server default]
user=mha
password=MhaP@ss
ssh_user=root
repl_user=repl_user
repl_password=ReplP@ss

master_binlog_dir=/var/log/mysql
remote_workdir=/tmp

[server1]
hostname=primary.example.com
candidate_master=1

[server2]
hostname=replica1.example.com
candidate_master=1

[server3]
hostname=replica2.example.com
no_master=1
EOF

# Lancer MHA Manager
nohup masterha_manager --conf=/etc/mha/app1.cnf &
```

### MaxScale Auto-Failover

**MaxScale** avec module `mariadbmon` :

```ini
# /etc/maxscale.cnf
[mariadbmon]
type=monitor
module=mariadbmon
servers=primary,replica1,replica2
user=maxscale
password=MaxScaleP@ss

auto_failover=true
auto_rejoin=true

failcount=3
failover_timeout=90s

[Read-Write-Service]
type=service
router=readwritesplit
servers=primary,replica1,replica2
user=maxscale
password=MaxScaleP@ss

master_failure_mode=fail_on_write
```

```bash
# DÃ©marrer MaxScale
systemctl start maxscale

# Monitorer
maxctrl list servers
```

---

## Tests de Failover

### Importance des tests

```
Test rÃ©gulier de failover:
âœ… Valide procÃ©dures
âœ… Forme Ã©quipes
âœ… Identifie gaps
âœ… Mesure RTO/RPO rÃ©els
âœ… RÃ©duit stress incidents

Sans tests:
âŒ ProcÃ©dures obsolÃ¨tes
âŒ Ã‰quipes non prÃ©parÃ©es
âŒ DÃ©couverte problÃ¨mes en prod
âŒ RTO/RPO inconnus
âŒ Panique lors incidents
```

### Planification des tests

**FrÃ©quence recommandÃ©e** :

| Type | FrÃ©quence | DurÃ©e |
|------|-----------|-------|
| Failover automatisÃ© | Mensuel | 30 min |
| Switchover planifiÃ© | Trimestriel | 1 heure |
| Disaster recovery complet | Annuel | 4 heures |

**Environnements de test** :

```
Niveau 1: Lab isolÃ©
- Topologie rÃ©plique production
- Zero risque
- Tests frÃ©quents

Niveau 2: Staging
- Proche production
- Traffic synthÃ©tique
- Tests trimestriels

Niveau 3: Production (fenÃªtre maintenance)
- RÃ©el
- Risque contrÃ´lÃ©
- Tests annuels
```

### Script de test automatisÃ©

```bash
#!/bin/bash
# test_failover.sh

set -e

PRIMARY="primary.example.com"
REPLICA1="replica1.example.com"
REPLICA2="replica2.example.com"

echo "=== Failover Test ==="
echo "Date: $(date)"
echo ""

# Phase 1: Ã‰tat initial
echo "[1/6] VÃ©rification Ã©tat initial..."
mysql -h $PRIMARY -e "SELECT @@hostname, @@read_only"
mysql -h $REPLICA1 -e "SHOW SLAVE STATUS\G" | grep -E "Slave_IO_Running|Slave_SQL_Running"

# Phase 2: InsÃ©rer donnÃ©es test
echo "[2/6] Insertion donnÃ©es test..."
mysql -h $PRIMARY -e "
  CREATE DATABASE IF NOT EXISTS failover_test;
  USE failover_test;
  CREATE TABLE IF NOT EXISTS test_log (
    id INT AUTO_INCREMENT PRIMARY KEY,
    event VARCHAR(255),
    ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  );
  INSERT INTO test_log (event) VALUES ('Before failover');
"

# Phase 3: Simuler crash Primary
echo "[3/6] Simulation crash Primary..."
ssh root@$PRIMARY "systemctl stop mariadb"
sleep 5

# Phase 4: Promouvoir Replica1
echo "[4/6] Promotion Replica1..."
mysql -h $REPLICA1 -e "
  STOP SLAVE;
  RESET SLAVE ALL;
  SET GLOBAL read_only = OFF;
  SET GLOBAL super_read_only = OFF;
"

# Phase 5: Reconfigurer Replica2
echo "[5/6] Reconfiguration Replica2..."
mysql -h $REPLICA2 -e "
  STOP SLAVE;
  CHANGE MASTER TO
    MASTER_HOST = '$REPLICA1',
    MASTER_USE_GTID = slave_pos;
  START SLAVE;
"

# Phase 6: Validation
echo "[6/6] Validation..."
mysql -h $REPLICA1 -e "
  USE failover_test;
  INSERT INTO test_log (event) VALUES ('After failover - new primary');
"

sleep 2

mysql -h $REPLICA2 -e "
  SELECT COUNT(*) AS replicated_events 
  FROM failover_test.test_log 
  WHERE event LIKE '%failover%';
"

echo ""
echo "âœ… Failover test completed successfully"
echo "New Primary: $REPLICA1"
echo "Replicas: $REPLICA2"

# Cleanup (optionnel)
echo ""
echo "Cleanup: RedÃ©marrer ancien Primary comme Replica..."
ssh root@$PRIMARY "systemctl start mariadb"
sleep 5
mysql -h $PRIMARY -e "
  SET GLOBAL read_only = ON;
  CHANGE MASTER TO
    MASTER_HOST = '$REPLICA1',
    MASTER_USE_GTID = slave_pos;
  START SLAVE;
"

echo "âœ… Cleanup completed"
```

### Checklist de test

```
PrÃ©-test:
â˜ Backup de toute la topologie
â˜ Documentation procÃ©dures Ã  jour
â˜ Ã‰quipe complÃ¨te disponible
â˜ Monitoring actif
â˜ FenÃªtre de temps allouÃ©e

Pendant le test:
â˜ ChronomÃ©trer chaque Ã©tape
â˜ Logger toutes les commandes
â˜ Capturer erreurs/warnings
â˜ Mesurer RTO effectif
â˜ Mesurer RPO effectif

Post-test:
â˜ VÃ©rifier intÃ©gritÃ© donnÃ©es
â˜ VÃ©rifier performance
â˜ Analyser mÃ©triques
â˜ Documenter leÃ§ons apprises
â˜ Mettre Ã  jour runbooks
```

---

## Bonnes Pratiques

### 1. GTID obligatoire

```sql
-- Activer GTID sur TOUTE la topologie
[mysqld]
gtid_domain_id = 0
gtid_strict_mode = ON
log_slave_updates = ON  -- Si cascade

-- Simplifie drastiquement failover/switchover
```

### 2. RÃ©plication semi-synchrone pour RPO=0

```sql
-- Sur Primary
SET GLOBAL rpl_semi_sync_master_enabled = ON;
SET GLOBAL rpl_semi_sync_master_timeout = 1000;

-- Sur Replicas
SET GLOBAL rpl_semi_sync_slave_enabled = ON;

-- Garantie: Zero data loss en failover
```

### 3. Identifier Replica candidat

```sql
-- Taguer les Replicas Ã©ligibles
CREATE TABLE replication_config (
  server_id INT PRIMARY KEY,
  hostname VARCHAR(255),
  role ENUM('primary', 'replica'),
  failover_candidate BOOLEAN,
  priority INT,  -- 1=highest
  notes TEXT
);

INSERT INTO replication_config VALUES
(1, 'primary.example.com', 'primary', FALSE, 0, 'Production primary'),
(2, 'replica1.example.com', 'replica', TRUE, 1, 'Hot standby - promote first'),
(3, 'replica2.example.com', 'replica', TRUE, 2, 'Fallback'),
(4, 'replica3.example.com', 'replica', FALSE, 0, 'Reporting only - never promote');
```

### 4. Automatiser reconfiguration applications

```bash
#!/bin/bash
# update_app_config.sh <new_primary_host>

NEW_PRIMARY=$1

# Mise Ã  jour fichiers config
for SERVER in app1 app2 app3; do
  ssh $SERVER "
    sed -i 's/^DB_HOST=.*/DB_HOST=$NEW_PRIMARY/' /etc/app/database.env
    systemctl reload app
  "
done

# Mise Ã  jour DNS (si utilisÃ©)
# aws route53 change-resource-record-sets ...

# Notification
curl -X POST $SLACK_WEBHOOK -d "{\"text\":\"Database primary updated to $NEW_PRIMARY\"}"
```

### 5. Virtual IP pour transparence

```bash
# Keepalived configuration
# /etc/keepalived/keepalived.conf

vrrp_script check_mariadb {
  script "/usr/local/bin/check_mariadb.sh"
  interval 2
  weight -20
}

vrrp_instance VI_1 {
  interface eth0
  state MASTER  # Sur Primary
  virtual_router_id 51
  priority 100  # Plus haut = preferred
  
  virtual_ipaddress {
    192.168.1.100/24
  }
  
  track_script {
    check_mariadb
  }
  
  notify_master "/usr/local/bin/notify_master.sh"
}

# Script de check
cat > /usr/local/bin/check_mariadb.sh <<'EOF'
#!/bin/bash
mysql -e "SELECT 1" > /dev/null 2>&1
exit $?
EOF

chmod +x /usr/local/bin/check_mariadb.sh
```

### 6. Documenter runbooks

```markdown
# Runbook: Failover d'Urgence

## DÃ©tection
- Alerte: "Primary DOWN"
- VÃ©rifier: `ping primary.example.com`
- Confirmer: Impossible de se connecter au Primary

## DÃ©cision
- Si < 5 minutes downtime attendu: ATTENDRE
- Si > 5 minutes ou hardware failure: FAILOVER

## ExÃ©cution
1. Identifier meilleur Replica
   ```sql
   SELECT @@gtid_slave_pos FROM replica1;
   ```

2. Promouvoir Replica
   ```sql
   STOP SLAVE; RESET SLAVE ALL;
   SET GLOBAL read_only = OFF;
   ```

3. Reconfigurer topologie
   [...]

## Validation
- Test Ã©criture sur nouveau Primary
- VÃ©rifier rÃ©plication autres Replicas
- Basculer applications

## Communication
- Notifier Ã©quipes
- Mettre Ã  jour documentation
- Post-mortem dans les 48h
```

### 7. Monitoring post-failover

```sql
-- CrÃ©er table de suivi
CREATE TABLE failover_history (
  id INT AUTO_INCREMENT PRIMARY KEY,
  event_type ENUM('failover', 'switchover'),
  old_primary VARCHAR(255),
  new_primary VARCHAR(255),
  initiated_by VARCHAR(255),
  reason TEXT,
  rto_seconds INT,  -- Temps downtime effectif
  rpo_seconds INT,  -- DonnÃ©es perdues estimÃ©es
  success BOOLEAN,
  notes TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Logger chaque Ã©vÃ©nement
INSERT INTO failover_history 
  (event_type, old_primary, new_primary, initiated_by, reason, rto_seconds, rpo_seconds, success) 
VALUES 
  ('failover', 'primary.example.com', 'replica1.example.com', 'dba@example.com', 
   'Hardware failure', 120, 2, TRUE);
```

---

## âœ… Points clÃ©s Ã  retenir

1. **Failover vs Switchover** : Non planifiÃ© vs planifiÃ©, urgence vs contrÃ´le

2. **RTO/RPO** : DÃ©finir et mesurer objectifs de rÃ©cupÃ©ration

3. **GTID essentiel** : Simplifie drastiquement les basculements

4. **Semi-sync** : RPO=0 pour donnÃ©es critiques

5. **Automatisation** : Orchestrator, MHA, MaxScale pour failover rapide

6. **Tests rÃ©guliers** : Valider procÃ©dures, former Ã©quipes, mesurer mÃ©triques

7. **Meilleur Replica** : Identifier par GTID position (le plus Ã  jour)

8. **Applications** : PrÃ©voir reconfiguration automatisÃ©e

9. **Virtual IP** : Transparence basculement pour applications

10. **Documentation** : Runbooks dÃ©taillÃ©s et Ã  jour, post-mortem systÃ©matique

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle MariaDB

- [ğŸ“– Replication Failover](https://mariadb.com/kb/en/replication-failover/)
- [ğŸ“– GTID and Failover](https://mariadb.com/kb/en/gtid/#using-gtid-for-failover)

### Outils

- **Orchestrator** : github.com/openark/orchestrator
- **MHA** : github.com/yoshinorim/mha4mysql-manager
- **MaxScale** : mariadb.com/products/skysql/mariadb-maxscale/

### Articles techniques

- [ğŸ”— Failover Best Practices](https://mariadb.com/resources/blog/replication-failover-best-practices/)
- [ğŸ”— Zero Downtime Migrations](https://www.percona.com/blog/zero-downtime-mysql-migrations/)

---

## â¡ï¸ Section suivante

**13.9 RÃ©plication semi-synchrone** : Configuration dÃ©taillÃ©e de la rÃ©plication semi-synchrone, modes AFTER_SYNC vs AFTER_COMMIT, impact performance, monitoring spÃ©cifique, et garanties de durabilitÃ©.

---


â­ï¸ [RÃ©plication semi-synchrone](/13-replication/09-replication-semi-synchrone.md)
