ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 13.1 Concepts de RÃ©plication : Asynchrone vs Semi-synchrone

> **Niveau** : AvancÃ©  
> **DurÃ©e estimÃ©e** : 1.5-2 heures  
> **PrÃ©requis** : 
> - ComprÃ©hension des transactions ACID (Chapitre 6)
> - Connaissance des binary logs (Section 11.5)
> - Bases en systÃ¨mes distribuÃ©s (consensus, latence rÃ©seau)
> - ThÃ©orÃ¨me CAP (Consistency, Availability, Partition tolerance)

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- Expliquer le fonctionnement dÃ©taillÃ© de la rÃ©plication asynchrone et semi-synchrone
- Comprendre les implications du thÃ©orÃ¨me CAP sur chaque mode
- Analyser les trade-offs performance vs durabilitÃ©
- Choisir le mode appropriÃ© selon les contraintes mÃ©tier
- Configurer et optimiser chaque mode de rÃ©plication
- Mesurer et interprÃ©ter les mÃ©triques de performance

---

## Introduction

La **rÃ©plication** dans MariaDB repose sur la propagation des modifications de donnÃ©es depuis un serveur source (Primary/Master) vers un ou plusieurs serveurs de destination (Replica/Slave). Le **mode de rÃ©plication** dÃ©termine le niveau de synchronisation entre ces serveurs et a un impact direct sur :

- La **durabilitÃ©** des donnÃ©es (garantie anti-perte)
- La **performance** des Ã©critures
- La **cohÃ©rence** lecture/Ã©criture
- Le **comportement en cas de panne**

MariaDB propose principalement deux modes :
1. **RÃ©plication asynchrone** (par dÃ©faut)
2. **RÃ©plication semi-synchrone** (plugin additionnel)

ğŸ’¡ **Analogie** : Pensez Ã  l'envoi d'un colis. La rÃ©plication asynchrone est comme dÃ©poser le colis dans une boÃ®te postale sans attendre de confirmation. La semi-synchrone, c'est attendre que le facteur confirme qu'il a bien le colis avant de partir.

---

## RÃ©plication Asynchrone

### Principe de fonctionnement

La rÃ©plication asynchrone est le mode **par dÃ©faut** de MariaDB. Elle fonctionne selon ce mÃ©canisme :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVEUR PRIMARY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Client COMMIT                                           â”‚
â”‚  2. Transaction Ã©crite dans InnoDB                          â”‚
â”‚  3. Transaction Ã©crite dans binlog                          â”‚
â”‚  4. COMMIT confirmÃ© au client âœ“                             â”‚
â”‚                                                             â”‚
â”‚  Le Primary n'attend PAS le Replica                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Binary Log Events
                     â”‚ (Transfert rÃ©seau asynchrone)
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVEUR REPLICA                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  5. IO Thread : RÃ©cupÃ¨re les Ã©vÃ©nements binlog              â”‚
â”‚  6. Ã‰criture dans relay log                                 â”‚
â”‚  7. SQL Thread : Applique les Ã©vÃ©nements                    â”‚
â”‚                                                             â”‚
â”‚  Aucune confirmation envoyÃ©e au Primary                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SÃ©quence dÃ©taillÃ©e** :

1. **Client envoie COMMIT** au Primary
2. **Primary Ã©crit dans InnoDB** (redo log, buffer pool)
3. **Primary Ã©crit dans binary log**
4. **Primary confirme COMMIT au client** âœ… **(sans attendre le Replica)**
5. **IO Thread du Replica** se connecte au Primary et lit le binlog
6. **Relay log** : Les Ã©vÃ©nements sont stockÃ©s localement sur le Replica
7. **SQL Thread du Replica** applique les Ã©vÃ©nements du relay log

### CaractÃ©ristiques techniques

**Threads impliquÃ©s** :

```sql
-- Sur le Primary
SHOW PROCESSLIST;
-- On voit : Binlog Dump thread(s) pour chaque Replica connectÃ©

-- Sur le Replica
SHOW PROCESSLIST;
-- On voit :
-- 1. Slave_IO thread (connexion au Primary)
-- 2. Slave_SQL thread (application des Ã©vÃ©nements)
```

**Flux de donnÃ©es** :

```
Primary Thread        Network         Replica Threads
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                      
Binlog Dump  â”€â”€â”€â”€â”€â†’  TCP/IP  â”€â”€â”€â”€â”€â†’  IO Thread
                                         â”‚
                                         â–¼
                                     Relay Log
                                         â”‚
                                         â–¼
                                     SQL Thread
                                         â”‚
                                         â–¼
                                      InnoDB
```

### Avantages

âœ… **Performance maximale**
- Latence d'Ã©criture minimale
- Le Primary ne bloque jamais en attendant le Replica
- Throughput optimal pour les charges OLTP

âœ… **SimplicitÃ©**
- Configuration par dÃ©faut, aucun plugin
- Gestion transparente des pannes Replica
- Le Primary continue mÃªme si tous les Replicas sont down

âœ… **ScalabilitÃ©**
- Le Primary peut avoir de nombreux Replicas sans impact
- Ajout/suppression de Replicas Ã  chaud

### InconvÃ©nients et risques

âŒ **Perte de donnÃ©es possible**

**ScÃ©nario de perte** :
```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

T0: Client COMMIT transaction T1 sur Primary âœ“
T1: Primary confirme au client
T2: Primary Ã©crit dans binlog
T3: Primary CRASH ğŸ’¥ (avant que le Replica ne rÃ©cupÃ¨re T1)
T4: Replica promu en nouveau Primary
    â†’ Transaction T1 est PERDUE âŒ
```

**Window de vulnÃ©rabilitÃ©** :
- DurÃ©e entre le COMMIT cÃ´tÃ© client et la rÃ©ception par le Replica
- Typiquement 100ms-1s selon la latence rÃ©seau et la charge
- Peut atteindre plusieurs secondes sous forte charge

âŒ **CohÃ©rence lecture/Ã©criture non garantie**

```sql
-- Application web
-- Serveur 1 (Write to Primary)
INSERT INTO orders (customer_id, total) VALUES (123, 99.99);
COMMIT; -- ConfirmÃ© âœ“

-- Serveur 2 (Read from Replica) - Quelques ms plus tard
SELECT COUNT(*) FROM orders WHERE customer_id = 123;
-- Peut ne PAS voir la nouvelle commande ! (lag de rÃ©plication)
```

âŒ **Lag de rÃ©plication variable**

- Le Replica peut Ãªtre en retard de quelques secondes Ã  plusieurs heures
- Facteurs : charge, bande passante rÃ©seau, DDL sur grosses tables
- Impact sur les lectures sur Replica (donnÃ©es "stale")

### Configuration

```ini
# /etc/mysql/mariadb.conf.d/50-server.cnf

[mysqld]
# Primary configuration
server-id = 1                    # Unique par serveur
log-bin = /var/log/mysql/mariadb-bin
binlog_format = ROW              # RecommandÃ©
max_binlog_size = 100M
expire_logs_days = 7

# Replica configuration
server-id = 2                    # DiffÃ©rent du Primary !
relay-log = /var/log/mysql/relay-bin
log-slave-updates = ON           # Si cascade ou multi-master
read-only = ON                   # Protection Ã©criture accidentelle
```

**Ã‰tablissement de la rÃ©plication** :

```sql
-- Sur le Primary : CrÃ©er utilisateur de rÃ©plication
CREATE USER 'repl_user'@'%' 
  IDENTIFIED BY 'SecureP@ssw0rd!';
  
GRANT REPLICATION SLAVE ON *.* 
  TO 'repl_user'@'%';

FLUSH PRIVILEGES;

-- Obtenir la position courante
SHOW MASTER STATUS;
-- +--------------------+----------+--------------+------------------+
-- | File               | Position | Binlog_Do_DB | Binlog_Ignore_DB |
-- +--------------------+----------+--------------+------------------+
-- | mariadb-bin.000042 |     4567 |              |                  |
-- +--------------------+----------+--------------+------------------+

-- Sur le Replica : Configurer la rÃ©plication
CHANGE MASTER TO
  MASTER_HOST = '192.168.1.100',
  MASTER_PORT = 3306,
  MASTER_USER = 'repl_user',
  MASTER_PASSWORD = 'SecureP@ssw0rd!',
  MASTER_LOG_FILE = 'mariadb-bin.000042',
  MASTER_LOG_POS = 4567;

START SLAVE;

-- VÃ©rifier l'Ã©tat
SHOW SLAVE STATUS\G
```

### Cas d'usage appropriÃ©s

**âœ“ ScÃ©narios recommandÃ©s** :

1. **ScalabilitÃ© en lecture**
   - Application Ã  forte charge de lecture
   - Reporting/Analytique sans impact sur production
   - TolÃ©rance Ã  des donnÃ©es lÃ©gÃ¨rement "stale"

2. **DÃ©veloppement/Testing**
   - Environnements non-critiques
   - CoÃ»t de perte de donnÃ©es acceptable

3. **GÃ©o-rÃ©plication longue distance**
   - Latence rÃ©seau Ã©levÃ©e (>50ms)
   - Impact performance semi-sync serait prohibitif

**âœ— ScÃ©narios dÃ©conseillÃ©s** :

1. DonnÃ©es financiÃ¨res critiques
2. ConformitÃ© rÃ©glementaire stricte (RGPD, HIPAA)
3. Applications nÃ©cessitant cohÃ©rence lecture aprÃ¨s Ã©criture

---

## RÃ©plication Semi-synchrone

### Principe de fonctionnement

La rÃ©plication semi-synchrone ajoute une **Ã©tape de confirmation** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVEUR PRIMARY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Client COMMIT                                           â”‚
â”‚  2. Transaction Ã©crite dans InnoDB                          â”‚
â”‚  3. Transaction Ã©crite dans binlog                          â”‚
â”‚  4. Envoi Ã©vÃ©nements â†’ Replica(s)                           â”‚
â”‚  5. â³ ATTENTE ACK d'au moins 1 Replica                     â”‚
â”‚  6. COMMIT confirmÃ© au client âœ“                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Binary Log Events
                     â”‚ + ACK obligatoire
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVEUR REPLICA                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  5. IO Thread : RÃ©cupÃ¨re les Ã©vÃ©nements binlog              â”‚
â”‚  6. Ã‰criture dans relay log                                 â”‚
â”‚  7. Envoi ACK au Primary âœ“                                  â”‚
â”‚  8. SQL Thread : Applique les Ã©vÃ©nements                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DiffÃ©rence clÃ©** : Le Primary **attend** qu'au moins un Replica ait confirmÃ© la **rÃ©ception** des Ã©vÃ©nements binlog avant de confirmer le COMMIT au client.

âš ï¸ **Important** : Le Replica confirme la **rÃ©ception** (Ã©criture relay log), PAS l'**application** (exÃ©cution SQL). Il peut donc y avoir un lÃ©ger lag mÃªme en semi-sync.

### Garanties de durabilitÃ©

**ScÃ©nario de crash avec semi-sync** :

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

T0: Client COMMIT transaction T1 sur Primary
T1: Primary Ã©crit dans binlog
T2: Primary envoie Ã  Replica
T3: Replica reÃ§oit et Ã©crit relay log
T4: Replica envoie ACK âœ“
T5: Primary confirme au client âœ“
T6: Primary CRASH ğŸ’¥
T7: Replica promu en nouveau Primary
    â†’ Transaction T1 est PRÃ‰SENTE âœ“ (dans relay log)
    â†’ Aucune perte de donnÃ©es
```

**Garantie** : Si le client reÃ§oit un COMMIT confirmÃ©, au moins un Replica possÃ¨de la transaction dans son relay log. Promotion de ce Replica = zero data loss.

### Architecture AFTER_SYNC vs AFTER_COMMIT

MariaDB 10.3+ propose deux modes semi-synchrones :

**1. AFTER_SYNC (par dÃ©faut, recommandÃ©)** :

```
SÃ©quence:
1. Ã‰criture binlog
2. Synchronisation disque binlog (fsync)
3. Envoi aux Replicas
4. Attente ACK
5. Commit InnoDB
6. Confirmation client
```

**Avantage** : Aucune transaction visible sur le Primary qui ne serait pas sur au moins un Replica. Garantie de cohÃ©rence maximale.

**2. AFTER_COMMIT (legacy)** :

```
SÃ©quence:
1. Ã‰criture binlog
2. Commit InnoDB
3. Envoi aux Replicas
4. Attente ACK
5. Confirmation client
```

**InconvÃ©nient** : FenÃªtre oÃ¹ une transaction est visible sur Primary mais pas encore ACKÃ©e par Replica. En cas de crash, lÃ©gÃ¨re incohÃ©rence possible.

**Configuration** :

```sql
-- Mode recommandÃ© (AFTER_SYNC)
SET GLOBAL rpl_semi_sync_master_wait_point = 'AFTER_SYNC';
```

### Installation et configuration

**1. Installation du plugin** :

```sql
-- Sur le Primary
INSTALL SONAME 'semisync_master';

-- VÃ©rifier l'installation
SHOW PLUGINS;
-- +-----------------------+--------+--------------------+---------+---------+
-- | Name                  | Status | Type               | Library | License |
-- +-----------------------+--------+--------------------+---------+---------+
-- | rpl_semi_sync_master  | ACTIVE | REPLICATION        | ...     | GPL     |
-- +-----------------------+--------+--------------------+---------+---------+
```

```sql
-- Sur chaque Replica
INSTALL SONAME 'semisync_slave';

SHOW PLUGINS;
-- +-----------------------+--------+--------------------+---------+---------+
-- | Name                  | Status | Type               | Library | License |
-- +-----------------------+--------+--------------------+---------+---------+
-- | rpl_semi_sync_slave   | ACTIVE | REPLICATION        | ...     | GPL     |
-- +-----------------------+--------+--------------------+---------+---------+
```

**2. Activation** :

```sql
-- Primary
SET GLOBAL rpl_semi_sync_master_enabled = ON;
SET GLOBAL rpl_semi_sync_master_timeout = 1000;  -- 1 seconde
SET GLOBAL rpl_semi_sync_master_wait_no_slave = ON;  -- Important !

-- Replica
SET GLOBAL rpl_semi_sync_slave_enabled = ON;

-- RedÃ©marrer IO thread pour activer
STOP SLAVE IO_THREAD;
START SLAVE IO_THREAD;
```

**3. Configuration permanente** :

```ini
# /etc/mysql/mariadb.conf.d/50-server.cnf

# PRIMARY
[mysqld]
plugin-load-add = semisync_master.so
rpl_semi_sync_master_enabled = ON
rpl_semi_sync_master_timeout = 1000
rpl_semi_sync_master_wait_point = AFTER_SYNC
rpl_semi_sync_master_wait_no_slave = ON

# REPLICA
[mysqld]
plugin-load-add = semisync_slave.so
rpl_semi_sync_slave_enabled = ON
```

### ParamÃ¨tres critiques

**rpl_semi_sync_master_timeout**

Temps maximal d'attente d'un ACK avant de basculer en mode asynchrone.

```sql
-- Default: 10000ms (10 secondes)
SET GLOBAL rpl_semi_sync_master_timeout = 1000;  -- 1 seconde

-- Trop court : Basculements frÃ©quents async â†” semi-sync
-- Trop long : Latence excessive en cas de problÃ¨me Replica
```

ğŸ’¡ **Recommandation** : 
- LAN : 500-1000ms
- WAN : 2000-5000ms
- Cross-datacenter : 5000-10000ms

**rpl_semi_sync_master_wait_no_slave**

Comportement si aucun Replica semi-sync n'est disponible.

```sql
-- ON (recommandÃ©) : Continue en mode asynchrone
SET GLOBAL rpl_semi_sync_master_wait_no_slave = ON;

-- OFF : Bloque les Ã©critures ! âš ï¸ Dangereux
SET GLOBAL rpl_semi_sync_master_wait_no_slave = OFF;
```

âš ï¸ **Attention** : Avec `OFF`, si tous les Replicas Ã©chouent, le Primary refuse les COMMIT ! Ã€ utiliser seulement si durabilitÃ© > disponibilitÃ©.

**rpl_semi_sync_master_wait_point**

```sql
-- AFTER_SYNC (recommandÃ©, par dÃ©faut depuis 10.3)
SET GLOBAL rpl_semi_sync_master_wait_point = 'AFTER_SYNC';

-- AFTER_COMMIT (legacy, pour compatibilitÃ©)
SET GLOBAL rpl_semi_sync_master_wait_point = 'AFTER_COMMIT';
```

### Monitoring de la semi-sync

**Variables de statut essentielles** :

```sql
-- Ã‰tat global
SHOW STATUS LIKE 'Rpl_semi_sync%';
```

**MÃ©triques clÃ©s** :

| Variable | Signification |
|----------|---------------|
| `Rpl_semi_sync_master_status` | ON = actif, OFF = fallback async |
| `Rpl_semi_sync_master_clients` | Nombre de Replicas semi-sync connectÃ©s |
| `Rpl_semi_sync_master_yes_tx` | Transactions confirmÃ©es en semi-sync |
| `Rpl_semi_sync_master_no_tx` | Transactions en mode async (timeout) |
| `Rpl_semi_sync_master_wait_sessions` | Sessions actuellement en attente ACK |
| `Rpl_semi_sync_master_avg_wait_time` | Temps moyen d'attente ACK (Âµs) |

**Exemple de monitoring** :

```sql
-- Taux de succÃ¨s semi-sync
SELECT 
  (Rpl_semi_sync_master_yes_tx / 
   (Rpl_semi_sync_master_yes_tx + Rpl_semi_sync_master_no_tx)) * 100 
  AS semi_sync_success_rate
FROM (
  SELECT 
    VARIABLE_VALUE AS Rpl_semi_sync_master_yes_tx
  FROM information_schema.GLOBAL_STATUS 
  WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_yes_tx'
) yes_tx,
(
  SELECT 
    VARIABLE_VALUE AS Rpl_semi_sync_master_no_tx
  FROM information_schema.GLOBAL_STATUS 
  WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_no_tx'
) no_tx;
```

**Alertes recommandÃ©es** :

```sql
-- Semi-sync dÃ©sactivÃ© (fallback async)
Rpl_semi_sync_master_status = OFF
â†’ Alert: Perte de garantie de durabilitÃ©

-- Trop de timeouts (>5%)
(Rpl_semi_sync_master_no_tx / total_tx) > 0.05
â†’ Alert: ProblÃ¨me rÃ©seau ou Replica surchargÃ©

-- Latence excessive (>100ms)
Rpl_semi_sync_master_avg_wait_time > 100000  -- Âµs
â†’ Alert: Optimiser rÃ©seau ou Replica
```

### Impact sur les performances

**Latence d'Ã©criture** :

```
Asynchrone:      5-10ms   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                          â†“
Semi-sync (LAN): 7-15ms   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                          â†“ +2-5ms
Semi-sync (WAN): 50-150ms â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                          â†“ +45-140ms
```

**Throughput** :

```sql
-- Benchmark sysbench (OLTP write-only)
-- Serveur : 16 vCPU, 64GB RAM, SSD NVMe
-- RÃ©seau : LAN 10Gbps

Mode           TPS     Latence P95    Latence P99
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Asynchrone     15,420  8.2ms          12.5ms
Semi-sync      13,850  10.1ms         15.8ms  (-10%)
```

**Facteurs d'impact** :

1. **Latence rÃ©seau** : Principal facteur
   - LAN (< 1ms) : Impact minimal (5-10%)
   - WAN (10-50ms) : Impact significatif (20-40%)
   - Intercontinental (100-200ms) : Impact majeur (50-80%)

2. **Nombre de Replicas semi-sync** :
   - Le Primary attend le **premier** ACK seulement
   - 1 Replica ou 10 Replicas : mÃªme latence !

3. **Configuration `sync_binlog`** :
   ```sql
   -- sync_binlog=1 (durabilitÃ© max, recommandÃ© avec semi-sync)
   SET GLOBAL sync_binlog = 1;  -- fsync Ã  chaque commit
   
   -- sync_binlog=0 (performance max, risque si crash OS)
   SET GLOBAL sync_binlog = 0;  -- fsync async par l'OS
   ```

### Avantages

âœ… **DurabilitÃ© garantie**
- Zero data loss en cas de crash Primary
- Au moins un Replica possÃ¨de toutes les transactions confirmÃ©es

âœ… **Failover simplifiÃ©**
- Le Replica semi-sync peut Ãªtre promu immÃ©diatement
- Pas de calcul de position binlog complexe

âœ… **ConformitÃ© rÃ©glementaire**
- RÃ©pond aux exigences de durabilitÃ© strictes
- AuditabilitÃ© : traÃ§abilitÃ© des transactions

### InconvÃ©nients

âŒ **Impact performance**
- Latence d'Ã©criture augmentÃ©e
- DÃ©pendance Ã  la latence rÃ©seau

âŒ **DisponibilitÃ© rÃ©duite**
- Si tous les Replicas semi-sync tombent : fallback async ou blocage (selon config)

âŒ **ComplexitÃ© opÃ©rationnelle**
- Configuration supplÃ©mentaire
- Monitoring additionnel requis

### Cas d'usage appropriÃ©s

**âœ“ ScÃ©narios recommandÃ©s** :

1. **DonnÃ©es financiÃ¨res critiques**
   - Transactions bancaires
   - Paiements e-commerce
   - ComptabilitÃ©

2. **ConformitÃ© rÃ©glementaire**
   - RGPD (donnÃ©es personnelles)
   - HIPAA (santÃ©)
   - PCI-DSS (cartes bancaires)

3. **SLA strict**
   - RPO (Recovery Point Objective) = 0
   - Zero data loss requirement

**âœ— ScÃ©narios dÃ©conseillÃ©s** :

1. GÃ©o-rÃ©plication longue distance (latence > 100ms)
2. Charges d'Ã©criture extrÃªmement Ã©levÃ©es (>10K TPS)
3. Budget latence trÃ¨s serrÃ© (< 10ms P99)

---

## Comparaison Approfondie

### Tableau rÃ©capitulatif

| CritÃ¨re | Asynchrone | Semi-synchrone |
|---------|------------|----------------|
| **DurabilitÃ©** | âš ï¸ Possible perte de donnÃ©es | âœ… Zero data loss |
| **Performance Ã©criture** | âœ… Maximale | âš ï¸ +5-50ms selon rÃ©seau |
| **Throughput** | âœ… Maximum | âš ï¸ -5-30% |
| **DisponibilitÃ©** | âœ… IndÃ©pendant des Replicas | âš ï¸ DÃ©pend d'au moins 1 Replica |
| **CohÃ©rence** | âš ï¸ Ã‰ventuelle | âœ… Forte |
| **ComplexitÃ©** | âœ… Simple | âš ï¸ Moyenne |
| **Failover** | âš ï¸ Calcul position requis | âœ… ImmÃ©diat |
| **Cas d'usage** | ScalabilitÃ© lecture | DurabilitÃ© critique |

### Trade-offs selon le thÃ©orÃ¨me CAP

Le [thÃ©orÃ¨me CAP](https://en.wikipedia.org/wiki/CAP_theorem) stipule qu'un systÃ¨me distribuÃ© ne peut garantir simultanÃ©ment que 2 des 3 propriÃ©tÃ©s :
- **C**onsistency (cohÃ©rence)
- **A**vailability (disponibilitÃ©)
- **P**artition tolerance (tolÃ©rance aux partitions)

**RÃ©plication asynchrone : AP (Availability + Partition tolerance)**

```
Primary     Replica
   â”‚           â”‚
   â”‚  Network  â”‚
   â”‚  Partitionâ”‚
   â”‚     â•³     â”‚
   â–¼           â–¼
 Writes     Reads
continue   continue
(Stale data possible)
```

- **Availability** : Le Primary continue mÃªme si Replica inaccessible
- **Partition tolerance** : TolÃ¨re la partition rÃ©seau
- **Consistency** : âš ï¸ SacrifiÃ©e (donnÃ©es "stale" sur Replica)

**RÃ©plication semi-synchrone : CP (Consistency + Partition tolerance)**

```
Primary     Replica
   â”‚           â”‚
   â”‚  Network  â”‚
   â”‚  Partitionâ”‚
   â”‚     â•³     â”‚
   â–¼           
 Writes
  BLOCK
(Si wait_no_slave=OFF)
```

- **Consistency** : Garantie par l'ACK obligatoire
- **Partition tolerance** : TolÃ¨re la partition (fallback async possible)
- **Availability** : âš ï¸ SacrifiÃ©e si `wait_no_slave=OFF`

### Choix du mode selon les contraintes

**Matrice de dÃ©cision** :

```
                        Latence rÃ©seau
                    Low (<10ms)    High (>50ms)
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  DurabilitÃ©      â”‚             â”‚              â”‚
  Critical        â”‚ Semi-sync   â”‚ Semi-sync    â”‚
  (RPO=0)         â”‚ (optimal)   â”‚ (coÃ»teux)    â”‚
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  DurabilitÃ©      â”‚ Async       â”‚ Async        â”‚
  Tolerant        â”‚ (optimal)   â”‚ (optimal)    â”‚
  (RPO>0)         â”‚             â”‚              â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Algorithme de dÃ©cision** :

```
SI (RPO = 0 ET latence_rÃ©seau < 20ms) 
  â†’ Semi-synchrone AFTER_SYNC

SINON SI (RPO = 0 ET latence_rÃ©seau > 20ms)
  â†’ Semi-synchrone AFTER_SYNC + timeout Ã©levÃ©
  â†’ OU Galera Cluster (chapitre 14)

SINON SI (RPO > 0 ET throughput > 10K TPS)
  â†’ Asynchrone

SINON SI (RPO > 0 ET latence_rÃ©seau > 100ms)
  â†’ Asynchrone

SINON
  â†’ Semi-synchrone (meilleure garantie par dÃ©faut)
FIN SI
```

---

## Configurations AvancÃ©es

### RÃ©plication semi-sync avec multiple ACKs

MariaDB 10.6+ permet d'exiger des ACK de **plusieurs** Replicas :

```sql
-- Attendre ACK de 2 Replicas minimum (quorum)
SET GLOBAL rpl_semi_sync_master_wait_for_slave_count = 2;
```

**Use case** : Garantie de durabilitÃ© encore plus forte, tolÃ©rance Ã  la panne d'un Replica.

âš ï¸ **Trade-off** : Latence augmentÃ©e (attente du 2Ã¨me Replica le plus lent).

### Hybrid mode : Semi-sync sur un sous-ensemble

```sql
-- Activer semi-sync uniquement sur les Replicas critiques
-- Replica 1 : Semi-sync (datacenter principal)
-- Replica 2 : Async (datacenter distant)
-- Replica 3 : Async (reporting)

-- Configuration Replica 1
SET GLOBAL rpl_semi_sync_slave_enabled = ON;

-- Configuration Replicas 2-3
SET GLOBAL rpl_semi_sync_slave_enabled = OFF;
```

### Monitoring complet

**Script de monitoring production-ready** :

```sql
-- CrÃ©er une vue de monitoring
CREATE OR REPLACE VIEW replication_health AS
SELECT 
  'semi_sync_status' AS metric,
  VARIABLE_VALUE AS value
FROM information_schema.GLOBAL_STATUS 
WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_status'

UNION ALL

SELECT 
  'semi_sync_clients',
  VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS 
WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_clients'

UNION ALL

SELECT 
  'semi_sync_avg_wait_ms',
  ROUND(VARIABLE_VALUE / 1000, 2)  -- Âµs â†’ ms
FROM information_schema.GLOBAL_STATUS 
WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_avg_wait_time'

UNION ALL

SELECT 
  'semi_sync_success_rate_pct',
  ROUND(
    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
     WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_yes_tx') /
    NULLIF((
      SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
      WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_yes_tx'
    ) + (
      SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
      WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_no_tx'
    ), 0) * 100,
    2
  );

-- Utilisation
SELECT * FROM replication_health;
-- +-----------------------------+----------+
-- | metric                      | value    |
-- +-----------------------------+----------+
-- | semi_sync_status            | ON       |
-- | semi_sync_clients           | 2        |
-- | semi_sync_avg_wait_ms       | 8.45     |
-- | semi_sync_success_rate_pct  | 98.76    |
-- +-----------------------------+----------+
```

**IntÃ©gration Prometheus** :

```yaml
# mysqld_exporter config
scrape_configs:
  - job_name: 'mariadb'
    static_configs:
      - targets: ['localhost:9104']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        
# Queries exportÃ©es
queries:
  - name: mariadb_semi_sync_status
    help: "Semi-sync replication status (1=ON, 0=OFF)"
    query: |
      SELECT 
        CASE WHEN VARIABLE_VALUE = 'ON' THEN 1 ELSE 0 END AS value
      FROM information_schema.GLOBAL_STATUS 
      WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_status';
      
  - name: mariadb_semi_sync_avg_wait_seconds
    help: "Average semi-sync wait time in seconds"
    query: |
      SELECT VARIABLE_VALUE / 1000000 AS value
      FROM information_schema.GLOBAL_STATUS 
      WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_avg_wait_time';
```

---

## Bonnes Pratiques de Production

### 1. Toujours tester le fallback async

```bash
#!/bin/bash
# Script de test : Que se passe-t-il si tous les Replicas tombent ?

echo "1. Ã‰tat initial semi-sync"
mariadb -e "SHOW STATUS LIKE 'Rpl_semi_sync_master_status';"

echo "2. ArrÃªt de tous les Replicas"
ssh replica1 "systemctl stop mariadb"
ssh replica2 "systemctl stop mariadb"

echo "3. Test Ã©criture (doit continuer en mode async)"
mariadb test -e "INSERT INTO test_table VALUES (NOW());"

echo "4. VÃ©rifier basculement automatique"
mariadb -e "SHOW STATUS LIKE 'Rpl_semi_sync_master_status';"
# Doit afficher OFF

echo "5. RedÃ©marrage Replicas"
ssh replica1 "systemctl start mariadb"
mariadb -e "SHOW STATUS LIKE 'Rpl_semi_sync_master_status';"
# Doit afficher ON
```

### 2. Dimensionner le timeout correctement

```sql
-- Mesurer la latence rÃ©seau rÃ©elle
-- Depuis le Primary
DO BENCHMARK(1000000, 1);  -- Warm-up

-- Test ping-pong avec Replica
-- Script Ã  exÃ©cuter en boucle
CREATE TABLE ping_test (ts TIMESTAMP(6));
INSERT INTO ping_test VALUES (NOW(6));

-- Sur Replica, mesurer le lag
SELECT TIMESTAMPDIFF(MICROSECOND, ts, NOW(6)) / 1000 AS lag_ms
FROM ping_test 
ORDER BY ts DESC 
LIMIT 1;

-- DÃ©finir timeout = P95 latence Ã— 2
SET GLOBAL rpl_semi_sync_master_timeout = 
  (SELECT CEIL(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY lag_ms) * 2)
   FROM lag_measurements);
```

### 3. Monitoring continu

```sql
-- CrÃ©er une alerte si semi-sync se dÃ©sactive
CREATE EVENT check_semi_sync_status
ON SCHEDULE EVERY 1 MINUTE
DO
  BEGIN
    DECLARE semi_sync_on BOOLEAN;
    
    SELECT VARIABLE_VALUE = 'ON' INTO semi_sync_on
    FROM information_schema.GLOBAL_STATUS
    WHERE VARIABLE_NAME = 'Rpl_semi_sync_master_status';
    
    IF NOT semi_sync_on THEN
      -- Logger dans une table
      INSERT INTO alerts (severity, message, created_at)
      VALUES ('CRITICAL', 'Semi-sync replication DISABLED', NOW());
      
      -- Ou envoyer alerte externe (webhook, etc.)
    END IF;
  END;
```

### 4. Documentation de la topologie

```sql
-- Documenter la configuration dans une table
CREATE TABLE replication_topology (
  host VARCHAR(255) PRIMARY KEY,
  role ENUM('primary', 'replica'),
  replication_mode ENUM('async', 'semi-sync'),
  datacenter VARCHAR(100),
  purpose VARCHAR(255),
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

INSERT INTO replication_topology VALUES
('db-primary-01.prod', 'primary', 'semi-sync', 'DC1', 'Production writes'),
('db-replica-01.prod', 'replica', 'semi-sync', 'DC1', 'Production reads + HA'),
('db-replica-02.prod', 'replica', 'async', 'DC2', 'DR site'),
('db-replica-03.prod', 'replica', 'async', 'DC1', 'Reporting/Analytics');
```

---

## âœ… Points clÃ©s Ã  retenir

1. **RÃ©plication asynchrone** : Mode par dÃ©faut, performance maximale, mais perte de donnÃ©es possible en cas de crash

2. **RÃ©plication semi-synchrone** : Garantit zero data loss, ajoute une latence rÃ©seau mais essentielle pour donnÃ©es critiques

3. **AFTER_SYNC vs AFTER_COMMIT** : Toujours utiliser AFTER_SYNC pour la cohÃ©rence maximale

4. **Timeout crucial** : Dimensionner `rpl_semi_sync_master_timeout` selon latence rÃ©seau rÃ©elle (P95 Ã— 2)

5. **ThÃ©orÃ¨me CAP** : Async privilÃ©gie disponibilitÃ© (AP), semi-sync privilÃ©gie cohÃ©rence (CP)

6. **Monitoring essentiel** : Surveiller `Rpl_semi_sync_master_status`, taux de succÃ¨s, et latence moyenne

7. **Fallback automatique** : Semi-sync bascule en async si timeout, assure disponibilitÃ©

8. **Impact performance** : -5 Ã  -30% throughput selon latence rÃ©seau, tester en conditions rÃ©elles

9. **Multiple ACKs** : `rpl_semi_sync_master_wait_for_slave_count > 1` pour tolÃ©rance Ã  la panne d'un Replica

10. **Use case driven** : Choisir selon contraintes mÃ©tier (RPO, RTO, throughput, latence)

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle MariaDB

- [ğŸ“– Replication Overview](https://mariadb.com/kb/en/replication-overview/)
- [ğŸ“– Semisynchronous Replication](https://mariadb.com/kb/en/semisynchronous-replication/)
- [ğŸ“– Replication and Binary Log System Variables](https://mariadb.com/kb/en/replication-and-binary-log-system-variables/)

### Articles techniques

- [ğŸ”— Understanding Semi-Synchronous Replication](https://mariadb.com/resources/blog/understanding-mariadb-semisynchronous-replication/)
- [ğŸ”— CAP Theorem and Databases](https://www.ibm.com/topics/cap-theorem)
- [ğŸ”— AFTER_SYNC vs AFTER_COMMIT](https://jfg-mysql.blogspot.com/2016/09/semisync-after-sync-vs-after-commit.html)

### Outils

- **pt-heartbeat** (Percona Toolkit) : Mesure prÃ©cise du lag
- **Orchestrator** : Gestion automatisÃ©e de topologies
- **PrometheusDB/Grafana** : Monitoring et alerting

---

## â¡ï¸ Section suivante

**13.2 RÃ©plication Master-Slave (Source-Replica)** : Configuration complÃ¨te Ã©tape par Ã©tape d'une topologie de rÃ©plication, crÃ©ation de l'utilisateur de rÃ©plication, Ã©tablissement de la connexion avec `CHANGE MASTER TO`, et sÃ©curisation de l'environnement.

Vous allez mettre en pratique les concepts de cette section !

---


â­ï¸ [RÃ©plication Master-Slave (Source-Replica)](/13-replication/02-replication-master-slave.md)
