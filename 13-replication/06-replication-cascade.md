üîù Retour au [Sommaire](/SOMMAIRE.md)

# 13.6 R√©plication en Cascade

> **Niveau** : Avanc√©  
> **Dur√©e estim√©e** : 2-2.5 heures  
> **Pr√©requis** : 
> - Sections 13.1 √† 13.5 (R√©plication fondamentale, GTID, multi-source)
> - Compr√©hension des architectures hi√©rarchiques
> - Notions de latence r√©seau et bande passante
> - Exp√©rience en troubleshooting de r√©plication

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :

- Comprendre l'architecture de r√©plication en cascade et ses cas d'usage
- Configurer une topologie de r√©plication multi-niveaux
- Activer et g√©rer `log_slave_updates` correctement
- Analyser et g√©rer le lag cumulatif dans une cascade
- Identifier les points de d√©faillance et les strat√©gies de mitigation
- Monitorer efficacement une topologie en cascade
- D√©cider quand utiliser (ou √©viter) la r√©plication en cascade

---

## Introduction

La **r√©plication en cascade** (ou **chained replication**) est une architecture o√π un serveur Replica agit **simultan√©ment** comme :
- **Replica** d'un Primary (re√ßoit les donn√©es)
- **Primary** pour d'autres Replicas (transmet les donn√©es)

```
Primary ‚Üí Relay1 ‚Üí Relay2 ‚Üí Replica
          (Replica + Primary)
```

### Le probl√®me r√©solu

**Sans cascade** : Surcharge du Primary

```
                    Primary
                      ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ
      ‚ñº       ‚ñº       ‚ñº       ‚ñº       ‚ñº       ‚ñº
   Replica1 Replica2 Replica3 Replica4 Replica5 Replica6

Probl√®me:
- Primary envoie binlog √† 6 Replicas
- 6 √ó threads Binlog Dump
- 6 √ó consommation bande passante
- Scalabilit√© limit√©e
```

**Avec cascade** : Distribution de charge

```
                Primary
                  ‚îÇ
                  ‚ñº
                Relay1
                  ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ       ‚îÇ       ‚îÇ
          ‚ñº       ‚ñº       ‚ñº
       Replica1 Replica2 Replica3
       
                Relay2
                  ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ       ‚îÇ       ‚îÇ
          ‚ñº       ‚ñº       ‚ñº
       Replica4 Replica5 Replica6

Avantages:
- Primary ‚Üí 2 connexions seulement
- Relay1/Relay2 distribuent la charge
- Scalabilit√© am√©lior√©e
```

### Historique

**Timeline** :

```
2003 - MySQL 4.0
       ‚îÇ Introduction concept cascade
       ‚îÇ log-slave-updates disponible
       ‚ñº
       
2014 - MariaDB 10.0
       ‚îÇ GTID facilite cascade multi-niveaux
       ‚îÇ Meilleure gestion lag cumulatif
       ‚ñº
       
2017 - MariaDB 10.3
       ‚îÇ Optimisations cascade avec GTID
       ‚îÇ Monitoring am√©lior√©
       ‚ñº
       
2025 - MariaDB 11.8 LTS
       ‚îÇ Cascade production-ready
       ‚îÇ Parallel replication optimis√©e pour cascade
```

---

## Architecture de R√©plication en Cascade

### Topologie simple (2 niveaux)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NIVEAU 1: PRIMARY                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ  Primary                             ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  server-id: 1                        ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  log-bin: ON                         ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  gtid_domain_id: 0                   ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ                                      ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  Binlog: mariadb-bin.000042          ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                    ‚îÇ                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ Replication
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NIVEAU 2: RELAY                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ  Relay (Intermediate)                ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  server-id: 2                        ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  log-bin: ON ‚úì                       ‚îÇ  ‚Üê CRITIQUE !     ‚îÇ
‚îÇ  ‚îÇ  log-slave-updates: ON ‚úì             ‚îÇ  ‚Üê CRITIQUE !     ‚îÇ
‚îÇ  ‚îÇ  gtid_domain_id: 0                   ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ                                      ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ R√¥le REPLICA (du Primary):     ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - IO Thread                    ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - SQL Thread                   ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Relay Log: relay-bin.*       ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ                                      ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ R√¥le PRIMARY (pour Replicas):  ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Binlog: mariadb-bin.000010   ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Binlog Dump Threads          ‚îÇ  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ              ‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ            ‚îÇ             ‚îÇ
               ‚ñº            ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  NIVEAU 3: REPLICAS FINAUX                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ Replica1 ‚îÇ     ‚îÇ Replica2 ‚îÇ     ‚îÇ Replica3 ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ id: 10   ‚îÇ     ‚îÇ id: 11   ‚îÇ     ‚îÇ id: 12   ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Topologie complexe (3+ niveaux)

```
                        Primary
                          ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                   ‚îÇ
                ‚ñº                   ‚ñº
             Relay1               Relay2
          (Datacenter A)      (Datacenter B)
                ‚îÇ                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ       ‚îÇ       ‚îÇ   ‚îÇ       ‚îÇ       ‚îÇ
        ‚ñº       ‚ñº       ‚ñº   ‚ñº       ‚ñº       ‚ñº
     Relay1A Relay1B Relay1C Relay2A Relay2B Relay2C
        ‚îÇ       ‚îÇ       ‚îÇ   ‚îÇ       ‚îÇ       ‚îÇ
        ‚ñº       ‚ñº       ‚ñº   ‚ñº       ‚ñº       ‚ñº
    Replicas Replicas Replicas Replicas Replicas Replicas
```

**Niveaux typiques** :
- **Niveau 1** : Primary source
- **Niveau 2** : Relays r√©gionaux (par datacenter, continent)
- **Niveau 3** : Relays locaux (par site, service)
- **Niveau 4** : Replicas finaux (application, backup)

‚ö†Ô∏è **Limitation** : Maximum **4-5 niveaux** recommand√©s (lag exponentiel au-del√†)

---

## Configuration log_slave_updates

Le param√®tre **`log_slave_updates`** est **CRITIQUE** pour la r√©plication en cascade.

### Fonctionnement

**Sans log_slave_updates (d√©faut)** :

```
Primary
  ‚Üì Binlog: INSERT INTO users...
Relay
  ‚îú‚îÄ Re√ßoit via IO Thread
  ‚îú‚îÄ Applique via SQL Thread
  ‚îî‚îÄ N'√âCRIT PAS dans son propre binlog ‚ùå

Replicas niveau 3 ‚Üí Ne re√ßoivent RIEN ‚ùå
```

**Avec log_slave_updates=ON** :

```
Primary
  ‚Üì Binlog: INSERT INTO users...
Relay
  ‚îú‚îÄ Re√ßoit via IO Thread
  ‚îú‚îÄ Applique via SQL Thread
  ‚îî‚îÄ √âCRIT dans son propre binlog ‚úì

Replicas niveau 3 ‚Üí Re√ßoivent les √©v√©nements ‚úì
```

### Configuration

**Sur TOUS les serveurs interm√©diaires (Relays)** :

```ini
# /etc/mysql/mariadb.conf.d/50-server.cnf

[mysqld]
# Configuration Relay
server-id = 2                # Unique !
log-bin = mariadb-bin        # Activer binlog
log-slave-updates = ON       # CRITIQUE pour cascade !

# Si GTID utilis√©
gtid_domain_id = 0
gtid_strict_mode = ON

# Relay log
relay-log = relay-bin
relay_log_recovery = ON

# Read-only (optionnel, selon use case)
read_only = ON
super_read_only = ON
```

**V√©rification** :

```sql
-- Sur le Relay
SELECT @@log_slave_updates;
-- +---------------------+
-- | @@log_slave_updates |
-- +---------------------+
-- |                   1 |  ‚Üê Doit √™tre 1 (ON)
-- +---------------------+

-- V√©rifier que le binlog est actif
SHOW MASTER STATUS;
-- +--------------------+----------+--------------+------------------+
-- | File               | Position | Binlog_Do_DB | Binlog_Ignore_DB |
-- +--------------------+----------+--------------+------------------+
-- | mariadb-bin.000010 | 5678     |              |                  |
-- +--------------------+----------+--------------+------------------+
```

### Impact sur les performances

**Overhead de log_slave_updates** :

```
Transaction sur Primary:
1. √âcriture InnoDB
2. √âcriture binlog Primary
3. R√©plication ‚Üí Relay
4. Relay: √âcriture InnoDB
5. Relay: √âcriture binlog (overhead log_slave_updates)
6. R√©plication ‚Üí Replicas niveau 3

Overhead:
- CPU: +5-10% (parsing + √©criture binlog)
- I/O disque: +20-30% (binlog suppl√©mentaire)
- M√©moire: Minime
```

**Benchmark** :

```
Sysbench OLTP Mixed (relay avec log_slave_updates)
Config: 16 vCPU, 64GB RAM, SSD NVMe

Sans log_slave_updates (Relay simple):
- TPS: 15,420
- Latence P95: 8.2ms
- Disk Write: 120 MB/s

Avec log_slave_updates (Relay cascade):
- TPS: 14,180 (-8%)
- Latence P95: 9.1ms (+0.9ms)
- Disk Write: 156 MB/s (+30%)

Conclusion: Overhead acceptable pour les avantages de la cascade
```

---

## Lag Cumulatif

### Le probl√®me

Le **lag se cumule** √† travers les niveaux de cascade :

```
Timeline d'une transaction:

T0: Primary ex√©cute INSERT
T1: Primary √©crit binlog (lag P = 0)
T2: Relay1 re√ßoit (lag R1 = 100ms)
T3: Relay1 applique (lag R1 = 100ms)
T4: Relay1 √©crit binlog
T5: Relay2 re√ßoit (lag R2 = 100ms + 100ms = 200ms)
T6: Replica final re√ßoit (lag = 300ms+)

Lag total = Lag Niveau1 + Lag Niveau2 + Lag Niveau3 + ...
```

### Calcul du lag cumulatif

**Formule** :

```
Lag_total = Œ£ (Lag_niveau_i + Latence_r√©seau_i)
            i=1 to N

O√π:
- N = Nombre de niveaux
- Lag_niveau_i = Secondes de retard application SQL thread
- Latence_r√©seau_i = Temps de transfert r√©seau
```

**Exemple concret** :

```
Topologie:
Primary ‚Üí Relay1 (WAN, 50ms) ‚Üí Relay2 (LAN, 2ms) ‚Üí Replica

Sc√©nario:
1. Primary ‚Üí Relay1
   - Latence r√©seau: 50ms
   - Application SQL: 20ms
   - Total niveau 1: 70ms

2. Relay1 ‚Üí Relay2
   - Latence r√©seau: 2ms
   - Application SQL: 15ms
   - Total niveau 2: 17ms

3. Relay2 ‚Üí Replica
   - Latence r√©seau: 1ms
   - Application SQL: 10ms
   - Total niveau 3: 11ms

Lag total: 70 + 17 + 11 = 98ms (acceptable)

Mais si chaque niveau a 1s de lag:
Lag total: 1000 + 1000 + 1000 = 3000ms (probl√©matique !)
```

### Visualisation du lag

```sql
-- Script pour tracer le lag √† travers la cascade

-- Sur Primary
CREATE TABLE heartbeat (
  ts TIMESTAMP(6),
  server_id INT,
  PRIMARY KEY (server_id)
) ENGINE=InnoDB;

-- Job p√©riodique sur Primary
INSERT INTO heartbeat VALUES (NOW(6), @@server_id)
ON DUPLICATE KEY UPDATE ts = NOW(6);

-- Sur chaque Relay et Replica
SELECT 
  @@hostname AS server,
  @@server_id AS server_id,
  TIMESTAMPDIFF(MICROSECOND, ts, NOW(6)) / 1000 AS lag_ms
FROM heartbeat
WHERE server_id = 1  -- Primary server_id
ORDER BY lag_ms DESC;
```

**Output cascade √† 3 niveaux** :

```
+------------------+-----------+--------+
| server           | server_id | lag_ms |
+------------------+-----------+--------+
| replica-final    | 12        | 350.5  |  ‚Üê Lag cumulatif
| relay2           | 3         | 180.2  |  ‚Üê Niveau 2
| relay1           | 2         | 85.1   |  ‚Üê Niveau 1
| primary          | 1         | 0.0    |  ‚Üê Source
+------------------+-----------+--------+
```

### Strat√©gies de r√©duction du lag

**1. Parall√©lisation agressive sur Relays**

```sql
-- Sur chaque Relay
SET GLOBAL slave_parallel_threads = 8;  -- Plus √©lev√© que Replica final
SET GLOBAL slave_parallel_mode = 'optimistic';
SET GLOBAL slave_domain_parallel_threads = 4;  -- Si multi-domain
```

**2. Hardware performant pour Relays**

```
Relays:
- SSD NVMe (I/O rapide pour binlog + relay log)
- CPU rapide (parsing binlog)
- RAM suffisante (buffer pool)
- R√©seau 10Gbps+ si haute volum√©trie

Replicas finaux:
- Peuvent √™tre moins puissants
- Le Relay absorbe la charge
```

**3. Minimiser les niveaux**

```
Mauvais (5 niveaux):
Primary ‚Üí R1 ‚Üí R2 ‚Üí R3 ‚Üí R4 ‚Üí Replica
Lag: 50 + 50 + 50 + 50 + 50 = 250ms

Meilleur (2 niveaux):
Primary ‚Üí R1 ‚Üí Replica
Lag: 50 + 50 = 100ms

Optimal (1 niveau, si possible):
Primary ‚Üí Replica
Lag: 50ms
```

**4. Compression pour WAN**

```sql
-- Sur connexions longue distance
CHANGE MASTER TO
  MASTER_HOST = 'remote-relay.example.com',
  MASTER_USE_GTID = slave_pos,
  MASTER_COMPRESSION_ALGORITHMS = 'zstd';  -- R√©duit latence r√©seau
```

**5. Monitoring continu**

```sql
-- Alerte si lag > seuil
SELECT 
  @@hostname,
  (SELECT TIMESTAMPDIFF(SECOND, ts, NOW()) 
   FROM heartbeat WHERE server_id = 1) AS lag_sec
HAVING lag_sec > 10;  -- Alerte si > 10 secondes
```

---

## Cas d'Usage

### 1. Distribution g√©ographique

**Sc√©nario** : Application multi-continents

```
Europe (Primary)
  ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ                     ‚îÇ                     ‚îÇ
  ‚ñº                     ‚ñº                     ‚ñº
US Relay            APAC Relay          LATAM Relay
(New York)          (Singapore)         (S√£o Paulo)
  ‚îÇ                     ‚îÇ                     ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îî‚îÄ‚îÄ‚îÄ Replicas
  ‚îÇ      ‚îÇ      ‚îÇ       ‚îÇ      ‚îÇ      ‚îÇ
  ‚ñº      ‚ñº      ‚ñº       ‚ñº      ‚ñº      ‚ñº
Replicas locales   Replicas locales
(Boston, Miami,    (Tokyo, Sydney,
 Chicago)           Mumbai)
```

**Avantages** :
- ‚úÖ Un seul flux transatlantique (Primary ‚Üí Relay)
- ‚úÖ Distribution locale rapide
- ‚úÖ √âconomie de bande passante internationale
- ‚úÖ Latence r√©duite pour lectures locales

**Configuration** :

```sql
-- US Relay (New York)
CHANGE MASTER TO
  MASTER_HOST = 'primary-eu.example.com',
  MASTER_USE_GTID = slave_pos,
  MASTER_COMPRESSION_ALGORITHMS = 'zstd';  -- WAN

-- Replicas US locales
CHANGE MASTER TO
  MASTER_HOST = 'relay-us-ny.example.com',  -- LAN
  MASTER_USE_GTID = slave_pos;
```

### 2. Scalabilit√© de lecture

**Sc√©nario** : 50+ Replicas en lecture

```
Primary
  ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ       ‚îÇ       ‚îÇ
  ‚ñº       ‚ñº       ‚ñº
Relay1  Relay2  Relay3
  ‚îÇ       ‚îÇ       ‚îÇ
  ‚îú‚îÄ...   ‚îú‚îÄ...   ‚îú‚îÄ...
  ‚ñº       ‚ñº       ‚ñº
20 Reps 15 Reps 15 Reps

Total: 50 Replicas
Primary connections: 3 (vs 50 sans cascade !)
```

**Avantages** :
- ‚úÖ Charge Primary r√©duite (3 connexions vs 50)
- ‚úÖ Scalabilit√© quasi-illimit√©e
- ‚úÖ Ajout/retrait Replica transparent

### 3. Migration progressive

**Sc√©nario** : Migration datacenter

```
Phase 1: Setup Relay dans nouveau DC
Old DC Primary ‚Üí New DC Relay
                     (pas encore de Replicas)

Phase 2: Ajouter Replicas dans nouveau DC
Old DC Primary ‚Üí New DC Relay ‚Üí New DC Replicas

Phase 3: Basculer Primary
New DC Primary ‚Üí Old DC Relay (r√©plication inverse)
              ‚Üí New DC Replicas

Phase 4: Nettoyer
New DC Primary ‚Üí New DC Replicas (direct)
Old DC d√©commission√©
```

### 4. Isolation environnements

**Sc√©nario** : Production ‚Üí Staging ‚Üí Dev

```
Production Primary
  ‚îÇ
  ‚ñº
Staging Relay (log_slave_updates)
  ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ             ‚îÇ
  ‚ñº             ‚ñº
Dev1 Replica  Dev2 Replica

Avantages:
- Production prot√©g√©e (1 seule connexion)
- Staging peut avoir donn√©es masqu√©es
- Devs ont donn√©es r√©centes sans impact prod
```

---

## Configuration Compl√®te Cascade

### √âtape 1 : Pr√©parer le Primary

```ini
# Primary
[mysqld]
server-id = 1
log-bin = mariadb-bin
binlog_format = ROW
gtid_domain_id = 0
max_binlog_size = 100M
sync_binlog = 1
```

```sql
-- Cr√©er utilisateur r√©plication
CREATE USER 'repl_relay'@'%' 
  IDENTIFIED BY 'Secure_Relay_P@ss_2025!';
  
GRANT REPLICATION SLAVE ON *.* 
  TO 'repl_relay'@'%';
```

### √âtape 2 : Configurer le Relay

```ini
# Relay (Intermediate)
[mysqld]
server-id = 2                # Unique !
log-bin = mariadb-bin        # Binlog actif
log-slave-updates = ON       # CRITIQUE !

binlog_format = ROW
gtid_domain_id = 0

# Relay log config
relay-log = relay-bin
relay_log_recovery = ON

# Performance
slave_parallel_threads = 4
slave_parallel_mode = optimistic

# Read-only (optionnel)
read_only = ON
super_read_only = ON
```

**Initialisation donn√©es** :

```bash
# Dump depuis Primary
mysqldump -u root -p \
  --all-databases \
  --single-transaction \
  --master-data=2 \
  --routines \
  --triggers \
  --events \
  > primary_dump.sql

# Restaurer sur Relay
mariadb -u root -p < primary_dump.sql
```

**Configuration r√©plication** :

```sql
-- Sur Relay
CHANGE MASTER TO
  MASTER_HOST = 'primary.example.com',
  MASTER_USER = 'repl_relay',
  MASTER_PASSWORD = 'Secure_Relay_P@ss_2025!',
  MASTER_USE_GTID = slave_pos;

START SLAVE;

-- V√©rifier
SHOW SLAVE STATUS\G
-- Slave_IO_Running: Yes
-- Slave_SQL_Running: Yes
```

**Cr√©er utilisateur pour Replicas niveau 3** :

```sql
-- Sur Relay
CREATE USER 'repl_final'@'%' 
  IDENTIFIED BY 'Secure_Final_P@ss_2025!';
  
GRANT REPLICATION SLAVE ON *.* 
  TO 'repl_final'@'%';
```

### √âtape 3 : Configurer Replicas finaux

```ini
# Replica final
[mysqld]
server-id = 10               # Unique !
relay-log = relay-bin
relay_log_recovery = ON
read_only = ON
super_read_only = ON
```

```bash
# Dump depuis Relay (ou Primary)
mysqldump -u root -p \
  --all-databases \
  --single-transaction \
  --master-data=2 \
  > relay_dump.sql

mariadb -u root -p < relay_dump.sql
```

```sql
-- Sur Replica final
CHANGE MASTER TO
  MASTER_HOST = 'relay.example.com',  -- Pointe vers Relay !
  MASTER_USER = 'repl_final',
  MASTER_PASSWORD = 'Secure_Final_P@ss_2025!',
  MASTER_USE_GTID = slave_pos;

START SLAVE;

SHOW SLAVE STATUS\G
```

### √âtape 4 : Validation

**Test de propagation** :

```sql
-- Sur Primary
CREATE DATABASE cascade_test;
USE cascade_test;
CREATE TABLE test (id INT PRIMARY KEY, data VARCHAR(100));
INSERT INTO test VALUES (1, 'Test at ' || NOW());

-- Sur Relay (attendre quelques secondes)
USE cascade_test;
SELECT * FROM test;
-- Doit afficher la ligne

-- Sur Replica final (attendre quelques secondes)
USE cascade_test;
SELECT * FROM test;
-- Doit afficher la ligne

-- Si OK : Cascade fonctionne ‚úì
```

---

## Monitoring Cascade

### M√©triques essentielles

**1. Lag par niveau**

```sql
-- Sur Primary: Ins√©rer heartbeat
CREATE EVENT heartbeat_event
ON SCHEDULE EVERY 1 SECOND
DO
  INSERT INTO heartbeat (ts, server_id) 
  VALUES (NOW(6), @@server_id)
  ON DUPLICATE KEY UPDATE ts = NOW(6);

-- Sur chaque serveur: Mesurer lag
SELECT 
  @@hostname AS server,
  @@server_id AS id,
  'Primary' AS level,
  0 AS lag_ms
FROM dual
WHERE @@server_id = 1

UNION ALL

SELECT 
  @@hostname,
  @@server_id,
  'Relay' AS level,
  TIMESTAMPDIFF(MICROSECOND, 
    (SELECT ts FROM heartbeat WHERE server_id = 1), 
    NOW(6)
  ) / 1000 AS lag_ms
FROM dual
WHERE @@server_id = 2

UNION ALL

SELECT 
  @@hostname,
  @@server_id,
  'Replica' AS level,
  TIMESTAMPDIFF(MICROSECOND, 
    (SELECT ts FROM heartbeat WHERE server_id = 1), 
    NOW(6)
  ) / 1000 AS lag_ms
FROM dual
WHERE @@server_id = 10;
```

**2. √âtat r√©plication √† chaque niveau**

```sql
-- Script consolid√© (√† ex√©cuter sur monitoring server)
SELECT 
  h.hostname,
  h.server_id,
  h.level,
  s.slave_io_running,
  s.slave_sql_running,
  s.seconds_behind_master,
  s.gtid_io_pos
FROM 
  (SELECT 'relay' AS hostname, 2 AS server_id, 'Relay1' AS level
   UNION SELECT 'replica1', 10, 'Replica1'
   UNION SELECT 'replica2', 11, 'Replica2') h
LEFT JOIN information_schema.SLAVE_STATUS s 
  ON h.server_id = @@server_id;
```

**3. Throughput par niveau**

```sql
-- √âv√©nements r√©pliqu√©s par seconde
SELECT 
  connection_name,
  @prev_pos := @curr_pos AS prev_pos,
  @curr_pos := read_master_log_pos AS curr_pos,
  @curr_pos - @prev_pos AS events_per_sec
FROM information_schema.SLAVE_STATUS,
     (SELECT @prev_pos := 0, @curr_pos := 0) vars
WHERE connection_name = '';

-- Ex√©cuter toutes les secondes pour calculer taux
```

### Dashboard Grafana

**Panel 1: Cascade Lag Overview**

```promql
# Prometheus query
mariadb_cascade_lag_seconds{level="relay1"}
mariadb_cascade_lag_seconds{level="relay2"}
mariadb_cascade_lag_seconds{level="replica_final"}

# Visualisation: Graph empil√© (stacked area)
```

**Panel 2: Cascade Health**

```promql
# Status binaire (1=OK, 0=ERROR)
mariadb_cascade_replication_status{server="relay1"}
mariadb_cascade_replication_status{server="relay2"}
mariadb_cascade_replication_status{server="replica"}

# Visualisation: Heatmap ou gauge
```

**Panel 3: Cumulative Lag**

```promql
# Somme du lag √† travers cascade
sum(mariadb_cascade_lag_seconds)

# Alert: > 10 seconds
```

### Script de monitoring automatis√©

```bash
#!/bin/bash
# cascade_health_check.sh

SERVERS=(
  "primary.example.com:primary"
  "relay1.example.com:relay1"
  "relay2.example.com:relay2"
  "replica1.example.com:replica1"
)

echo "=== Cascade Replication Health ==="
echo "Timestamp: $(date)"
echo ""

TOTAL_LAG=0
ERROR_COUNT=0

for SERVER_INFO in "${SERVERS[@]}"; do
  IFS=':' read -r HOST ROLE <<< "$SERVER_INFO"
  
  echo "[$ROLE] $HOST"
  
  if [ "$ROLE" = "primary" ]; then
    # Primary: V√©rifier binlog actif
    BINLOG=$(mysql -h $HOST -N -e "SELECT @@log_bin")
    if [ "$BINLOG" = "1" ]; then
      echo "  ‚úì Binlog: Active"
    else
      echo "  ‚úó Binlog: Inactive"
      ERROR_COUNT=$((ERROR_COUNT + 1))
    fi
  else
    # Replica/Relay: V√©rifier r√©plication
    STATUS=$(mysql -h $HOST -N -e "
      SELECT CONCAT(
        CASE WHEN slave_io_running='Yes' AND slave_sql_running='Yes' 
        THEN 'RUNNING' ELSE 'STOPPED' END,
        '|',
        IFNULL(seconds_behind_master, 'NULL')
      )
      FROM information_schema.SLAVE_STATUS 
      LIMIT 1
    ")
    
    IFS='|' read -r STATE LAG <<< "$STATUS"
    
    if [ "$STATE" = "RUNNING" ]; then
      echo "  ‚úì Status: $STATE"
      echo "  ‚è± Lag: ${LAG}s"
      TOTAL_LAG=$((TOTAL_LAG + LAG))
    else
      echo "  ‚úó Status: $STATE"
      ERROR_COUNT=$((ERROR_COUNT + 1))
    fi
    
    # V√©rifier log_slave_updates si Relay
    if [[ "$ROLE" == relay* ]]; then
      LSU=$(mysql -h $HOST -N -e "SELECT @@log_slave_updates")
      if [ "$LSU" = "1" ]; then
        echo "  ‚úì log_slave_updates: ON"
      else
        echo "  ‚úó log_slave_updates: OFF (CRITICAL!)"
        ERROR_COUNT=$((ERROR_COUNT + 1))
      fi
    fi
  fi
  
  echo ""
done

echo "=== Summary ==="
echo "Total cumulative lag: ${TOTAL_LAG}s"
echo "Errors: $ERROR_COUNT"

if [ $ERROR_COUNT -gt 0 ]; then
  echo "‚ùå HEALTH CHECK FAILED"
  exit 2
elif [ $TOTAL_LAG -gt 60 ]; then
  echo "‚ö†Ô∏è  WARNING: High cumulative lag"
  exit 1
else
  echo "‚úÖ HEALTH CHECK PASSED"
  exit 0
fi
```

---

## Troubleshooting Cascade

### Probl√®me 1 : Relay ne transmet pas aux Replicas

```sql
-- Sur Replica final
SHOW SLAVE STATUS\G
-- Last_IO_Error: Got fatal error 1236 from master when reading data from binary log
```

**Cause** : `log_slave_updates` non activ√© sur Relay

**Diagnostic** :

```sql
-- Sur Relay
SELECT @@log_slave_updates;
-- +---------------------+
-- | @@log_slave_updates |
-- +---------------------+
-- |                   0 |  ‚Üê PROBL√àME ! Doit √™tre 1
-- +---------------------+
```

**Solution** :

```sql
-- Sur Relay: Activer log_slave_updates (n√©cessite red√©marrage)
-- Dans my.cnf:
[mysqld]
log_slave_updates = ON

-- Red√©marrer
systemctl restart mariadb

-- V√©rifier
SELECT @@log_slave_updates;
-- Doit √™tre 1

-- Sur Replica final: Reconfigurer
STOP SLAVE;
CHANGE MASTER TO
  MASTER_HOST = 'relay.example.com',
  MASTER_USE_GTID = slave_pos;
START SLAVE;
```

### Probl√®me 2 : Lag exponentiel

```sql
-- Lag observ√©
Primary ‚Üí Relay1: 100ms
Relay1 ‚Üí Relay2: 500ms
Relay2 ‚Üí Replica: 2000ms

Total: 2600ms (inacceptable)
```

**Causes** :
- Relays sous-dimensionn√©s
- Pas de parall√©lisation
- Trop de niveaux

**Solutions** :

```sql
-- 1. Activer parall√©lisation agressive
-- Sur Relay1
SET GLOBAL slave_parallel_threads = 8;
SET GLOBAL slave_parallel_mode = 'optimistic';

-- Sur Relay2
SET GLOBAL slave_parallel_threads = 8;
SET GLOBAL slave_parallel_mode = 'optimistic';

-- 2. V√©rifier hardware
-- CPU surcharg√© ?
top
# Si load average > nombre de cores ‚Üí Upgrade CPU

-- Disque satur√© ?
iostat -x 1
# Si %util > 80% ‚Üí Upgrade vers SSD NVMe

-- 3. R√©duire niveaux (si possible)
-- √âliminer Relay2:
Primary ‚Üí Relay1 ‚Üí Replicas (direct)
```

### Probl√®me 3 : Relay tombe, Replicas orphelins

```sql
-- Relay1 CRASH üí•

-- Sur Replica final
SHOW SLAVE STATUS\G
-- Last_IO_Error: Error connecting to master 'repl_final@relay1:3306'
```

**Solution imm√©diate : Basculer sur Primary directement**

```sql
-- Sur Replica final
STOP SLAVE;

CHANGE MASTER TO
  MASTER_HOST = 'primary.example.com',  -- Direct vers Primary
  MASTER_USER = 'repl_user',
  MASTER_PASSWORD = 'password',
  MASTER_USE_GTID = slave_pos;  -- Position recalcul√©e auto avec GTID

START SLAVE;
```

**Solution long terme : Redondance Relays**

```
Primary
  ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ          ‚îÇ          ‚îÇ
  ‚ñº          ‚ñº          ‚ñº
Relay1A   Relay1B   Relay1C
  ‚îÇ          ‚îÇ          ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ           ‚îÇ
       ‚ñº           ‚ñº
    Replicas   Replicas

Si Relay1A tombe:
- Basculer vers Relay1B ou Relay1C
- Avec GTID: Automatique
```

### Probl√®me 4 : GTID out of order

```sql
-- Sur Replica niveau 3
SHOW SLAVE STATUS\G
-- Last_SQL_Error: An attempt was made to binlog GTID 0-1-1000 which would create an out-of-order sequence number
```

**Cause** : Transactions non ordonn√©es √† travers cascade

**Solution** :

```sql
-- Activer gtid_strict_mode sur TOUS les serveurs
-- Primary
SET GLOBAL gtid_strict_mode = ON;

-- Relay
SET GLOBAL gtid_strict_mode = ON;

-- Replicas
SET GLOBAL gtid_strict_mode = ON;

-- Red√©marrer r√©plication
STOP SLAVE;
START SLAVE;
```

---

## Bonnes Pratiques

### 1. Limiter les niveaux

```
‚úÖ Recommand√©:
Primary ‚Üí Relay ‚Üí Replicas (2 niveaux)
Primary ‚Üí Relay1 ‚Üí Relay2 ‚Üí Replicas (3 niveaux)

‚ö†Ô∏è √Ä √©viter:
Primary ‚Üí R1 ‚Üí R2 ‚Üí R3 ‚Üí R4 ‚Üí Replicas (5+ niveaux)
Lag cumulatif excessif
```

### 2. Dimensionner les Relays

**Hardware Relay** :
- CPU : 2-4√ó plus puissant que Replica standard
- RAM : Buffer pool g√©n√©reux (‚â• 50% de RAM)
- Disque : SSD NVMe (binlog + relay log I/O intense)
- R√©seau : 10Gbps+ si haute volum√©trie

**Configuration** :

```ini
[mysqld]
# Relay optimis√©
innodb_buffer_pool_size = 32G       # 50-70% RAM
innodb_log_file_size = 2G
innodb_flush_log_at_trx_commit = 2  # Moins strict (Relay peut √™tre reconstruit)
sync_binlog = 0                     # Moins strict
slave_parallel_threads = 8
slave_parallel_mode = optimistic
```

### 3. Toujours utiliser GTID

```sql
-- GTID simplifie radicalement la cascade

-- Ajout Replica niveau 3: Trivial avec GTID
CHANGE MASTER TO
  MASTER_HOST = 'relay.example.com',
  MASTER_USE_GTID = slave_pos;  -- Position auto !

-- Sans GTID: Complexe
-- Trouver position exacte dans binlog du Relay
-- Risque d'erreur
```

### 4. Redondance Relays critiques

```
Pour Relays critiques (geo-distribution):

          Primary
             ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚ñº               ‚ñº
  Relay-A         Relay-B  (Redondance)
     ‚îÇ               ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
          Replicas

Si Relay-A tombe:
- Replicas basculent vers Relay-B
- Avec Orchestrator: Automatique
```

### 5. Monitoring proactif

```sql
-- Alerte si lag > seuil acceptable
CREATE EVENT check_cascade_lag
ON SCHEDULE EVERY 1 MINUTE
DO
  BEGIN
    DECLARE total_lag INT;
    
    SELECT SUM(seconds_behind_master) INTO total_lag
    FROM information_schema.SLAVE_STATUS;
    
    IF total_lag > 30 THEN
      INSERT INTO alerts (severity, message, created_at)
      VALUES ('WARNING', 
              CONCAT('Cascade lag: ', total_lag, 's'), 
              NOW());
    END IF;
  END;
```

### 6. Documentation topologie

```sql
CREATE TABLE cascade_topology (
  server_id INT PRIMARY KEY,
  hostname VARCHAR(255),
  role ENUM('primary', 'relay', 'replica'),
  level INT,  -- Niveau dans cascade
  parent_server_id INT,
  gtid_domain INT,
  notes TEXT,
  FOREIGN KEY (parent_server_id) REFERENCES cascade_topology(server_id)
);

INSERT INTO cascade_topology VALUES
(1, 'primary.example.com', 'primary', 1, NULL, 0, 'Production primary'),
(2, 'relay1.example.com', 'relay', 2, 1, 0, 'US relay'),
(3, 'relay2.example.com', 'relay', 2, 1, 0, 'EU relay'),
(10, 'replica1.example.com', 'replica', 3, 2, 0, 'US read replica'),
(11, 'replica2.example.com', 'replica', 3, 3, 0, 'EU read replica');

-- Requ√™te topologie
SELECT 
  CONCAT(REPEAT('  ', level-1), hostname) AS topology
FROM cascade_topology
ORDER BY level, server_id;
-- primary.example.com
--   relay1.example.com
--     replica1.example.com
--   relay2.example.com
--     replica2.example.com
```

---

## ‚úÖ Points cl√©s √† retenir

1. **log_slave_updates** : Param√®tre CRITIQUE pour cascade, doit √™tre ON sur TOUS les Relays

2. **Lag cumulatif** : Se cumule √† travers les niveaux, limiter √† 2-3 niveaux maximum

3. **GTID essentiel** : Simplifie configuration et failover de mani√®re dramatique

4. **Hardware Relays** : Doivent √™tre plus puissants que Replicas standards

5. **Use cases** : Geo-distribution, scalabilit√© lecture, isolation environnements

6. **Monitoring** : Mesurer lag √† chaque niveau avec table heartbeat

7. **Parall√©lisation** : Agressive sur Relays pour r√©duire lag

8. **Redondance** : Relays critiques doivent √™tre redondants

9. **Limitation** : Maximum 4-5 niveaux avant lag inacceptable

10. **Documentation** : Topologie doit √™tre document√©e et √† jour

---

## üîó Ressources et r√©f√©rences

### Documentation officielle MariaDB

- [üìñ Replication Overview](https://mariadb.com/kb/en/replication-overview/)
- [üìñ log_slave_updates](https://mariadb.com/kb/en/replication-and-binary-log-system-variables/#log_slave_updates)
- [üìñ Parallel Replication](https://mariadb.com/kb/en/parallel-replication/)

### Articles techniques

- [üîó Chained Replication Best Practices](https://mariadb.com/resources/blog/)
- [üîó Measuring Replication Lag](https://www.percona.com/blog/)

### Outils

- **pt-heartbeat** (Percona Toolkit) : Mesure pr√©cise lag cascade
- **Orchestrator** : Gestion topologies complexes avec cascade
- **MaxScale** : Routing intelligent multi-niveaux

---

## ‚û°Ô∏è Section suivante

**13.7 Monitoring et troubleshooting** : Commandes SHOW REPLICA STATUS d√©taill√©es, analyse du lag, diagnostic d'erreurs courantes, outils de monitoring (pt-heartbeat, Orchestrator), et m√©triques critiques de production.

---


‚è≠Ô∏è [Monitoring et troubleshooting](/13-replication/07-monitoring-troubleshooting.md)
