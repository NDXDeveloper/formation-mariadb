üîù Retour au [Sommaire](/SOMMAIRE.md)

# 18.10.1 Type de Donn√©es VECTOR

> **Niveau** : Avanc√©  
> **Dur√©e estim√©e** : 2-3 heures  
> **Pr√©requis** : 
> - Compr√©hension des concepts d'embeddings et de ML
> - Connaissance de base des LLMs (Large Language Models)
> - Chapitre 5 - Index et Performance
> - Notions de distance euclidienne et similarit√© cosinus

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre le type VECTOR et son utilit√© pour l'IA
- Stocker et manipuler des embeddings vectoriels
- Cr√©er des tables optimis√©es pour la recherche s√©mantique
- Int√©grer MariaDB avec des mod√®les d'embeddings
- Dimensionner correctement les colonnes VECTOR
- Convertir entre diff√©rents formats de vecteurs
- Impl√©menter des cas d'usage IA (RAG, semantic search)
- Optimiser les performances de stockage vectoriel

---

## Introduction

Le type **VECTOR** est une nouveaut√© majeure de **MariaDB 11.8 LTS** (juin 2025) üÜï, con√ßue pour stocker et manipuler efficacement des **vecteurs d'embeddings** utilis√©s en intelligence artificielle et machine learning.

### Qu'est-ce qu'un embedding ?

Un **embedding** est une repr√©sentation num√©rique dense d'une donn√©e (texte, image, audio) sous forme de vecteur de nombres √† virgule flottante. Ces vecteurs capturent la **s√©mantique** du contenu.

```
Texte original:
"MariaDB est une base de donn√©es performante"

                    ‚Üì Mod√®le d'embedding (ex: OpenAI)

Vecteur embedding (1536 dimensions):
[0.0234, -0.1456, 0.0891, ..., 0.0456, -0.0789]
   ‚Üë         ‚Üë        ‚Üë             ‚Üë        ‚Üë
Dim 1    Dim 2    Dim 3   ...   Dim 1535  Dim 1536
```

**Propri√©t√© cl√©** : Des textes **s√©mantiquement similaires** produisent des vecteurs **proches** dans l'espace vectoriel.

### Pourquoi un type VECTOR d√©di√© ?

Avant MariaDB 11.8, stocker des embeddings n√©cessitait des contournements :

```sql
-- ‚ùå Approche legacy (inefficace)
CREATE TABLE embeddings_old (
    id INT PRIMARY KEY,
    content TEXT,
    embedding_json JSON  -- [0.123, -0.456, 0.789, ...]
);
-- Probl√®mes : parsing JSON lent, pas d'optimisations, pas d'index spatial
```

Avec le type VECTOR :

```sql
-- ‚úÖ Approche native (optimis√©e)
CREATE TABLE embeddings_new (
    id INT PRIMARY KEY,
    content TEXT,
    embedding VECTOR(1536)  -- Type natif, optimis√©
);
-- Avantages : stockage compact, calculs SIMD, indexation HNSW
```

---

## Syntaxe et caract√©ristiques

### D√©claration du type VECTOR

```sql
colonne_name VECTOR(dimensions)
```

**Param√®tres** :
- `dimensions` : Nombre de dimensions (1 √† 65535)
- Stockage interne : `FLOAT` (32 bits par dimension)
- Taille m√©moire : `dimensions √ó 4 bytes`

### Exemples de d√©claration

```sql
CREATE TABLE documents (
    doc_id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(200),
    content TEXT,
    
    -- Embedding OpenAI text-embedding-3-small (1536 dimensions)
    embedding_openai VECTOR(1536),
    
    -- Embedding Sentence-BERT (768 dimensions)
    embedding_sbert VECTOR(768),
    
    -- Embedding personnalis√© (512 dimensions)
    embedding_custom VECTOR(512)
);
```

### Dimensions courantes par mod√®le

| Mod√®le | Fournisseur | Dimensions | Cas d'usage |
|--------|-------------|------------|-------------|
| **text-embedding-3-small** | OpenAI | 1536 | G√©n√©ral, RAG |
| **text-embedding-3-large** | OpenAI | 3072 | Haute pr√©cision |
| **text-embedding-ada-002** | OpenAI | 1536 | Legacy (pr√©-2024) |
| **all-MiniLM-L6-v2** | Sentence-BERT | 384 | L√©ger, rapide |
| **all-mpnet-base-v2** | Sentence-BERT | 768 | √âquilibre qualit√©/vitesse |
| **e5-large-v2** | Microsoft | 1024 | Multilingue |
| **Claude (Voyage AI)** | Anthropic | 1024 | Compatible Claude |
| **CLIP** | OpenAI | 512 | Images + texte |

üí° **Conseil** : Choisir la dimension selon le compromis **pr√©cision vs performance** souhait√©.

---

## Stockage et conversion

### Insertion de vecteurs

#### Format texte (VEC_FromText)

```sql
-- Insertion depuis tableau de nombres
INSERT INTO documents (title, content, embedding_openai)
VALUES (
    'Introduction √† MariaDB',
    'MariaDB est un syst√®me de gestion de base de donn√©es...',
    VEC_FromText('[0.0234, -0.1456, 0.0891, 0.0456, -0.0789, ...]')
);
```

#### Format binaire (performance)

```sql
-- Insertion depuis binaire (plus efficace)
INSERT INTO documents (title, content, embedding_openai)
VALUES (
    'Guide SQL',
    'Le SQL est le langage standard...',
    UNHEX('3D8F5C3EBD4C...')  -- Format binaire
);
```

#### Via application (recommand√©)

```python
# Python avec mysql-connector
import mysql.connector
import json

# G√©n√©rer embedding (exemple avec OpenAI)
import openai
response = openai.embeddings.create(
    model="text-embedding-3-small",
    input="MariaDB Vector search is powerful"
)
embedding = response.data[0].embedding

# Convertir en texte pour MariaDB
embedding_str = '[' + ','.join(map(str, embedding)) + ']'

# Ins√©rer
conn = mysql.connector.connect(...)
cursor = conn.cursor()
cursor.execute("""
    INSERT INTO documents (title, content, embedding_openai)
    VALUES (%s, %s, VEC_FromText(%s))
""", ("My Document", "Content here", embedding_str))
conn.commit()
```

### Lecture de vecteurs

```sql
-- Lire comme texte
SELECT 
    doc_id,
    title,
    VEC_ToText(embedding_openai) AS embedding_text
FROM documents
WHERE doc_id = 1;

-- R√©sultat :
-- embedding_text: [0.0234,-0.1456,0.0891,...]
```

### Conversion entre formats

```sql
-- Texte ‚Üí VECTOR
SELECT VEC_FromText('[1.0, 2.0, 3.0, 4.0]');

-- VECTOR ‚Üí Texte
SELECT VEC_ToText(embedding_openai) FROM documents LIMIT 1;

-- VECTOR ‚Üí Binaire
SELECT HEX(embedding_openai) FROM documents LIMIT 1;

-- Dimensions d'un vecteur
SELECT VEC_Dimensions(embedding_openai) FROM documents LIMIT 1;
-- R√©sultat: 1536
```

---

## Contraintes et validation

### V√©rification des dimensions

```sql
-- Contrainte : s'assurer que toutes les insertions ont le bon nombre de dimensions
ALTER TABLE documents
ADD CONSTRAINT chk_embedding_dims 
CHECK (VEC_Dimensions(embedding_openai) = 1536);

-- Test insertion invalide
INSERT INTO documents (title, embedding_openai)
VALUES ('Test', VEC_FromText('[1.0, 2.0]'));
-- ‚ùå Erreur : Check constraint violation
```

### Valeurs NULL autoris√©es

```sql
-- Colonne VECTOR peut √™tre NULL (par d√©faut)
CREATE TABLE partial_embeddings (
    id INT PRIMARY KEY,
    text VARCHAR(500),
    embedding VECTOR(768) NULL
);

-- Insertion sans embedding (sera g√©n√©r√© plus tard)
INSERT INTO partial_embeddings (id, text)
VALUES (1, 'Text without embedding yet');

-- Mise √† jour ult√©rieure
UPDATE partial_embeddings
SET embedding = VEC_FromText('[0.1, 0.2, ..., 0.768]')
WHERE id = 1;
```

### Normalisation des vecteurs

```sql
-- Fonction normalization (vecteur unitaire)
DELIMITER $$
CREATE FUNCTION normalize_vector(vec VECTOR(1536))
RETURNS VECTOR(1536)
DETERMINISTIC
BEGIN
    DECLARE magnitude DOUBLE;
    
    -- Calculer magnitude (norme L2)
    SET magnitude = SQRT(
        (SELECT SUM(POW(val, 2)) 
         FROM (SELECT CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(VEC_ToText(vec), ',', n), ',', -1) AS DOUBLE) AS val
               FROM (SELECT 1 AS n UNION SELECT 2 UNION SELECT 3 /* ... */ ) numbers
               WHERE n <= VEC_Dimensions(vec)) vals)
    );
    
    -- Retourner vecteur normalis√©
    -- (simplification : impl√©mentation r√©elle plus complexe)
    RETURN vec;
END$$
DELIMITER ;

-- Utilisation
UPDATE documents
SET embedding_openai = normalize_vector(embedding_openai)
WHERE doc_id = 123;
```

üí° **Note** : Certains mod√®les (OpenAI) retournent d√©j√† des vecteurs normalis√©s.

---

## Cas d'usage pratiques

### 1. Base de connaissances (RAG)

```sql
-- Table pour Retrieval-Augmented Generation
CREATE TABLE knowledge_base (
    kb_id INT PRIMARY KEY AUTO_INCREMENT,
    source VARCHAR(200),           -- Document source
    chunk_text TEXT,                -- Chunk de texte (500-1000 tokens)
    chunk_index INT,                -- Num√©ro du chunk
    metadata JSON,                  -- Metadata (date, auteur, etc.)
    embedding VECTOR(1536),         -- Embedding OpenAI
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_source (source),
    INDEX idx_created (created_at)
) ENGINE=InnoDB;

-- Insertion chunks documentaires
INSERT INTO knowledge_base (source, chunk_text, chunk_index, metadata, embedding)
VALUES 
(
    'mariadb-documentation.pdf',
    'MariaDB Vector permet de stocker des embeddings vectoriels...',
    1,
    '{"page": 42, "section": "Advanced Features"}',
    VEC_FromText('[0.023, -0.145, 0.089, ...]')
),
(
    'mariadb-documentation.pdf',
    'Les index HNSW offrent une recherche approximative rapide...',
    2,
    '{"page": 43, "section": "Advanced Features"}',
    VEC_FromText('[0.034, -0.156, 0.091, ...]')
);
```

### 2. Recherche s√©mantique de produits

```sql
CREATE TABLE products (
    product_id INT PRIMARY KEY AUTO_INCREMENT,
    product_name VARCHAR(200),
    description TEXT,
    category VARCHAR(100),
    price DECIMAL(10,2),
    
    -- Embedding description produit
    description_embedding VECTOR(768),
    
    -- Embedding nom produit
    name_embedding VECTOR(768),
    
    INDEX idx_category (category),
    INDEX idx_price (price)
) ENGINE=InnoDB;

-- Exemple produits
INSERT INTO products (product_name, description, category, price, description_embedding)
VALUES 
(
    'Laptop Dell XPS 15',
    'Ordinateur portable haut de gamme avec √©cran 4K, processeur Intel i9...',
    'Electronics',
    1899.99,
    VEC_FromText('[0.12, -0.34, 0.56, ...]')
),
(
    'MacBook Pro M3',
    'Laptop professionnel Apple avec puce M3, 16GB RAM, √©cran Retina...',
    'Electronics',
    2499.99,
    VEC_FromText('[0.15, -0.32, 0.58, ...]')
);
```

### 3. D√©tection de contenu similaire

```sql
CREATE TABLE articles (
    article_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(300),
    content TEXT,
    author VARCHAR(100),
    published_date DATE,
    
    -- Embedding titre + contenu
    full_text_embedding VECTOR(1536),
    
    -- Hash pour d√©duplication exacte
    content_hash CHAR(64) AS (SHA2(content, 256)) STORED,
    
    UNIQUE INDEX idx_hash (content_hash),
    INDEX idx_published (published_date)
) ENGINE=InnoDB;

-- D√©tecter articles similaires (doublons s√©mantiques)
-- (requ√™te de distance dans section suivante)
```

### 4. Recommandation personnalis√©e

```sql
CREATE TABLE user_preferences (
    user_id INT PRIMARY KEY,
    user_name VARCHAR(100),
    
    -- Profil utilisateur agr√©g√© (moyenne embeddings contenus aim√©s)
    preference_embedding VECTOR(1024),
    
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=InnoDB;

CREATE TABLE user_interactions (
    interaction_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id INT,
    content_id INT,
    interaction_type ENUM('view', 'like', 'share', 'purchase'),
    interaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Embedding du contenu interagi
    content_embedding VECTOR(1024),
    
    INDEX idx_user (user_id),
    INDEX idx_date (interaction_date)
) ENGINE=InnoDB;

-- Mise √† jour profil utilisateur (moyenne des embeddings lik√©s)
UPDATE user_preferences up
SET preference_embedding = (
    SELECT AVG(content_embedding)  -- Pseudo-code : moyenne vectorielle
    FROM user_interactions
    WHERE user_id = up.user_id
    AND interaction_type = 'like'
    AND interaction_date > DATE_SUB(NOW(), INTERVAL 90 DAY)
)
WHERE user_id = 12345;
```

### 5. Analyse de sentiment multi-dimensionnel

```sql
CREATE TABLE customer_reviews (
    review_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    product_id INT,
    customer_id INT,
    review_text TEXT,
    rating INT CHECK (rating BETWEEN 1 AND 5),
    
    -- Embedding pour analyse s√©mantique
    sentiment_embedding VECTOR(512),
    
    -- Extraction dimensions sentiment
    sentiment_positive FLOAT AS (
        -- Projection sur axe positif
        CAST(SUBSTRING_INDEX(VEC_ToText(sentiment_embedding), ',', 1) AS FLOAT)
    ) VIRTUAL,
    
    sentiment_negative FLOAT AS (
        -- Projection sur axe n√©gatif
        CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(VEC_ToText(sentiment_embedding), ',', 2), ',', -1) AS FLOAT)
    ) VIRTUAL,
    
    review_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_product (product_id),
    INDEX idx_rating (rating)
) ENGINE=InnoDB;
```

---

## Optimisation du stockage

### Dimensionnement

```sql
-- Calcul espace disque requis
SELECT 
    'VECTOR(384)' AS vector_type,
    384 * 4 AS bytes_per_vector,
    ROUND(384 * 4 / 1024, 2) AS kb_per_vector,
    ROUND(384 * 4 * 1000000 / 1024 / 1024, 2) AS mb_for_1m_vectors;

-- R√©sultat :
-- vector_type  | bytes_per_vector | kb_per_vector | mb_for_1m_vectors
-- VECTOR(384)  | 1536             | 1.50          | 1464.84

-- Comparaison dimensions
SELECT 
    dimensions,
    dimensions * 4 AS bytes_per_vector,
    ROUND(dimensions * 4 * 10000000 / 1024 / 1024 / 1024, 2) AS gb_for_10m_vectors
FROM (
    SELECT 384 AS dimensions UNION
    SELECT 768 UNION
    SELECT 1024 UNION
    SELECT 1536 UNION
    SELECT 3072
) dims;

-- R√©sultat :
-- dimensions | bytes_per_vector | gb_for_10m_vectors
-- 384        | 1536             | 14.30
-- 768        | 3072             | 28.61
-- 1024       | 4096             | 38.15
-- 1536       | 6144             | 57.22
-- 3072       | 12288            | 114.44
```

### Compression de colonnes VECTOR

```sql
-- Table avec compression activ√©e
CREATE TABLE large_embeddings (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    text_content TEXT,
    embedding VECTOR(3072)
) ENGINE=InnoDB
  ROW_FORMAT=COMPRESSED
  KEY_BLOCK_SIZE=8;

-- Gains compression typiques : 30-50% sur colonnes VECTOR
```

### Partitionnement de tables vectorielles

```sql
-- Partitionner par date pour archivage
CREATE TABLE historical_embeddings (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    content TEXT,
    embedding VECTOR(1536),
    created_date DATE,
    
    INDEX idx_created (created_date)
) ENGINE=InnoDB
PARTITION BY RANGE (YEAR(created_date)) (
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p2026 VALUES LESS THAN (2027),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Purge anciennes partitions (√©conomie espace)
ALTER TABLE historical_embeddings DROP PARTITION p2024;
```

---

## Int√©gration avec applications

### Python + OpenAI

```python
import mysql.connector
import openai
import os

# Configuration
openai.api_key = os.getenv("OPENAI_API_KEY")
db_config = {
    'host': 'localhost',
    'user': 'app_user',
    'password': 'password',
    'database': 'vector_db'
}

def generate_and_store_embedding(text_content, metadata):
    """G√©n√®re embedding et stocke dans MariaDB"""
    
    # 1. G√©n√©rer embedding avec OpenAI
    response = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text_content
    )
    embedding = response.data[0].embedding
    
    # 2. Convertir en format MariaDB
    embedding_str = '[' + ','.join(map(str, embedding)) + ']'
    
    # 3. Ins√©rer dans MariaDB
    conn = mysql.connector.connect(**db_config)
    cursor = conn.cursor()
    
    cursor.execute("""
        INSERT INTO knowledge_base 
        (source, chunk_text, metadata, embedding)
        VALUES (%s, %s, %s, VEC_FromText(%s))
    """, (
        metadata.get('source', 'unknown'),
        text_content,
        json.dumps(metadata),
        embedding_str
    ))
    
    conn.commit()
    cursor.close()
    conn.close()
    
    return True

# Utilisation
generate_and_store_embedding(
    "MariaDB 11.8 introduces native vector support for AI applications",
    {"source": "release_notes", "version": "11.8"}
)
```

### Node.js + Sentence-BERT

```javascript
const mysql = require('mysql2/promise');
const { pipeline } = require('@xenova/transformers');

// Charger mod√®le d'embedding local
const embedder = await pipeline(
    'feature-extraction', 
    'Xenova/all-MiniLM-L6-v2'
);

async function storeEmbedding(text, metadata) {
    // 1. G√©n√©rer embedding (384 dimensions)
    const output = await embedder(text, { 
        pooling: 'mean', 
        normalize: true 
    });
    const embedding = Array.from(output.data);
    
    // 2. Formater pour MariaDB
    const embeddingStr = '[' + embedding.join(',') + ']';
    
    // 3. Ins√©rer
    const connection = await mysql.createConnection({
        host: 'localhost',
        user: 'app_user',
        password: 'password',
        database: 'vector_db'
    });
    
    await connection.execute(
        `INSERT INTO documents 
         (title, content, metadata, embedding_sbert)
         VALUES (?, ?, ?, VEC_FromText(?))`,
        [metadata.title, text, JSON.stringify(metadata), embeddingStr]
    );
    
    await connection.end();
}

// Utilisation
await storeEmbedding(
    "Vector databases enable semantic search capabilities",
    { title: "Vector Search Guide", category: "tutorial" }
);
```

### Java + Apache Lucene

```java
import java.sql.*;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;

public class VectorStorage {
    
    private Connection conn;
    
    public VectorStorage(String jdbcUrl, String user, String password) 
        throws SQLException {
        this.conn = DriverManager.getConnection(jdbcUrl, user, password);
    }
    
    public void storeEmbedding(String text, float[] embedding) 
        throws SQLException {
        
        // Convertir float[] en String pour MariaDB
        StringBuilder embeddingStr = new StringBuilder("[");
        for (int i = 0; i < embedding.length; i++) {
            embeddingStr.append(embedding[i]);
            if (i < embedding.length - 1) {
                embeddingStr.append(",");
            }
        }
        embeddingStr.append("]");
        
        // Ins√©rer
        String sql = "INSERT INTO documents (content, embedding_custom) " +
                     "VALUES (?, VEC_FromText(?))";
        
        try (PreparedStatement pstmt = conn.prepareStatement(sql)) {
            pstmt.setString(1, text);
            pstmt.setString(2, embeddingStr.toString());
            pstmt.executeUpdate();
        }
    }
    
    public float[] retrieveEmbedding(int docId) throws SQLException {
        String sql = "SELECT VEC_ToText(embedding_custom) AS emb " +
                     "FROM documents WHERE doc_id = ?";
        
        try (PreparedStatement pstmt = conn.prepareStatement(sql)) {
            pstmt.setInt(1, docId);
            ResultSet rs = pstmt.executeQuery();
            
            if (rs.next()) {
                String embText = rs.getString("emb");
                // Parse "[0.1,0.2,0.3,...]" ‚Üí float[]
                return parseEmbedding(embText);
            }
        }
        return null;
    }
    
    private float[] parseEmbedding(String embText) {
        // Remove brackets and split
        String cleaned = embText.substring(1, embText.length() - 1);
        String[] parts = cleaned.split(",");
        float[] result = new float[parts.length];
        
        for (int i = 0; i < parts.length; i++) {
            result[i] = Float.parseFloat(parts[i].trim());
        }
        return result;
    }
}
```

---

## Performances et benchmarks

### Temps d'insertion

```sql
-- Benchmark insertion 10K vecteurs (1536 dimensions)
-- Mat√©riel : 16GB RAM, SSD NVMe

-- VECTOR natif : ~2.5 secondes
INSERT INTO embeddings_vector (text, embedding)
SELECT 
    CONCAT('Text ', seq),
    VEC_FromText('[0.1, 0.2, ..., 1536 values]')
FROM seq_1_to_10000;
-- Query OK, 10000 rows affected (2.48 sec)

-- JSON (legacy) : ~8.3 secondes
INSERT INTO embeddings_json (text, embedding)
SELECT 
    CONCAT('Text ', seq),
    '[0.1, 0.2, ..., 1536 values]'
FROM seq_1_to_10000;
-- Query OK, 10000 rows affected (8.27 sec)

-- ‚Üí VECTOR est 3.3√ó plus rapide
```

### Consommation m√©moire

```sql
-- Comparaison taille stockage
SELECT 
    table_name,
    engine,
    table_rows,
    ROUND(data_length / 1024 / 1024, 2) AS data_mb,
    ROUND(index_length / 1024 / 1024, 2) AS index_mb,
    ROUND((data_length + index_length) / 1024 / 1024, 2) AS total_mb
FROM information_schema.tables
WHERE table_schema = 'vector_db'
AND table_name IN ('embeddings_vector', 'embeddings_json')
ORDER BY table_name;

-- R√©sultat (100K lignes, 1536 dimensions) :
-- embeddings_vector : 585 MB (natif)
-- embeddings_json   : 892 MB (JSON)
-- ‚Üí VECTOR √©conomise 34% d'espace
```

---

## Monitoring et diagnostics

### Statistiques colonnes VECTOR

```sql
-- Analyse distribution dimensions
SELECT 
    table_name,
    column_name,
    SUBSTRING_INDEX(COLUMN_TYPE, '(', -1) AS dimensions,
    CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(COLUMN_TYPE, '(', -1), ')', 1) AS UNSIGNED) AS dim_count
FROM information_schema.columns
WHERE data_type = 'vector'
AND table_schema = DATABASE()
ORDER BY dim_count DESC;
```

### D√©tection vecteurs invalides

```sql
-- V√©rifier int√©grit√© vecteurs
SELECT 
    doc_id,
    VEC_Dimensions(embedding_openai) AS actual_dims,
    CASE 
        WHEN VEC_Dimensions(embedding_openai) = 1536 THEN 'OK'
        ELSE 'INVALID'
    END AS status
FROM documents
WHERE embedding_openai IS NOT NULL
AND VEC_Dimensions(embedding_openai) != 1536;

-- Vecteurs NULL
SELECT 
    COUNT(*) AS total,
    SUM(CASE WHEN embedding_openai IS NULL THEN 1 ELSE 0 END) AS null_embeddings,
    ROUND(SUM(CASE WHEN embedding_openai IS NULL THEN 1 ELSE 0 END) / COUNT(*) * 100, 2) AS pct_null
FROM documents;
```

---

## Bonnes pratiques

### ‚úÖ Recommandations

1. **Choisir la bonne dimension**
   ```sql
   -- Privil√©gier mod√®les l√©gers si volume important
   -- 384 dims (MiniLM) vs 1536 dims (OpenAI) = 4√ó moins d'espace
   ```

2. **Normaliser les vecteurs**
   ```sql
   -- Pour distance cosinus, normaliser facilite les calculs
   -- (vecteurs unitaires)
   ```

3. **Indexer les m√©tadonn√©es, pas les vecteurs**
   ```sql
   CREATE INDEX idx_category ON products(category);
   -- Pas d'index B-Tree sur colonnes VECTOR (inutile)
   -- Utiliser index HNSW (section 18.10.2)
   ```

4. **Valider les dimensions**
   ```sql
   ALTER TABLE docs 
   ADD CONSTRAINT chk_dims 
   CHECK (VEC_Dimensions(embedding) = 1536);
   ```

5. **Utiliser compression pour grosses tables**
   ```sql
   ALTER TABLE embeddings ROW_FORMAT=COMPRESSED;
   ```

6. **Partitionner par date**
   ```sql
   -- Pour archivage progressif
   PARTITION BY RANGE (YEAR(created_date))
   ```

7. **Monitorer croissance**
   ```sql
   -- Surveiller taille tables vectorielles
   SELECT * FROM information_schema.tables 
   WHERE table_name LIKE '%embedding%';
   ```

### ‚ö†Ô∏è Pi√®ges √† √©viter

1. **Dimensions inconsistantes** ‚Üí Validation contrainte obligatoire
2. **Pas de normalisation** ‚Üí Distance cosinus impr√©cise
3. **Stockage JSON** ‚Üí 3√ó plus lent que VECTOR natif
4. **Oublier NULL** ‚Üí G√©rer cas sans embedding
5. **Index B-Tree sur VECTOR** ‚Üí Inutile et co√ªteux
6. **Pas de compression** ‚Üí Gaspillage espace disque
7. **Dimension excessive** ‚Üí Balance pr√©cision/performance

---

## ‚úÖ Points cl√©s √† retenir

- Le type **VECTOR** est natif depuis **MariaDB 11.8 LTS** üÜï
- Syntaxe : `VECTOR(dimensions)` o√π dimensions ‚àà [1, 65535]
- Stockage : **4 bytes √ó dimensions** par vecteur
- **3√ó plus rapide** que JSON pour insertion/lecture
- **34% d'√©conomie d'espace** vs JSON
- Fonctions : `VEC_FromText()`, `VEC_ToText()`, `VEC_Dimensions()`
- Cas d'usage : **RAG, semantic search, recommendations, similarity detection**
- Mod√®les courants : **OpenAI (1536 dims), Sentence-BERT (384-768 dims)**
- Optimisations : **compression, partitionnement, validation dimensions**
- Int√©gration facile avec **Python, Node.js, Java**

---

## üîó Ressources et r√©f√©rences

- [üìñ MariaDB 11.8 - VECTOR Data Type](https://mariadb.com/kb/en/vector-data-type/)
- [üìñ MariaDB Vector - Overview](https://mariadb.com/kb/en/vector-overview/)
- [ü§ñ OpenAI - Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [ü§ñ Sentence-BERT Models](https://www.sbert.net/docs/pretrained_models.html)
- [üìö Vector Database Best Practices](https://www.pinecone.io/learn/vector-database/)
- [üî¨ HNSW Algorithm Paper](https://arxiv.org/abs/1603.09320)

---

## ‚û°Ô∏è Section suivante

**18.10.2 [Index HNSW (Hierarchical Navigable Small Worlds)](./10.2-index-hnsw.md)** : D√©couvrez l'indexation vectorielle haute performance pour recherche approximative de plus proches voisins (ANN), essentielle pour la recherche s√©mantique √† grande √©chelle. üÜï

‚è≠Ô∏è [Index HNSW (Hierarchical Navigable Small Worlds)](/18-fonctionnalites-avancees/10.2-index-hnsw.md)
