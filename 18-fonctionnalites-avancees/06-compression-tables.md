üîù Retour au [Sommaire](/SOMMAIRE.md)

# 18.6 Compression de Tables

> **Niveau** : Avanc√©  
> **Dur√©e estim√©e** : 1.5-2 heures  
> **Pr√©requis** : Chapitre 7 (Moteurs de stockage InnoDB), compr√©hension des performances I/O

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :

- Comprendre les **deux m√©thodes de compression InnoDB** (COMPRESSED vs PAGE_COMPRESSED)
- Configurer **ROW_FORMAT=COMPRESSED** avec KEY_BLOCK_SIZE appropri√©
- Utiliser **PAGE_COMPRESSED** avec compression transparente
- Analyser l'**impact sur performance** (CPU, I/O, espace disque)
- Choisir la **m√©thode de compression** selon le cas d'usage
- Optimiser la compression pour **cloud et SSD**
- Mesurer le **ratio de compression** et l'√©conomie d'espace
- Identifier les **tables candidates** √† la compression

---

## Introduction

La **compression de tables** permet de r√©duire l'espace disque utilis√© et, dans certains cas, d'am√©liorer les performances I/O en √©changeant de la puissance CPU contre une r√©duction du volume de donn√©es lues/√©crites. MariaDB InnoDB offre deux m√©thodes principales de compression, chacune avec ses avantages et cas d'usage.

### Pourquoi Compresser les Tables ?

**B√©n√©fices de la compression** :

1. **üíæ R√©duction de l'Espace Disque**
   - √âconomie 40-70% d'espace selon les donn√©es
   - Stockage de plus de donn√©es sur m√™me hardware
   - R√©duction des co√ªts cloud (stockage factur√© au Go)

2. **‚ö° Am√©lioration des Performances I/O**
   - Moins de donn√©es √† lire/√©crire sur disque
   - R√©duction de la latence I/O (surtout HDD)
   - Buffer pool plus efficace (plus de donn√©es en cache)

3. **üí∞ Optimisation des Co√ªts**
   - Stockage cloud moins cher (AWS EBS, Azure Disks)
   - Moins de IOPS consomm√©s
   - Bande passante r√©seau r√©duite (r√©plication, backup)

4. **üìà Scalabilit√© Am√©lior√©e**
   - Archivage de plus d'historique
   - R√©tention de logs plus longue
   - Capacit√© accrue sans agrandissement disque

**Compromis √† consid√©rer** :

- ‚ö†Ô∏è **CPU** : Compression/d√©compression consomme du CPU
- ‚ö†Ô∏è **Complexit√©** : Configuration et tuning n√©cessaires
- ‚ö†Ô∏è **Performances variables** : D√©pend du type de donn√©es et workload

**Quand compresser** :
- ‚úÖ Tables volumineuses peu modifi√©es (logs, archives)
- ‚úÖ Donn√©es textuelles (JSON, XML, logs applicatifs)
- ‚úÖ Environnements cloud avec stockage co√ªteux
- ‚úÖ Tables avec ratio lecture/√©criture √©lev√©

**Quand NE PAS compresser** :
- ‚ùå Tables OLTP √† forte √©criture (overhead CPU)
- ‚ùå Donn√©es d√©j√† compress√©es (images, vid√©os, archives ZIP)
- ‚ùå Tables tr√®s sollicit√©es en UPDATE/DELETE
- ‚ùå Hardware ancien avec CPU limit√©

---

## Les Deux M√©thodes de Compression

MariaDB InnoDB propose deux approches distinctes de compression :

### 1. ROW_FORMAT=COMPRESSED (Compression Page-Level)

**Principe** : Compresse les pages InnoDB (16KB par d√©faut) vers une taille plus petite (1K, 2K, 4K, 8K).

```sql
CREATE TABLE logs_compressed (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  timestamp TIMESTAMP,
  message TEXT,
  metadata JSON
) ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;
```

**Caract√©ristiques** :
- Compression au niveau page InnoDB
- Taille de page compress√©e configurable (1, 2, 4, 8 KB)
- Stocke version non compress√©e en buffer pool
- Algorithme zlib (deflate)

**Avantages** :
- ‚úÖ Compatible avec toutes versions InnoDB
- ‚úÖ Transparente pour application
- ‚úÖ Bon ratio compression/performance

**Inconv√©nients** :
- ‚ö†Ô∏è Overhead CPU mod√©r√© √† √©lev√©
- ‚ö†Ô∏è Configuration KEY_BLOCK_SIZE requise
- ‚ö†Ô∏è Double stockage en m√©moire (compressed + uncompressed)

### 2. PAGE_COMPRESSED (Compression Transparente)

**Principe** : Utilise la compression au niveau filesystem avec "punch hole" (sparse files).

```sql
CREATE TABLE logs_page_compressed (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  timestamp TIMESTAMP,
  message TEXT,
  metadata JSON
) PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=6;
```

**Caract√©ristiques** :
- Compression transparente au niveau OS
- "Punch hole" lib√®re blocs inutilis√©s
- Algorithmes multiples (zlib, lz4, lzo, lzma, bzip2, snappy)
- N√©cessite filesystem supportant punch hole (XFS, ext4, Btrfs)

**Avantages** :
- ‚úÖ Overhead CPU g√©n√©ralement plus faible
- ‚úÖ Pas de double stockage en m√©moire
- ‚úÖ Configuration simple (niveau 1-9)
- ‚úÖ Meilleur ratio compression souvent

**Inconv√©nients** :
- ‚ö†Ô∏è N√©cessite filesystem compatible
- ‚ö†Ô∏è Moins de contr√¥le granulaire
- ‚ö†Ô∏è Support variable selon OS/version

---

## ROW_FORMAT=COMPRESSED - Compression Page-Level

### Configuration et Param√®tres

#### KEY_BLOCK_SIZE : Taille de Page Compress√©e

```sql
-- Tailles disponibles : 1, 2, 4, 8 (en KB)
-- Plus petit = meilleure compression, mais plus de CPU

-- KEY_BLOCK_SIZE=8 (recommand√© pour d√©marrer)
CREATE TABLE moderate_compression (
  id INT PRIMARY KEY,
  data TEXT
) ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8;

-- KEY_BLOCK_SIZE=4 (compression plus agressive)
CREATE TABLE high_compression (
  id INT PRIMARY KEY,
  data TEXT
) ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4;

-- KEY_BLOCK_SIZE=1 (compression maximale, CPU √©lev√©)
CREATE TABLE ultra_compression (
  id INT PRIMARY KEY,
  data TEXT
) ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=1;
```

**R√®gles de choix KEY_BLOCK_SIZE** :

| KEY_BLOCK_SIZE | Ratio Compression | CPU Overhead | Usage |
|----------------|-------------------|--------------|-------|
| 8K | Mod√©r√© (30-40%) | Faible (+10-15%) | Tables fr√©quemment lues |
| 4K | Bon (40-50%) | Moyen (+20-30%) | Logs, archives r√©centes |
| 2K | √âlev√© (50-60%) | √âlev√© (+30-50%) | Archives, rarement lues |
| 1K | Maximum (60-70%) | Tr√®s √©lev√© (+50-100%) | Archivage long terme |

#### Variables de Configuration InnoDB

```sql
-- Activer file-per-table (requis pour compression)
SET GLOBAL innodb_file_per_table = 1;

-- Taille page InnoDB (doit √™tre >= KEY_BLOCK_SIZE)
-- Par d√©faut 16K, compatible avec tous KEY_BLOCK_SIZE
SHOW VARIABLES LIKE 'innodb_page_size';

-- Format de fichier compatible compression
SET GLOBAL innodb_file_format = 'Barracuda';  -- MariaDB 10.2+
SET GLOBAL innodb_file_format_max = 'Barracuda';

-- Niveau compression zlib (0-9, d√©faut 6)
SET GLOBAL innodb_compression_level = 6;
-- Plus √©lev√© = meilleure compression, plus de CPU

-- Taux d'√©chec acceptable avant recompression
SET GLOBAL innodb_compression_failure_threshold_pct = 5;
-- Si >5% pages ne rentrent pas, recompression automatique

-- Padding pour √©viter recompression fr√©quente
SET GLOBAL innodb_compression_pad_pct_max = 50;
-- Laisse 50% d'espace libre pour UPDATE
```

### Exemple Complet de Cr√©ation

```sql
-- Configuration serveur (my.cnf)
-- [mysqld]
-- innodb_file_per_table = 1
-- innodb_compression_level = 6
-- innodb_compression_failure_threshold_pct = 5

-- Table de logs applicatifs
CREATE TABLE application_logs (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  application VARCHAR(50),
  level ENUM('DEBUG','INFO','WARN','ERROR','FATAL'),
  message TEXT,
  context JSON,
  stack_trace TEXT,
  created_at TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP(6),
  
  -- Index
  INDEX idx_app_time (application, created_at),
  INDEX idx_level (level, created_at)
  
) ENGINE=InnoDB 
  ROW_FORMAT=COMPRESSED 
  KEY_BLOCK_SIZE=4
  COMMENT='Logs applicatifs compress√©s, r√©tention 90 jours';

-- V√©rifier configuration
SHOW CREATE TABLE application_logs;
```

### Impact sur Performances

**Benchmark typique** (table de logs, 10M lignes) :

```sql
-- Table non compress√©e
CREATE TABLE logs_uncompressed (
  log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
  timestamp TIMESTAMP,
  level VARCHAR(10),
  message TEXT,
  metadata JSON
) ENGINE=InnoDB;

-- Table compress√©e KEY_BLOCK_SIZE=4
CREATE TABLE logs_compressed_4k (
  log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
  timestamp TIMESTAMP,
  level VARCHAR(10),
  message TEXT,
  metadata JSON
) ENGINE=InnoDB ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4;

-- Ins√©rer 10M lignes de test
-- ...

-- R√©sultats typiques :
```

| M√©trique | Non Compress√©e | Compress√©e 4K | Diff√©rence |
|----------|---------------|---------------|------------|
| **Taille disque** | 2.5 GB | 1.1 GB | -56% |
| **INSERT 100K rows** | 12.5 sec | 15.8 sec | +26% |
| **SELECT COUNT(*)** | 3.2 sec | 3.5 sec | +9% |
| **SELECT avec WHERE** | 1.8 sec | 1.4 sec | -22% (I/O r√©duit) |
| **UPDATE 10K rows** | 2.1 sec | 3.4 sec | +62% |
| **DELETE 10K rows** | 1.9 sec | 2.8 sec | +47% |

**Observations** :
- ‚úÖ √âconomie espace disque significative (50-60%)
- ‚ö†Ô∏è INSERT/UPDATE/DELETE plus lents (compression overhead)
- ‚úÖ SELECT parfois plus rapides (moins de I/O)
- Optimal pour tables **read-heavy**

---

## PAGE_COMPRESSED - Compression Transparente

### Configuration et Utilisation

#### Activer PAGE_COMPRESSED

```sql
-- Compression transparente avec niveau par d√©faut (6)
CREATE TABLE logs_page (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  timestamp TIMESTAMP,
  message TEXT,
  metadata JSON
) PAGE_COMPRESSED=1;

-- Avec niveau de compression sp√©cifique (1-9)
CREATE TABLE logs_page_level9 (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  timestamp TIMESTAMP,
  message TEXT,
  metadata JSON
) PAGE_COMPRESSED=1 
  PAGE_COMPRESSION_LEVEL=9;  -- Compression maximale

-- Algorithme de compression personnalis√©
CREATE TABLE logs_lz4 (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  timestamp TIMESTAMP,
  message TEXT
) PAGE_COMPRESSED=1 
  PAGE_COMPRESSION_LEVEL=6
  COMPRESSION='lz4';  -- Algorithme LZ4 (plus rapide)
```

#### Algorithmes de Compression Disponibles

| Algorithme | Ratio | Vitesse | CPU | Usage |
|------------|-------|---------|-----|-------|
| **zlib** | Bon | Moyenne | Moyen | D√©faut, √©quilibr√© |
| **lz4** | Mod√©r√© | Tr√®s rapide | Faible | OLTP, performance |
| **lzo** | Mod√©r√© | Rapide | Faible | Alternative √† lz4 |
| **lzma** | Excellent | Lente | √âlev√© | Archivage |
| **bzip2** | Tr√®s bon | Lente | √âlev√© | Archivage |
| **snappy** | Mod√©r√© | Tr√®s rapide | Tr√®s faible | Google, performance |

```sql
-- Exemple avec LZ4 (recommand√© pour OLTP)
CREATE TABLE orders_lz4 (
  order_id INT PRIMARY KEY,
  customer_id INT,
  order_data JSON
) PAGE_COMPRESSED=1 COMPRESSION='lz4';

-- Exemple avec LZMA (archivage long terme)
CREATE TABLE archives_lzma (
  archive_id BIGINT PRIMARY KEY,
  archive_data LONGTEXT
) PAGE_COMPRESSED=1 
  COMPRESSION='lzma' 
  PAGE_COMPRESSION_LEVEL=9;
```

### Pr√©requis Filesystem

**Filesystems supportant punch hole** :
- ‚úÖ **XFS** (recommand√©, support natif excellent)
- ‚úÖ **ext4** (avec feature "extents" activ√©e)
- ‚úÖ **Btrfs** (support natif)
- ‚ùå **ext3** (non support√©)
- ‚ùå **NTFS** (Windows, non support√©)

**V√©rifier support punch hole** :
```bash
# Linux : V√©rifier filesystem
df -T /var/lib/mysql
# Filesystem     Type
# /dev/sda1      xfs   ‚Üê OK

# Tester punch hole manuellement
fallocate -p -o 0 -l 4096 testfile
# Si pas d'erreur ‚Üí punch hole support√©

# MariaDB : V√©rifier dans error log au d√©marrage
grep -i "punch hole" /var/log/mysql/error.log
# InnoDB: Using punch hole for PAGE_COMPRESSED
```

**Configuration my.cnf** :
```ini
[mysqld]
# Activer page compression
innodb_file_per_table = 1

# Algorithme par d√©faut (zlib, lz4, lzo, etc.)
innodb_compression_algorithm = zlib

# Niveau par d√©faut si non sp√©cifi√©
innodb_compression_default = 6

# D√©sactiver double-write pour PAGE_COMPRESSED (optionnel, risqu√©)
# innodb_doublewrite = 0  # Am√©liore perfs mais risque corruption crash
```

### Avantages PAGE_COMPRESSED

```sql
-- Comparaison ROW_FORMAT vs PAGE_COMPRESSED
-- Table test : 5M lignes, colonnes TEXT/JSON

-- ROW_FORMAT=COMPRESSED
CREATE TABLE test_row_compressed (
  id INT PRIMARY KEY,
  data TEXT
) ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4;

-- PAGE_COMPRESSED
CREATE TABLE test_page_compressed (
  id INT PRIMARY KEY,
  data TEXT
) PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=6;

-- R√©sultats typiques :
```

| M√©trique | ROW_COMPRESSED | PAGE_COMPRESSED | Avantage |
|----------|---------------|-----------------|----------|
| **Taille disque** | 1.2 GB | 0.9 GB | PAGE (-25%) |
| **M√©moire buffer pool** | 2.4 GB (double) | 1.2 GB | PAGE (-50%) |
| **INSERT 100K** | 18 sec | 14 sec | PAGE (-22%) |
| **SELECT COUNT(*)** | 4 sec | 3.2 sec | PAGE (-20%) |
| **CPU usage** | 35% | 28% | PAGE (-20%) |

**PAGE_COMPRESSED gagne g√©n√©ralement** sur :
- ‚úÖ Consommation m√©moire (pas de double stockage)
- ‚úÖ CPU overhead plus faible (algorithmes optimis√©s)
- ‚úÖ Performance globale meilleure
- ‚úÖ Ratio compression souvent sup√©rieur

---

## Cas d'Usage et Exemples Concrets

### 1. Table de Logs Applicatifs

**Besoin** : Logs JSON volumineux, r√©tention 90 jours, lectures rares.

```sql
-- Table de logs avec compression agressive
CREATE TABLE application_logs (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  application VARCHAR(50),
  environment ENUM('DEV','STAGING','PROD'),
  level ENUM('DEBUG','INFO','WARN','ERROR','FATAL'),
  timestamp TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP(6),
  
  -- Donn√©es volumineuses
  message TEXT,
  context JSON,
  stack_trace TEXT,
  request_headers JSON,
  response_body TEXT,
  
  -- M√©tadonn√©es
  user_id INT,
  session_id VARCHAR(64),
  ip_address VARCHAR(45),
  
  -- Index
  INDEX idx_app_env_time (application, environment, timestamp),
  INDEX idx_level_time (level, timestamp),
  INDEX idx_user (user_id, timestamp)
  
) ENGINE=InnoDB
  PAGE_COMPRESSED=1
  PAGE_COMPRESSION_LEVEL=6
  COMPRESSION='zlib'
  COMMENT='Logs applicatifs, r√©tention 90j, compression ~60%';

-- Partitionnement par mois pour archivage
ALTER TABLE application_logs
PARTITION BY RANGE (UNIX_TIMESTAMP(timestamp)) (
  PARTITION p202501 VALUES LESS THAN (UNIX_TIMESTAMP('2025-02-01')),
  PARTITION p202502 VALUES LESS THAN (UNIX_TIMESTAMP('2025-03-01')),
  PARTITION p202503 VALUES LESS THAN (UNIX_TIMESTAMP('2025-04-01')),
  -- ...
  PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- R√©sultat typique :
-- Sans compression : 50 GB/mois
-- Avec compression : 18 GB/mois (-64%)
-- √âconomie : 32 GB/mois, ~400 GB/an
```

### 2. Archivage de Donn√©es Historiques

**Besoin** : Commandes >2 ans, rarement consult√©es, r√©tention l√©gale 7 ans.

```sql
-- Table d'archive avec compression maximale
CREATE TABLE orders_archive (
  order_id BIGINT PRIMARY KEY,
  customer_id INT,
  order_date DATE,
  
  -- Donn√©es commande (snapshot complet)
  order_data JSON,          -- D√©tail items, prix, promos
  shipping_info JSON,       -- Adresse, transporteur, tracking
  payment_info JSON,        -- Mode paiement, transaction_id
  customer_snapshot JSON,   -- √âtat client √† date commande
  
  -- M√©tadonn√©es
  archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  original_table VARCHAR(50),
  
  INDEX idx_customer (customer_id, order_date),
  INDEX idx_date (order_date)
  
) ENGINE=InnoDB
  PAGE_COMPRESSED=1
  PAGE_COMPRESSION_LEVEL=9  -- Compression maximale
  COMPRESSION='lzma'        -- Algorithme le plus efficace
  COMMENT='Archive commandes >2 ans, r√©tention 7 ans';

-- Proc√©dure d'archivage mensuel
DELIMITER $$
CREATE PROCEDURE archive_old_orders()
BEGIN
  DECLARE v_cutoff_date DATE;
  SET v_cutoff_date = DATE_SUB(CURDATE(), INTERVAL 2 YEAR);
  
  -- Copier vers archive
  INSERT INTO orders_archive (
    order_id, customer_id, order_date, order_data, 
    shipping_info, payment_info, customer_snapshot, original_table
  )
  SELECT 
    o.order_id,
    o.customer_id,
    o.order_date,
    JSON_OBJECT(
      'items', (SELECT JSON_ARRAYAGG(JSON_OBJECT(
        'product_id', oi.product_id,
        'quantity', oi.quantity,
        'unit_price', oi.unit_price
      )) FROM order_items oi WHERE oi.order_id = o.order_id),
      'total', o.total_amount,
      'status', o.status
    ) AS order_data,
    JSON_OBJECT('address', o.shipping_address, 'method', o.shipping_method) AS shipping_info,
    JSON_OBJECT('method', o.payment_method, 'transaction', o.transaction_id) AS payment_info,
    JSON_OBJECT('name', c.name, 'email', c.email) AS customer_snapshot,
    'orders' AS original_table
  FROM orders o
  INNER JOIN customers c ON o.customer_id = c.customer_id
  WHERE o.order_date < v_cutoff_date
    AND o.order_id NOT IN (SELECT order_id FROM orders_archive);
  
  -- Supprimer de table active
  DELETE FROM orders WHERE order_date < v_cutoff_date;
  
  SELECT 
    ROW_COUNT() AS orders_archived,
    v_cutoff_date AS cutoff_date;
END$$
DELIMITER ;

-- Planifier ex√©cution mensuelle
CREATE EVENT archive_orders_monthly
ON SCHEDULE EVERY 1 MONTH
STARTS '2025-01-01 02:00:00'
DO CALL archive_old_orders();

-- R√©sultat typique :
-- Sans compression : 200 GB archives
-- Avec LZMA level 9 : 45 GB (-77%)
```

### 3. Table JSON de Donn√©es Semi-Structur√©es

**Besoin** : Documents JSON, recherche occasionnelle, stockage optimis√©.

```sql
-- Table de documents JSON compress√©s
CREATE TABLE documents (
  document_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  document_type VARCHAR(50),
  
  -- Document JSON complet
  content JSON,
  
  -- M√©tadonn√©es (colonnes g√©n√©r√©es pour recherche)
  title VARCHAR(255) AS (content->>'$.title') VIRTUAL,
  author VARCHAR(100) AS (content->>'$.author') VIRTUAL,
  created_date DATE AS (content->>'$.created_date') VIRTUAL,
  tags JSON AS (content->'$.tags') VIRTUAL,
  
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  
  -- Index sur colonnes virtuelles
  INDEX idx_title (title),
  INDEX idx_author (author),
  INDEX idx_type_date (document_type, created_date),
  FULLTEXT INDEX ft_content (content)
  
) ENGINE=InnoDB
  PAGE_COMPRESSED=1
  PAGE_COMPRESSION_LEVEL=7
  COMPRESSION='zlib'
  COMMENT='Documents JSON, compression ~65%';

-- Insertion exemple
INSERT INTO documents (document_type, content) VALUES
('article', JSON_OBJECT(
  'title', 'Introduction to MariaDB Compression',
  'author', 'Jane Doe',
  'created_date', '2025-01-15',
  'tags', JSON_ARRAY('database', 'compression', 'performance'),
  'body', 'MariaDB offers two main compression methods...',  -- Texte long
  'metadata', JSON_OBJECT('word_count', 2500, 'language', 'en')
));

-- Recherche optimis√©e (utilise index sur colonne virtuelle)
SELECT document_id, title, author, created_date
FROM documents
WHERE author = 'Jane Doe'
  AND created_date >= '2025-01-01';

-- R√©sultat typique :
-- Documents JSON moyens : 50 KB/document
-- 1M documents : 50 GB non compress√©
-- Avec compression : 17 GB (-66%)
```

### 4. Tables de Mesures IoT/T√©l√©m√©trie

**Besoin** : Millions de mesures/jour, r√©tention 1 an, lectures agr√©g√©es.

```sql
-- Table de mesures IoT
CREATE TABLE sensor_readings (
  reading_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  sensor_id INT,
  timestamp TIMESTAMP(6),
  
  -- Mesures (souvent r√©p√©titives, compressent bien)
  temperature DECIMAL(5,2),
  humidity DECIMAL(5,2),
  pressure DECIMAL(7,2),
  battery_level TINYINT,
  
  -- M√©tadonn√©es (JSON, compresse tr√®s bien)
  raw_data JSON,  -- Donn√©es brutes capteur
  
  INDEX idx_sensor_time (sensor_id, timestamp),
  INDEX idx_timestamp (timestamp)
  
) ENGINE=InnoDB
  PAGE_COMPRESSED=1
  PAGE_COMPRESSION_LEVEL=4  -- Compromis perf/compression
  COMPRESSION='lz4'         -- Rapide pour insertions fr√©quentes
  COMMENT='Mesures IoT, compression LZ4 pour performance INSERT'
PARTITION BY RANGE (UNIX_TIMESTAMP(timestamp)) (
  PARTITION p_2025_01 VALUES LESS THAN (UNIX_TIMESTAMP('2025-02-01')),
  PARTITION p_2025_02 VALUES LESS THAN (UNIX_TIMESTAMP('2025-03-01')),
  -- ... partitions mensuelles
  PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Insertion batch (100K lectures/batch)
INSERT INTO sensor_readings (sensor_id, timestamp, temperature, humidity, pressure, battery_level, raw_data)
SELECT 
  sensor_id,
  FROM_UNIXTIME(UNIX_TIMESTAMP('2025-01-15 00:00:00') + seq * 60) AS timestamp,
  20 + RAND() * 10 AS temperature,
  40 + RAND() * 30 AS humidity,
  1000 + RAND() * 30 AS pressure,
  90 - (seq % 100) AS battery_level,
  JSON_OBJECT('raw', CONCAT('DATA_', seq)) AS raw_data
FROM seq_1_to_100000
CROSS JOIN (SELECT sensor_id FROM sensors) AS s;

-- Agr√©gation (b√©n√©ficie de compression = moins de I/O)
SELECT 
  sensor_id,
  DATE(timestamp) AS reading_date,
  AVG(temperature) AS avg_temp,
  MIN(temperature) AS min_temp,
  MAX(temperature) AS max_temp,
  COUNT(*) AS reading_count
FROM sensor_readings
WHERE timestamp >= '2025-01-01'
  AND timestamp < '2025-02-01'
GROUP BY sensor_id, DATE(timestamp);

-- R√©sultat typique :
-- 10M lectures/jour : 3 GB/jour non compress√©
-- Avec LZ4 level 4 : 1.2 GB/jour (-60%)
-- Performance INSERT : -5% vs non compress√© (LZ4 tr√®s rapide)
```

---

## Monitoring et Mesure de la Compression

### V√©rifier Ratio de Compression

```sql
-- Requ√™te pour voir taille r√©elle vs taille compress√©e
SELECT 
  table_schema AS 'Database',
  table_name AS 'Table',
  ROUND(data_length / 1024 / 1024, 2) AS 'Data (MB)',
  ROUND(index_length / 1024 / 1024, 2) AS 'Index (MB)',
  ROUND((data_length + index_length) / 1024 / 1024, 2) AS 'Total (MB)',
  table_rows AS 'Rows',
  ROW_FORMAT,
  CREATE_OPTIONS
FROM information_schema.TABLES
WHERE table_schema NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys')
  AND (ROW_FORMAT = 'Compressed' OR CREATE_OPTIONS LIKE '%PAGE_COMPRESSED%')
ORDER BY (data_length + index_length) DESC;

-- Exemple r√©sultat :
-- Database | Table             | Data (MB) | Index (MB) | Total (MB) | Rows    | ROW_FORMAT | CREATE_OPTIONS
-- mydb     | application_logs  | 1234.56   | 234.12     | 1468.68    | 5000000 | Compressed | KEY_BLOCK_SIZE=4
-- mydb     | sensor_readings   | 890.23    | 123.45     | 1013.68    | 10000000| Dynamic    | page_compressed=1
```

### Statistiques InnoDB Compression

```sql
-- Statistiques d√©taill√©es de compression
SELECT * FROM information_schema.INNODB_CMP;

-- Colonnes importantes :
-- page_size : Taille page compress√©e (KEY_BLOCK_SIZE)
-- compress_ops : Nombre d'op√©rations de compression
-- compress_ops_ok : Compressions r√©ussies
-- compress_time : Temps CPU compression (microsecondes)
-- uncompress_ops : Nombre de d√©compressions
-- uncompress_time : Temps CPU d√©compression

-- Taux d'√©chec de compression
SELECT 
  page_size,
  compress_ops,
  compress_ops - compress_ops_ok AS compress_failures,
  ROUND((compress_ops - compress_ops_ok) / compress_ops * 100, 2) AS failure_rate_pct
FROM information_schema.INNODB_CMP
WHERE compress_ops > 0;

-- Si failure_rate_pct > 5% ‚Üí Augmenter KEY_BLOCK_SIZE ou d√©sactiver compression
```

### Monitoring en Production

```sql
-- Vue pour monitoring r√©gulier
CREATE VIEW compression_stats AS
SELECT 
  t.table_schema,
  t.table_name,
  t.row_format,
  t.table_rows,
  ROUND((t.data_length + t.index_length) / 1024 / 1024, 2) AS total_mb,
  ROUND(t.data_length / t.table_rows / 1024, 2) AS kb_per_row,
  c.compress_ops,
  c.uncompress_ops,
  ROUND(c.compress_time / 1000000, 2) AS compress_time_sec,
  ROUND(c.uncompress_time / 1000000, 2) AS uncompress_time_sec,
  ROUND((c.compress_ops - c.compress_ops_ok) / c.compress_ops * 100, 2) AS failure_pct
FROM information_schema.TABLES t
LEFT JOIN information_schema.INNODB_CMP c 
  ON c.page_size = SUBSTRING_INDEX(SUBSTRING_INDEX(t.create_options, 'KEY_BLOCK_SIZE=', -1), ' ', 1) * 1024
WHERE t.table_schema NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys')
  AND (t.row_format = 'Compressed' OR t.create_options LIKE '%PAGE_COMPRESSED%');

-- Alertes
SELECT * FROM compression_stats WHERE failure_pct > 5;
```

---

## Migration vers Compression

### Ajouter Compression √† Table Existante

```sql
-- Table existante non compress√©e
CREATE TABLE logs (
  log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  message TEXT,
  created_at TIMESTAMP
) ENGINE=InnoDB;

-- 10M lignes existantes, 5 GB

-- Option 1 : ALTER TABLE (bloque table, peut √™tre long)
ALTER TABLE logs 
  ROW_FORMAT=COMPRESSED 
  KEY_BLOCK_SIZE=4;
-- Dur√©e typique : 15-30 min pour 5 GB, table LOCKED

-- Option 2 : Cr√©er nouvelle table + copie + swap (moins de downtime)
-- √âtape 1 : Cr√©er table compress√©e
CREATE TABLE logs_compressed LIKE logs;
ALTER TABLE logs_compressed 
  ROW_FORMAT=COMPRESSED 
  KEY_BLOCK_SIZE=4;

-- √âtape 2 : Copier donn√©es (batch pour r√©duire lock)
INSERT INTO logs_compressed 
SELECT * FROM logs 
WHERE log_id <= 5000000;  -- Premier batch

INSERT INTO logs_compressed 
SELECT * FROM logs 
WHERE log_id > 5000000 AND log_id <= 10000000;  -- Deuxi√®me batch

-- √âtape 3 : Synchroniser nouvelles donn√©es (pendant migration)
INSERT INTO logs_compressed 
SELECT * FROM logs l
WHERE l.log_id > (SELECT MAX(log_id) FROM logs_compressed);

-- √âtape 4 : Swap tables (rapide)
RENAME TABLE 
  logs TO logs_old,
  logs_compressed TO logs;

-- √âtape 5 : V√©rifier, puis supprimer ancienne
-- DROP TABLE logs_old;

-- R√©sultat : 5 GB ‚Üí 2.1 GB (-58%)
```

### Utiliser gh-ost ou pt-online-schema-change

```bash
# gh-ost : Migration sans downtime
gh-ost \
  --host=localhost \
  --database=mydb \
  --table=logs \
  --alter="ROW_FORMAT=COMPRESSED, KEY_BLOCK_SIZE=4" \
  --initially-drop-ghost-table \
  --initially-drop-old-table \
  --execute

# Avantages :
# - Pas de lock de la table originale
# - Migrations longues possibles
# - Pause/resume si besoin
# - Rollback facile
```

---

## Optimisation et Tuning

### Choisir le Bon Niveau de Compression

```sql
-- Test : Cr√©er 4 tables avec diff√©rents niveaux
CREATE TABLE test_level_1 (...) PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=1;
CREATE TABLE test_level_3 (...) PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=3;
CREATE TABLE test_level_6 (...) PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=6;
CREATE TABLE test_level_9 (...) PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=9;

-- Ins√©rer m√™mes donn√©es dans toutes
INSERT INTO test_level_1 SELECT * FROM source_data;
INSERT INTO test_level_3 SELECT * FROM source_data;
INSERT INTO test_level_6 SELECT * FROM source_data;
INSERT INTO test_level_9 SELECT * FROM source_data;

-- Comparer taille et performance
SELECT 
  table_name,
  ROUND((data_length + index_length) / 1024 / 1024, 2) AS size_mb,
  SUBSTRING_INDEX(create_options, 'PAGE_COMPRESSION_LEVEL=', -1) AS level
FROM information_schema.TABLES
WHERE table_name LIKE 'test_level_%';

-- R√©sultats typiques (1 GB donn√©es source) :
-- Level 1 : 680 MB, INSERT rapide
-- Level 3 : 540 MB, INSERT moyen
-- Level 6 : 420 MB, INSERT lent
-- Level 9 : 380 MB, INSERT tr√®s lent

-- Choisir niveau 6 (bon compromis)
```

### Compression pour SSD vs HDD

```sql
-- SSD : Privil√©gier vitesse (CPU < I/O)
-- Recommandation : LZ4 ou Snappy
CREATE TABLE ssd_optimized (
  id INT PRIMARY KEY,
  data TEXT
) PAGE_COMPRESSED=1 
  COMPRESSION='lz4' 
  PAGE_COMPRESSION_LEVEL=3;

-- HDD : Privil√©gier compression (I/O critique)
-- Recommandation : zlib level 6-9
CREATE TABLE hdd_optimized (
  id INT PRIMARY KEY,
  data TEXT
) PAGE_COMPRESSED=1 
  COMPRESSION='zlib' 
  PAGE_COMPRESSION_LEVEL=7;
```

### Compression Cloud (AWS, Azure, GCP)

```sql
-- Cloud : Optimiser co√ªts stockage + IOPS
-- EBS GP3 : $0.08/GB + $0.005/IOPS
-- Compression 60% = √©conomie $48/TB/mois

-- Configuration recommand√©e AWS RDS / Azure Database
CREATE TABLE cloud_table (
  id BIGINT PRIMARY KEY,
  data JSON
) PAGE_COMPRESSED=1 
  COMPRESSION='zlib'
  PAGE_COMPRESSION_LEVEL=6;

-- B√©n√©fices :
-- - R√©duction stockage (facturation GB)
-- - R√©duction IOPS (moins de lectures)
-- - R√©duction co√ªts snapshots/backups
-- - R√©duction bande passante r√©plication

-- √âconomie typique :
-- 100 GB ‚Üí 40 GB apr√®s compression
-- √âconomie : $4.80/mois stockage
-- + R√©duction IOPS : ~$10-20/mois
-- Total : ~$15-25/mois par 100 GB
```

---

## Best Practices

### 1. Tester avant Production

```sql
-- ‚úÖ Toujours tester sur copie avec donn√©es r√©elles
CREATE TABLE test_compression LIKE production_table;
ALTER TABLE test_compression PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=6;

-- Copier √©chantillon repr√©sentatif
INSERT INTO test_compression 
SELECT * FROM production_table LIMIT 1000000;

-- Benchmark
SELECT BENCHMARK(1000, (SELECT COUNT(*) FROM test_compression));
SELECT BENCHMARK(100, (SELECT * FROM test_compression WHERE id = RAND() * 1000000));
```

### 2. Monitoring Continu

```sql
-- ‚úÖ Surveiller taux d'√©chec compression
SELECT failure_pct FROM compression_stats WHERE failure_pct > 5;

-- ‚úÖ Alertes si ratio compression < 20%
SELECT table_name, 
  ROUND((1 - (data_length / (table_rows * avg_row_length))) * 100, 2) AS compression_ratio
FROM information_schema.TABLES
WHERE compression_ratio < 20;
```

### 3. Adapter selon Workload

```sql
-- ‚úÖ OLTP haute √©criture : LZ4, niveau bas
CREATE TABLE oltp_table (...) 
  PAGE_COMPRESSED=1 COMPRESSION='lz4' PAGE_COMPRESSION_LEVEL=3;

-- ‚úÖ OLAP lecture intensive : zlib, niveau √©lev√©
CREATE TABLE analytics_table (...) 
  PAGE_COMPRESSED=1 COMPRESSION='zlib' PAGE_COMPRESSION_LEVEL=7;

-- ‚úÖ Archivage : LZMA, niveau max
CREATE TABLE archive_table (...) 
  PAGE_COMPRESSED=1 COMPRESSION='lzma' PAGE_COMPRESSION_LEVEL=9;
```

### 4. Partitionnement + Compression

```sql
-- ‚úÖ Combiner pour optimisation maximale
CREATE TABLE logs_optimized (
  log_id BIGINT AUTO_INCREMENT,
  created_at TIMESTAMP,
  message TEXT,
  PRIMARY KEY (log_id, created_at)
) PAGE_COMPRESSED=1 COMPRESSION='zlib' PAGE_COMPRESSION_LEVEL=6
PARTITION BY RANGE (UNIX_TIMESTAMP(created_at)) (
  PARTITION p_current VALUES LESS THAN (UNIX_TIMESTAMP('2025-02-01')),
  PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- Archiver partition ancienne avec compression max
ALTER TABLE logs_optimized 
  REORGANIZE PARTITION p_current INTO (
    PARTITION p_2025_01 VALUES LESS THAN (UNIX_TIMESTAMP('2025-02-01'))
      PAGE_COMPRESSED=1 COMPRESSION='lzma' PAGE_COMPRESSION_LEVEL=9
  );
```

---

## ‚úÖ Points cl√©s √† retenir

### Deux M√©thodes de Compression
- ‚úÖ **ROW_FORMAT=COMPRESSED** : Compression page-level InnoDB (KEY_BLOCK_SIZE: 1, 2, 4, 8)
- ‚úÖ **PAGE_COMPRESSED** : Compression transparente OS avec punch hole (niveaux 1-9)
- ‚úÖ **Recommandation g√©n√©rale** : PAGE_COMPRESSED (meilleure performance, ratio sup√©rieur)

### Performance
- ‚úÖ **√âconomie espace** : 40-70% selon donn√©es (texte/JSON = meilleur)
- ‚ö†Ô∏è **CPU overhead** : +10-50% selon algorithme et niveau
- ‚úÖ **I/O am√©lior√©s** : Moins de donn√©es lues/√©crites (b√©n√©fice SSD++)
- ‚ö†Ô∏è **√âcritures plus lentes** : INSERT/UPDATE +15-50%
- ‚úÖ **Lectures variables** : Parfois plus rapides (moins I/O), parfois plus lentes (CPU)

### Algorithmes
- ‚úÖ **zlib** : D√©faut, √©quilibr√© (ratio/vitesse)
- ‚úÖ **lz4** : Rapide, OLTP, faible overhead CPU
- ‚úÖ **lzma** : Maximum compression, archivage
- ‚úÖ **snappy** : Tr√®s rapide, Google

### Cas d'Usage Id√©aux
- ‚úÖ **Logs applicatifs** : Texte, JSON, r√©tention longue
- ‚úÖ **Archives** : Donn√©es >1-2 ans, lectures rares
- ‚úÖ **JSON/XML** : Documents semi-structur√©s
- ‚úÖ **IoT/T√©l√©m√©trie** : Millions de mesures, agr√©gations
- ‚úÖ **Cloud** : R√©duction co√ªts stockage et IOPS

### Best Practices
- ‚úÖ Tester sur copie avec donn√©es r√©elles avant production
- ‚úÖ Commencer niveau 6 (√©quilibr√©), ajuster selon besoins
- ‚úÖ LZ4 pour OLTP, zlib pour OLAP, LZMA pour archives
- ‚úÖ Monitoring continu (taux √©chec, ratio compression)
- ‚úÖ Combiner avec partitionnement pour archivage optimal
- ‚úÖ SSD : Privil√©gier vitesse (LZ4), HDD : Privil√©gier compression (zlib)

### Limitations
- ‚ö†Ô∏è PAGE_COMPRESSED n√©cessite filesystem compatible (XFS, ext4)
- ‚ö†Ô∏è Pas b√©n√©fique sur donn√©es d√©j√† compress√©es (images, vid√©os)
- ‚ö†Ô∏è Overhead CPU peut √™tre prohibitif sur hardware ancien
- ‚ö†Ô∏è Tables OLTP haute √©criture : √©valuer compromis

---

## üîó Ressources et r√©f√©rences

### Documentation Officielle MariaDB
- üìñ [InnoDB Compression](https://mariadb.com/kb/en/innodb-page-compression/) - Guide complet
- üìñ [ROW_FORMAT=COMPRESSED](https://mariadb.com/kb/en/innodb-row-formats-overview/#compressed) - Page-level
- üìñ [PAGE_COMPRESSED](https://mariadb.com/kb/en/innodb-page-compression/) - Transparente
- üìñ [Compression Algorithms](https://mariadb.com/kb/en/compression-algorithms/) - D√©tails algorithmes

### Performance et Benchmarks
- üìù [InnoDB Compression Performance](https://mariadb.com/resources/blog/innodb-compression-performance/)
- üìù [Compression for SSD vs HDD](https://mariadb.com/kb/en/compression-ssd-vs-hdd/)
- üìù [Cloud Cost Optimization with Compression](https://mariadb.com/resources/blog/cloud-cost-compression/)

### Outils
- üõ†Ô∏è [pt-table-checksum](https://www.percona.com/doc/percona-toolkit/) - V√©rifier int√©grit√© post-compression
- üõ†Ô∏è [sysbench](https://github.com/akopytov/sysbench) - Benchmark compression
- üõ†Ô∏è [mysqltuner](https://github.com/major/MySQLTuner-perl) - Recommandations compression

---

## ‚û°Ô∏è Section suivante

**[18.7 Encryption at Rest](./07-encryption-at-rest.md)** : D√©couvrez comment prot√©ger vos donn√©es stock√©es avec le chiffrement InnoDB, la gestion des cl√©s (file, AWS KMS, Vault), et l'impact sur les performances.

---


‚è≠Ô∏è [Encryption at rest](/18-fonctionnalites-avancees/07-encryption-at-rest.md)
