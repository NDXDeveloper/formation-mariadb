üîù Retour au [Sommaire](/SOMMAIRE.md)

# 18.10 MariaDB Vector : Recherche Vectorielle pour l'IA/RAG üÜï

> **Niveau** : Avanc√© / Expert  
> **Dur√©e estim√©e** : 3-4 heures  
> **Pr√©requis** : Compr√©hension IA/ML de base, embeddings, concept de similarit√© vectorielle

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :

- Comprendre les **embeddings vectoriels** et la recherche s√©mantique
- Utiliser le **type VECTOR** pour stocker des vecteurs
- Cr√©er des **index HNSW** pour recherche rapide
- Ma√Ætriser les **fonctions de distance** (cosine, euclidean, dot product)
- Impl√©menter un syst√®me **RAG** (Retrieval Augmented Generation)
- Int√©grer MariaDB avec **OpenAI, Hugging Face, LLMs**
- Optimiser les **performances** (SIMD, index tuning)
- Combiner recherche **vectorielle + SQL** classique
- Comparer MariaDB Vector aux **bases vectorielles sp√©cialis√©es**

---

## Introduction

**MariaDB Vector** (introduit dans MariaDB 11.7, stable 11.8 LTS) apporte le support natif de la **recherche vectorielle** dans MariaDB, permettant de stocker et rechercher des **embeddings** g√©n√©r√©s par des mod√®les d'IA pour des cas d'usage comme la recherche s√©mantique, les syst√®mes RAG, et la recommandation.

### Qu'est-ce que la Recherche Vectorielle ?

**D√©finition** : Technique permettant de trouver des √©l√©ments similaires en comparant leurs repr√©sentations num√©riques (vecteurs) dans un espace multidimensionnel.

**Exemple concret** :
```python
# Texte ‚Üí Vecteur (embedding)
"MariaDB est une base de donn√©es SQL"
  ‚Üì [Mod√®le AI : OpenAI text-embedding-3-small]
  ‚Üì
[0.023, -0.145, 0.892, ..., 0.234]  # Vecteur 1536 dimensions

"MySQL est un syst√®me de gestion de base de donn√©es"
  ‚Üì [Mod√®le AI]
  ‚Üì
[0.019, -0.139, 0.881, ..., 0.227]  # Vecteur similaire !

# Similarit√© cosine : 0.94 (tr√®s similaire)
# ‚Üí Ces deux phrases ont un sens proche
```

**Diff√©rence avec recherche textuelle classique** :

| Recherche Classique (FULLTEXT) | Recherche Vectorielle |
|--------------------------------|------------------------|
| Correspondance de **mots-cl√©s** | Correspondance de **sens** |
| "base de donn√©es" ‚â† "SGBD" | "base de donn√©es" ‚âà "SGBD" |
| "chat" (animal) = "chat" (discussion) | "chat" (contexte) distingu√© |
| Exact/proximit√© lexicale | Similarit√© s√©mantique |
| Rapide pour mots exacts | Rapide avec index HNSW |

**Exemple limitation recherche textuelle** :
```sql
-- Recherche FULLTEXT classique
SELECT * FROM documents
WHERE MATCH(content) AGAINST('base de donn√©es');
-- ‚úÖ Trouve : "base de donn√©es SQL"
-- ‚ùå Ne trouve PAS : "SGBD relationnel"
-- ‚ùå Ne trouve PAS : "syst√®me de gestion de donn√©es"

-- Recherche Vectorielle
SELECT * FROM documents
ORDER BY VEC_DISTANCE_COSINE(embedding, query_vector)
LIMIT 10;
-- ‚úÖ Trouve : "base de donn√©es SQL"
-- ‚úÖ Trouve : "SGBD relationnel" (sens similaire)
-- ‚úÖ Trouve : "syst√®me de gestion de donn√©es" (synonyme)
```

### Pourquoi MariaDB Vector ?

**Probl√®me** : Avant MariaDB Vector, stack typique pour recherche vectorielle :

```
Application
    ‚Üì
MariaDB (donn√©es m√©tier) + Pinecone/Weaviate/Qdrant (vecteurs)
    ‚Üì
Synchronisation complexe, 2 bases √† maintenir
```

**Solution** : MariaDB Vector unifie tout dans une seule base :

```
Application
    ‚Üì
MariaDB (donn√©es m√©tier + vecteurs + recherche hybride)
    ‚Üì
Une seule base, transactions ACID, jointures SQL+Vector
```

**Avantages MariaDB Vector** :

1. **üîÑ Int√©gration Transparente**
   - Vecteurs dans m√™me base que donn√©es m√©tier
   - Transactions ACID sur donn√©es + vecteurs
   - Jointures SQL classique + recherche vectorielle

2. **üí∞ Co√ªt R√©duit**
   - Pas de base vectorielle s√©par√©e (Pinecone = $$)
   - Infrastructure simplifi√©e
   - Moins de synchronisation

3. **üöÄ Performance**
   - Index HNSW optimis√©
   - Support SIMD (AVX2, AVX-512)
   - Calcul vectoriel au niveau moteur

4. **üîí S√©curit√© et Fiabilit√©**
   - Encryption at rest native
   - Backup/restore standard MariaDB
   - R√©plication ma√Ætre-esclave

5. **üõ†Ô∏è Flexibilit√© SQL**
   - Requ√™tes hybrides (filtre SQL + similarit√©)
   - Agr√©gations, GROUP BY sur r√©sultats
   - CTEs, window functions

---

## Architecture MariaDB Vector

### Composants Principaux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Application / LLM                     ‚îÇ
‚îÇ  (OpenAI, Hugging Face, Local Model)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ Embeddings (vecteurs)
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MariaDB Vector Storage                ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Table avec colonne VECTOR                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  CREATE TABLE docs (                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    id INT,                                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    content TEXT,                           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    embedding VECTOR(1536)  ‚Üê Type natif    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  )                                         ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Index HNSW (Approximate Nearest Neighbor) ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  CREATE INDEX idx_emb ON docs(embedding)   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    USING HNSW;                             ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Fonctions de Distance                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - VEC_DISTANCE_COSINE(v1, v2)             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - VEC_DISTANCE_EUCLIDEAN(v1, v2)          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - VEC_DISTANCE_DOT(v1, v2)                ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Optimisations SIMD                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - AVX2 (256-bit)                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - AVX-512 (512-bit)                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Calculs parall√®les CPU                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Type VECTOR

**D√©claration** :
```sql
-- VECTOR(dimensions)
-- dimensions : 1 √† 65535 (limite pratique ~10000)

CREATE TABLE embeddings (
  id INT PRIMARY KEY AUTO_INCREMENT,
  text_content TEXT,
  
  -- Vecteur 1536 dimensions (OpenAI text-embedding-3-small)
  embedding VECTOR(1536),
  
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Autres dimensions courantes
embedding_small VECTOR(384)   -- sentence-transformers/all-MiniLM-L6-v2
embedding_medium VECTOR(768)  -- BERT base
embedding_large VECTOR(1536)  -- OpenAI text-embedding-3-small
embedding_huge VECTOR(3072)   -- OpenAI text-embedding-3-large
```

**Stockage** :
```sql
-- Format : [val1, val2, val3, ..., valN]
-- Valeurs : FLOAT (32-bit)

-- Ins√©rer vecteur
INSERT INTO embeddings (text_content, embedding) VALUES (
  'MariaDB supporte maintenant les vecteurs',
  '[0.023, -0.145, 0.892, ..., 0.234]'
  -- Cha√Æne JSON convertie automatiquement en VECTOR
);

-- Depuis variable/programme
SET @vec = '[0.1, 0.2, 0.3, 0.4]';
INSERT INTO embeddings (text_content, embedding) VALUES (
  'Test',
  @vec
);
```

### Index HNSW

**HNSW** (Hierarchical Navigable Small World) : Algorithme d'indexation pour recherche rapide de voisins les plus proches (k-NN).

**Principe** :
```
Structure multi-niveau hi√©rarchique

Niveau 2 (sparse) :  o‚îÄ‚îÄ‚îÄ‚îÄ‚îÄo‚îÄ‚îÄ‚îÄ‚îÄ‚îÄo
                     ‚îÇ     ‚îÇ     ‚îÇ
Niveau 1 (medium) :  o‚îÄ‚îÄo‚îÄ‚îÄo‚îÄ‚îÄo‚îÄ‚îÄo‚îÄ‚îÄo
                     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ
Niveau 0 (dense)  :  o‚îÄo‚îÄo‚îÄo‚îÄo‚îÄo‚îÄo‚îÄo‚îÄo

Recherche : Commence en haut, descend progressivement
Complexit√© : O(log n) au lieu de O(n) pour scan complet
```

**Cr√©ation** :
```sql
-- Cr√©er index HNSW sur colonne VECTOR
CREATE INDEX idx_embedding ON embeddings(embedding)
  USING HNSW;

-- Param√®tres optionnels (tuning avanc√©)
CREATE INDEX idx_embedding ON embeddings(embedding)
  USING HNSW
  WITH (
    M = 16,              -- Nombre de connexions par n≈ìud
    ef_construction = 200, -- Qualit√© construction index
    distance = 'cosine'   -- M√©trique de distance
  );
```

**Param√®tres HNSW** :

| Param√®tre | Description | D√©faut | Impact |
|-----------|-------------|--------|--------|
| **M** | Connexions par n≈ìud | 16 | ‚Üë M = meilleure pr√©cision, + m√©moire |
| **ef_construction** | Qualit√© construction | 200 | ‚Üë ef = meilleur index, construction + lente |
| **distance** | M√©trique | cosine | cosine, euclidean, dot |

**Trade-offs** :
- M=8 : Rapide, moins pr√©cis, compact
- M=16 : √âquilibr√© (recommand√©)
- M=32 : Tr√®s pr√©cis, plus lent, volumineux

### Fonctions de Distance

**3 fonctions principales** :

#### 1. VEC_DISTANCE_COSINE (Recommand√©)

**Formule** : 
```
distance = 1 - (A¬∑B) / (||A|| * ||B||)
o√π A¬∑B = produit scalaire
||A|| = norme euclidienne de A
```

**Plage** : 0 (identique) √† 2 (oppos√©)  
**Usage** : Similarit√© s√©mantique (texte, images)

```sql
-- Recherche documents similaires
SELECT 
  id,
  text_content,
  VEC_DISTANCE_COSINE(embedding, @query_vector) AS distance
FROM embeddings
ORDER BY distance ASC
LIMIT 10;
-- distance proche de 0 = tr√®s similaire
```

**Avantage** : Insensible √† la magnitude, compare direction dans l'espace.

#### 2. VEC_DISTANCE_EUCLIDEAN

**Formule** : 
```
distance = ‚àö(Œ£(Ai - Bi)¬≤)
```

**Plage** : 0 (identique) √† ‚àû  
**Usage** : Distance g√©om√©trique absolue

```sql
SELECT 
  id,
  VEC_DISTANCE_EUCLIDEAN(embedding, @query_vector) AS distance
FROM embeddings
ORDER BY distance ASC;
```

**Avantage** : M√©trique classique, intuitive.  
**Inconv√©nient** : Sensible √† la magnitude des vecteurs.

#### 3. VEC_DISTANCE_DOT

**Formule** : 
```
distance = -(A¬∑B)
Produit scalaire n√©gatif (pour ORDER BY ASC)
```

**Plage** : -‚àû √† ‚àû (n√©gatif de produit scalaire)  
**Usage** : Vecteurs normalis√©s, calcul rapide

```sql
SELECT 
  id,
  VEC_DISTANCE_DOT(embedding, @query_vector) AS distance
FROM embeddings
ORDER BY distance ASC;
```

**Avantage** : Calcul le plus rapide.  
**Note** : √âquivalent √† cosine si vecteurs normalis√©s.

**Comparaison** :

| Distance | Vitesse | Pr√©cision Texte | Normalis√© Requis |
|----------|---------|-----------------|------------------|
| **Cosine** | Moyen | ‚úÖ Excellent | ‚ùå Non |
| **Euclidean** | Moyen | ‚ö†Ô∏è Bon | ‚ùå Non |
| **Dot Product** | ‚úÖ Rapide | ‚úÖ Excellent | ‚úÖ Oui |

**Recommandation** : **Cosine** pour texte/s√©mantique, **Dot** si vecteurs normalis√©s (gain performance).

---

## Workflow RAG Complet

### Architecture RAG (Retrieval Augmented Generation)

```
1. Indexation (offline)
   Document ‚Üí Chunking ‚Üí Embeddings ‚Üí MariaDB Vector

2. Recherche (online)
   Question ‚Üí Embedding ‚Üí Similarit√© ‚Üí Top-K Documents

3. G√©n√©ration (online)
   Question + Documents ‚Üí LLM ‚Üí R√©ponse augment√©e
```

### Exemple Complet : Base de Connaissances Technique

#### √âtape 1 : Cr√©ation Table

```sql
-- Table pour base de connaissances
CREATE TABLE knowledge_base (
  doc_id INT PRIMARY KEY AUTO_INCREMENT,
  
  -- M√©tadonn√©es
  title VARCHAR(255),
  category VARCHAR(100),
  url VARCHAR(500),
  source VARCHAR(100),
  
  -- Contenu
  chunk_text TEXT,  -- Fragment de texte (500-1000 tokens)
  chunk_index INT,  -- Num√©ro du fragment dans document
  
  -- Embedding vectoriel
  embedding VECTOR(1536),  -- OpenAI text-embedding-3-small
  
  -- Audit
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  
  -- Index
  INDEX idx_category (category),
  INDEX idx_source (source),
  FULLTEXT INDEX ft_chunk (chunk_text),  -- Recherche hybride
  INDEX idx_embedding ON (embedding) USING HNSW  -- ‚Üê Index vectoriel
);
```

#### √âtape 2 : Insertion Documents (avec Python)

```python
import mariadb
import openai
from typing import List

# Configuration
openai.api_key = "sk-..."
conn = mariadb.connect(
    user="user",
    password="pass",
    host="localhost",
    database="rag_db"
)
cursor = conn.cursor()

def chunk_text(text: str, chunk_size: int = 500) -> List[str]:
    """D√©couper texte en fragments de ~chunk_size mots"""
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = ' '.join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

def get_embedding(text: str) -> List[float]:
    """G√©n√©rer embedding via OpenAI"""
    response = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

def index_document(title: str, content: str, category: str, url: str):
    """Indexer un document dans la base vectorielle"""
    chunks = chunk_text(content, chunk_size=500)
    
    for idx, chunk in enumerate(chunks):
        # G√©n√©rer embedding
        embedding = get_embedding(chunk)
        embedding_str = '[' + ','.join(map(str, embedding)) + ']'
        
        # Ins√©rer dans MariaDB
        cursor.execute("""
            INSERT INTO knowledge_base 
            (title, category, url, source, chunk_text, chunk_index, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            title,
            category,
            url,
            'documentation',
            chunk,
            idx,
            embedding_str
        ))
    
    conn.commit()
    print(f"Indexed: {title} ({len(chunks)} chunks)")

# Exemple : Indexer documentation MariaDB
index_document(
    title="MariaDB Vector Documentation",
    content="""
    MariaDB Vector enables semantic search and AI applications.
    The VECTOR data type stores embeddings generated by ML models.
    HNSW index provides fast approximate nearest neighbor search.
    Vector search can be combined with traditional SQL queries.
    [... contenu complet ...]
    """,
    category="Database",
    url="https://mariadb.com/kb/en/vector/"
)

# Indexer plusieurs documents
documents = [
    {"title": "Python Best Practices", "content": "...", "category": "Programming"},
    {"title": "SQL Optimization Guide", "content": "...", "category": "Database"},
    # ...
]

for doc in documents:
    index_document(**doc)
```

#### √âtape 3 : Recherche S√©mantique

```python
def semantic_search(query: str, limit: int = 5, category: str = None):
    """Recherche s√©mantique dans la base de connaissances"""
    
    # 1. G√©n√©rer embedding de la question
    query_embedding = get_embedding(query)
    query_embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
    
    # 2. Recherche vectorielle
    sql = """
        SELECT 
            doc_id,
            title,
            category,
            chunk_text,
            url,
            VEC_DISTANCE_COSINE(embedding, ?) AS similarity
        FROM knowledge_base
    """
    
    # Filtrage optionnel par cat√©gorie (recherche hybride)
    params = [query_embedding_str]
    if category:
        sql += " WHERE category = ?"
        params.append(category)
    
    sql += """
        ORDER BY similarity ASC
        LIMIT ?
    """
    params.append(limit)
    
    cursor.execute(sql, params)
    results = cursor.fetchall()
    
    return [{
        'doc_id': row[0],
        'title': row[1],
        'category': row[2],
        'text': row[3],
        'url': row[4],
        'similarity': row[5]
    } for row in results]

# Exemple recherche
results = semantic_search(
    query="Comment optimiser les requ√™tes SQL lentes ?",
    limit=5,
    category="Database"
)

for i, result in enumerate(results, 1):
    print(f"\n{i}. {result['title']} (similarity: {result['similarity']:.4f})")
    print(f"   Category: {result['category']}")
    print(f"   Text preview: {result['text'][:200]}...")
    print(f"   URL: {result['url']}")

# R√©sultat typique :
# 1. SQL Optimization Guide (similarity: 0.1234)
#    Category: Database
#    Text preview: To optimize slow queries, start by using EXPLAIN...
#    URL: https://...
```

#### √âtape 4 : G√©n√©ration RAG (Question + Contexte ‚Üí R√©ponse)

```python
def rag_query(question: str, context_limit: int = 3):
    """RAG : Retrieval + Augmented Generation"""
    
    # 1. Retrieval : Rechercher contexte pertinent
    context_docs = semantic_search(question, limit=context_limit)
    
    # 2. Construire prompt avec contexte
    context_text = "\n\n".join([
        f"Document {i+1} ({doc['title']}):\n{doc['text']}"
        for i, doc in enumerate(context_docs)
    ])
    
    prompt = f"""
    Tu es un assistant technique expert. Utilise UNIQUEMENT les documents fournis ci-dessous pour r√©pondre √† la question.
    Si la r√©ponse n'est pas dans les documents, dis-le clairement.
    
    DOCUMENTS DE R√âF√âRENCE:
    {context_text}
    
    QUESTION: {question}
    
    R√âPONSE:
    """
    
    # 3. G√©n√©ration avec LLM
    response = openai.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": "Tu es un assistant technique pr√©cis."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3  # R√©ponse factuelle, peu cr√©ative
    )
    
    answer = response.choices[0].message.content
    
    return {
        'question': question,
        'answer': answer,
        'sources': [
            {'title': doc['title'], 'url': doc['url']}
            for doc in context_docs
        ]
    }

# Exemple utilisation
result = rag_query("Comment cr√©er un index HNSW dans MariaDB ?")

print(f"Question: {result['question']}\n")
print(f"R√©ponse: {result['answer']}\n")
print("Sources:")
for source in result['sources']:
    print(f"  - {source['title']}: {source['url']}")

# R√©sultat typique :
# Question: Comment cr√©er un index HNSW dans MariaDB ?
# 
# R√©ponse: Pour cr√©er un index HNSW dans MariaDB, utilisez la syntaxe :
# CREATE INDEX idx_name ON table(vector_column) USING HNSW;
# Vous pouvez √©galement sp√©cifier des param√®tres comme M et ef_construction...
# 
# Sources:
#   - MariaDB Vector Documentation: https://mariadb.com/kb/en/vector/
#   - HNSW Index Guide: https://...
```

#### √âtape 5 : Recherche Hybride (SQL + Vector)

```sql
-- Combiner filtres SQL classiques + similarit√© vectorielle

-- Recherche dans cat√©gorie sp√©cifique
SELECT 
  doc_id,
  title,
  chunk_text,
  VEC_DISTANCE_COSINE(embedding, @query_vec) AS similarity
FROM knowledge_base
WHERE category = 'Database'  -- Filtre SQL
  AND created_at >= DATE_SUB(NOW(), INTERVAL 6 MONTH)  -- Documents r√©cents
ORDER BY similarity ASC
LIMIT 10;

-- Recherche multi-crit√®res
SELECT 
  doc_id,
  title,
  category,
  chunk_text,
  VEC_DISTANCE_COSINE(embedding, @query_vec) AS vec_similarity,
  MATCH(chunk_text) AGAINST('optimization' IN BOOLEAN MODE) AS text_score
FROM knowledge_base
WHERE 
  category IN ('Database', 'Performance')
  AND (
    -- Texte contient mot-cl√© OU s√©mantiquement similaire
    MATCH(chunk_text) AGAINST('optimization index' IN BOOLEAN MODE)
    OR VEC_DISTANCE_COSINE(embedding, @query_vec) < 0.3
  )
ORDER BY 
  vec_similarity ASC,
  text_score DESC
LIMIT 20;

-- Agr√©gation sur r√©sultats
SELECT 
  category,
  COUNT(*) AS relevant_docs,
  AVG(VEC_DISTANCE_COSINE(embedding, @query_vec)) AS avg_similarity
FROM knowledge_base
GROUP BY category
HAVING avg_similarity < 0.5
ORDER BY avg_similarity ASC;
```

---

## Cas d'Usage Concrets

### 1. Chatbot Support Client

**Architecture** :
```
Client Question ‚Üí Embedding ‚Üí MariaDB Vector ‚Üí Top-3 FAQ similaires ‚Üí LLM ‚Üí R√©ponse
```

**Impl√©mentation** :
```sql
CREATE TABLE faq (
  faq_id INT PRIMARY KEY AUTO_INCREMENT,
  question TEXT,
  answer TEXT,
  category VARCHAR(50),
  language VARCHAR(5) DEFAULT 'fr',
  view_count INT DEFAULT 0,
  helpful_count INT DEFAULT 0,
  embedding VECTOR(1536),
  
  INDEX idx_category (category),
  INDEX idx_embedding ON (embedding) USING HNSW
);

-- Indexer FAQ
INSERT INTO faq (question, answer, category, embedding) VALUES (
  'Comment r√©initialiser mon mot de passe ?',
  'Pour r√©initialiser votre mot de passe : 1. Cliquez sur "Mot de passe oubli√©"...',
  'Account',
  '[0.12, -0.34, ...]'  -- Embedding de la question
);

-- Recherche question similaire
SET @user_question_vec = get_embedding('J''ai oubli√© mon mot de passe');

SELECT 
  faq_id,
  question,
  answer,
  VEC_DISTANCE_COSINE(embedding, @user_question_vec) AS similarity
FROM faq
WHERE language = 'fr'
ORDER BY similarity ASC
LIMIT 3;

-- Mettre √† jour statistiques
UPDATE faq 
SET view_count = view_count + 1 
WHERE faq_id = 123;
```

### 2. Recommandation de Produits

**Principe** : Similarit√© entre embeddings de descriptions produits.

```sql
CREATE TABLE products (
  product_id INT PRIMARY KEY AUTO_INCREMENT,
  name VARCHAR(255),
  description TEXT,
  category VARCHAR(100),
  price DECIMAL(10,2),
  
  -- Embedding de la description
  description_embedding VECTOR(1536),
  
  INDEX idx_category (category),
  INDEX idx_price (price),
  INDEX idx_emb ON (description_embedding) USING HNSW
);

-- Recommandation : Produits similaires
SELECT 
  p2.product_id,
  p2.name,
  p2.price,
  VEC_DISTANCE_COSINE(
    p2.description_embedding,
    p1.description_embedding
  ) AS similarity
FROM products p1
CROSS JOIN products p2
WHERE p1.product_id = 42  -- Produit de r√©f√©rence
  AND p2.product_id != p1.product_id
  AND p2.category = p1.category  -- M√™me cat√©gorie
  AND ABS(p2.price - p1.price) < 100  -- Prix similaire
ORDER BY similarity ASC
LIMIT 5;
```

### 3. Recherche d'Images par Similarit√©

**Principe** : Embeddings d'images (CLIP, ResNet).

```sql
CREATE TABLE images (
  image_id INT PRIMARY KEY AUTO_INCREMENT,
  filename VARCHAR(255),
  url VARCHAR(500),
  description TEXT,
  
  -- Embedding visuel (CLIP ViT-L/14)
  visual_embedding VECTOR(768),
  
  -- Tags
  tags JSON,
  
  uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  
  INDEX idx_emb ON (visual_embedding) USING HNSW
);

-- Recherche images visuellement similaires
SET @query_image_vec = get_clip_embedding('path/to/query.jpg');

SELECT 
  image_id,
  filename,
  description,
  VEC_DISTANCE_COSINE(visual_embedding, @query_image_vec) AS similarity
FROM images
ORDER BY similarity ASC
LIMIT 10;

-- Recherche cross-modale : Texte ‚Üí Images similaires
SET @text_query_vec = get_clip_text_embedding('sunset over mountains');

SELECT 
  image_id,
  filename,
  VEC_DISTANCE_COSINE(visual_embedding, @text_query_vec) AS similarity
FROM images
ORDER BY similarity ASC
LIMIT 10;
```

### 4. D√©tection de Contenu Dupliqu√©

**Principe** : Documents avec embeddings tr√®s proches = potentiellement dupliqu√©s.

```sql
-- Trouver documents similaires (potentiel plagiat)
SELECT 
  d1.doc_id AS doc1_id,
  d1.title AS doc1_title,
  d2.doc_id AS doc2_id,
  d2.title AS doc2_title,
  VEC_DISTANCE_COSINE(d1.embedding, d2.embedding) AS similarity
FROM documents d1
CROSS JOIN documents d2
WHERE d1.doc_id < d2.doc_id  -- √âviter doublons (A,B) = (B,A)
  AND VEC_DISTANCE_COSINE(d1.embedding, d2.embedding) < 0.05  -- Tr√®s similaire
ORDER BY similarity ASC;

-- R√©sultat :
-- doc1_id | doc1_title        | doc2_id | doc2_title         | similarity
--    123  | "Article X"       |   456   | "Article X (copy)" | 0.001
--    789  | "Tutorial Python" |   234   | "Python Guide"     | 0.042
```

---

## Performance et Optimisations

### Optimisations SIMD

**SIMD** (Single Instruction Multiple Data) : Instructions CPU parall√®les pour calculs vectoriels.

**Support MariaDB Vector** :
- ‚úÖ **AVX2** (256-bit) : CPU Intel/AMD depuis 2013
- ‚úÖ **AVX-512** (512-bit) : CPU Intel Skylake-X+, AMD Zen 4+
- ‚úÖ **ARM NEON** : Processeurs ARM

**V√©rification support** :
```bash
# Linux : V√©rifier flags CPU
cat /proc/cpuinfo | grep -E 'avx2|avx512'
# Si pr√©sent ‚Üí SIMD activ√© automatiquement

# MariaDB : V√©rifier optimisations
SHOW VARIABLES LIKE '%vector%';
-- vector_simd_enabled: ON
```

**Impact performance** :

| Op√©ration | Sans SIMD | AVX2 | AVX-512 | Gain |
|-----------|-----------|------|---------|------|
| Distance cosine (1536d) | 12 Œºs | 3 Œºs | 1.5 Œºs | **8x** |
| Distance euclidean | 15 Œºs | 4 Œºs | 2 Œºs | **7.5x** |
| Dot product | 8 Œºs | 2 Œºs | 1 Œºs | **8x** |

### Tuning Index HNSW

**Param√®tres √† ajuster** :

```sql
-- M : Nombre de connexions par n≈ìud
-- Trade-off : Pr√©cision vs M√©moire

-- M=8 : Rapide, compact (recommand√© si < 100K vecteurs)
CREATE INDEX idx_emb ON docs(embedding) USING HNSW WITH (M = 8);

-- M=16 : √âquilibr√© (d√©faut, recommand√© g√©n√©ral)
CREATE INDEX idx_emb ON docs(embedding) USING HNSW WITH (M = 16);

-- M=32 : Haute pr√©cision (recommand√© si > 1M vecteurs)
CREATE INDEX idx_emb ON docs(embedding) USING HNSW WITH (M = 32);

-- ef_construction : Qualit√© construction index
-- Plus √©lev√© = meilleur index, mais construction plus lente

-- ef_construction=100 : Rapide (dev/test)
CREATE INDEX idx_emb ON docs(embedding) USING HNSW 
  WITH (ef_construction = 100);

-- ef_construction=200 : √âquilibr√© (d√©faut, prod)
CREATE INDEX idx_emb ON docs(embedding) USING HNSW 
  WITH (ef_construction = 200);

-- ef_construction=500 : Haute qualit√© (critique)
CREATE INDEX idx_emb ON docs(embedding) USING HNSW 
  WITH (ef_construction = 500);
```

**Recommandations** :

| Taille Dataset | M | ef_construction | Taille Index | Recall@10 |
|----------------|---|-----------------|--------------|-----------|
| < 10K | 8 | 100 | Petit | 95% |
| 10K - 100K | 16 | 200 | Moyen | 98% |
| 100K - 1M | 16 | 300 | Grand | 99% |
| > 1M | 32 | 400 | Tr√®s grand | 99.5% |

**Recall@10** : Probabilit√© que les 10 meilleurs vrais voisins soient dans les 10 r√©sultats de l'index.

### Benchmark Comparatif

**Configuration test** :
- Dataset : 1M vecteurs 1536 dimensions
- Hardware : 16 cores, 64 GB RAM, SSD NVMe
- Mod√®le : OpenAI text-embedding-3-small

**R√©sultats** :

| M√©thode | Temps Recherche | Recall@10 | M√©moire Index |
|---------|----------------|-----------|---------------|
| **Scan Complet** | 2,500 ms | 100% | 0 MB (pas d'index) |
| **HNSW M=8** | 12 ms | 95% | 450 MB |
| **HNSW M=16** | 18 ms | 98% | 850 MB |
| **HNSW M=32** | 28 ms | 99.5% | 1,650 MB |

**Observations** :
- HNSW M=16 : **140x plus rapide** que scan complet
- Trade-off acceptable : -2% recall pour 850 MB m√©moire
- Pour applications critiques : M=32 (-0.5% recall)

---

## Comparaison avec Bases Vectorielles Sp√©cialis√©es

### MariaDB Vector vs Pinecone/Weaviate/Qdrant

| Aspect | MariaDB Vector | Pinecone | Weaviate | Qdrant |
|--------|----------------|----------|----------|--------|
| **Type** | SQL + Vector | Vector pure | GraphQL + Vector | REST + Vector |
| **D√©ploiement** | Self-hosted | Cloud (SaaS) | Self-hosted | Self-hosted |
| **Co√ªt** | ‚úÖ Gratuit | $$$ (usage-based) | Gratuit/Entreprise | Gratuit |
| **Indexation** | HNSW | Propri√©taire | HNSW | HNSW |
| **Dimensions max** | 65,535 | 20,000 | 65,536 | 65,536 |
| **Transactions** | ‚úÖ ACID | ‚ùå Non | ‚ö†Ô∏è Limit√© | ‚ùå Non |
| **SQL natif** | ‚úÖ Complet | ‚ùå Non | ‚ùå Non | ‚ùå Non |
| **Joins** | ‚úÖ Oui | ‚ùå Non | ‚ö†Ô∏è GraphQL | ‚ùå Non |
| **Filtrage** | ‚úÖ SQL WHERE | ‚ö†Ô∏è Metadata | ‚úÖ GraphQL | ‚úÖ Payload |
| **Performance** | Tr√®s bon | Excellent | Excellent | Excellent |
| **Scalabilit√©** | Horizontale | Auto | Manuelle | Manuelle |
| **Backup** | ‚úÖ Standard SQL | ‚úÖ Automatique | ‚ö†Ô∏è Manuel | ‚ö†Ô∏è Manuel |
| **Encryption** | ‚úÖ At-rest | ‚úÖ Transit | ‚úÖ Both | ‚úÖ Both |

### Quand Choisir MariaDB Vector ?

**‚úÖ Utiliser MariaDB Vector si** :
- Donn√©es m√©tier + vecteurs dans m√™me syst√®me
- Besoin de transactions ACID
- Requ√™tes hybrides SQL + similarit√©
- Infrastructure existante MariaDB
- Budget limit√© (pas de SaaS)
- < 10M vecteurs (performant jusqu'√† cette √©chelle)

**‚ùå Utiliser base vectorielle sp√©cialis√©e si** :
- > 100M vecteurs (scalabilit√© extr√™me)
- Latence < 1 ms critique (edge cases)
- √âquipe d√©di√©e ML/Vector ops
- Budget permet SaaS g√©r√© (Pinecone)
- Pas de donn√©es m√©tier relationnelles

**üîÄ Approche hybride** :
```
MariaDB (donn√©es m√©tier + vecteurs r√©cents/actifs)
    +
Pinecone/Weaviate (archive vecteurs anciens/massifs)
```

---

## Best Practices

### 1. Design de Sch√©ma

```sql
-- ‚úÖ Bonnes pratiques

CREATE TABLE documents (
  -- ID num√©rique (performant)
  doc_id BIGINT PRIMARY KEY AUTO_INCREMENT,
  
  -- M√©tadonn√©es filtrables (index classiques)
  category VARCHAR(100),
  language VARCHAR(5),
  created_at TIMESTAMP,
  INDEX idx_cat_lang (category, language),
  INDEX idx_created (created_at),
  
  -- Contenu textuel (FULLTEXT pour recherche hybride)
  title VARCHAR(500),
  content TEXT,
  FULLTEXT INDEX ft_content (title, content),
  
  -- Vecteur (dimensions fixes selon mod√®le)
  embedding VECTOR(1536) NOT NULL,
  INDEX idx_emb ON (embedding) USING HNSW WITH (M = 16),
  
  -- Normalisation : S√©parer texte long si > 64KB
  -- Utiliser table li√©e si n√©cessaire
  
  -- Version embedding (pour re-indexation)
  embedding_model VARCHAR(50) DEFAULT 'text-embedding-3-small',
  embedding_version INT DEFAULT 1
);
```

### 2. Chunking Optimal

```python
def smart_chunk(text: str, max_tokens: int = 500, overlap: int = 50):
    """
    D√©coupage intelligent avec overlap
    
    Overlap : Contexte partag√© entre chunks adjacents
    Am√©liore r√©sultats pour phrases √† cheval sur 2 chunks
    """
    import tiktoken
    
    enc = tiktoken.get_encoding("cl100k_base")
    tokens = enc.encode(text)
    
    chunks = []
    start = 0
    
    while start < len(tokens):
        end = start + max_tokens
        chunk_tokens = tokens[start:end]
        chunk_text = enc.decode(chunk_tokens)
        
        chunks.append({
            'text': chunk_text,
            'start_token': start,
            'end_token': end
        })
        
        start = end - overlap  # Overlap
    
    return chunks

# Longueur optimale par use case :
# - FAQ/Support : 200-500 tokens (questions courtes)
# - Documentation : 500-1000 tokens (paragraphes complets)
# - Articles : 1000-2000 tokens (sections logiques)
```

### 3. Gestion des Versions d'Embeddings

```sql
-- Probl√®me : Mod√®le d'embedding √©volue (v1 ‚Üí v2)
-- Solution : Versioning + re-indexation progressive

ALTER TABLE documents
ADD COLUMN embedding_v2 VECTOR(1536),
ADD INDEX idx_emb_v2 ON (embedding_v2) USING HNSW;

-- Re-indexer progressivement (batch)
-- Script Python ex√©cut√© en background
UPDATE documents
SET 
  embedding_v2 = get_embedding_v2(content),
  embedding_version = 2
WHERE embedding_version = 1
  AND doc_id BETWEEN ? AND ?
LIMIT 1000;

-- Une fois compl√®te, swap colonnes
ALTER TABLE documents
DROP COLUMN embedding,
CHANGE COLUMN embedding_v2 embedding VECTOR(1536);
```

### 4. Monitoring et Alertes

```sql
-- M√©triques √† surveiller

-- 1. Taille index HNSW
SELECT 
  table_name,
  index_name,
  ROUND(stat_value / 1024 / 1024, 2) AS index_size_mb
FROM information_schema.INNODB_SYS_TABLESTATS
WHERE index_name LIKE '%embedding%';

-- 2. Distribution similarit√©
SELECT 
  CASE 
    WHEN similarity < 0.1 THEN 'Tr√®s similaire'
    WHEN similarity < 0.3 THEN 'Similaire'
    WHEN similarity < 0.5 THEN 'Moyennement similaire'
    ELSE 'Peu similaire'
  END AS similarity_range,
  COUNT(*) AS count
FROM (
  SELECT VEC_DISTANCE_COSINE(embedding, @query_vec) AS similarity
  FROM documents
  LIMIT 10000
) AS distances
GROUP BY similarity_range;

-- 3. Performance requ√™tes
-- Utiliser Performance Schema
SELECT 
  DIGEST_TEXT,
  COUNT_STAR,
  AVG_TIMER_WAIT / 1000000000 AS avg_time_ms
FROM performance_schema.events_statements_summary_by_digest
WHERE DIGEST_TEXT LIKE '%VEC_DISTANCE%'
ORDER BY avg_time_ms DESC
LIMIT 10;
```

---

## ‚úÖ Points cl√©s √† retenir

### Concepts Fondamentaux
- ‚úÖ **Recherche vectorielle** : Similarit√© s√©mantique vs correspondance lexicale
- ‚úÖ **Embeddings** : Repr√©sentation num√©rique texte/image (vecteurs haute dimension)
- ‚úÖ **Type VECTOR** : Stockage natif vecteurs (1 √† 65535 dimensions)
- ‚úÖ **Index HNSW** : Recherche approximative k-NN rapide (O(log n))

### Fonctions de Distance
- ‚úÖ **VEC_DISTANCE_COSINE** : Recommand√© pour texte/s√©mantique (0-2)
- ‚úÖ **VEC_DISTANCE_EUCLIDEAN** : Distance g√©om√©trique (0-‚àû)
- ‚úÖ **VEC_DISTANCE_DOT** : Plus rapide si vecteurs normalis√©s
- üí° **Choix** : Cosine par d√©faut, Dot si normalis√©

### RAG (Retrieval Augmented Generation)
- ‚úÖ **Workflow** : Document ‚Üí Chunking ‚Üí Embedding ‚Üí Index ‚Üí Recherche ‚Üí LLM
- ‚úÖ **Chunking** : 500-1000 tokens avec overlap 50-100
- ‚úÖ **Recherche hybride** : SQL WHERE + similarit√© vectorielle
- ‚úÖ **Top-K** : R√©cup√©rer 3-5 documents les plus pertinents

### Performance
- ‚úÖ **SIMD** : AVX2/AVX-512 = 8x plus rapide
- ‚úÖ **HNSW tuning** : M=16 (d√©faut), M=32 (haute pr√©cision)
- ‚úÖ **Benchmark** : 18ms recherche sur 1M vecteurs (140x vs scan)
- ‚úÖ **Recall@10** : 98% avec M=16

### Int√©gration
- ‚úÖ **OpenAI** : text-embedding-3-small (1536d), text-embedding-3-large (3072d)
- ‚úÖ **Hugging Face** : sentence-transformers (384-768d)
- ‚úÖ **CLIP** : Multimodal texte-image (768d)
- ‚úÖ **Local** : Ollama, llama.cpp (open source)

### Cas d'Usage
- ‚úÖ **Chatbot/FAQ** : Recherche questions similaires
- ‚úÖ **Base connaissances** : Documentation technique searchable
- ‚úÖ **Recommandation** : Produits/contenus similaires
- ‚úÖ **D√©tection duplicatas** : Plagiat, contenu similaire

### vs Bases Vectorielles Sp√©cialis√©es
- ‚úÖ **Avantages MariaDB** : ACID, SQL, Joins, Gratuit, Une seule base
- ‚ö†Ô∏è **Limites** : Scalabilit√© < Pinecone (performant jusqu'√† 10M vecteurs)
- üí° **Sweet spot** : Applications avec donn√©es m√©tier + vecteurs

### Best Practices
- ‚úÖ Versioning embeddings (embedding_version)
- ‚úÖ Chunking intelligent avec overlap
- ‚úÖ Index HNSW sur colonnes vectorielles
- ‚úÖ Recherche hybride (SQL + similarit√©)
- ‚úÖ Monitoring taille index et performance

---

## üîó Ressources et r√©f√©rences

### Documentation Officielle MariaDB
- üìñ [MariaDB Vector](https://mariadb.com/kb/en/vector/) - Documentation compl√®te
- üìñ [VECTOR Data Type](https://mariadb.com/kb/en/vector-data-type/)
- üìñ [HNSW Index](https://mariadb.com/kb/en/vector-indexes/)
- üìñ [Vector Functions](https://mariadb.com/kb/en/vector-functions/)

### Embeddings et Mod√®les
- ü§ñ [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings) - API officielle
- ü§ó [Hugging Face Models](https://huggingface.co/models?pipeline_tag=sentence-similarity)
- üìù [Sentence Transformers](https://www.sbert.net/) - Mod√®les open source

### RAG et Applications
- üìö [RAG Architecture](https://arxiv.org/abs/2005.11401) - Paper original
- üîß [LangChain](https://python.langchain.com/) - Framework RAG
- ü¶ú [LlamaIndex](https://www.llamaindex.ai/) - Alternative RAG

### Algorithmes et Performance
- üìÑ [HNSW Paper](https://arxiv.org/abs/1603.09320) - Algorithme original
- üìä [Vector Search Benchmarks](https://github.com/erikbern/ann-benchmarks)

### Comparaisons
- üîÑ [Pinecone](https://www.pinecone.io/) - Base vectorielle SaaS
- üîÑ [Weaviate](https://weaviate.io/) - Base vectorielle open source
- üîÑ [Qdrant](https://qdrant.tech/) - Alternative Rust

---

## ‚û°Ô∏è Sous-sections suivantes

### **18.10.1 Type de Donn√©es VECTOR**
D√©tails du type VECTOR, stockage interne, limites de dimensions, et conversions.

### **18.10.2 Index HNSW**
Architecture d√©taill√©e HNSW, param√®tres avanc√©s (M, ef_construction, ef_search), tuning performance.

### **18.10.3 Fonctions de Distance**
Formules math√©matiques, cas d'usage par fonction, normalisation, optimisations SIMD.

### **18.10.4 Int√©gration avec LLMs**
Exemples complets OpenAI, Hugging Face, Ollama, gestion tokens, co√ªts, rate limits.

---


‚è≠Ô∏è [Type de donn√©es VECTOR](/18-fonctionnalites-avancees/10.1-type-donnees-vector.md)
