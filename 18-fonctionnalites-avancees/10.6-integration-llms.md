üîù Retour au [Sommaire](/SOMMAIRE.md)

# 18.10.6 Int√©gration avec LLMs (OpenAI, Claude, LLaMA)

> **Niveau** : Expert  
> **Dur√©e estim√©e** : 3-4 heures  
> **Pr√©requis** : 
> - Sections 18.10.1 √† 18.10.5 - MariaDB Vector
> - Compr√©hension des LLMs et RAG
> - Exp√©rience avec APIs REST
> - Connaissances en Python/JavaScript

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Int√©grer MariaDB avec OpenAI pour g√©n√©ration d'embeddings
- Utiliser Claude (Anthropic) avec MariaDB Vector
- D√©ployer des mod√®les open source (LLaMA, Mistral)
- Impl√©menter un syst√®me RAG complet
- Construire un chatbot avec m√©moire vectorielle
- Optimiser les co√ªts d'API et de stockage
- G√©rer le versioning des mod√®les d'embeddings
- Mettre en production des applications IA/DB

---

## Introduction

L'int√©gration de **MariaDB Vector** avec les **Large Language Models** permet de construire des applications d'IA augment√©e combinant :
- **G√©n√©ration de texte** (LLM) avec **recherche s√©mantique** (MariaDB Vector)
- **M√©moire longue terme** (embeddings stock√©s) avec **contexte dynamique** (RAG)
- **Scalabilit√©** (MariaDB) avec **intelligence** (LLMs)

### Architecture RAG typique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. INDEXATION (Offline)                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Documents ‚Üí Chunking ‚Üí LLM Embeddings         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚Üì                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        MariaDB Vector Storage                 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. RETRIEVAL (Runtime)                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ User Query ‚Üí Embedding ‚Üí Vector Search        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚Üì                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Top-K relevant chunks from MariaDB         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. GENERATION (Runtime)                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ User Query + Retrieved Context ‚Üí LLM          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚Üì                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Generated Answer (grounded in data)        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Int√©gration OpenAI

### Configuration API

```python
# Installation
# pip install openai mysql-connector-python

import openai
import mysql.connector
from typing import List, Dict, Any
import os

# Configuration OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# Configuration MariaDB
db_config = {
    'host': 'localhost',
    'user': 'rag_user',
    'password': os.getenv("MARIADB_PASSWORD"),
    'database': 'knowledge_base'
}
```

### G√©n√©ration d'embeddings

```python
class OpenAIEmbedder:
    """G√©n√©ration d'embeddings via OpenAI API"""
    
    def __init__(self, model="text-embedding-3-small"):
        """
        Mod√®les disponibles:
        - text-embedding-3-small: 1536 dims, $0.02/1M tokens
        - text-embedding-3-large: 3072 dims, $0.13/1M tokens
        - text-embedding-ada-002: 1536 dims, $0.10/1M tokens (legacy)
        """
        self.model = model
        self.dimensions = 1536 if "small" in model or "ada" in model else 3072
    
    def embed(self, texts: List[str]) -> List[List[float]]:
        """G√©n√©rer embeddings pour une liste de textes"""
        # OpenAI accepte jusqu'√† 2048 textes par requ√™te
        if len(texts) > 2048:
            raise ValueError("Max 2048 texts per batch")
        
        response = openai.embeddings.create(
            model=self.model,
            input=texts
        )
        
        return [item.embedding for item in response.data]
    
    def embed_single(self, text: str) -> List[float]:
        """G√©n√©rer embedding pour un texte unique"""
        return self.embed([text])[0]

# Utilisation
embedder = OpenAIEmbedder(model="text-embedding-3-small")

text = "MariaDB 11.8 introduces native vector support for AI applications"
embedding = embedder.embed_single(text)

print(f"Embedding dimensions: {len(embedding)}")  # 1536
print(f"First 5 values: {embedding[:5]}")
```

### Stockage dans MariaDB

```python
class VectorStore:
    """Interface MariaDB pour stockage vectoriel"""
    
    def __init__(self, db_config: Dict[str, str], embedder):
        self.db_config = db_config
        self.embedder = embedder
        self._init_db()
    
    def _init_db(self):
        """Cr√©er table si n√©cessaire"""
        conn = mysql.connector.connect(**self.db_config)
        cursor = conn.cursor()
        
        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS documents (
                doc_id BIGINT PRIMARY KEY AUTO_INCREMENT,
                content TEXT NOT NULL,
                metadata JSON,
                embedding VECTOR({self.embedder.dimensions}),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                
                INDEX idx_embedding (embedding) USING HNSW
            ) ENGINE=InnoDB
        """)
        
        conn.commit()
        cursor.close()
        conn.close()
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """
        Ajouter documents avec g√©n√©ration automatique embeddings
        documents: [{"content": "...", "metadata": {...}}, ...]
        """
        conn = mysql.connector.connect(**self.db_config)
        cursor = conn.cursor()
        
        # G√©n√©rer embeddings par batch
        contents = [doc["content"] for doc in documents]
        embeddings = self.embedder.embed(contents)
        
        # Insertion
        insert_stmt = """
            INSERT INTO documents (content, metadata, embedding)
            VALUES (%s, %s, VEC_FromText(%s))
        """
        
        data = []
        for doc, embedding in zip(documents, embeddings):
            emb_str = '[' + ','.join(map(str, embedding)) + ']'
            metadata_json = json.dumps(doc.get("metadata", {}))
            data.append((doc["content"], metadata_json, emb_str))
        
        cursor.executemany(insert_stmt, data)
        conn.commit()
        
        print(f"‚úì Inserted {len(documents)} documents")
        
        cursor.close()
        conn.close()
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Recherche s√©mantique"""
        # G√©n√©rer embedding de la query
        query_embedding = self.embedder.embed_single(query)
        query_emb_str = '[' + ','.join(map(str, query_embedding)) + ']'
        
        conn = mysql.connector.connect(**self.db_config)
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute("""
            SELECT 
                doc_id,
                content,
                metadata,
                1 - VEC_DISTANCE_COSINE(
                    embedding, 
                    VEC_FromText(%s)
                ) AS similarity
            FROM documents
            ORDER BY similarity DESC
            LIMIT %s
        """, (query_emb_str, top_k))
        
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        return results

# Utilisation
store = VectorStore(db_config, embedder)

# Indexer documents
documents = [
    {
        "content": "MariaDB Vector provides native support for AI embeddings",
        "metadata": {"source": "documentation", "topic": "vector"}
    },
    {
        "content": "HNSW index enables fast approximate nearest neighbor search",
        "metadata": {"source": "documentation", "topic": "performance"}
    }
]

store.add_documents(documents)

# Recherche
results = store.search("How to use vectors in MariaDB?", top_k=3)

for result in results:
    print(f"Similarity: {result['similarity']:.4f}")
    print(f"Content: {result['content']}")
    print("---")
```

### Syst√®me RAG complet avec OpenAI

```python
class RAGSystem:
    """Retrieval-Augmented Generation avec OpenAI + MariaDB"""
    
    def __init__(self, vector_store: VectorStore, llm_model="gpt-4o-mini"):
        self.vector_store = vector_store
        self.llm_model = llm_model
    
    def query(self, 
              question: str, 
              top_k: int = 5,
              temperature: float = 0.1) -> Dict[str, Any]:
        """
        Poser une question avec RAG
        
        Retourne:
        - answer: R√©ponse g√©n√©r√©e
        - sources: Documents sources utilis√©s
        - similarity_scores: Scores de similarit√©
        """
        # 1. Retrieval : chercher documents pertinents
        retrieved_docs = self.vector_store.search(question, top_k=top_k)
        
        if not retrieved_docs:
            return {
                "answer": "Je n'ai pas trouv√© d'informations pertinentes.",
                "sources": [],
                "similarity_scores": []
            }
        
        # 2. Construire contexte
        context = "\n\n".join([
            f"Document {i+1}:\n{doc['content']}"
            for i, doc in enumerate(retrieved_docs)
        ])
        
        # 3. Prompt engineering
        system_prompt = """Tu es un assistant qui r√©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.
Si la r√©ponse n'est pas dans le contexte, dis-le clairement.
Cite les documents sources dans ta r√©ponse."""
        
        user_prompt = f"""Contexte:
{context}

Question: {question}

R√©ponse:"""
        
        # 4. Generation : appeler LLM
        response = openai.chat.completions.create(
            model=self.llm_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature
        )
        
        answer = response.choices[0].message.content
        
        return {
            "answer": answer,
            "sources": [doc['content'] for doc in retrieved_docs],
            "similarity_scores": [doc['similarity'] for doc in retrieved_docs],
            "tokens_used": response.usage.total_tokens
        }

# Utilisation
rag = RAGSystem(store, llm_model="gpt-4o-mini")

result = rag.query("What is HNSW and how does it work?")

print("Question:", "What is HNSW and how does it work?")
print("\nAnswer:", result['answer'])
print(f"\nTokens used: {result['tokens_used']}")
print(f"\nTop source similarity: {result['similarity_scores'][0]:.4f}")
```

---

## Int√©gration Claude (Anthropic)

### Configuration API Anthropic

```python
# Installation
# pip install anthropic

import anthropic
import json

# Configuration
client = anthropic.Anthropic(
    api_key=os.getenv("ANTHROPIC_API_KEY")
)
```

### Embeddings via Voyage AI (partenaire Anthropic)

```python
# Claude n'a pas d'API embeddings native
# Utiliser Voyage AI (recommand√© par Anthropic)
# pip install voyageai

import voyageai

voyage_client = voyageai.Client(api_key=os.getenv("VOYAGE_API_KEY"))

class VoyageEmbedder:
    """Embeddings Voyage AI (compatible Claude)"""
    
    def __init__(self, model="voyage-2"):
        """
        Mod√®les:
        - voyage-2: 1024 dims, optimis√© g√©n√©ral
        - voyage-large-2: 1536 dims, haute pr√©cision
        - voyage-code-2: 1536 dims, code
        """
        self.client = voyageai.Client()
        self.model = model
        self.dimensions = 1024 if model == "voyage-2" else 1536
    
    def embed(self, texts: List[str]) -> List[List[float]]:
        """G√©n√©rer embeddings"""
        result = self.client.embed(
            texts=texts,
            model=self.model
        )
        return result.embeddings
    
    def embed_single(self, text: str) -> List[float]:
        return self.embed([text])[0]

# Utilisation
voyage_embedder = VoyageEmbedder(model="voyage-2")
```

### RAG avec Claude

```python
class ClaudeRAG:
    """RAG avec Claude (Anthropic)"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.client = anthropic.Anthropic()
    
    def query(self,
              question: str,
              top_k: int = 5,
              model="claude-3-5-sonnet-20241022",
              max_tokens: int = 1024) -> Dict[str, Any]:
        """
        Query avec Claude
        
        Mod√®les disponibles:
        - claude-3-5-sonnet-20241022: Optimal qualit√©/vitesse
        - claude-3-5-haiku-20241022: Ultra rapide, √©conomique
        - claude-3-opus-20240229: Pr√©cision maximale
        """
        # Retrieval
        retrieved_docs = self.vector_store.search(question, top_k=top_k)
        
        if not retrieved_docs:
            return {"answer": "Aucune information pertinente trouv√©e."}
        
        # Contexte
        context = "\n\n".join([
            f"<document index=\"{i+1}\">\n{doc['content']}\n</document>"
            for i, doc in enumerate(retrieved_docs)
        ])
        
        # Prompt (style Claude optimis√©)
        prompt = f"""Voici des documents pertinents:

<documents>
{context}
</documents>

Question: {question}

Instructions:
- R√©ponds en te basant UNIQUEMENT sur les documents fournis
- Cite tes sources avec <cite index="X">...</cite>
- Si l'information n'est pas dans les documents, dis-le clairement
- Sois concis et pr√©cis"""
        
        # Appel API Claude
        message = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        answer = message.content[0].text
        
        return {
            "answer": answer,
            "sources": [doc['content'] for doc in retrieved_docs],
            "model": model,
            "tokens_input": message.usage.input_tokens,
            "tokens_output": message.usage.output_tokens
        }

# Utilisation
claude_rag = ClaudeRAG(store)

result = claude_rag.query(
    "Explain vector search optimization techniques",
    model="claude-3-5-sonnet-20241022"
)

print("Answer:", result['answer'])
print(f"Tokens: {result['tokens_input']} in, {result['tokens_output']} out")
```

---

## Int√©gration LLaMA et mod√®les open source

### D√©ploiement LLaMA local

```python
# Installation
# pip install llama-cpp-python sentence-transformers

from llama_cpp import Llama
from sentence_transformers import SentenceTransformer

class LocalLLMRAG:
    """RAG avec LLaMA local (pas d'API externe)"""
    
    def __init__(self, 
                 llm_model_path: str,
                 embedding_model: str = "all-MiniLM-L6-v2"):
        """
        llm_model_path: Chemin vers mod√®le GGUF
        embedding_model: Mod√®le sentence-transformers
        """
        # LLM local
        self.llm = Llama(
            model_path=llm_model_path,
            n_ctx=4096,  # Context window
            n_threads=8,  # CPU threads
            n_gpu_layers=35  # Layers sur GPU si disponible
        )
        
        # Embeddings local
        self.embedder = SentenceTransformer(embedding_model)
        self.dimensions = self.embedder.get_sentence_embedding_dimension()
        
        # MariaDB
        self.vector_store = VectorStore(db_config, self)
    
    def embed(self, texts: List[str]) -> List[List[float]]:
        """G√©n√©rer embeddings localement"""
        embeddings = self.embedder.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()
    
    def embed_single(self, text: str) -> List[float]:
        return self.embed([text])[0]
    
    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """RAG complet local (0 co√ªt API)"""
        # Retrieval
        retrieved_docs = self.vector_store.search(question, top_k=top_k)
        
        if not retrieved_docs:
            return {"answer": "No relevant information found."}
        
        # Contexte
        context = "\n\n".join([
            f"Document {i+1}:\n{doc['content']}"
            for i, doc in enumerate(retrieved_docs)
        ])
        
        # Prompt pour LLaMA
        prompt = f"""### Context:
{context}

### Question:
{question}

### Answer:
"""
        
        # G√©n√©ration locale
        output = self.llm(
            prompt,
            max_tokens=512,
            temperature=0.1,
            top_p=0.9,
            stop=["###", "\n\n\n"]
        )
        
        answer = output['choices'][0]['text'].strip()
        
        return {
            "answer": answer,
            "sources": [doc['content'] for doc in retrieved_docs],
            "model": "local_llama"
        }

# Utilisation
# T√©l√©charger mod√®le GGUF depuis Hugging Face
# Exemple: llama-2-7b-chat.Q4_K_M.gguf

local_rag = LocalLLMRAG(
    llm_model_path="/models/llama-2-7b-chat.Q4_K_M.gguf",
    embedding_model="all-MiniLM-L6-v2"
)

result = local_rag.query("What are the benefits of vector databases?")
print(result['answer'])
```

### Ollama integration

```python
# Alternative : Ollama (plus simple)
# Installation : curl -fsSL https://ollama.com/install.sh | sh
# pip install ollama

import ollama

class OllamaRAG:
    """RAG avec Ollama (LLaMA, Mistral, etc.)"""
    
    def __init__(self, model: str = "llama3.1"):
        """
        Mod√®les disponibles via Ollama:
        - llama3.1: 8B, 70B params
        - mistral: 7B params
        - mixtral: 8x7B MoE
        - phi3: 3.8B params (tr√®s rapide)
        """
        self.model = model
        
        # V√©rifier mod√®le install√©
        try:
            ollama.show(model)
        except:
            print(f"Downloading {model}...")
            ollama.pull(model)
    
    def query_with_context(self, question: str, context: str) -> str:
        """G√©n√©rer r√©ponse avec contexte"""
        prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {question}

Answer:"""
        
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options={
                "temperature": 0.1,
                "top_p": 0.9
            }
        )
        
        return response['response']

# Utilisation
ollama_rag = OllamaRAG(model="llama3.1")

# Combiner avec MariaDB Vector
retrieved_docs = store.search("vector search", top_k=3)
context = "\n\n".join([doc['content'] for doc in retrieved_docs])

answer = ollama_rag.query_with_context(
    "How does vector search work?",
    context
)
```

---

## Cas d'usage avanc√©s

### 1. Chatbot avec m√©moire conversationnelle

```python
class ConversationalRAG:
    """Chatbot avec historique et m√©moire vectorielle"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.client = openai
        self.conversation_history = []
    
    def chat(self, user_message: str, session_id: str) -> str:
        """Conversation avec contexte et m√©moire"""
        # 1. Recherche dans base de connaissances
        relevant_docs = self.vector_store.search(user_message, top_k=3)
        
        # 2. Construire contexte avec historique
        context_docs = "\n".join([
            f"- {doc['content']}"
            for doc in relevant_docs
        ])
        
        system_prompt = f"""Tu es un assistant expert. Utilise ces informations:

{context_docs}

R√©ponds de mani√®re conversationnelle en te basant sur le contexte et l'historique."""
        
        # 3. Construire messages avec historique
        messages = [{"role": "system", "content": system_prompt}]
        
        # Ajouter historique (max 10 derniers messages)
        for msg in self.conversation_history[-10:]:
            messages.append(msg)
        
        # Ajouter message utilisateur
        messages.append({"role": "user", "content": user_message})
        
        # 4. G√©n√©rer r√©ponse
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7
        )
        
        assistant_message = response.choices[0].message.content
        
        # 5. Sauvegarder dans historique
        self.conversation_history.append(
            {"role": "user", "content": user_message}
        )
        self.conversation_history.append(
            {"role": "assistant", "content": assistant_message}
        )
        
        # 6. Optionnel : Stocker conversation dans MariaDB
        self._save_conversation(session_id, user_message, assistant_message)
        
        return assistant_message
    
    def _save_conversation(self, session_id: str, user_msg: str, bot_msg: str):
        """Sauvegarder conversation pour analyse"""
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO conversations 
            (session_id, user_message, bot_response, timestamp)
            VALUES (%s, %s, %s, NOW())
        """, (session_id, user_msg, bot_msg))
        
        conn.commit()
        cursor.close()
        conn.close()

# Utilisation
chatbot = ConversationalRAG(store)

# Session utilisateur
session = "user_12345"

print(chatbot.chat("What is MariaDB Vector?", session))
print(chatbot.chat("How do I create an index?", session))
print(chatbot.chat("What about performance?", session))
# ‚Üí Le chatbot maintient le contexte de la conversation
```

### 2. Support client multilingue

```python
class MultilingualSupport:
    """Support client avec d√©tection langue et RAG"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.client = openai
    
    def detect_language(self, text: str) -> str:
        """D√©tecter langue du texte"""
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": f"Detect the language of this text and respond with only the ISO 639-1 code (e.g., 'en', 'fr', 'es'): {text}"
            }],
            temperature=0
        )
        return response.choices[0].message.content.strip().lower()
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """R√©pondre dans la langue de la question"""
        # D√©tecter langue
        language = self.detect_language(question)
        
        # Traduire en anglais si n√©cessaire (base de connaissances en anglais)
        if language != 'en':
            translation = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": f"Translate to English: {question}"
                }],
                temperature=0
            )
            english_question = translation.choices[0].message.content
        else:
            english_question = question
        
        # Recherche vectorielle (en anglais)
        docs = self.vector_store.search(english_question, top_k=3)
        context = "\n".join([doc['content'] for doc in docs])
        
        # G√©n√©rer r√©ponse dans langue d'origine
        lang_names = {
            'en': 'English', 'fr': 'French', 'es': 'Spanish',
            'de': 'German', 'it': 'Italian', 'pt': 'Portuguese'
        }
        
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": f"""Context: {context}

Question: {question}

Answer the question in {lang_names.get(language, 'the original language')} based on the context."""
            }],
            temperature=0.3
        )
        
        return {
            "answer": response.choices[0].message.content,
            "detected_language": language,
            "sources": [doc['content'] for doc in docs]
        }

# Utilisation
support = MultilingualSupport(store)

# Question en fran√ßais
result = support.answer_question("Comment fonctionne la recherche vectorielle?")
print(f"Language: {result['detected_language']}")
print(f"Answer: {result['answer']}")

# Question en espagnol
result = support.answer_question("¬øQu√© es MariaDB Vector?")
print(f"Language: {result['detected_language']}")
print(f"Answer: {result['answer']}")
```

### 3. Analyse de sentiment avec embeddings

```python
class SentimentAnalyzer:
    """Analyse sentiment + clustering s√©mantique"""
    
    def __init__(self):
        self.embedder = OpenAIEmbedder()
        self.conn = mysql.connector.connect(**db_config)
    
    def analyze_reviews(self, reviews: List[Dict[str, str]]):
        """Analyser sentiment et cr√©er clusters"""
        # G√©n√©rer embeddings
        texts = [r['text'] for r in reviews]
        embeddings = self.embedder.embed(texts)
        
        # Analyser sentiment avec LLM
        sentiments = []
        for text in texts:
            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": f"Analyze sentiment (positive/negative/neutral): {text}"
                }],
                temperature=0
            )
            sentiments.append(response.choices[0].message.content)
        
        # Stocker avec embeddings
        cursor = self.conn.cursor()
        for review, embedding, sentiment in zip(reviews, embeddings, sentiments):
            emb_str = '[' + ','.join(map(str, embedding)) + ']'
            cursor.execute("""
                INSERT INTO reviews 
                (product_id, review_text, sentiment, embedding)
                VALUES (%s, %s, %s, VEC_FromText(%s))
            """, (review['product_id'], review['text'], sentiment, emb_str))
        
        self.conn.commit()
        cursor.close()
    
    def find_similar_complaints(self, complaint: str, top_k: int = 10):
        """Trouver plaintes similaires pour analyse tendances"""
        embedding = self.embedder.embed_single(complaint)
        emb_str = '[' + ','.join(map(str, embedding)) + ']'
        
        cursor = self.conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                product_id,
                review_text,
                sentiment,
                1 - VEC_DISTANCE_COSINE(embedding, VEC_FromText(%s)) AS similarity
            FROM reviews
            WHERE sentiment LIKE '%negative%'
            ORDER BY similarity DESC
            LIMIT %s
        """, (emb_str, top_k))
        
        return cursor.fetchall()
```

---

## Optimisation des co√ªts

### Strat√©gies de r√©duction

```python
class CostOptimizedRAG:
    """RAG optimis√© pour r√©duire co√ªts API"""
    
    def __init__(self):
        self.cache = {}  # Cache embeddings
        self.embedder = OpenAIEmbedder()
    
    def embed_with_cache(self, text: str) -> List[float]:
        """Cache embeddings pour √©viter recalcul"""
        # Hash du texte
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # G√©n√©rer et cacher
        embedding = self.embedder.embed_single(text)
        self.cache[text_hash] = embedding
        
        return embedding
    
    def query_with_hybrid_search(self, question: str) -> Dict[str, Any]:
        """
        Recherche hybride : keyword + vector
        R√©duit nombre de calculs vectoriels
        """
        # 1. Pre-filter avec full-text search (rapide, gratuit)
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute("""
            SELECT doc_id, content
            FROM documents
            WHERE MATCH(content) AGAINST(%s IN BOOLEAN MODE)
            LIMIT 50
        """, (question,))
        
        candidates = cursor.fetchall()
        
        # 2. Re-rank avec embeddings (seulement 50 au lieu de tous)
        if not candidates:
            return {"answer": "No results"}
        
        query_embedding = self.embed_with_cache(question)
        query_emb_str = '[' + ','.join(map(str, query_embedding)) + ']'
        
        candidate_ids = [c['doc_id'] for c in candidates]
        placeholders = ','.join(['%s'] * len(candidate_ids))
        
        cursor.execute(f"""
            SELECT 
                doc_id, content,
                1 - VEC_DISTANCE_COSINE(embedding, VEC_FromText(%s)) AS similarity
            FROM documents
            WHERE doc_id IN ({placeholders})
            ORDER BY similarity DESC
            LIMIT 5
        """, (query_emb_str, *candidate_ids))
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        return {"results": results}
```

### Comparaison co√ªts

```python
# Co√ªts OpenAI (D√©cembre 2024)
costs = {
    "embeddings": {
        "text-embedding-3-small": 0.02 / 1_000_000,  # $/token
        "text-embedding-3-large": 0.13 / 1_000_000,
        "text-embedding-ada-002": 0.10 / 1_000_000
    },
    "llm": {
        "gpt-4o-mini": {
            "input": 0.150 / 1_000_000,
            "output": 0.600 / 1_000_000
        },
        "gpt-4o": {
            "input": 2.50 / 1_000_000,
            "output": 10.00 / 1_000_000
        }
    }
}

def estimate_monthly_cost(
    docs_count: int,
    avg_doc_tokens: int,
    queries_per_day: int,
    avg_query_tokens: int,
    avg_response_tokens: int
):
    """Estimer co√ªts mensuels"""
    
    # Co√ªt indexation (one-time par mois si r√©indexation)
    embedding_cost = (
        docs_count * avg_doc_tokens * 
        costs["embeddings"]["text-embedding-3-small"]
    )
    
    # Co√ªt queries (quotidien √ó 30)
    queries_embedding_cost = (
        queries_per_day * 30 * avg_query_tokens *
        costs["embeddings"]["text-embedding-3-small"]
    )
    
    # Co√ªt LLM g√©n√©ration
    llm_cost = queries_per_day * 30 * (
        avg_query_tokens * costs["llm"]["gpt-4o-mini"]["input"] +
        avg_response_tokens * costs["llm"]["gpt-4o-mini"]["output"]
    )
    
    total = embedding_cost + queries_embedding_cost + llm_cost
    
    print(f"""
Estimation co√ªts mensuels:
- Indexation : ${embedding_cost:.2f}
- Queries embeddings : ${queries_embedding_cost:.2f}
- LLM g√©n√©ration : ${llm_cost:.2f}
-------------------------
Total : ${total:.2f}/mois
    """)
    
    return total

# Exemple : Support client
estimate_monthly_cost(
    docs_count=10_000,          # 10K articles support
    avg_doc_tokens=500,         # 500 tokens/article
    queries_per_day=1_000,      # 1K questions/jour
    avg_query_tokens=50,        # 50 tokens/question
    avg_response_tokens=200     # 200 tokens/r√©ponse
)
# R√©sultat : ~$65/mois
```

---

## Monitoring et m√©triques

### Tracking performance RAG

```python
class RAGMetrics:
    """Monitoring syst√®me RAG"""
    
    def __init__(self):
        self.conn = mysql.connector.connect(**db_config)
        self._init_metrics_table()
    
    def _init_metrics_table(self):
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rag_metrics (
                metric_id BIGINT PRIMARY KEY AUTO_INCREMENT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                query_text TEXT,
                retrieval_time_ms FLOAT,
                generation_time_ms FLOAT,
                total_time_ms FLOAT,
                top_similarity_score FLOAT,
                tokens_used INT,
                cost_usd FLOAT,
                user_feedback ENUM('positive', 'negative', 'neutral')
            )
        """)
        self.conn.commit()
        cursor.close()
    
    def log_query(self, metrics: Dict[str, Any]):
        """Logger m√©triques d'une query"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO rag_metrics 
            (query_text, retrieval_time_ms, generation_time_ms, 
             total_time_ms, top_similarity_score, tokens_used, cost_usd)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """, (
            metrics['query'],
            metrics['retrieval_time'],
            metrics['generation_time'],
            metrics['total_time'],
            metrics['top_similarity'],
            metrics['tokens'],
            metrics['cost']
        ))
        self.conn.commit()
        cursor.close()
    
    def get_daily_stats(self, days: int = 7):
        """Statistiques quotidiennes"""
        cursor = self.conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                DATE(timestamp) AS date,
                COUNT(*) AS queries,
                AVG(total_time_ms) AS avg_latency,
                AVG(top_similarity_score) AS avg_similarity,
                SUM(cost_usd) AS total_cost,
                SUM(CASE WHEN user_feedback = 'positive' THEN 1 ELSE 0 END) / 
                    COUNT(*) AS satisfaction_rate
            FROM rag_metrics
            WHERE timestamp >= DATE_SUB(NOW(), INTERVAL %s DAY)
            GROUP BY DATE(timestamp)
            ORDER BY date DESC
        """, (days,))
        
        return cursor.fetchall()
```

---

## Bonnes pratiques

### ‚úÖ Recommandations

1. **Chunking intelligent**
   ```python
   # Taille optimale : 500-1000 tokens
   # Avec overlap 50-100 tokens
   def smart_chunk(text, chunk_size=500, overlap=50):
       # Utiliser sentence splitter
       sentences = sent_tokenize(text)
       # Regrouper jusqu'√† chunk_size tokens
       # ...
   ```

2. **Cache embeddings**
   ```python
   # √âviter recalculs co√ªteux
   @lru_cache(maxsize=10000)
   def get_embedding(text_hash):
       return embedder.embed(text)
   ```

3. **Batch processing**
   ```python
   # Grouper requ√™tes API
   embeddings = embedder.embed(texts)  # Batch
   # vs
   # for text in texts:  # ‚ùå Lent, co√ªteux
   #     embedder.embed(text)
   ```

4. **Hybrid search**
   ```python
   # Combiner keyword + vector
   # Keyword filter ‚Üí Vector rerank
   ```

5. **Monitoring co√ªts**
   ```python
   # Tracker chaque appel API
   total_cost += tokens * cost_per_token
   ```

6. **Versioning mod√®les**
   ```sql
   ALTER TABLE documents 
   ADD COLUMN embedding_model VARCHAR(50);
   -- Permet migration progressive
   ```

### ‚ö†Ô∏è Pi√®ges √† √©viter

1. **Chunks trop grands** ‚Üí Context overflow LLM
2. **Pas de cache** ‚Üí Co√ªts explosent
3. **Ignorer similarit√©** ‚Üí Mauvais contexte au LLM
4. **Prompts non optimis√©s** ‚Üí Tokens gaspill√©s
5. **Pas de monitoring** ‚Üí D√©rive qualit√© invisible
6. **Mod√®le unique** ‚Üí Pas de A/B testing

---

## ‚úÖ Points cl√©s √† retenir

- **RAG** = Retrieval (Vector DB) + Augmented (Context) + Generation (LLM)
- **OpenAI** = Solution standard, text-embedding-3-small recommand√©
- **Claude** = Via Voyage AI embeddings, excellent pour reasoning
- **LLaMA/Ollama** = 0 co√ªt API, bon pour prototypage/privacy
- **Co√ªts** : Embedding ~$0.02/1M tokens, LLM ~$0.15-$10/1M tokens
- **Optimisation** : Cache, batch, hybrid search, mod√®les l√©gers
- **Monitoring** : Latence, similarit√©, co√ªts, satisfaction utilisateur
- **Production** : Versioning mod√®les, A/B testing, feedback loop
- MariaDB Vector = **Infrastructure critique** pour RAG scalable
- **Hybrid search** (keyword + vector) = Meilleur rapport performance/co√ªt

---

## üîó Ressources et r√©f√©rences

- [üìñ OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [üìñ Anthropic Claude API](https://docs.anthropic.com/claude/reference)
- [üìñ Voyage AI Documentation](https://docs.voyageai.com/)
- [ü§ñ LangChain Framework](https://python.langchain.com/)
- [ü§ñ LlamaIndex](https://docs.llamaindex.ai/)
- [üöÄ Ollama](https://ollama.com/)
- [üìö RAG Best Practices](https://www.anthropic.com/research/retrieval-augmented-generation)

---

**üéì F√©licitations !** Vous ma√Ætrisez maintenant l'int√©gration compl√®te MariaDB Vector avec les LLMs pour construire des applications d'IA augment√©e production-ready ! üöÄ

‚è≠Ô∏è [Online Schema Change (ALTER TABLE non-bloquant)](/18-fonctionnalites-avancees/11-online-schema-change.md)
