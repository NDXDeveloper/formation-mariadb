ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 18.10.2 Index HNSW (Hierarchical Navigable Small Worlds)

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : 
> - Section 18.10.1 - Type de donnÃ©es VECTOR
> - ComprÃ©hension des algorithmes de graphes
> - Notions de complexitÃ© algorithmique (Big O)
> - Chapitre 5 - Index et Performance
> - Concepts de recherche approximative (ANN)

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- Comprendre le principe des index HNSW pour la recherche vectorielle
- CrÃ©er et configurer des index HNSW optimisÃ©s
- Tuner les paramÃ¨tres M, ef_construction et ef_search
- Mesurer et optimiser la prÃ©cision vs performance
- ImplÃ©menter la recherche de similaritÃ© Ã  grande Ã©chelle
- Choisir entre HNSW et recherche brute force
- Diagnostiquer et rÃ©soudre les problÃ¨mes de performance
- Dimensionner l'infrastructure pour des millions de vecteurs

---

## Introduction

L'index **HNSW** (Hierarchical Navigable Small Worlds) est un algorithme de recherche approximative des plus proches voisins (Approximate Nearest Neighbor - ANN) introduit dans **MariaDB 11.8 LTS** ğŸ†•. Il permet de rechercher efficacement des vecteurs similaires parmi des millions d'embeddings.

### ProblÃ¨me : Recherche de similaritÃ© Ã  grande Ã©chelle

Sans index, la recherche du plus proche voisin nÃ©cessite de comparer le vecteur requÃªte avec **tous** les vecteurs de la base :

```
Recherche brute force (scan complet) :
Query vector : [0.1, 0.2, 0.3, ..., 0.1536]
                    â†“ Calculer distance avec TOUS les vecteurs
Database : 10,000,000 vecteurs
           â†“
Temps : ~15-30 secondes (inacceptable pour production)

ComplexitÃ© : O(n Ã— d)
- n = nombre de vecteurs
- d = dimensions
```

Avec index HNSW :

```
Recherche avec HNSW :
Query vector : [0.1, 0.2, 0.3, ..., 0.1536]
                    â†“ Navigation intelligente dans le graphe
HNSW Index : 10,000,000 vecteurs
           â†“
Temps : ~5-20 millisecondes (acceptable)

ComplexitÃ© : O(log n Ã— d)
- n = nombre de vecteurs
- d = dimensions
```

ğŸ’¡ **Gain** : **HNSW est 1000-5000Ã— plus rapide** que le scan complet sur gros volumes.

---

## Principe de fonctionnement

### Architecture en graphe hiÃ©rarchique

HNSW construit un **graphe multi-couches** oÃ¹ chaque vecteur est un nÅ“ud connectÃ© Ã  ses voisins :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Couche 2 (niveau supÃ©rieur - sparse)               â”‚
â”‚  â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹                       â”‚
â”‚  â”‚            â”‚             â”‚                       â”‚
â”‚  â”‚            â”‚             â”‚                       â”‚
â””â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚            â”‚             â”‚
â”Œâ”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Couche 1 (niveau intermÃ©diaire)                    â”‚
â”‚  â—‹â”€â”€â—‹â”€â”€â”€â”€â—‹â”€â”€â”€â”€â—‹â”€â”€â”€â—‹â”€â”€â”€â”€â—‹â”€â”€â”€â”€â—‹â”€â”€â—‹                    â”‚
â”‚  â”‚  â”‚    â”‚    â”‚   â”‚    â”‚    â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚    â”‚    â”‚   â”‚    â”‚    â”‚  â”‚                    â”‚
â””â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚  â”‚    â”‚    â”‚   â”‚    â”‚    â”‚  â”‚
â”Œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Couche 0 (niveau de base - dense)                  â”‚
â”‚  â—‹â”€â—‹â—‹â—‹â”€â—‹â—‹â—‹â”€â—‹â—‹â—‹â”€â—‹â—‹â—‹â”€â—‹â—‹â—‹â”€â—‹â—‹â—‹â”€â—‹â—‹â—‹                      â”‚
â”‚  Tous les vecteurs sont prÃ©sents                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Processus de recherche :
1. Entrer par la couche supÃ©rieure (sparse)
2. Naviguer vers la zone approximative
3. Descendre de couche en couche
4. Affiner dans la couche 0 (dense)
5. Retourner les K plus proches voisins
```

### Algorithme de recherche

```
Fonction recherche_hnsw(query_vector, k, index):
    current = point_entree_index
    
    // Descendre les couches
    Pour chaque couche de top Ã  1:
        current = chercher_plus_proche(query, current, couche)
    
    // Recherche finale couche 0
    candidats = chercher_k_plus_proches(query, current, couche_0, k)
    
    Retourner top_k(candidats)

ComplexitÃ© : O(log n) sauts Ã— O(d) calcul distance
```

---

## CrÃ©ation d'index HNSW

### Syntaxe de base

```sql
CREATE INDEX index_name 
ON table_name (vector_column)
USING HNSW;
```

### Exemple simple

```sql
-- Table avec embeddings
CREATE TABLE documents (
    doc_id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(200),
    content TEXT,
    embedding VECTOR(1536)
) ENGINE=InnoDB;

-- CrÃ©er index HNSW
CREATE INDEX idx_embedding_hnsw 
ON documents(embedding) 
USING HNSW;

-- VÃ©rification
SHOW INDEX FROM documents WHERE Key_name = 'idx_embedding_hnsw';
```

### Avec paramÃ¨tres personnalisÃ©s

```sql
-- Index HNSW avec tuning avancÃ©
CREATE INDEX idx_embedding_optimized 
ON documents(embedding) 
USING HNSW
WITH (
    M = 16,                    -- Nombre de connexions par nÅ“ud
    ef_construction = 200,     -- Taille liste candidats construction
    distance_metric = 'cosine' -- MÃ©trique de distance
);
```

---

## ParamÃ¨tres de configuration

### M (nombre de connexions)

**M** dÃ©termine le nombre de connexions bidirectionnelles par nÅ“ud dans le graphe.

| M | PrÃ©cision | Vitesse recherche | Taille index | Temps construction |
|---|-----------|-------------------|--------------|-------------------|
| 8 | Faible | TrÃ¨s rapide | Petit | Rapide |
| **16** | **Bonne** (dÃ©faut) | **Rapide** | **Moyen** | **Moyen** |
| 32 | TrÃ¨s bonne | Moyen | Grand | Lent |
| 64 | Excellente | Lent | TrÃ¨s grand | TrÃ¨s lent |

```sql
-- M faible : recherche ultra-rapide, prÃ©cision rÃ©duite
CREATE INDEX idx_fast ON docs(embedding) 
USING HNSW WITH (M = 8);

-- M Ã©levÃ© : prÃ©cision maximale, plus lent
CREATE INDEX idx_accurate ON docs(embedding) 
USING HNSW WITH (M = 48);
```

**RÃ¨gles empiriques** :
- **M = 16** : Bon compromis pour la plupart des cas
- **M = 8-12** : Si vitesse critique (latence < 10ms)
- **M = 32-64** : Si prÃ©cision critique (recall > 99%)

ğŸ’¡ **Taille index** : Augmenter M de 16 â†’ 32 double la taille de l'index.

### ef_construction (construction)

**ef_construction** contrÃ´le la taille de la liste de candidats pendant la construction de l'index.

| ef_construction | QualitÃ© index | Temps construction |
|-----------------|---------------|-------------------|
| 100 | Faible | Rapide |
| **200** | **Bonne** (dÃ©faut) | **Moyen** |
| 400 | TrÃ¨s bonne | Lent |
| 800 | Excellente | TrÃ¨s lent |

```sql
-- Construction rapide (dev/test)
CREATE INDEX idx_dev ON docs(embedding) 
USING HNSW WITH (ef_construction = 100);

-- Construction qualitÃ© production
CREATE INDEX idx_prod ON docs(embedding) 
USING HNSW WITH (ef_construction = 400);
```

**Impact** :
- Plus Ã©levÃ© â†’ Index de meilleure qualitÃ© â†’ Recherches plus prÃ©cises
- Plus Ã©levÃ© â†’ Temps de construction augmente linÃ©airement
- **Ne peut pas Ãªtre modifiÃ©** aprÃ¨s crÃ©ation (rebuild nÃ©cessaire)

### ef_search (recherche runtime)

**ef_search** contrÃ´le la taille de la liste de candidats **pendant les recherches**.

```sql
-- ParamÃ¨tre dynamique (modifiable sans rebuild)
SET SESSION hnsw_ef_search = 100;  -- Rapide, moins prÃ©cis
SET SESSION hnsw_ef_search = 200;  -- DÃ©faut
SET SESSION hnsw_ef_search = 500;  -- Lent, trÃ¨s prÃ©cis
```

| ef_search | PrÃ©cision | Latence |
|-----------|-----------|---------|
| 50 | ~85% recall | ~5ms |
| 100 | ~92% recall | ~10ms |
| **200** | **~96% recall** (dÃ©faut) | **~20ms** |
| 500 | ~99% recall | ~50ms |
| 1000 | ~99.5% recall | ~100ms |

**Avantage clÃ©** : Ajustable **par requÃªte** sans rebuild :

```sql
-- Recherche rapide
SET hnsw_ef_search = 100;
SELECT ... ORDER BY VEC_DISTANCE_COSINE(...) LIMIT 10;

-- Recherche prÃ©cise
SET hnsw_ef_search = 500;
SELECT ... ORDER BY VEC_DISTANCE_COSINE(...) LIMIT 10;
```

---

## MÃ©triques de distance

### Distance euclidienne (L2)

**Formule** : âˆš(Î£(a[i] - b[i])Â²)

```sql
CREATE INDEX idx_euclidean ON docs(embedding) 
USING HNSW WITH (distance_metric = 'euclidean');

-- Recherche
SELECT 
    doc_id, 
    title,
    VEC_DISTANCE_EUCLIDEAN(embedding, VEC_FromText('[0.1, 0.2, ...]')) AS distance
FROM documents
ORDER BY distance
LIMIT 10;
```

**Cas d'usage** :
- Images (CLIP embeddings)
- CoordonnÃ©es spatiales
- DonnÃ©es scientifiques

### SimilaritÃ© cosinus

**Formule** : 1 - (AÂ·B) / (||A|| Ã— ||B||)

```sql
CREATE INDEX idx_cosine ON docs(embedding) 
USING HNSW WITH (distance_metric = 'cosine');

-- Recherche
SELECT 
    doc_id, 
    title,
    VEC_DISTANCE_COSINE(embedding, VEC_FromText('[0.1, 0.2, ...]')) AS distance,
    1 - VEC_DISTANCE_COSINE(embedding, VEC_FromText('[0.1, 0.2, ...]')) AS similarity
FROM documents
ORDER BY distance
LIMIT 10;
```

**Cas d'usage** (le plus courant) :
- Texte (OpenAI, Sentence-BERT)
- Recherche sÃ©mantique
- Recommandations

ğŸ’¡ **Attention** : Pour distance cosinus, les vecteurs doivent Ãªtre **normalisÃ©s** (norme = 1).

### Distance Manhattan (L1)

**Formule** : Î£|a[i] - b[i]|

```sql
CREATE INDEX idx_manhattan ON docs(embedding) 
USING HNSW WITH (distance_metric = 'manhattan');
```

**Cas d'usage** :
- Optimisation gÃ©omÃ©trique
- Moins sensible aux outliers

### Produit scalaire (Inner Product)

**Formule** : -AÂ·B (nÃ©gation pour ordre croissant)

```sql
CREATE INDEX idx_inner ON docs(embedding) 
USING HNSW WITH (distance_metric = 'inner_product');
```

**Cas d'usage** :
- Vecteurs non normalisÃ©s
- Certains modÃ¨les ML spÃ©cifiques

---

## Recherche de similaritÃ©

### Recherche K plus proches voisins (KNN)

```sql
-- Trouver les 10 documents les plus similaires
SELECT 
    doc_id,
    title,
    VEC_DISTANCE_COSINE(
        embedding, 
        VEC_FromText('[0.023, -0.145, 0.089, ...]')
    ) AS distance
FROM documents
ORDER BY distance ASC
LIMIT 10;

-- RÃ©sultat :
-- doc_id | title                        | distance
-- -------|------------------------------|----------
-- 4523   | Introduction to Vector DB    | 0.0234
-- 8821   | MariaDB 11.8 Features        | 0.0456
-- 1092   | Semantic Search Guide        | 0.0521
-- ...
```

### Recherche avec seuil de distance

```sql
-- Documents avec similaritÃ© > 0.8 (distance < 0.2)
SELECT 
    doc_id,
    title,
    1 - VEC_DISTANCE_COSINE(embedding, @query_vector) AS similarity
FROM documents
WHERE VEC_DISTANCE_COSINE(embedding, @query_vector) < 0.2
ORDER BY similarity DESC
LIMIT 50;
```

### Recherche hybride (vecteur + filtres)

```sql
-- Recherche sÃ©mantique + filtres mÃ©tadonnÃ©es
SELECT 
    doc_id,
    title,
    category,
    published_date,
    VEC_DISTANCE_COSINE(embedding, @query_vector) AS distance
FROM documents
WHERE category = 'Technology'
  AND published_date >= '2024-01-01'
  AND VEC_DISTANCE_COSINE(embedding, @query_vector) < 0.3
ORDER BY distance ASC
LIMIT 20;

-- âš ï¸ Important : Filtrer AVANT la recherche vectorielle si possible
```

### Recherche avec agrÃ©gation

```sql
-- Top produits par catÃ©gorie basÃ© sur similaritÃ©
SELECT 
    category,
    product_id,
    product_name,
    VEC_DISTANCE_COSINE(description_embedding, @query) AS distance,
    ROW_NUMBER() OVER (PARTITION BY category ORDER BY distance) AS rank_in_category
FROM products
WHERE VEC_DISTANCE_COSINE(description_embedding, @query) < 0.5
HAVING rank_in_category <= 3
ORDER BY category, distance;
```

---

## Performance et benchmarks

### Temps de construction

```sql
-- Benchmark : CrÃ©er index HNSW sur 1M vecteurs (1536 dims)
-- MatÃ©riel : 32GB RAM, 16 cores

-- M=16, ef_construction=200 (dÃ©faut)
CREATE INDEX idx_default ON embeddings(vec) USING HNSW;
-- Temps : ~45 minutes

-- M=32, ef_construction=400 (haute qualitÃ©)
CREATE INDEX idx_quality ON embeddings(vec) 
USING HNSW WITH (M=32, ef_construction=400);
-- Temps : ~2.5 heures

-- M=8, ef_construction=100 (rapide)
CREATE INDEX idx_fast ON embeddings(vec) 
USING HNSW WITH (M=8, ef_construction=100);
-- Temps : ~20 minutes
```

### Latence de recherche

```sql
-- Benchmark : Recherche top-10 sur 1M vecteurs

-- Brute force (pas d'index)
SELECT ... ORDER BY VEC_DISTANCE_COSINE(...) LIMIT 10;
-- Latence : ~3500ms (3.5 secondes)

-- HNSW avec ef_search=100
SET hnsw_ef_search = 100;
SELECT ... ORDER BY VEC_DISTANCE_COSINE(...) LIMIT 10;
-- Latence : ~8ms
-- Recall : ~92%

-- HNSW avec ef_search=200 (dÃ©faut)
SET hnsw_ef_search = 200;
SELECT ... ORDER BY VEC_DISTANCE_COSINE(...) LIMIT 10;
-- Latence : ~15ms
-- Recall : ~96%

-- HNSW avec ef_search=500
SET hnsw_ef_search = 500;
SELECT ... ORDER BY VEC_DISTANCE_COSINE(...) LIMIT 10;
-- Latence : ~35ms
-- Recall : ~99%

-- â†’ HNSW est 230Ã— plus rapide (ef=200) avec 96% prÃ©cision
```

### Taille index vs nombre de vecteurs

```sql
-- Estimation taille index HNSW
SELECT 
    @num_vectors AS vectors,
    @dimensions AS dims,
    @M AS connections,
    -- Formule approximative
    ROUND(
        @num_vectors * @dimensions * 4 + -- Stockage vecteurs
        @num_vectors * @M * 8             -- Stockage graphe
    / 1024 / 1024 / 1024, 2) AS index_size_gb
FROM (SELECT 
    1000000 AS @num_vectors,
    1536 AS @dimensions,
    16 AS @M
) params;

-- RÃ©sultat :
-- vectors  | dims | connections | index_size_gb
-- 1000000  | 1536 | 16          | 5.95
```

| Vecteurs | Dimensions | M | Taille index |
|----------|------------|---|--------------|
| 100K | 384 | 16 | ~180 MB |
| 100K | 1536 | 16 | ~650 MB |
| 1M | 384 | 16 | ~1.8 GB |
| 1M | 1536 | 16 | ~6.5 GB |
| 10M | 1536 | 16 | ~65 GB |
| 10M | 1536 | 32 | ~120 GB |

---

## Cas d'usage avancÃ©s

### 1. Recherche sÃ©mantique RAG

```sql
-- Base de connaissances pour RAG (Retrieval-Augmented Generation)
CREATE TABLE knowledge_chunks (
    chunk_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    document_source VARCHAR(200),
    chunk_text TEXT,
    chunk_index INT,
    token_count INT,
    embedding VECTOR(1536),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_source (document_source),
    INDEX idx_embedding_hnsw (embedding) USING HNSW 
        WITH (M = 16, ef_construction = 200)
) ENGINE=InnoDB;

-- ProcÃ©dure recherche pour RAG
DELIMITER $$
CREATE PROCEDURE rag_search(
    IN query_embedding TEXT,
    IN max_results INT,
    IN min_similarity FLOAT
)
BEGIN
    SET @query_vec = VEC_FromText(query_embedding);
    
    SELECT 
        chunk_id,
        document_source,
        chunk_text,
        1 - VEC_DISTANCE_COSINE(embedding, @query_vec) AS similarity,
        token_count
    FROM knowledge_chunks
    WHERE VEC_DISTANCE_COSINE(embedding, @query_vec) < (1 - min_similarity)
    ORDER BY similarity DESC
    LIMIT max_results;
END$$
DELIMITER ;

-- Utilisation
CALL rag_search(
    '[0.023, -0.145, ...]',  -- Query embedding
    5,                        -- Top 5 chunks
    0.7                       -- SimilaritÃ© minimale 70%
);
```

### 2. DÃ©duplication sÃ©mantique

```sql
-- DÃ©tection de contenu dupliquÃ© ou trÃ¨s similaire
CREATE TABLE articles (
    article_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(300),
    content TEXT,
    author VARCHAR(100),
    published_date DATE,
    content_embedding VECTOR(1536),
    
    INDEX idx_embedding (content_embedding) USING HNSW
) ENGINE=InnoDB;

-- Trouver doublons potentiels
SELECT 
    a1.article_id AS original_id,
    a1.title AS original_title,
    a2.article_id AS duplicate_id,
    a2.title AS duplicate_title,
    1 - VEC_DISTANCE_COSINE(a1.content_embedding, a2.content_embedding) AS similarity
FROM articles a1
JOIN articles a2 
    ON a1.article_id < a2.article_id
WHERE VEC_DISTANCE_COSINE(a1.content_embedding, a2.content_embedding) < 0.05  -- 95%+ similaritÃ©
ORDER BY similarity DESC
LIMIT 100;
```

### 3. Recommandation multi-facteurs

```sql
-- SystÃ¨me de recommandation hybride
CREATE TABLE user_profiles (
    user_id INT PRIMARY KEY,
    preference_embedding VECTOR(768),  -- Profil agrÃ©gÃ©
    last_updated TIMESTAMP
) ENGINE=InnoDB;

CREATE TABLE content_items (
    item_id INT PRIMARY KEY,
    title VARCHAR(200),
    category VARCHAR(50),
    popularity_score FLOAT,
    content_embedding VECTOR(768),
    
    INDEX idx_embedding (content_embedding) USING HNSW,
    INDEX idx_category (category),
    INDEX idx_popularity (popularity_score)
) ENGINE=InnoDB;

-- Recommandations personnalisÃ©es avec boost popularitÃ©
SELECT 
    ci.item_id,
    ci.title,
    ci.category,
    ci.popularity_score,
    (
        (1 - VEC_DISTANCE_COSINE(ci.content_embedding, up.preference_embedding)) * 0.7 +
        (ci.popularity_score / 100) * 0.3
    ) AS combined_score
FROM content_items ci
CROSS JOIN user_profiles up
WHERE up.user_id = 12345
  AND VEC_DISTANCE_COSINE(ci.content_embedding, up.preference_embedding) < 0.4
ORDER BY combined_score DESC
LIMIT 20;
```

### 4. Recherche multi-modale (texte + image)

```sql
-- Table avec embeddings texte ET image
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    name VARCHAR(200),
    description TEXT,
    image_url VARCHAR(500),
    
    -- Embedding texte (description)
    text_embedding VECTOR(1536),
    
    -- Embedding image (CLIP)
    image_embedding VECTOR(512),
    
    INDEX idx_text_emb (text_embedding) USING HNSW,
    INDEX idx_image_emb (image_embedding) USING HNSW
) ENGINE=InnoDB;

-- Recherche par texte
SELECT product_id, name
FROM products
ORDER BY VEC_DISTANCE_COSINE(text_embedding, @query_text_embedding)
LIMIT 10;

-- Recherche par image similaire
SELECT product_id, name
FROM products
ORDER BY VEC_DISTANCE_COSINE(image_embedding, @query_image_embedding)
LIMIT 10;

-- Recherche hybride texte + image
SELECT 
    product_id,
    name,
    (
        VEC_DISTANCE_COSINE(text_embedding, @query_text_embedding) * 0.6 +
        VEC_DISTANCE_COSINE(image_embedding, @query_image_embedding) * 0.4
    ) AS combined_distance
FROM products
ORDER BY combined_distance
LIMIT 10;
```

### 5. Clustering et segmentation

```sql
-- Identifier clusters de documents similaires
WITH vector_distances AS (
    SELECT 
        d1.doc_id AS doc1,
        d2.doc_id AS doc2,
        VEC_DISTANCE_COSINE(d1.embedding, d2.embedding) AS distance
    FROM documents d1
    JOIN documents d2 ON d1.doc_id < d2.doc_id
    WHERE VEC_DISTANCE_COSINE(d1.embedding, d2.embedding) < 0.3
)
SELECT 
    doc1,
    GROUP_CONCAT(doc2) AS similar_docs,
    COUNT(*) AS cluster_size
FROM vector_distances
GROUP BY doc1
HAVING cluster_size >= 3
ORDER BY cluster_size DESC;
```

---

## Tuning et optimisation

### StratÃ©gie de tuning

```sql
-- 1. Commencer avec paramÃ¨tres par dÃ©faut
CREATE INDEX idx_baseline ON docs(embedding) USING HNSW;
-- M=16, ef_construction=200

-- 2. Tester diffÃ©rents ef_search
SET hnsw_ef_search = 50;
-- Mesurer latence et recall

SET hnsw_ef_search = 100;
-- Mesurer latence et recall

SET hnsw_ef_search = 200;
-- Mesurer latence et recall

-- 3. Si recall < 95%, augmenter M et rebuilder
DROP INDEX idx_baseline ON docs;
CREATE INDEX idx_improved ON docs(embedding) 
USING HNSW WITH (M = 24, ef_construction = 300);

-- 4. Affiner ef_search pour trouver sweet spot
SET hnsw_ef_search = 150;
```

### MÃ©triques de qualitÃ©

```sql
-- Vue pour monitoring qualitÃ© index
CREATE VIEW hnsw_quality_metrics AS
SELECT 
    table_name,
    index_name,
    stat_value AS index_size_pages,
    ROUND(stat_value * @@innodb_page_size / 1024 / 1024, 2) AS index_size_mb
FROM mysql.innodb_index_stats
WHERE index_name LIKE '%hnsw%'
AND stat_name = 'size'
ORDER BY stat_value DESC;

SELECT * FROM hnsw_quality_metrics;
```

### Recall testing

```python
# Script Python pour mesurer recall
import mysql.connector
import numpy as np

def calculate_recall(db_config, test_vectors, k=10):
    """
    Compare HNSW results vs brute force (ground truth)
    """
    conn = mysql.connector.connect(**db_config)
    cursor = conn.cursor()
    
    total_recall = 0
    
    for query_vec in test_vectors:
        vec_str = '[' + ','.join(map(str, query_vec)) + ']'
        
        # Ground truth (brute force)
        cursor.execute(f"""
            SELECT doc_id
            FROM documents
            ORDER BY VEC_DISTANCE_COSINE(embedding, VEC_FromText('{vec_str}'))
            LIMIT {k}
        """)
        ground_truth = set(row[0] for row in cursor.fetchall())
        
        # HNSW results
        cursor.execute(f"""
            SELECT doc_id
            FROM documents USE INDEX (idx_embedding_hnsw)
            ORDER BY VEC_DISTANCE_COSINE(embedding, VEC_FromText('{vec_str}'))
            LIMIT {k}
        """)
        hnsw_results = set(row[0] for row in cursor.fetchall())
        
        # Recall = intersection / k
        recall = len(ground_truth & hnsw_results) / k
        total_recall += recall
    
    avg_recall = total_recall / len(test_vectors)
    print(f"Average Recall@{k}: {avg_recall:.4f}")
    
    cursor.close()
    conn.close()
    return avg_recall

# Utilisation
test_vecs = [np.random.rand(1536) for _ in range(100)]
recall = calculate_recall(db_config, test_vecs, k=10)
# Average Recall@10: 0.9623 (96.23%)
```

---

## Monitoring et diagnostics

### Ã‰tat de l'index

```sql
-- VÃ©rifier indices HNSW
SELECT 
    TABLE_NAME,
    INDEX_NAME,
    INDEX_TYPE,
    COMMENT
FROM information_schema.STATISTICS
WHERE INDEX_TYPE = 'HNSW'
AND TABLE_SCHEMA = DATABASE()
ORDER BY TABLE_NAME, INDEX_NAME;
```

### Performance queries

```sql
-- Analyser utilisation index
EXPLAIN SELECT doc_id, title
FROM documents
ORDER BY VEC_DISTANCE_COSINE(embedding, VEC_FromText('[0.1, 0.2, ...]'))
LIMIT 10;

-- RÃ©sultat attendu :
-- type: index
-- key: idx_embedding_hnsw
-- Extra: Using index; Using filesort
```

### Statistiques index

```sql
-- Taille et fragmentation
SELECT 
    index_name,
    ROUND(stat_value * @@innodb_page_size / 1024 / 1024, 2) AS size_mb,
    stat_description
FROM mysql.innodb_index_stats
WHERE table_name = 'documents'
AND index_name LIKE '%hnsw%'
AND stat_name IN ('size', 'n_leaf_pages', 'n_diff_pfx01')
ORDER BY index_name, stat_name;
```

---

## Limitations et contraintes

### Limites techniques

| Limite | Valeur | Impact |
|--------|--------|--------|
| **Max vecteurs** | Plusieurs milliards | LimitÃ© par RAM/disque |
| **Max dimensions** | 65535 | Pratique : 384-3072 |
| **M max** | 256 | Au-delÃ , rendements dÃ©croissants |
| **ef_construction max** | 65535 | Temps construction prohibitif |
| **Concurrent inserts** | Support | Lock granulaire |

### Contraintes d'utilisation

```sql
-- âŒ ERREUR : Index HNSW sur colonne non-VECTOR
CREATE INDEX idx_fail ON docs(text_column) USING HNSW;
-- ERROR: HNSW index requires VECTOR column

-- âŒ ERREUR : Index composite HNSW
CREATE INDEX idx_fail ON docs(embedding, category) USING HNSW;
-- ERROR: HNSW index supports single column only

-- âœ… OK : Index HNSW + B-Tree sÃ©parÃ©s
CREATE INDEX idx_embedding ON docs(embedding) USING HNSW;
CREATE INDEX idx_category ON docs(category);
```

### OpÃ©rations non supportÃ©es

- âŒ Index HNSW sur colonnes VIRTUAL
- âŒ Index composite (HNSW + autre colonne)
- âŒ Modification paramÃ¨tres M/ef_construction sans rebuild
- âŒ Recherche exacte (toujours approximative)

---

## Bonnes pratiques

### âœ… Recommandations

1. **DÃ©marrer avec dÃ©fauts, tuner ensuite**
   ```sql
   CREATE INDEX idx ON docs(embedding) USING HNSW;
   -- M=16, ef_construction=200, ef_search=200
   ```

2. **Mesurer recall avant optimisation**
   ```python
   # Ã‰tablir baseline : recall cible > 95%
   ```

3. **Ajuster ef_search selon latence cible**
   ```sql
   -- Latence < 20ms : ef_search = 100-150
   -- Latence < 50ms : ef_search = 200-300
   -- PrÃ©cision max : ef_search = 500+
   ```

4. **Construire index hors heures de pointe**
   ```bash
   # Planifier construction index HNSW la nuit
   mysql -e "CREATE INDEX idx_hnsw ..." &
   ```

5. **Monitorer taille index vs RAM**
   ```sql
   -- Index doit tenir en RAM pour performances optimales
   SELECT @@innodb_buffer_pool_size / 1024 / 1024 / 1024 AS buffer_pool_gb;
   ```

6. **Utiliser compression si espace limitÃ©**
   ```sql
   ALTER TABLE docs ROW_FORMAT=COMPRESSED;
   ```

7. **Partitionner trÃ¨s grosses tables**
   ```sql
   -- SÃ©parer hot data (rÃ©cent) et cold data (ancien)
   PARTITION BY RANGE (YEAR(created_date)) (...)
   ```

### âš ï¸ PiÃ¨ges Ã  Ã©viter

1. **M trop Ã©levÃ©** â†’ Index Ã©norme, construction lente, gain minimal
2. **ef_construction trop faible** â†’ Index de mauvaise qualitÃ©, recall < 90%
3. **Oublier de tester recall** â†’ Faux sentiment de performance
4. **Index HNSW sur donnÃ©es volatiles** â†’ Rebuild frÃ©quent coÃ»teux
5. **NÃ©gliger RAM** â†’ Index sur disque = performances dÃ©gradÃ©es
6. **Pas de filtrage prÃ©-recherche** â†’ Scan inutile de vecteurs hors scope
7. **Vecteurs non normalisÃ©s** â†’ Distance cosinus incorrecte

---

## Comparaison avec alternatives

### HNSW vs Brute Force

| CritÃ¨re | Brute Force | HNSW |
|---------|-------------|------|
| **PrÃ©cision** | 100% (exacte) | 95-99% (approximative) |
| **Latence (100K vec)** | ~500ms | ~10ms |
| **Latence (10M vec)** | ~50s | ~20ms |
| **Taille index** | 0 (aucun) | ~3Ã— taille donnÃ©es |
| **ScalabilitÃ©** | Mauvaise | Excellente |
| **Cas d'usage** | < 10K vecteurs | > 100K vecteurs |

### HNSW vs IVF (Inverted File)

| CritÃ¨re | HNSW | IVF |
|---------|------|-----|
| **PrÃ©cision** | 95-99% | 90-95% |
| **Latence** | TrÃ¨s faible | Faible |
| **Construction** | Lente | Rapide |
| **RequÃªtes/sec** | TrÃ¨s Ã©levÃ© | Ã‰levÃ© |
| **FlexibilitÃ©** | Excellente | Bonne |
| **ImplÃ©mentation** | MariaDB 11.8+ | Faiss, Qdrant |

ğŸ’¡ **Recommandation** : HNSW pour 99% des cas d'usage production MariaDB.

---

## âœ… Points clÃ©s Ã  retenir

- **HNSW** = Hierarchical Navigable Small Worlds, algorithme ANN de rÃ©fÃ©rence
- NouveautÃ© **MariaDB 11.8 LTS** ğŸ†• pour recherche vectorielle haute performance
- **1000-5000Ã— plus rapide** que brute force sur gros volumes
- ParamÃ¨tres clÃ©s : **M** (connexions), **ef_construction** (qualitÃ©), **ef_search** (runtime)
- **M=16, ef_construction=200** = bon compromis par dÃ©faut
- **ef_search** ajustable dynamiquement sans rebuild
- MÃ©triques de distance : **cosinus** (texte), **euclidienne** (images), manhattan, inner product
- **Recall 95-99%** typique en production
- Taille index : **~3-5Ã— taille vecteurs** selon M
- Cas d'usage : **RAG, semantic search, recommandations, dÃ©duplication**
- NÃ©cessite vecteurs **normalisÃ©s** pour distance cosinus

---

## ğŸ”— Ressources et rÃ©fÃ©rences

- [ğŸ“– MariaDB - HNSW Index](https://mariadb.com/kb/en/hnsw-index/)
- [ğŸ“– MariaDB Vector - Performance Tuning](https://mariadb.com/kb/en/vector-performance/)
- [ğŸ“š HNSW Original Paper (Malkov & Yashunin)](https://arxiv.org/abs/1603.09320)
- [ğŸ”¬ Benchmark ANN Algorithms](http://ann-benchmarks.com/)
- [ğŸ“Š HNSW Parameter Guide](https://www.pinecone.io/learn/hnsw/)
- [ğŸ¤– Vector Search Best Practices](https://www.pinecone.io/learn/vector-search/)

---

## â¡ï¸ Section suivante

**18.10.3 [Fonctions de distance vectorielle](./10.3-fonctions-distance.md)** : Explorez en dÃ©tail les fonctions de calcul de distance (VEC_DISTANCE_EUCLIDEAN, VEC_DISTANCE_COSINE, etc.), leurs propriÃ©tÃ©s mathÃ©matiques et leurs cas d'usage spÃ©cifiques. ğŸ†•

â­ï¸ [Fonctions de distance (VEC_DISTANCE_EUCLIDEAN, VEC_DISTANCE_COSINE)](/18-fonctionnalites-avancees/10.3-fonctions-distance.md)
