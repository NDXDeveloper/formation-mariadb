ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# Partie 6 : RÃ©plication et Haute DisponibilitÃ© (DBA/DevOps)

> **Niveau** : Expert â€” DBA Senior, DevOps/SRE, Architectes Infrastructure  
> **DurÃ©e estimÃ©e** : 3-4 jours  
> **PrÃ©requis** : MaÃ®trise administration MariaDB, expÃ©rience systÃ¨mes distribuÃ©s, comprÃ©hension rÃ©seaux et tolÃ©rance aux pannes, notions de consensus distribuÃ©

---

## ğŸ¯ L'infrastructure qui ne dort jamais

Dans le monde moderne des services 24/7, **l'indisponibilitÃ© n'est plus acceptable**. Un site e-commerce qui tombe perd $5,600 par minute en moyenne. Une application bancaire inaccessible gÃ©nÃ¨re un prÃ©judice rÃ©putationnel majeur. Un systÃ¨me mÃ©dical hors ligne peut mettre des vies en danger. La haute disponibilitÃ© n'est pas un luxe â€” c'est une **exigence business, contractuelle, et parfois lÃ©gale**.

La diffÃ©rence entre 99.9% et 99.99% de disponibilitÃ© semble minime sur le papier, mais elle reprÃ©sente un Ã©cart de **8h43 contre 52 minutes d'indisponibilitÃ© annuelle**. Pour atteindre 99.999% ("five nines"), vous ne disposez que de **5 minutes 15 secondes par an** pour toutes les pannes combinÃ©es. Cette exigence transforme fondamentalement l'architecture : les composants simples (single point of failure) deviennent inadmissibles, et chaque dÃ©cision technique doit intÃ©grer la rÃ©silience.

La rÃ©plication et la haute disponibilitÃ© dans MariaDB rÃ©pondent Ã  trois objectifs stratÃ©giques :

1. **TolÃ©rance aux pannes** : Continuer Ã  fonctionner malgrÃ© la dÃ©faillance de serveurs, disques, datacenters
2. **ScalabilitÃ© en lecture** : Distribuer la charge sur plusieurs nÅ“uds pour performances accrues
3. **Disaster Recovery** : RÃ©cupÃ©ration rapide aprÃ¨s incident majeur (incendie, catastrophe naturelle, cyberattaque)

Cette partie vous enseigne comment concevoir, implÃ©menter, et opÃ©rer des **architectures MariaDB rÃ©silientes** capables de maintenir le service mÃªme face Ã  des dÃ©faillances multiples. Vous maÃ®triserez la rÃ©plication asynchrone et semi-synchrone pour scaling et redondance, Galera Cluster pour synchronisation multi-master, MaxScale pour load balancing et failover automatique, et les stratÃ©gies de rÃ©cupÃ©ration aprÃ¨s incident.

Ces compÃ©tences sont **essentielles pour tout DBA ou DevOps responsable d'infrastructures critiques**. Vous apprendrez non seulement les technologies, mais surtout comment concevoir des systÃ¨mes qui continuent de fonctionner quand (et non "si") les composants tombent.

---

## ğŸ“š Les deux modules de cette partie

### Module 13 : RÃ©plication
**10 sections | DurÃ©e : ~2 jours**

Ce module couvre l'ensemble du spectre de la rÃ©plication MariaDB, des fondamentaux Ã  l'exploitation en production :

#### ğŸ”„ Fondamentaux de la rÃ©plication
- **Concepts** : Asynchrone vs semi-synchrone vs synchrone
- **Topologies** : Source-Replica, multi-tier, hub-and-spoke
- **Use cases** : Scaling en lecture, backup, disaster recovery, analytics

#### ğŸ—ï¸ RÃ©plication Master-Slave (Source-Replica)
- **Configuration Primary** : Binary logs, server-id, binlog_format
- **Configuration Replica** : Connexion au primary, threads SQL/IO
- **CHANGE MASTER TO** : Configuration de la rÃ©plication
- **Monitoring** : `SHOW SLAVE STATUS`, lag, threads health

#### ğŸ“ MÃ©thodes de rÃ©plication
- **Binlog coordinates** : RÃ©plication basÃ©e sur positions (fichier + position)
- **GTID (Global Transaction Identifier)** : RÃ©plication sans positions ğŸ”„
  - Avantages : Failover simplifiÃ©, topologies flexibles
  - Configuration et migration vers GTID
  - Transactions sur plusieurs serveurs

#### ğŸŒ³ Topologies avancÃ©es
- **Multi-source replication** : Un replica reÃ§oit de plusieurs primaries
  - Use case : Consolidation de donnÃ©es, fÃ©dÃ©ration
  - Configuration et gestion des canaux
- **RÃ©plication en cascade** : Primary â†’ Intermediate â†’ Replicas
  - RÃ©duction de charge sur primary
  - Distribution gÃ©ographique

#### ğŸ” Monitoring et troubleshooting
- **MÃ©triques critiques** : `Seconds_Behind_Master`, lag accumulation, thread status
- **Erreurs courantes** : Duplicate key, network issues, disk full
- **Outils de diagnostic** : `SHOW PROCESSLIST`, error logs, binary log inspection
- **Performance tuning** : Parallel replication, compression

#### ğŸš¨ Failover et switchover
- **Switchover planifiÃ©** : Migration contrÃ´lÃ©e vers nouveau primary
- **Failover d'urgence** : Promotion automatique en cas de panne
- **RÃ©conciliation** : Resynchronisation aprÃ¨s failover
- **Tools** : MaxScale, Orchestrator, MHA

#### âš¡ RÃ©plication semi-synchrone
- **Garanties de durabilitÃ©** : Confirmation avant commit
- **Trade-offs** : Latence vs consistance
- **Configuration** : Plugins et timeouts
- **Use cases** : SystÃ¨mes financiers, donnÃ©es critiques

#### ğŸ†• Optimistic ALTER TABLE (11.8)
- **ProblÃ¨me** : `ALTER TABLE` bloque la rÃ©plication (lag majeur)
- **Solution** : RÃ©plication continue pendant DDL
- **Impact** : RÃ©duction lag de 90%+ sur grandes tables
- **Configuration** : Activation et prÃ©cautions

ğŸ’¡ **Impact opÃ©rationnel** : La rÃ©plication correctement configurÃ©e permet de maintenir le service pendant maintenance, pannes matÃ©rielles, et croissance de charge. Elle est la base de toute stratÃ©gie HA.

---

### Module 14 : Haute DisponibilitÃ©
**10 sections | DurÃ©e : ~2 jours**

Ce module vous enseigne les architectures et outils pour Ã©liminer les single points of failure :

#### ğŸ›ï¸ Architectures HA : Fondamentaux
- **Principes** : Redondance, dÃ©tection rapide, failover automatique
- **Topologies** : Active-Passive, Active-Active, N+1, N+M
- **Trade-offs** : CoÃ»t vs disponibilitÃ©, simplicitÃ© vs rÃ©silience
- **SLA calculations** : De l'infrastructure au service

#### ğŸ”— MariaDB Galera Cluster
- **Architecture synchrone multi-master** : Tous les nÅ“uds acceptent Ã©critures
- **Certification-based replication** : CohÃ©rence garantie
- **Configuration et dÃ©ploiement** : Bootstrap, joindre un cluster
- **State transfers** : SST (State Snapshot Transfer) vs IST (Incremental)
- **Performance** : Write scaling limitations, certification overhead

#### âš–ï¸ Split-brain et quorum
- **Le problÃ¨me du split-brain** : Partitions rÃ©seau crÃ©ent clusters indÃ©pendants
- **MÃ©canismes de quorum** : MajoritÃ© simple, weighted voting
- **Prevention** : Garbd (Galera Arbitrator), odd number of nodes
- **Recovery** : RÃ©unification aprÃ¨s partition

#### ğŸŒ MaxScale : Le couteau suisse de la HA
- **Architecture** : Proxy intelligent entre apps et bases ğŸ”„
- **Load Balancing** : Distribution de charge automatique
- **Read/Write Split** : Routage intelligent selon type de requÃªte
- **Query Routing** : RÃ¨gles complexes de redirection
- **Database Firewall** : Protection contre SQL injection

#### ğŸ†• MaxScale 25.01 : Innovations majeures
- **Workload Capture** : Enregistrement du trafic production ğŸ†•
  - Capture complÃ¨te : requÃªtes, timing, transactions
  - Anonymisation pour conformitÃ©
  - Storage compressÃ©
- **Workload Replay** : Rejeu de charge rÃ©elle ğŸ†•
  - Tests de performance rÃ©alistes
  - Validation de migrations/upgrades
  - Capacity planning basÃ© sur donnÃ©es rÃ©elles
- **Diff Router** : Comparaison de versions ğŸ†•
  - Routage vers deux backends simultanÃ©ment
  - Comparaison de rÃ©sultats en temps rÃ©el
  - Validation d'upgrades MariaDB sans risque

#### ğŸ¤– Failover automatique
- **DÃ©tection de pannes** : Health checks, timeouts, consensus
- **Promotion de replica** : SÃ©lection du meilleur candidat
- **Reconfiguration automatique** : RÃ©orientation du trafic
- **Notifications** : Alerting et escalation
- **Tools** : MaxScale auto-failover, Corosync/Pacemaker

#### ğŸŒ Virtual IP et keepalived
- **Floating VIP** : IP qui migre avec le primary actif
- **VRRP protocol** : Virtual Router Redundancy Protocol
- **Configuration** : keepalived setup et priority
- **Integration** : Avec rÃ©plication ou Galera

#### ğŸ”„ Transaction Replay et Connection Migration (11.8) ğŸ†•
- **Transaction Replay** : Re-exÃ©cution automatique aprÃ¨s Ã©chec temporaire
- **Connection Migration** : Transfert transparent de sessions entre nÅ“uds
- **Use cases** : Rolling upgrades sans downtime, maintenance transparente
- **Configuration** : Activation et limites

#### ğŸ›¡ï¸ StratÃ©gies de rÃ©cupÃ©ration aprÃ¨s incident
- **Incident classification** : Mineur (1 node) vs Majeur (DC entier)
- **Runbooks** : ProcÃ©dures documentÃ©es et testÃ©es
- **Escalation** : Qui appeler, quand, comment
- **Post-mortem** : Analyse et amÃ©lioration continue

#### ğŸ”€ Alternatives : ProxySQL et HAProxy
- **ProxySQL** : Proxy avancÃ© avec query caching et rewrite
- **HAProxy** : Load balancer gÃ©nÃ©rique pour MariaDB
- **Comparaison** : Forces et faiblesses vs MaxScale

ğŸ’¡ **Impact business** : Une architecture HA bien conÃ§ue peut atteindre 99.99% de disponibilitÃ© (52 min/an downtime) voire 99.999% avec multi-DC. C'est la diffÃ©rence entre un service fiable et un service de classe mondiale.

---

## ğŸ†• Innovations MariaDB 11.8 et MaxScale 25.01 pour la HA

### RÃ©duction drastique du lag de rÃ©plication : Optimistic ALTER TABLE

#### Le problÃ¨me historique

```sql
-- ScÃ©nario : Table de 500M lignes, index Ã  ajouter
ALTER TABLE large_table ADD INDEX idx_email (email);
-- Temps : 2 heures

-- Pendant ces 2 heures :
-- âŒ RÃ©plication complÃ¨tement bloquÃ©e sur replicas
-- âŒ Lag accumulation : 2 heures de retard
-- âŒ Risque : Replicas inutilisables pour lectures
```

**Impact** : Sur une application avec 10,000 requÃªtes/sec, un lag de 2h signifie 72 millions de requÃªtes en retard.

#### La solution MariaDB 11.8 : Optimistic ALTER TABLE ğŸ†•

```sql
-- MariaDB 11.8 : RÃ©plication continue pendant ALTER
SET GLOBAL replicate_alter_as_create_select = ON;

ALTER TABLE large_table ADD INDEX idx_email (email);
-- Primary : 2 heures
-- Replicas : RÃ©plication continue, lag < 10 secondes

-- RÃ©sultat :
-- âœ… RÃ©plication parallÃ¨le au DDL
-- âœ… Lag rÃ©duit de 99.9% (2h â†’ 10s)
-- âœ… Replicas restent utilisables
```

**Impact opÃ©rationnel** : Maintenance sans interruption de service, scaling sans downtime.

---

### ContinuitÃ© de service : Transaction Replay et Connection Migration ğŸ†•

#### Transaction Replay : RÃ©silience automatique

```python
# Application : Transaction bancaire
try:
    connection.begin()
    connection.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
    connection.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")
    connection.commit()
except TransientError:
    # MariaDB 11.8 : Transaction Replay automatique
    # La transaction est re-exÃ©cutÃ©e sur un autre nÅ“ud
    # Transparent pour l'application
    pass
```

**BÃ©nÃ©fices** :
- âœ… Ã‰checs temporaires rÃ©solus automatiquement
- âœ… Moins de code de retry dans applications
- âœ… Meilleure expÃ©rience utilisateur

#### Connection Migration : Maintenance transparente

```sql
-- ScÃ©nario : Maintenance d'un nÅ“ud Galera
-- Avant 11.8 : 1000+ connexions perdues, erreurs applicatives

-- Avec Connection Migration :
-- 1. MaxScale dÃ©tecte node en maintenance
-- 2. Migrations progressives des connexions vers autres nÅ“uds
-- 3. Transactions en cours relocalisÃ©es automatiquement
-- 4. ZÃ©ro erreur applicative

-- RÃ©sultat : Rolling upgrades sans downtime
```

**Impact** : Maintenance et upgrades sans fenÃªtre de downtime planifiÃ©e.

---

### MaxScale 25.01 : Testing et validation nouvelle gÃ©nÃ©ration

#### Workload Capture : Production comme jeu de donnÃ©es de test ğŸ†•

```bash
# Capturer trafic production pendant 1 heure
maxctrl call command workload capture start \
  --duration=3600 \
  --output=/data/capture/prod-2025-12-15.wcap

# Analyse du workload capturÃ©
maxctrl call command workload analyze /data/capture/prod-2025-12-15.wcap
# Output :
# - Queries per second: 15,432
# - Read/Write ratio: 75/25
# - Peak connections: 842
# - Top 10 queries
# - Transaction patterns
```

**Use cases** :
- ğŸ“Š Capacity planning basÃ© sur trafic rÃ©el
- ğŸ”¬ Optimisation de requÃªtes avec donnÃ©es rÃ©elles
- ğŸ“ˆ Benchmark avant scaling infrastructure

#### Workload Replay : Validation Ã  l'Ã©chelle production ğŸ†•

```bash
# Rejouer workload capturÃ© sur nouvel environnement
maxctrl call command workload replay start \
  --input=/data/capture/prod-2025-12-15.wcap \
  --target=mariadb-new-11.8 \
  --speed=1.0  # Temps rÃ©el

# Monitoring pendant replay
maxctrl call command workload replay status
# Output :
# - Progress: 35%
# - Queries executed: 5,432,890
# - Errors: 12
# - Average latency: 4.2ms (vs 4.5ms prod)
# - 95th percentile: 15ms (vs 18ms prod)
```

**Avantages rÃ©volutionnaires** :
- âœ… Tests de charge avec trafic rÃ©el, pas synthÃ©tique
- âœ… Validation d'upgrades MariaDB sans risque
- âœ… Identification de rÃ©gressions de performance
- âœ… Capacity planning prÃ©cis

#### Diff Router : Comparaison de versions en temps rÃ©el ğŸ†•

```yaml
# Configuration MaxScale Diff Router
[Diff-Service]
type=service
router=diffsvc
servers=mariadb-11.4-prod, mariadb-11.8-test
user=maxscale
password=***

# Routage vers les deux versions simultanÃ©ment
# Comparaison automatique des rÃ©sultats
```

**Workflow de validation** :
```bash
# 1. Routage vers anciennes et nouvelles versions
# 2. MaxScale compare rÃ©sultats en temps rÃ©el
# 3. Logging des diffÃ©rences dÃ©tectÃ©es
maxctrl call command diff-router analyze

# Output :
# - Total queries: 1,250,000
# - Identical results: 1,249,987 (99.999%)
# - Differences detected: 13
#   â†’ Query ID 4521: Row order difference (acceptable)
#   â†’ Query ID 8934: Aggregation precision difference (investigate)
```

**Impact** : Migrations et upgrades MariaDB avec validation exhaustive et zÃ©ro surprise.

---

## âœ… CompÃ©tences acquises

Ã€ la fin de cette sixiÃ¨me partie, vous serez capable de :

### Conception d'architectures rÃ©silientes
- âœ… **Concevoir** des topologies de rÃ©plication adaptÃ©es aux besoins (read scaling, HA, DR)
- âœ… **Choisir** entre rÃ©plication asynchrone, semi-synchrone, et synchrone selon contraintes
- âœ… **Dimensionner** des clusters Galera pour charge de travail donnÃ©e
- âœ… **ImplÃ©menter** des architectures multi-datacenter pour disaster recovery
- âœ… **Calculer** la disponibilitÃ© thÃ©orique d'une architecture (SLA)
- âœ… **Identifier** et Ã©liminer les single points of failure

### OpÃ©ration en production
- âœ… **Configurer** la rÃ©plication MariaDB avec GTID
- âœ… **DÃ©ployer** et maintenir un cluster Galera en production
- âœ… **ImplÃ©menter** MaxScale pour load balancing et failover
- âœ… **Monitorer** la santÃ© de la rÃ©plication (lag, threads, errors)
- âœ… **Diagnostiquer** et rÃ©soudre les problÃ¨mes de rÃ©plication
- âœ… **Effectuer** des failovers planifiÃ©s sans downtime

### ZÃ©ro downtime operations
- âœ… **Planifier** et exÃ©cuter des maintenances sans interruption
- âœ… **Upgrader** MariaDB sans arrÃªt de service (rolling upgrades)
- âœ… **Utiliser** Workload Capture/Replay pour tests rÃ©alistes
- âœ… **Valider** des migrations avec Diff Router
- âœ… **Appliquer** Transaction Replay pour rÃ©silience applicative

### Disaster recovery
- âœ… **Concevoir** des stratÃ©gies de rÃ©cupÃ©ration aprÃ¨s incident
- âœ… **DÃ©finir** RTO et RPO adaptÃ©s aux besoins business
- âœ… **Documenter** des runbooks de failover testÃ©s
- âœ… **Effectuer** des drill exercises rÃ©guliers
- âœ… **RÃ©cupÃ©rer** d'un incident de split-brain
- âœ… **Coordonner** une rÃ©ponse Ã  incident majeur

### Expertise avancÃ©e
- âœ… **Optimiser** la performance de la rÃ©plication
- âœ… **PrÃ©venir** les problÃ¨mes de quorum et split-brain
- âœ… **Configurer** Virtual IP avec keepalived
- âœ… **IntÃ©grer** monitoring et alerting pour HA
- âœ… **Automatiser** le failover avec dÃ©tection intelligente

---

## ğŸ“ Parcours recommandÃ©s

Cette partie est **absolument critique** pour DBA et DevOps gÃ©rant des systÃ¨mes en production.

| Parcours | Importance | Justification |
|----------|------------|---------------|
| ğŸ” **Administrateur/DBA** | â­â­â­ ABSOLUMENT CRITIQUE | La HA est une responsabilitÃ© fondamentale du DBA. Sans maÃ®trise de la rÃ©plication et Galera, impossible d'opÃ©rer en production critique. |
| âš™ï¸ **DevOps/SRE** | â­â­â­ ABSOLUMENT CRITIQUE | DevOps responsables du uptime doivent maÃ®triser failover, monitoring, et rÃ©cupÃ©ration aprÃ¨s incident. Non nÃ©gociable pour atteindre SLA. |
| ğŸ—ï¸ **Architecte Infrastructure** | â­â­â­ ESSENTIEL | Conception d'architectures rÃ©silientes nÃ©cessite comprÃ©hension profonde des capacitÃ©s et limitations de chaque solution HA. |
| ğŸ”§ **DÃ©veloppeur Senior** | â­â­ IMPORTANT | Comprendre la HA aide Ã  Ã©crire du code rÃ©silient (retry logic, timeouts, idempotence). ParticuliÃ¨rement pour dÃ©veloppeurs d'applications critiques. |

### Pourquoi cette partie est non nÃ©gociable ?

#### Pour les DBA
ResponsabilitÃ© contractuelle d'assurer la disponibilitÃ© :
- **SLA 99.9%** = Maximum 8h43 downtime/an
- **SLA 99.99%** = Maximum 52 min downtime/an  
- **SLA 99.999%** = Maximum 5 min 15s downtime/an

**Sans maÃ®trise de la HA, ces SLA sont impossibles Ã  atteindre.**

#### Pour les DevOps/SRE
La rÃ©plication et HA sont au cÅ“ur du mÃ©tier SRE :
- **Toil reduction** : Automatisation du failover
- **Error budgets** : Calcul basÃ© sur disponibilitÃ© rÃ©elle
- **Incident response** : ProcÃ©dures de rÃ©cupÃ©ration testÃ©es
- **Capacity planning** : Dimensionnement pour redondance

#### Pour les Architectes
DÃ©cisions structurelles impactant des annÃ©es :
- **Multi-DC** : Latence vs consistance
- **Active-Active** : ComplexitÃ© vs performance
- **Costs** : 2x infrastructure pour N+1, 3x pour N+M
- **Trade-offs** : CAP theorem dans le monde rÃ©el

---

## ğŸš¨ ScÃ©narios de dÃ©faillance et stratÃ©gies de rÃ©cupÃ©ration

### Taxonomie des pannes

#### Type 1 : DÃ©faillance d'un nÅ“ud (FrÃ©quence : Mensuelle)

**ScÃ©nario** : Un serveur replica tombe (problÃ¨me disque, OOM, crash)

**Impact** :
- âš ï¸ RÃ©plication interrompue sur ce nÅ“ud
- âš ï¸ Lectures distribuÃ©es vers nÅ“uds restants
- âœ… Ã‰critures non affectÃ©es (sur primary)

**RÃ©cupÃ©ration** :
```bash
# 1. Identifier le problÃ¨me
mariadb -e "SHOW SLAVE STATUS\G" | grep -E "(Slave_IO_Running|Slave_SQL_Running|Last_Error)"

# 2. Si erreur rÃ©parable
mariadb -e "STOP SLAVE; SET GLOBAL sql_slave_skip_counter = 1; START SLAVE;"

# 3. Si corruption, resync complet
mariabackup --backup --target-dir=/backup/resync
# Restaurer sur replica
mariabackup --copy-back --target-dir=/backup/resync
mariadb -e "CHANGE MASTER TO MASTER_LOG_FILE='binlog.xxx', MASTER_LOG_POS=yyy; START SLAVE;"
```

**RTO** : 10-30 minutes  
**RPO** : 0 (pas de perte de donnÃ©es)

---

#### Type 2 : DÃ©faillance du primary (FrÃ©quence : Trimestrielle)

**ScÃ©nario** : Le serveur primary tombe (hardware failure, data center issue)

**Impact** :
- ğŸ”´ Ã‰critures bloquÃ©es â†’ Downtime applicatif
- âš ï¸ Lectures fonctionnent sur replicas
- ğŸš¨ Urgence : Promouvoir un replica en primary

**RÃ©cupÃ©ration automatique avec MaxScale** :
```bash
# MaxScale dÃ©tecte panne primary (health check Ã©choue)
# â†’ Auto-promotion du replica le plus Ã  jour
# â†’ Reconfiguration du routage
# â†’ Notifications

# Timeline :
# T+0s : Primary tombe
# T+5s : MaxScale dÃ©tecte (health check interval)
# T+10s : Promotion du meilleur replica
# T+15s : Routage reconfigurÃ©
# T+20s : Applications fonctionnelles

# RTO : 20-30 secondes
```

**RÃ©cupÃ©ration manuelle (sans MaxScale)** :
```bash
# 1. Identifier le replica le plus Ã  jour
mariadb -h replica1 -e "SHOW SLAVE STATUS\G" | grep Exec_Master_Log_Pos
mariadb -h replica2 -e "SHOW SLAVE STATUS\G" | grep Exec_Master_Log_Pos

# 2. Promouvoir le meilleur replica
mariadb -h replica1 -e "STOP SLAVE; RESET SLAVE ALL;"

# 3. Repointer autres replicas
mariadb -h replica2 -e "STOP SLAVE; CHANGE MASTER TO MASTER_HOST='replica1'; START SLAVE;"

# 4. Reconfigurer applications vers nouveau primary
# (IdÃ©alement via VIP pour transparence)

# RTO : 5-15 minutes (manuel)
```

**RPO** : 
- RÃ©plication async : 0-30 secondes (lag typique)
- RÃ©plication semi-sync : 0 (garanti)

---

#### Type 3 : Split-brain Galera (FrÃ©quence : Annuelle)

**ScÃ©nario** : Partition rÃ©seau divise un cluster Galera 4 nÅ“uds en deux groupes (2+2)

**Impact** :
- ğŸ”´ Deux groupes indÃ©pendants pensent Ãªtre le cluster primaire
- ğŸ”´ Risque de divergence de donnÃ©es (write conflicts)
- ğŸ”´ Requiert intervention manuelle

**PrÃ©vention** :
```ini
# galera.cnf : Toujours nombre impair de nÅ“uds
wsrep_cluster_size = 3  # ou 5, 7, etc.

# Avec Garbd (Galera Arbitrator) si contrainte budgÃ©taire
# Node 1, 2 : Serveurs complets
# Node 3 : Garbd (lÃ©ger, vote uniquement)
garbd --group=my_cluster --address=gcomm://node1,node2
```

**RÃ©cupÃ©ration** :
```bash
# 1. Identifier la partition avec le quorum
# Partition avec majoritÃ© continue (2 nÅ“uds sur 3)
# Partition minoritaire se met en "non-primary" state

# 2. Sur partition minoritaire, forcer shutdown
systemctl stop mariadb

# 3. AprÃ¨s rÃ©solution rÃ©seau, rejoindre cluster
# Le nÅ“ud va faire un IST (Incremental State Transfer)
systemctl start mariadb

# VÃ©rification
mariadb -e "SHOW STATUS LIKE 'wsrep_cluster_status';"
# Output : Primary (OK)
```

**RTO** : 10-30 minutes (selon durÃ©e partition rÃ©seau)  
**RPO** : 0 (pas de perte si rÃ©solution correcte)

---

#### Type 4 : Disaster complet d'un datacenter (FrÃ©quence : Tous les 5-10 ans)

**ScÃ©nario** : Incendie, inondation, cyberattaque, coupure Ã©lectrique prolongÃ©e â†’ DC entier hors ligne

**Impact** :
- ğŸ”´ğŸ”´ğŸ”´ Service entiÃ¨rement down si pas de multi-DC
- ğŸ”´ğŸ”´ğŸ”´ Perte de donnÃ©es si RPO > durÃ©e incident

**Architecture prÃ©ventive : Multi-DC avec Galera** :
```plaintext
Datacenter A (Paris)       Datacenter B (Frankfurt)
â”œâ”€ Galera Node 1          â”œâ”€ Galera Node 3
â”œâ”€ Galera Node 2          â”œâ”€ Galera Node 4
â””â”€ Garbd                  â””â”€ Garbd

Latence inter-DC : 15-20ms (acceptable pour Galera)
Write Performance : ImpactÃ© par latence, mais disponibilitÃ© garantie
```

**RÃ©cupÃ©ration** :
```bash
# Scenario : DC A en feu, DC B continue seul

# 1. DC B maintient le service automatiquement (quorum 2/4 + Garbd)
# 2. Applications re-routÃ©es vers DC B (DNS, load balancer)
# 3. OpÃ©ration continue avec lÃ©gÃ¨re dÃ©gradation performance

# AprÃ¨s reconstruction DC A :
# 1. Provisionner nouveaux serveurs
# 2. Joindre cluster existant
# 3. SST automatique depuis DC B
# 4. Service complÃ¨tement restaurÃ©

# RTO : 0 (transparent pour utilisateurs)
# RPO : 0 (synchronous replication)
```

**CoÃ»t** : 2x infrastructure minimum (Active-Active multi-DC)

---

#### Type 5 : Corruption de donnÃ©es silencieuse (FrÃ©quence : Rare mais critique)

**ScÃ©nario** : Bug applicatif, ransomware, ou erreur humaine corrompt des donnÃ©es

**Impact** :
- ğŸ”´ DonnÃ©es incorrectes rÃ©pliquÃ©es sur tous les nÅ“uds
- ğŸ”´ DÃ©couverte souvent tardive (heures/jours aprÃ¨s)
- ğŸ”´ NÃ©cessite restauration depuis backup

**RÃ©cupÃ©ration PITR (Point-in-Time Recovery)** :
```bash
# Scenario : DÃ©couverte d'une suppression accidentelle Ã  14:35
# Last good backup : 02:00 ce matin

# 1. Identifier position binlog du backup
cat /backup/xtrabackup_binlog_info
# Output : binlog.000042  12847392

# 2. Restaurer backup sur serveur temporaire
mariabackup --copy-back --target-dir=/backup/full-02h00

# 3. Rejouer binlogs jusqu'Ã  14:34 (avant corruption)
mysqlbinlog --start-position=12847392 \
            --stop-datetime="2025-12-15 14:34:00" \
            /var/lib/mysql/binlog.0000* \
  | mariadb

# 4. Valider les donnÃ©es
mariadb -e "SELECT COUNT(*) FROM critical_table WHERE deleted_at IS NULL;"

# 5. Migrer donnÃ©es corrigÃ©es vers production
# (Via dump/restore ou rÃ©plication temporaire)

# RTO : 2-6 heures (selon taille base)
# RPO : <1 minute (granularitÃ© binlog)
```

---

### Matrice de rÃ©cupÃ©ration

| Type d'incident | FrÃ©quence | RTO | RPO | Automatisable | CoÃ»t prÃ©vention |
|-----------------|-----------|-----|-----|---------------|-----------------|
| NÅ“ud replica down | Mensuel | 10-30 min | 0 | âœ… Oui (monitoring) | Faible |
| Primary down | Trimestriel | 20s-15min | 0-30s | âœ… Oui (MaxScale) | Moyen |
| Split-brain Galera | Annuel | 10-30 min | 0 | âš ï¸ Partiellement | Faible (Garbd) |
| DC disaster | 5-10 ans | 0-4h | 0-1h | âœ… Oui (Multi-DC) | Ã‰levÃ© (2x infra) |
| Data corruption | Rare | 2-6h | 1 min | âŒ Non (PITR manuel) | Moyen (backups) |

---

## ğŸ“Š SLA et objectifs RTO/RPO

### Comprendre les SLA (Service Level Agreements)

Un SLA dÃ©finit contractuellement le niveau de service garanti. Pour une base de donnÃ©es, le critÃ¨re principal est la **disponibilitÃ©**.

#### Calcul de disponibilitÃ©

```
DisponibilitÃ© (%) = (Temps total - Temps indisponible) / Temps total Ã— 100
```

#### Tableau de rÃ©fÃ©rence des SLA

| SLA | Downtime annuel | Downtime mensuel | Downtime hebdomadaire | Classe de service |
|-----|-----------------|------------------|----------------------|-------------------|
| 90% | 36.5 jours | 72 heures | 16.8 heures | Inacceptable |
| 95% | 18.25 jours | 36 heures | 8.4 heures | Basic |
| 99% | 3.65 jours | 7.2 heures | 1.68 heures | Standard |
| 99.9% ("three nines") | **8h 43min** | 43.2 minutes | 10.1 minutes | High Availability |
| 99.95% | 4h 22min | 21.6 minutes | 5 minutes | Very High Availability |
| 99.99% ("four nines") | **52 minutes** | 4.3 minutes | 1 minute | Mission Critical |
| 99.999% ("five nines") | **5min 15s** | 26 secondes | 6 secondes | Ultra Critical |

#### Impact business selon SLA

```
E-commerce $10M/an de CA (24/7) :
- 99%    â†’ 3.65 jours down  â†’ $100,000 de perte potentielle
- 99.9%  â†’ 8h43min down     â†’ $3,650 de perte potentielle  
- 99.99% â†’ 52 minutes down  â†’ $360 de perte potentielle

Banking application :
- 99.99% minimum requis par rÃ©gulation
- PÃ©nalitÃ©s contractuelles si dÃ©passement
```

---

### RTO (Recovery Time Objective)

**DÃ©finition** : Temps maximum acceptable entre une panne et la restauration du service.

#### Exemples par cas d'usage

| Cas d'usage | RTO typique | Architecture requise |
|-------------|-------------|----------------------|
| Blog personnel | 24 heures | Single server + backup hebdomadaire |
| Site e-commerce PME | 4 heures | RÃ©plication async + backup quotidien |
| Application SaaS B2B | 1 heure | RÃ©plication semi-sync + MaxScale |
| Banking core system | 15 minutes | Galera cluster + Multi-DC |
| Trading platform | 30 secondes | Galera multi-DC + auto-failover |
| Payment processing | 0 (continuous) | Multi-DC active-active + edge caching |

#### Facteurs influenÃ§ant RTO

1. **DÃ©tection de panne** : Monitoring frÃ©quence (1s, 5s, 30s)
2. **Failover automatique** : AutomatisÃ© vs manuel
3. **Taille des donnÃ©es** : Impact sur restauration
4. **ComplexitÃ© architecture** : Plus de composants = plus de points de dÃ©faillance
5. **CompÃ©tences Ã©quipe** : Runbooks documentÃ©s et testÃ©s

---

### RPO (Recovery Point Objective)

**DÃ©finition** : QuantitÃ© maximale acceptable de donnÃ©es perdues, mesurÃ©e en temps.

#### Exemples par cas d'usage

| Cas d'usage | RPO typique | StratÃ©gie backup/rÃ©plication |
|-------------|-------------|------------------------------|
| Blog personnel | 24 heures | Backup hebdomadaire |
| Analytics database | 6 heures | Backup quotidien + binlogs |
| CMS/CRM | 1 heure | Backup quotidien + binlog streaming |
| E-commerce | 5 minutes | RÃ©plication async continue |
| Financial transactions | 0 (zero data loss) | RÃ©plication semi-sync ou Galera |
| Banking core | 0 (zero data loss) | Galera multi-DC synchronous |

#### Relation RTO/RPO et architecture

```plaintext
Architecture Simple (Single Server + Backup)
â”œâ”€ RTO : 4-24 heures (restauration manuelle)
â””â”€ RPO : 1-24 heures (frÃ©quence backup)

Architecture RÃ©plication Async
â”œâ”€ RTO : 5-15 minutes (failover manuel)
â””â”€ RPO : 0-60 secondes (lag rÃ©plication)

Architecture RÃ©plication Semi-Sync + MaxScale
â”œâ”€ RTO : 20-60 secondes (failover automatique)
â””â”€ RPO : 0 secondes (garanti)

Architecture Galera Multi-DC
â”œâ”€ RTO : 0-5 secondes (transparente)
â””â”€ RPO : 0 secondes (synchronous)
```

---

### CoÃ»t de la haute disponibilitÃ©

**Principe fondamental** : Chaque "nine" supplÃ©mentaire coÃ»te ~10x plus cher.

#### Matrice coÃ»t/disponibilitÃ©

| SLA cible | Architecture minimale | CoÃ»t relatif | ComplexitÃ© |
|-----------|----------------------|--------------|------------|
| 99% | Single server + backup | 1x | Faible |
| 99.9% | Primary + 2 replicas + backup | 3x | Moyenne |
| 99.99% | Galera 3 nodes + MaxScale + monitoring | 5-7x | Ã‰levÃ©e |
| 99.999% | Galera multi-DC + edge caching + automation | 10-15x | TrÃ¨s Ã©levÃ©e |

#### Analyse coÃ»t/bÃ©nÃ©fice

```
Exemple : E-commerce $10M CA/an

Option A : 99.9% SLA
- Infrastructure : $50k/an
- Downtime potentiel : 8h43min â†’ $3,650 perte
- Total Cost of Ownership : $53,650

Option B : 99.99% SLA
- Infrastructure : $200k/an
- Downtime potentiel : 52min â†’ $360 perte
- Total Cost of Ownership : $200,360

DiffÃ©rentiel : $146,710/an
BÃ©nÃ©fice : -$3,290 de pertes Ã©vitÃ©es

â†’ Pour ce cas, 99.9% est optimal sauf si :
  - Impact rÃ©putationnel significatif
  - PÃ©nalitÃ©s contractuelles
  - Exigence rÃ©glementaire
```

**RÃ¨gle pragmatique** : Viser le SLA le plus bas acceptable business, pas le plus Ã©levÃ© techniquement possible.

---

## ğŸ’¡ Principes d'architecture rÃ©siliente

### Les 7 piliers de la haute disponibilitÃ©

#### 1ï¸âƒ£ Redondance Ã  tous les niveaux

```plaintext
Application Layer     [LB] â†’ [App1] [App2] [App3]
                       â†“
Database Proxy Layer  [MaxScale1] [MaxScale2]
                       â†“
Database Layer        [Primary] [Replica1] [Replica2]
                       â†“
Storage Layer         [SAN] avec RAID10 + snapshots
                       â†“
Network Layer         [Switch1] [Switch2] avec bonding
                       â†“
Power Layer           [UPS] + [Generator]
```

**Principe** : Aucun single point of failure Ã  aucun niveau.

#### 2ï¸âƒ£ DÃ©tection rapide et prÃ©cise

```bash
# Monitoring multi-niveaux
- Health check TCP : 1 seconde
- Health check SQL : 5 secondes (SELECT 1)
- Health check applicatif : 10 secondes (query complexe)
- Monitoring mÃ©trique : 30 secondes (lag, connections, etc.)
```

**Principe** : DÃ©tecter avant que l'utilisateur ne soit impactÃ©.

#### 3ï¸âƒ£ Failover automatique testÃ©

```yaml
# Automatisation complÃ¨te
Detection â†’ Decision â†’ Action â†’ Validation
   â†“           â†“         â†“          â†“
  5s         2s       10s       3s
  
Total : 20 secondes RTO
```

**Principe** : L'humain est trop lent pour maintenir des SLA Ã©levÃ©s.

#### 4ï¸âƒ£ Isolation des dÃ©faillances

```plaintext
Failure Domain 1     Failure Domain 2     Failure Domain 3
â”œâ”€ Rack A           â”œâ”€ Rack B           â”œâ”€ Rack C
â”œâ”€ Switch A         â”œâ”€ Switch B         â”œâ”€ Switch C
â”œâ”€ Node 1           â”œâ”€ Node 2           â”œâ”€ Node 3
â””â”€ Subnet 10.0.1.x  â””â”€ Subnet 10.0.2.x  â””â”€ Subnet 10.0.3.x
```

**Principe** : Une dÃ©faillance ne doit affecter qu'un domaine.

#### 5ï¸âƒ£ Graceful degradation

```python
# Application : DÃ©gradation gracieuse
if primary_db.available():
    return full_featured_query()
elif replica_db.available():
    return read_only_query()  # Mode dÃ©gradÃ©
elif cache.available():
    return cached_result()    # Mode trÃ¨s dÃ©gradÃ©
else:
    return error_page()       # Dernier recours
```

**Principe** : Service dÃ©gradÃ© > pas de service.

#### 6ï¸âƒ£ Chaos Engineering

```bash
# Tests de rÃ©silience rÃ©guliers
- Kill random node monthly
- Inject network latency weekly
- Simulate disk full quarterly
- Full DR drill annually
```

**Principe** : Casser volontairement pour valider la rÃ©silience.

#### 7ï¸âƒ£ Documentation et runbooks

```markdown
# Runbook : Primary Database Failure
## Symptoms
- MaxScale logs : "Server primary marked as down"
- Application errors : Connection refused

## Impact
- Write operations blocked
- Read operations functioning

## Actions
1. Verify primary really down (ssh, ping, telnet)
2. Check replica lag : SHOW SLAVE STATUS
3. If lag < 10s : Promote best replica
4. If lag > 10s : Wait for catch-up or force promotion
5. Reconfigure applications/MaxScale
6. Notify team

## Rollback
- Keep old primary offline until sync
- Rejoin as replica when fixed

## Post-mortem
- Root cause analysis
- Update automation if needed
```

**Principe** : ProcÃ©dures Ã©crites, testÃ©es, et Ã  jour.

---

## ğŸ¯ PrÃ©requis pour cette partie

Cette partie est **la plus exigeante** de la formation. Assurez-vous de maÃ®triser :

### Techniques
- âœ… Administration MariaDB avancÃ©e (Parties 3-5)
- âœ… Transactions, verrouillage, MVCC
- âœ… Binary logs et leur rÃ´le
- âœ… Backup/restauration (Mariabackup, PITR)

### SystÃ¨mes distribuÃ©s
- âœ… Concepts de consensus (Paxos, Raft basics)
- âœ… ThÃ©orÃ¨me CAP (Consistency, Availability, Partition tolerance)
- âœ… Trade-offs latence vs cohÃ©rence
- âœ… Split-brain et quorum

### RÃ©seaux
- âœ… TCP/IP, latence, bande passante
- âœ… Virtual IPs et routage
- âœ… Firewalls et segmentation rÃ©seau
- âœ… DNS et load balancers

### OpÃ©rationnel
- âœ… Monitoring et alerting
- âœ… Incident response
- âœ… Linux administration avancÃ©e
- âœ… Scripting (bash, Python)

Si certains concepts ne sont pas clairs, revoyez les parties prÃ©cÃ©dentes ou documentez-vous sur les systÃ¨mes distribuÃ©s avant de continuer.

---

## âš ï¸ Avertissements critiques

### Les erreurs qui cassent la HA

1. **âŒ Sous-estimer la latence rÃ©seau**
   - Galera multi-DC avec 100ms latency â†’ performances catastrophiques
   - **Solution** : <20ms pour Galera, sinon rÃ©plication async

2. **âŒ NÃ©gliger les tests de failover**
   - "Notre failover fonctionne" basÃ© sur thÃ©orie, pas pratique
   - **Solution** : Drill mensuel, failover planifiÃ© trimestriel

3. **âŒ Nombre pair de nÅ“uds Galera**
   - 2 ou 4 nÅ“uds â†’ risque de split-brain 50/50
   - **Solution** : Toujours impair (3, 5, 7) ou Garbd

4. **âŒ Ignorer le monitoring**
   - DÃ©couvrir le lag de rÃ©plication via utilisateurs
   - **Solution** : Alerting proactif sur toutes les mÃ©triques critiques

5. **âŒ Oublier les dÃ©pendances externes**
   - Base HA mais DNS single point of failure
   - **Solution** : HA Ã  tous les niveaux, pas seulement DB

6. **âŒ Surcharger le primary avec lectures**
   - Primary surchargÃ© â†’ lent â†’ replicas en lag â†’ tout dÃ©gradÃ©
   - **Solution** : Read/Write split strict avec MaxScale

7. **âŒ Pas de plan de rollback**
   - Upgrade ratÃ© sans possibilitÃ© de revenir en arriÃ¨re
   - **Solution** : Toujours un plan B documentÃ© et testÃ©

---

## ğŸš€ PrÃªt pour l'infrastructure critique ?

Cette partie vous prÃ©pare Ã  gÃ©rer des systÃ¨mes oÃ¹ **chaque seconde d'indisponibilitÃ© compte**. Vous allez acquÃ©rir les compÃ©tences pour :

- âœ… Concevoir des architectures capables de rÃ©sister Ã  des dÃ©faillances multiples
- âœ… Atteindre des SLA de 99.99% ou plus
- âœ… OpÃ©rer des maintenances sans downtime
- âœ… RÃ©cupÃ©rer d'incidents en quelques secondes
- âœ… Valider des upgrades sans risque avec Workload Replay
- âœ… Dormir tranquille en sachant votre infrastructure rÃ©siliente

Les compÃ©tences de cette partie sont **diffÃ©renciantes sur le marchÃ©**. Les DBA et DevOps maÃ®trisant Galera, MaxScale, et les stratÃ©gies de HA avancÃ©es sont trÃ¨s recherchÃ©s et bien rÃ©munÃ©rÃ©s.

**PrÃ©parez-vous Ã  construire des systÃ¨mes qui ne tombent jamais.** ğŸ›¡ï¸

---

## â¡ï¸ Prochaine Ã©tape

**Module 13 : RÃ©plication** â†’ MaÃ®trisez la rÃ©plication asynchrone et semi-synchrone, GTID, topologies avancÃ©es, et techniques de failover. La base de toute stratÃ©gie de haute disponibilitÃ©.

Bienvenue dans le monde de l'infrastructure qui ne dort jamais ! ğŸŒ

---

**MariaDB** : Version 11.8 LTS  
**MaxScale** : Version 25.01

â­ï¸ [RÃ©plication](/13-replication/README.md)
