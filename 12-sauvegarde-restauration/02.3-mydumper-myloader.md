ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 12.2.3 mydumper/myloader : ParallÃ©lisme

> **Niveau** : AvancÃ©  
> **DurÃ©e estimÃ©e** : 3 heures  
> **PrÃ©requis** : Sections 12.2.1 et 12.2.2, comprÃ©hension des threads et du parallÃ©lisme

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- Comprendre l'architecture parallÃ¨le de mydumper et ses avantages sur mysqldump
- Installer et configurer mydumper/myloader pour des backups haute performance
- Exploiter le parallÃ©lisme pour rÃ©duire drastiquement les temps de backup et restauration
- Effectuer des restaurations sÃ©lectives granulaires (base, table, chunk)
- Optimiser les paramÃ¨tres selon la taille de votre base et vos ressources
- IntÃ©grer mydumper dans vos stratÃ©gies de continuitÃ© d'activitÃ©
- Automatiser les backups parallÃ¨les avec monitoring et alertes

---

## Introduction

**mydumper** est un utilitaire open-source de backup logique multi-threadÃ© dÃ©veloppÃ© pour pallier les limitations de performance de `mysqldump`. AssociÃ© Ã  son compagnon **myloader** pour la restauration, il offre des performances jusqu'Ã  **10x supÃ©rieures** sur les grandes bases de donnÃ©es.

### Pourquoi mydumper ?

#### Limitations de mysqldump

```bash
# mysqldump : Single-threaded, sÃ©quentiel
mysqldump production_db
# Table 1 (1 GB)  â†’ 3 min
# Table 2 (5 GB)  â†’ 15 min
# Table 3 (2 GB)  â†’ 6 min
# Total : 24 minutes
```

#### Approche mydumper

```bash
# mydumper : Multi-threaded, parallÃ¨le
mydumper -B production_db -t 8
# Table 1 (1 GB)  â†’ Thread 1 }
# Table 2 (5 GB)  â†’ Thread 2, 3, 4 } En parallÃ¨le
# Table 3 (2 GB)  â†’ Thread 5 }
# Total : 6 minutes (4x plus rapide)
```

### Avantages clÃ©s

| FonctionnalitÃ© | mysqldump | mydumper |
|----------------|-----------|----------|
| **ParallÃ©lisme** | âŒ Single-thread | âœ… Multi-thread |
| **Chunking tables** | âŒ Non | âœ… Oui (par plage) |
| **Compression native** | âŒ Via pipe | âœ… IntÃ©grÃ© (gzip/zstd) |
| **Restauration sÃ©lective** | âš ï¸ Difficile | âœ… Facile (1 fichier = 1 table) |
| **CohÃ©rence** | âœ… --single-transaction | âœ… Snapshot global |
| **MÃ©tadonnÃ©es** | âš ï¸ LimitÃ©es | âœ… Riches (metadata file) |
| **Vitesse backup** | 50-200 MB/s | 500-2000 MB/s |
| **Vitesse restauration** | 100-300 MB/s | 1000-5000 MB/s |

ğŸ’¡ **Cas d'usage idÃ©al** : Bases > 50 GB, besoin de restauration rapide, restauration sÃ©lective frÃ©quente.

---

## Architecture et fonctionnement

### Principe du parallÃ©lisme

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      mydumper Master Thread     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Thread Pool (8 threads)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼           â–¼               â–¼               â–¼           â–¼
    Thread 1    Thread 2        Thread 3        Thread 4    Thread 5
    Table 1     Table 2         Table 3         Table 4     Table 5
    (users)     (orders)        (products)      (logs)      (stats)
        â”‚           â”‚               â”‚               â”‚           â”‚
        â–¼           â–¼               â–¼               â–¼           â–¼
    users.sql   orders.sql      products.sql    logs.sql    stats.sql
```

### Chunking de tables volumineuses

Pour les tables trÃ¨s grandes, mydumper peut les **diviser en chunks** :

```
Table orders (10 GB, 50M rows)
    â”‚
    â”œâ”€ Thread 1 â†’ orders.00000.sql (id: 1-10M)
    â”œâ”€ Thread 2 â†’ orders.00001.sql (id: 10M-20M)
    â”œâ”€ Thread 3 â†’ orders.00002.sql (id: 20M-30M)
    â”œâ”€ Thread 4 â†’ orders.00003.sql (id: 30M-40M)
    â””â”€ Thread 5 â†’ orders.00004.sql (id: 40M-50M)

5 threads en parallÃ¨le = 5x plus rapide
```

ğŸ’¡ **Avantage** : Chaque chunk peut Ãªtre traitÃ© indÃ©pendamment, permettant un parallÃ©lisme maximal.

### Structure de sortie

```bash
backup_dir/
â”œâ”€â”€ metadata                      # Informations globales
â”œâ”€â”€ production_db-schema-create.sql  # CREATE DATABASE
â”œâ”€â”€ production_db.users-schema.sql   # CREATE TABLE users
â”œâ”€â”€ production_db.users.sql          # DonnÃ©es users
â”œâ”€â”€ production_db.orders-schema.sql  # CREATE TABLE orders
â”œâ”€â”€ production_db.orders.00000.sql   # Chunk 1 (id: 1-1M)
â”œâ”€â”€ production_db.orders.00001.sql   # Chunk 2 (id: 1M-2M)
â”œâ”€â”€ production_db.products-schema.sql
â”œâ”€â”€ production_db.products.sql
â””â”€â”€ production_db-schema-post.sql    # Triggers, procÃ©dures, views
```

**MÃ©tadonnÃ©es** contient :

```ini
Started dump at: 2025-12-13 14:30:45
SHOW MASTER STATUS:
    Log: mariadb-bin.000123
    Pos: 987654321
SHOW SLAVE STATUS:
    Host: primary.example.com
    Log: mariadb-bin.000122
    Pos: 456789012
    GTID: 0-1-12345
Finished dump at: 2025-12-13 14:35:12
```

ğŸ’¡ **Crucial pour PITR** : Position binlog exacte du snapshot.

---

## Installation

### Depuis les dÃ©pÃ´ts officiels

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install mydumper

# CentOS/RHEL 8+
sudo dnf install mydumper

# Fedora
sudo dnf install mydumper
```

### Compilation depuis les sources (derniÃ¨re version)

```bash
# DÃ©pendances
sudo apt install -y cmake gcc g++ libglib2.0-dev \
  libmysqlclient-dev libssl-dev libpcre3-dev zlib1g-dev

# Clone du repository
git clone https://github.com/mydumper/mydumper.git
cd mydumper

# Compilation
cmake .
make
sudo make install

# VÃ©rification
mydumper --version
myloader --version
```

### VÃ©rification de l'installation

```bash
# Tester mydumper
mydumper --help | head -20

# Tester myloader
myloader --help | head -20

# Versions recommandÃ©es
# mydumper 0.15.x+ (support MariaDB 11.x)
```

---

## Options principales de mydumper

### Options de connexion

```bash
mydumper \
  --host=localhost \          # ou -h
  --port=3306 \              # ou -P
  --user=backup_user \       # ou -u
  --password=SecurePass \    # ou -p (demande interactive si omis)
  --socket=/var/run/mysqld/mysqld.sock  # ou -S
```

ğŸ’¡ **SÃ©curitÃ©** : Comme pour mysqldump, prÃ©fÃ©rez un fichier de configuration plutÃ´t que le mot de passe en ligne de commande.

```ini
# ~/.my.cnf
[client]
user=backup_user
password=SecurePassword123
```

```bash
# Utilisation
mydumper --defaults-file=~/.my.cnf -B production_db
```

---

### Options de sÃ©lection

```bash
# Backup d'une base spÃ©cifique
mydumper --database=production_db
# Alias : -B production_db

# Backup de plusieurs bases
mydumper --regex='^(db1|db2|db3)\.'

# Backup de toutes les bases (sauf systÃ¨me)
mydumper --all-databases
# Exclut automatiquement : mysql, information_schema, performance_schema

# Backup d'une table spÃ©cifique
mydumper -B production_db --regex='production_db\.users'

# Backup de plusieurs tables
mydumper -B production_db --regex='production_db\.(users|orders|products)'

# Ignorer certaines tables
mydumper -B production_db --regex='production_db\.(?!temp_|cache_)'
```

ğŸ’¡ **Regex power** : mydumper utilise les expressions rÃ©guliÃ¨res PCRE pour un filtrage trÃ¨s flexible.

---

### Options de parallÃ©lisme

```bash
# Nombre de threads (dÃ©faut : 4)
mydumper -B production_db --threads=8
# Alias : -t 8

# RÃ¨gle empirique :
# - Disque HDD : 2-4 threads
# - SSD SATA : 4-8 threads
# - NVMe : 8-16 threads
# - RÃ©seau : selon bande passante
```

#### Optimisation du nombre de threads

```bash
# Benchmark rapide pour trouver le nombre optimal
for threads in 2 4 8 16; do
    echo "Testing with $threads threads..."
    time mydumper -B test_db -t $threads -o /tmp/test_$threads
    rm -rf /tmp/test_$threads
done

# Exemple de rÃ©sultats :
# 2 threads  : 12m 30s
# 4 threads  : 7m 15s
# 8 threads  : 4m 45s  â† Optimal
# 16 threads : 5m 10s  (overhead de coordination)
```

---

### Options de chunking

```bash
# Activer le chunking pour tables volumineuses
mydumper -B production_db \
  --chunk-filesize=100  # Taille de chunk en MB
  # Alias : -F 100

# Nombre de lignes par chunk
mydumper -B production_db \
  --rows=1000000  # 1M lignes par fichier
  # Alias : -r 1000000
```

#### Exemple : Table de 10 GB

```bash
# Sans chunking (1 seul fichier de 10 GB)
mydumper -B production_db --regex='production_db\.huge_table'
# â†’ production_db.huge_table.sql (10 GB)

# Avec chunking (100 MB par fichier)
mydumper -B production_db --regex='production_db\.huge_table' -F 100
# â†’ production_db.huge_table.00000.sql (100 MB)
# â†’ production_db.huge_table.00001.sql (100 MB)
# â†’ ...
# â†’ production_db.huge_table.00099.sql (100 MB)
# 100 fichiers de 100 MB chacun
```

ğŸ’¡ **Avantage multi-threading** : 8 threads peuvent charger 8 chunks simultanÃ©ment lors de la restauration.

---

### Options de cohÃ©rence

```bash
# Snapshot cohÃ©rent (Ã©quivalent --single-transaction)
mydumper -B production_db \
  --trx-consistency-only
  # Active START TRANSACTION WITH CONSISTENT SNAPSHOT

# Verrous globaux (Ã©quivalent --lock-all-tables)
mydumper -B production_db \
  --lock-all-tables
  # Alias : -x
```

ğŸ’¡ **Par dÃ©faut**, mydumper utilise `--trx-consistency-only` pour InnoDB.

---

### Options de compression

```bash
# Compression gzip (dÃ©faut si disponible)
mydumper -B production_db --compress
# Alias : -c

# Niveau de compression gzip (1-9)
mydumper -B production_db --compress --compress-level=6

# Compression zstd (plus rapide, meilleur ratio)
mydumper -B production_db --compress-protocol=zstd
```

#### Comparaison des compresseurs

| Compresseur | Ratio | Vitesse compression | Vitesse dÃ©compression |
|-------------|-------|---------------------|----------------------|
| gzip -6 | 10:1 | 50 MB/s | 200 MB/s |
| gzip -9 | 11:1 | 15 MB/s | 200 MB/s |
| zstd -3 | 10:1 | 400 MB/s | 800 MB/s |
| zstd -9 | 12:1 | 100 MB/s | 700 MB/s |

âœ… **Recommandation** : `zstd -3` pour le meilleur compromis vitesse/ratio.

---

### Options de dump

```bash
# Dump complet avec tous les objets
mydumper -B production_db \
  --triggers \           # Inclure les triggers (dÃ©faut)
  --routines \          # Inclure procÃ©dures et fonctions
  --events              # Inclure les Ã©vÃ©nements planifiÃ©s

# SchÃ©ma uniquement (sans donnÃ©es)
mydumper -B production_db --no-data
# Alias : -m

# DonnÃ©es uniquement (sans schÃ©ma)
mydumper -B production_db --no-schemas
# Alias : -d
```

---

### Options de sortie

```bash
# RÃ©pertoire de destination
mydumper -B production_db \
  --outputdir=/backups/mariadb/$(date +%Y%m%d)
  # Alias : -o /backups/mariadb/$(date +%Y%m%d)

# Ã‰craser le rÃ©pertoire existant
mydumper -B production_db -o /backups/latest --overwrite-tables

# VerbositÃ© (0-3, dÃ©faut: 2)
mydumper -B production_db --verbose=3
# Alias : -v 3
```

---

### Options avancÃ©es pour PITR

```bash
# Enregistrer la position binlog
mydumper -B production_db \
  --binlog-snapshot \
  --outputdir=/backups/$(date +%Y%m%d)

# Vider les logs avant backup (nouveau binlog)
mydumper -B production_db \
  --binlog-snapshot \
  --flush-logs

# Dump des binlogs
mydumper -B production_db \
  --binlog-snapshot \
  --binlog-dir=/var/lib/mysql \
  --outputdir=/backups/full_$(date +%Y%m%d)
```

Le fichier `metadata` contiendra :

```
SHOW MASTER STATUS:
    Log: mariadb-bin.000042
    Pos: 123456789
    GTID: 0-1-98765
```

---

## Commande mydumper complÃ¨te (production-ready)

```bash
#!/bin/bash
# Backup production avec mydumper

BACKUP_DIR="/backups/mariadb/$(date +%Y%m%d_%H%M%S)"

mydumper \
  --database=production_db \
  --outputdir="$BACKUP_DIR" \
  --threads=8 \
  --chunk-filesize=100 \
  --compress-protocol=zstd \
  --trx-consistency-only \
  --triggers \
  --routines \
  --events \
  --binlog-snapshot \
  --flush-logs \
  --verbose=2

if [ $? -eq 0 ]; then
    echo "âœ… Backup completed: $BACKUP_DIR"
    # Afficher la position binlog
    cat "$BACKUP_DIR/metadata"
else
    echo "âŒ Backup FAILED"
    exit 1
fi
```

---

## Options principales de myloader

### Options de connexion

```bash
myloader \
  --host=localhost \
  --port=3306 \
  --user=restore_user \
  --password=SecurePass
```

---

### Options de sÃ©lection

```bash
# Restaurer depuis un rÃ©pertoire
myloader --directory=/backups/20251213
# Alias : -d /backups/20251213

# Restaurer une base spÃ©cifique
myloader -d /backups/20251213 \
  --source-db=production_db \
  --database=restored_db
# Restaure production_db vers restored_db (renommage)

# Restaurer des tables spÃ©cifiques
myloader -d /backups/20251213 \
  --source-db=production_db \
  --regex='production_db\.(users|orders)'
```

---

### Options de parallÃ©lisme

```bash
# Nombre de threads (dÃ©faut : 4)
myloader -d /backups/20251213 --threads=16
# Alias : -t 16

# Pour restauration ultra-rapide :
# threads = (nombre de cores CPU) * 1.5
# Ex : 12 cores â†’ 16-18 threads
```

ğŸ’¡ **Restauration parallÃ¨le** : Avec 16 threads, myloader peut restaurer 16 tables/chunks simultanÃ©ment.

---

### Options de performance

```bash
# DÃ©sactiver les index pendant l'import
myloader -d /backups/20251213 \
  --innodb-optimize-keys
# Les index secondaires sont reconstruits Ã  la fin (bien plus rapide)

# Taille du buffer d'insertion
myloader -d /backups/20251213 \
  --max-allowed-packet=1G

# Nombre de queries par transaction
myloader -d /backups/20251213 \
  --queries-per-transaction=1000
```

---

### Options de sÃ©curitÃ©

```bash
# Activer le mode "overwrite" (Ã©crase tables existantes)
myloader -d /backups/20251213 \
  --overwrite-tables

# Activer les contraintes d'intÃ©gritÃ©
myloader -d /backups/20251213 \
  --enable-setnames
```

---

### Options de monitoring

```bash
# VerbositÃ© (0-3)
myloader -d /backups/20251213 --verbose=3
# Alias : -v 3

# Logs dans un fichier
myloader -d /backups/20251213 \
  --logfile=/var/log/myloader_restore.log
```

---

## Commande myloader complÃ¨te (production-ready)

```bash
#!/bin/bash
# Restauration production avec myloader

BACKUP_DIR="/backups/mariadb/20251213_143045"
LOG_FILE="/var/log/myloader_$(date +%Y%m%d_%H%M%S).log"

# PrÃ©-checks
if [ ! -d "$BACKUP_DIR" ]; then
    echo "âŒ Backup directory not found: $BACKUP_DIR"
    exit 1
fi

if [ ! -f "$BACKUP_DIR/metadata" ]; then
    echo "âŒ Metadata file missing"
    exit 1
fi

echo "ğŸ“¦ Restoring from: $BACKUP_DIR"
echo "ğŸ“Š Metadata:"
cat "$BACKUP_DIR/metadata"

# Restauration
myloader \
  --directory="$BACKUP_DIR" \
  --threads=16 \
  --innodb-optimize-keys \
  --max-allowed-packet=1G \
  --queries-per-transaction=1000 \
  --overwrite-tables \
  --verbose=3 \
  --logfile="$LOG_FILE"

if [ $? -eq 0 ]; then
    echo "âœ… Restore completed successfully"
    echo "ğŸ“„ Logs: $LOG_FILE"
else
    echo "âŒ Restore FAILED"
    echo "ğŸ“„ Check logs: $LOG_FILE"
    exit 1
fi
```

---

## Restauration sÃ©lective : Le grand avantage

### Restaurer une seule table

```bash
# Restaurer uniquement la table 'users'
myloader -d /backups/20251213 \
  --source-db=production_db \
  --regex='production_db\.users'

# Fichiers restaurÃ©s :
# - production_db.users-schema.sql
# - production_db.users.sql (ou .00000.sql, .00001.sql... si chunkÃ©)
```

### Restaurer dans une base diffÃ©rente

```bash
# Cloner production_db vers test_db
myloader -d /backups/20251213 \
  --source-db=production_db \
  --database=test_db
```

### Restaurer seulement certaines tables

```bash
# Restaurer users, orders et products uniquement
myloader -d /backups/20251213 \
  --source-db=production_db \
  --regex='production_db\.(users|orders|products)'
```

### Restaurer un chunk spÃ©cifique

```bash
# Restaurer seulement un sous-ensemble de donnÃ©es
# (ex: dernier chunk d'une table pour debug)
myloader -d /backups/20251213 \
  --source-db=production_db \
  --regex='production_db\.orders\.00099'
# Restaure uniquement orders.00099.sql (derniÃ¨res lignes)
```

---

## Cas d'usage pratiques en production

### 1. Backup quotidien complet avec rÃ©tention

```bash
#!/bin/bash
# /opt/scripts/mydumper_daily_backup.sh

set -euo pipefail

# Configuration
DB="production_db"
BACKUP_ROOT="/backups/mariadb"
RETENTION_DAYS=7
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="$BACKUP_ROOT/$DATE"

# CrÃ©ation du rÃ©pertoire
mkdir -p "$BACKUP_DIR"

# Backup
echo "[$(date)] Starting backup..."

mydumper \
  --database="$DB" \
  --outputdir="$BACKUP_DIR" \
  --threads=8 \
  --chunk-filesize=100 \
  --compress-protocol=zstd \
  --trx-consistency-only \
  --triggers \
  --routines \
  --events \
  --binlog-snapshot \
  --flush-logs \
  --verbose=2

BACKUP_STATUS=$?

if [ $BACKUP_STATUS -ne 0 ]; then
    echo "[$(date)] âŒ Backup FAILED"
    exit 1
fi

echo "[$(date)] âœ… Backup completed: $BACKUP_DIR"

# Statistiques
BACKUP_SIZE=$(du -sh "$BACKUP_DIR" | cut -f1)
FILE_COUNT=$(find "$BACKUP_DIR" -type f | wc -l)

echo "ğŸ“Š Backup stats:"
echo "  - Size: $BACKUP_SIZE"
echo "  - Files: $FILE_COUNT"
echo "  - Location: $BACKUP_DIR"

# Afficher position binlog
echo "ğŸ“ Binlog position:"
grep -A 3 "SHOW MASTER STATUS" "$BACKUP_DIR/metadata"

# Nettoyage des backups anciens
echo "[$(date)] Cleaning old backups (retention: $RETENTION_DAYS days)..."
find "$BACKUP_ROOT" -maxdepth 1 -type d -mtime +$RETENTION_DAYS \
  -exec rm -rf {} \;

# CrÃ©er un lien symbolique 'latest'
rm -f "$BACKUP_ROOT/latest"
ln -s "$BACKUP_DIR" "$BACKUP_ROOT/latest"

echo "[$(date)] ğŸ‰ Backup process completed"
```

---

### 2. Backup parallÃ¨le de plusieurs bases

```bash
#!/bin/bash
# Backup de toutes les bases (sauf systÃ¨me)

BACKUP_DIR="/backups/mariadb/all_databases_$(date +%Y%m%d)"

mydumper \
  --outputdir="$BACKUP_DIR" \
  --threads=16 \
  --chunk-filesize=100 \
  --compress-protocol=zstd \
  --trx-consistency-only \
  --triggers \
  --routines \
  --events \
  --binlog-snapshot \
  --flush-logs

# Le rÃ©sultat :
# backup_dir/
# â”œâ”€â”€ metadata
# â”œâ”€â”€ db1-schema-create.sql
# â”œâ”€â”€ db1.table1-schema.sql
# â”œâ”€â”€ db1.table1.sql.zst
# â”œâ”€â”€ db2-schema-create.sql
# â”œâ”€â”€ db2.table1-schema.sql
# â”œâ”€â”€ db2.table1.sql.zst
# â””â”€â”€ ...
```

---

### 3. Clone rapide d'une base pour dÃ©veloppement

```bash
#!/bin/bash
# Clone production_db â†’ dev_db

BACKUP_DIR="/tmp/production_clone_$(date +%s)"

# 1. Dump de production
echo "ğŸ“¦ Dumping production_db..."
mydumper \
  --database=production_db \
  --outputdir="$BACKUP_DIR" \
  --threads=8 \
  --compress-protocol=zstd \
  --trx-consistency-only

# 2. Restauration vers dev_db
echo "â™»ï¸  Restoring to dev_db..."
myloader \
  --directory="$BACKUP_DIR" \
  --source-db=production_db \
  --database=dev_db \
  --threads=16 \
  --overwrite-tables

# 3. Anonymisation des donnÃ©es sensibles
echo "ğŸ”’ Anonymizing sensitive data..."
mariadb dev_db <<EOF
UPDATE users SET email = CONCAT('user', id, '@example.com');
UPDATE users SET phone = CONCAT('555-', LPAD(id, 7, '0'));
UPDATE customers SET credit_card = '************1234';
EOF

# 4. Nettoyage
rm -rf "$BACKUP_DIR"

echo "âœ… Clone completed: production_db â†’ dev_db (anonymized)"
```

---

### 4. Restauration sÃ©lective de table corrompue

```bash
#!/bin/bash
# ScÃ©nario : La table 'orders' est corrompue, restaurer uniquement celle-ci

LATEST_BACKUP="/backups/mariadb/latest"

echo "âš ï¸  Corrupted table detected: orders"
echo "ğŸ“¦ Restoring from latest backup: $LATEST_BACKUP"

# 1. VÃ©rifier que le backup contient la table
if ! ls "$LATEST_BACKUP"/production_db.orders* 1>/dev/null 2>&1; then
    echo "âŒ Table 'orders' not found in backup"
    exit 1
fi

# 2. Drop de la table corrompue
echo "ğŸ—‘ï¸  Dropping corrupted table..."
mariadb production_db -e "DROP TABLE IF EXISTS orders;"

# 3. Restauration sÃ©lective
echo "â™»ï¸  Restoring 'orders' table..."
myloader \
  --directory="$LATEST_BACKUP" \
  --source-db=production_db \
  --database=production_db \
  --regex='production_db\.orders' \
  --threads=8 \
  --innodb-optimize-keys

if [ $? -eq 0 ]; then
    echo "âœ… Table 'orders' restored successfully"
    
    # VÃ©rifier l'intÃ©gritÃ©
    ROWS=$(mariadb production_db -N -e "SELECT COUNT(*) FROM orders;")
    echo "ğŸ“Š Restored rows: $ROWS"
else
    echo "âŒ Restore FAILED"
    exit 1
fi
```

---

### 5. Backup avec validation automatique

```bash
#!/bin/bash
# Backup avec tests de validitÃ© automatiques

BACKUP_DIR="/backups/mariadb/$(date +%Y%m%d_%H%M%S)"

# 1. Backup
mydumper \
  --database=production_db \
  --outputdir="$BACKUP_DIR" \
  --threads=8 \
  --compress-protocol=zstd \
  --trx-consistency-only \
  --triggers \
  --routines \
  --events \
  --binlog-snapshot

BACKUP_STATUS=$?

if [ $BACKUP_STATUS -ne 0 ]; then
    echo "âŒ Backup failed"
    exit 1
fi

# 2. Validation : VÃ©rifier les fichiers essentiels
EXPECTED_FILES=(
    "metadata"
    "production_db-schema-create.sql"
    "production_db.users-schema.sql"
    "production_db.orders-schema.sql"
)

for file in "${EXPECTED_FILES[@]}"; do
    if [ ! -f "$BACKUP_DIR/$file" ]; then
        echo "âŒ Missing file: $file"
        exit 1
    fi
done

echo "âœ… All expected files present"

# 3. Validation : VÃ©rifier l'intÃ©gritÃ© de la compression
for zstd_file in "$BACKUP_DIR"/*.sql.zst; do
    if ! zstd -t "$zstd_file" 2>/dev/null; then
        echo "âŒ Corrupted compressed file: $zstd_file"
        exit 1
    fi
done

echo "âœ… All compressed files are valid"

# 4. Validation : Compter les fichiers de donnÃ©es
SQL_FILES=$(find "$BACKUP_DIR" -name "*.sql*" -type f | wc -l)
echo "ğŸ“Š Backup contains $SQL_FILES SQL files"

# 5. Test de restauration sur base temporaire (optionnel, lourd)
if [ "${VALIDATE_RESTORE:-false}" = "true" ]; then
    echo "ğŸ§ª Testing restore on temporary database..."
    
    mariadb -e "CREATE DATABASE IF NOT EXISTS test_restore_$(date +%s);"
    
    myloader \
      --directory="$BACKUP_DIR" \
      --source-db=production_db \
      --database=test_restore_$(date +%s) \
      --threads=4 \
      --verbose=0
    
    if [ $? -eq 0 ]; then
        echo "âœ… Test restore successful"
        mariadb -e "DROP DATABASE test_restore_$(date +%s);"
    else
        echo "âŒ Test restore FAILED"
        exit 1
    fi
fi

echo "âœ… Backup validated: $BACKUP_DIR"
```

---

## Performances et benchmarks

### Comparaison mydumper vs mysqldump

**Configuration test** :
- Base : 100 GB (500 tables, 1 milliard de lignes)
- Serveur : 16 cores, 64 GB RAM, NVMe
- RÃ©seau : Local

| Outil | Threads | Backup | Restauration | Taille (zstd) |
|-------|---------|--------|--------------|---------------|
| mysqldump | 1 | 85 min | 210 min | 12 GB |
| mydumper | 4 | 28 min | 45 min | 11.8 GB |
| mydumper | 8 | 16 min | 24 min | 11.8 GB |
| mydumper | 16 | 11 min | 15 min | 11.8 GB |

ğŸ’¡ **RÃ©sultats** :
- **Backup** : mydumper (16 threads) = **7.7x plus rapide**
- **Restauration** : myloader (16 threads) = **14x plus rapide**

### Impact du chunking

**Table test** : orders (50 GB, 200M lignes)

| Chunk size | Fichiers | Threads | Backup | Restauration |
|------------|----------|---------|--------|--------------|
| Pas de chunk | 1 | 1 | 45 min | 120 min |
| 1 GB | 50 | 8 | 12 min | 28 min |
| 500 MB | 100 | 16 | 9 min | 18 min |
| 100 MB | 500 | 16 | 8 min | 16 min |

ğŸ’¡ **Sweet spot** : 100-500 MB par chunk avec 8-16 threads.

### Impact de la compression

**Base test** : 100 GB non compressÃ©e

| Compression | Taille | Temps backup | Temps restauration |
|-------------|--------|--------------|-------------------|
| Aucune | 100 GB | 8 min | 12 min |
| gzip -6 | 11 GB | 14 min | 18 min |
| gzip -9 | 10 GB | 22 min | 18 min |
| zstd -3 | 11.5 GB | 9 min | 13 min |
| zstd -9 | 10.5 GB | 13 min | 14 min |

âœ… **Recommandation** : zstd -3 (excellent compromis)

---

## Comparaison avec Mariabackup

| CritÃ¨re | mydumper | Mariabackup |
|---------|----------|-------------|
| **Type** | Logique | Physique |
| **Vitesse backup** | 500-2000 MB/s | 1000-3000 MB/s |
| **Vitesse restauration** | 1000-5000 MB/s | 2000-8000 MB/s |
| **PortabilitÃ©** | âœ… Excellente | âš ï¸ MÃªme version/config |
| **Restauration sÃ©lective** | âœ… Table par table | âŒ Tout ou rien |
| **Taille (compressÃ©)** | ~10-15% original | ~10-15% original |
| **CohÃ©rence** | âœ… Snapshot global | âœ… Natif |
| **Downtime** | âŒ Aucun | âŒ Aucun |
| **PITR** | âœ… Via binlog | âœ… Via binlog |
| **Cas d'usage idÃ©al** | < 500 GB, restore sÃ©lectif | > 500 GB, full restore |

ğŸ’¡ **StratÃ©gie hybride** :
- **Full backup hebdo** : Mariabackup (physique, rapide)
- **Backup quotidien** : mydumper (logique, granulaire)
- **Binlogs** : Pour PITR entre deux backups

---

## Automatisation avancÃ©e

### Avec systemd timer

```ini
# /etc/systemd/system/mydumper-backup.service
[Unit]
Description=mydumper Daily Backup
After=mariadb.service

[Service]
Type=oneshot
User=backup
ExecStart=/opt/scripts/mydumper_daily_backup.sh
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```ini
# /etc/systemd/system/mydumper-backup.timer
[Unit]
Description=mydumper Daily Backup Timer
Requires=mydumper-backup.service

[Timer]
OnCalendar=daily
OnCalendar=02:00
Persistent=true

[Install]
WantedBy=timers.target
```

```bash
# Activer
sudo systemctl enable mydumper-backup.timer
sudo systemctl start mydumper-backup.timer

# VÃ©rifier
sudo systemctl list-timers mydumper-backup.timer
```

---

### Avec monitoring Prometheus

```bash
#!/bin/bash
# Script avec mÃ©triques Prometheus

PROMETHEUS_PUSHGATEWAY="http://pushgateway:9091"

START_TIME=$(date +%s)

# Backup
mydumper -B production_db -o /backups/latest --threads=8

BACKUP_STATUS=$?
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

# Envoyer les mÃ©triques
if [ $BACKUP_STATUS -eq 0 ]; then
    SUCCESS=1
    BACKUP_SIZE=$(du -sb /backups/latest | cut -f1)
else
    SUCCESS=0
    BACKUP_SIZE=0
fi

cat <<EOF | curl --data-binary @- "$PROMETHEUS_PUSHGATEWAY/metrics/job/mydumper/instance/$(hostname)"
# HELP mydumper_backup_duration_seconds Duration of the backup
# TYPE mydumper_backup_duration_seconds gauge
mydumper_backup_duration_seconds $DURATION

# HELP mydumper_backup_size_bytes Size of the backup
# TYPE mydumper_backup_size_bytes gauge
mydumper_backup_size_bytes $BACKUP_SIZE

# HELP mydumper_backup_success Success status (1=success, 0=failure)
# TYPE mydumper_backup_success gauge
mydumper_backup_success $SUCCESS

# HELP mydumper_backup_timestamp_seconds Timestamp of last backup
# TYPE mydumper_backup_timestamp_seconds gauge
mydumper_backup_timestamp_seconds $END_TIME
EOF
```

**Alertes Prometheus** :

```yaml
# prometheus-alerts.yml
groups:
  - name: mydumper
    interval: 5m
    rules:
      - alert: MydumperBackupFailed
        expr: mydumper_backup_success == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "mydumper backup failed on {{ $labels.instance }}"
          
      - alert: MydumperBackupTooOld
        expr: time() - mydumper_backup_timestamp_seconds > 86400
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "No mydumper backup in last 24h on {{ $labels.instance }}"
          
      - alert: MydumperBackupTooSlow
        expr: mydumper_backup_duration_seconds > 3600
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "mydumper backup took > 1h on {{ $labels.instance }}"
```

---

## IntÃ©gration cloud-native

### Sauvegarde vers S3 (AWS/MinIO)

```bash
#!/bin/bash
# Backup mydumper â†’ S3

BACKUP_DIR="/tmp/mydumper_$(date +%s)"
S3_BUCKET="s3://my-backups/mariadb"
DATE=$(date +%Y%m%d_%H%M%S)

# 1. Backup local
mydumper -B production_db -o "$BACKUP_DIR" --threads=8 --compress-protocol=zstd

# 2. Upload vers S3
aws s3 sync "$BACKUP_DIR" "$S3_BUCKET/$DATE/" --storage-class STANDARD_IA

# 3. Nettoyage local
rm -rf "$BACKUP_DIR"

# 4. RÃ©tention S3 (supprimer backups > 30 jours)
aws s3 ls "$S3_BUCKET/" | while read -r line; do
    FOLDER_DATE=$(echo "$line" | awk '{print $2}' | tr -d '/')
    FOLDER_AGE=$(( ($(date +%s) - $(date -d "$FOLDER_DATE" +%s)) / 86400 ))
    
    if [ "$FOLDER_AGE" -gt 30 ]; then
        echo "Deleting old backup: $FOLDER_DATE"
        aws s3 rm "$S3_BUCKET/$FOLDER_DATE/" --recursive
    fi
done

echo "âœ… Backup uploaded to $S3_BUCKET/$DATE/"
```

---

### Restauration depuis S3

```bash
#!/bin/bash
# Restauration mydumper depuis S3

S3_BACKUP="s3://my-backups/mariadb/20251213_143000"
LOCAL_DIR="/tmp/restore_$(date +%s)"

# 1. Download depuis S3
aws s3 sync "$S3_BACKUP" "$LOCAL_DIR"

# 2. VÃ©rifier l'intÃ©gritÃ©
if [ ! -f "$LOCAL_DIR/metadata" ]; then
    echo "âŒ Invalid backup (metadata missing)"
    exit 1
fi

# 3. Restauration
myloader -d "$LOCAL_DIR" --threads=16 --overwrite-tables

# 4. Nettoyage
rm -rf "$LOCAL_DIR"

echo "âœ… Restore completed from S3"
```

---

## PiÃ¨ges et limitations

### PiÃ¨ge 1 : Pas de support pour vues/triggers cross-database

```bash
# âŒ PROBLÃˆME : Si une vue dÃ©pend d'une table d'une autre base
CREATE VIEW db1.summary AS SELECT * FROM db2.source;

# mydumper ne gÃ¨re pas bien ces dÃ©pendances inter-bases
# Solution : Utiliser mysqldump pour ces cas, ou documenter manuellement
```

---

### PiÃ¨ge 2 : Chunking avec tables sans PRIMARY KEY

```bash
# âš ï¸ ATTENTION : Chunking impossible sans clÃ© primaire

# VÃ©rifier les tables sans PK
SELECT table_schema, table_name 
FROM information_schema.tables t
WHERE table_schema NOT IN ('information_schema', 'performance_schema', 'mysql')
AND NOT EXISTS (
    SELECT 1 FROM information_schema.key_column_usage k
    WHERE k.table_schema = t.table_schema
    AND k.table_name = t.table_name
    AND k.constraint_name = 'PRIMARY'
);

# Solution : Ajouter une PK ou Ã©viter --chunk-filesize pour ces tables
```

---

### PiÃ¨ge 3 : Restauration dÃ©sordonnÃ©e (dÃ©pendances FK)

```bash
# âŒ PROBLÃˆME : myloader restaure en parallÃ¨le sans ordre
# Si table 'orders' rÃ©fÃ©rence 'customers' via FK, peut Ã©chouer

# Solution 1 : DÃ©sactiver temporairement les FK
myloader -d /backups/latest --threads=16 \
  --enable-setnames \
  --overwrite-tables

# Dans MariaDB, avant restore :
SET FOREIGN_KEY_CHECKS=0;

# Solution 2 : mydumper inclut dÃ©jÃ  Ã§a dans les dumps
# VÃ©rifier production_db-schema-create.sql :
# SET FOREIGN_KEY_CHECKS=0;
```

---

### PiÃ¨ge 4 : MÃ©moire insuffisante avec trop de threads

```bash
# âš ï¸ SymptÃ´me : OOM (Out of Memory) pendant restore

# Cause : myloader charge plusieurs chunks en RAM simultanÃ©ment
# 16 threads * 100 MB par chunk = 1.6 GB minimum

# Solution : RÃ©duire les threads si RAM limitÃ©e
myloader -d /backups/latest --threads=4  # Au lieu de 16
```

---

### PiÃ¨ge 5 : CaractÃ¨res spÃ©ciaux dans les noms de tables

```bash
# âŒ Tables avec caractÃ¨res spÃ©ciaux posent problÃ¨me

# Exemple : table nommÃ©e "my-special-table"
# mydumper crÃ©Ã© : production_db.my-special-table.sql
# Le tiret cause des problÃ¨mes de parsing

# Solution : Ã‰viter les caractÃ¨res spÃ©ciaux, utiliser underscores
ALTER TABLE `my-special-table` RENAME TO `my_special_table`;
```

---

## âœ… Points clÃ©s Ã  retenir

- **mydumper** est un outil de backup logique **multi-threadÃ©** offrant des performances jusqu'Ã  **10x supÃ©rieures** Ã  mysqldump
- **ParallÃ©lisme** : Utilisez 4-16 threads selon votre matÃ©riel (HDD: 2-4, SSD: 4-8, NVMe: 8-16)
- **Chunking** : Option `--chunk-filesize=100` divise les grandes tables en morceaux parallÃ©lisables
- **Compression zstd** : Meilleur compromis vitesse/ratio (`--compress-protocol=zstd`)
- **Restauration sÃ©lective** : 1 fichier = 1 table permet de restaurer prÃ©cisÃ©ment ce dont vous avez besoin
- **Structure de sortie** : RÃ©pertoire organisÃ© (metadata, schemas, donnÃ©es sÃ©parÃ©es)
- **myloader** : Restauration parallÃ¨le avec `--innodb-optimize-keys` pour performances maximales
- **PITR** : Option `--binlog-snapshot` enregistre position binlog dans metadata
- **Cas d'usage idÃ©al** : Bases 50-500 GB, besoin de restauration sÃ©lective frÃ©quente
- **ComplÃ©mentaire Ã  Mariabackup** : mydumper pour granularitÃ©, Mariabackup pour vitesse pure

---

## ğŸ”— Ressources et rÃ©fÃ©rences

- ğŸ“– [mydumper Documentation officielle](https://github.com/mydumper/mydumper)
- ğŸ“– [mydumper Wiki](https://github.com/mydumper/mydumper/wiki)
- ğŸ› ï¸ [mydumper GitHub Repository](https://github.com/mydumper/mydumper)
- ğŸ“Š [Benchmark: mydumper vs mysqldump](https://www.percona.com/blog/mydumper-benchmark/)
- ğŸ“– [Best Practices for mydumper](https://mariadb.com/kb/en/mydumper/)
- ğŸ¥ [mydumper Webinar - Percona](https://www.percona.com/resources/videos/mydumper-myloader)

---

## â¡ï¸ Section suivante

**12.3 Sauvegarde physique (Mariabackup)** : Backups au niveau fichiers pour performances ultimes, support incrÃ©mental, et nouveautÃ© MariaDB 11.8 avec BACKUP STAGE pour cohÃ©rence optimale.

â­ï¸ [Sauvegarde physique (Mariabackup)](/12-sauvegarde-restauration/03-sauvegarde-physique-mariabackup.md)
