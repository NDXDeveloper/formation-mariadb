ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 12.7 Tests de restauration et plan de reprise (PRA)

> **Niveau** : AvancÃ©  
> **DurÃ©e estimÃ©e** : 4-5 heures  
> **PrÃ©requis** : Sections 12.1-12.6, Gestion de crise, ComprÃ©hension des RTO/RPO

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- **Concevoir** une mÃ©thodologie de tests de restauration exhaustive
- **Planifier** des exercices de simulation (tabletop et live)
- **Documenter** un Plan de Reprise d'ActivitÃ© (PRA) complet
- **DÃ©finir** des scÃ©narios d'incident et arbres de dÃ©cision
- **Organiser** la communication de crise entre Ã©quipes
- **Mesurer** et amÃ©liorer les temps de restauration (RTO)
- **Valider** rÃ©guliÃ¨rement la viabilitÃ© des procÃ©dures
- **Former** les Ã©quipes aux situations d'urgence

---

## Introduction

Le test de restauration est **la seule preuve** qu'un backup est exploitable. Sans tests rÃ©guliers, une organisation opÃ¨re avec une **fausse impression de sÃ©curitÃ©**.

### La dure rÃ©alitÃ© des backups non testÃ©s

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DÃ©couverte lors d'un incident rÃ©el           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Vendredi 15h : Panne disque serveur production      â”‚
â”‚  Vendredi 15h30: DÃ©but restauration depuis backup    â”‚
â”‚  Vendredi 17h : Ã‰chec restauration                   â”‚
â”‚                                                      â”‚
â”‚  Causes dÃ©couvertes :                                â”‚
â”‚  â”œâ”€ Binary logs manquants (rotation incorrecte)      â”‚
â”‚  â”œâ”€ Backup compressÃ© corrompu (jamais testÃ©)         â”‚
â”‚  â”œâ”€ ProcÃ©dure obsolÃ¨te (MySQLâ†’MariaDB migration)     â”‚
â”‚  â””â”€ Serveur restauration sous-dimensionnÃ©            â”‚
â”‚                                                      â”‚
â”‚  RÃ©sultat :                                          â”‚
â”‚  â”œâ”€ Downtime : 36 heures                             â”‚
â”‚  â”œâ”€ Perte donnÃ©es : 3 jours                          â”‚
â”‚  â”œâ”€ CoÃ»t : 2Mâ‚¬                                       â”‚
â”‚  â””â”€ RÃ©putation : Gravement affectÃ©e                  â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ’¡ **Principe fondamental** : *"Faire confiance Ã  un backup non testÃ©, c'est jouer Ã  la roulette russe avec ses donnÃ©es"*.

### Statistiques Ã©difiantes

D'aprÃ¨s Gartner et Veeam (2023-2024) :

```
Organisations testant leurs restaurations :
â”œâ”€ Jamais                : 34% âš ï¸
â”œâ”€ Annuellement          : 31%
â”œâ”€ Trimestriellement     : 23%
â””â”€ Mensuellement ou plus : 12%

Causes d'Ã©chec de restauration lors de tests :
â”œâ”€ ProcÃ©dure incorrecte/obsolÃ¨te    : 42%
â”œâ”€ Fichiers manquants/corrompus     : 28%
â”œâ”€ Infrastructure inadÃ©quate        : 18%
â””â”€ Erreur humaine (stress)          : 12%

Impact business d'un Ã©chec restauration :
â”œâ”€ < 1h downtime   : 100Kâ‚¬ - 500Kâ‚¬
â”œâ”€ 1-4h downtime   : 500Kâ‚¬ - 2Mâ‚¬
â”œâ”€ 4-24h downtime  : 2Mâ‚¬ - 10Mâ‚¬
â””â”€ > 24h downtime  : 10Mâ‚¬+ + fermeture possible
```

---

## MÃ©thodologie de tests de restauration

### Niveaux de tests

Les tests de restauration suivent une approche **pyramidale** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Pyramide des tests de restauration     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     â•±â–²                          â”‚
â”‚                    â•±  â•²                         â”‚
â”‚                   â•±    â•²                        â”‚
â”‚                  â•±      â•²   Niveau 4            â”‚
â”‚                 â•±â”€â”€â”€â”€â”€â”€â”€â”€â•²  Disaster Recovery   â”‚
â”‚                â•±          â•² (Annuel)            â”‚
â”‚               â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²                    â”‚
â”‚              â•±              â•² Niveau 3          â”‚
â”‚             â•±   Full DR      â•² PITR Complet     â”‚
â”‚            â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•² (Trimestriel)   â”‚
â”‚           â•±                    â•²                â”‚
â”‚          â•±  Niveau 2            â•²               â”‚
â”‚         â•±  Restauration Partielleâ•²              â”‚
â”‚        â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•² (Mensuel)   â”‚
â”‚       â•±                            â•²            â”‚
â”‚      â•±     Niveau 1                 â•²           â”‚
â”‚     â•±   Validation IntÃ©gritÃ©         â•²          â”‚
â”‚    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²         â”‚
â”‚   â•±         (Quotidien/Hebdo)          â•²        â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Niveau 1 : Validation d'intÃ©gritÃ© (Quotidien/Hebdomadaire)

**Objectif** : VÃ©rifier que les backups sont cohÃ©rents et non corrompus.

```bash
#!/bin/bash
# level1_integrity_check.sh

BACKUP_DIR="/backups/mariadb/full/latest"
LOG_FILE="/var/log/backup_validation.log"

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"
}

# Test 1: Fichiers prÃ©sents
log "Test 1: Checking files presence..."
required_files=(
  "xtrabackup_checkpoints"
  "xtrabackup_info"
  "xtrabackup_binlog_info"
)

for file in "${required_files[@]}"; do
  if [[ ! -f "$BACKUP_DIR/$file" ]]; then
    log "ERROR: Missing required file: $file"
    exit 1
  fi
done
log "âœ… All required files present"

# Test 2: Checksum validation
log "Test 2: Validating checksums..."
if [[ -f "$BACKUP_DIR.sha256" ]]; then
  cd "$(dirname "$BACKUP_DIR")" || exit 1
  if sha256sum -c "$(basename "$BACKUP_DIR").sha256"; then
    log "âœ… Checksums valid"
  else
    log "ERROR: Checksum validation failed"
    exit 1
  fi
else
  log "WARNING: No checksum file found"
fi

# Test 3: Compressed files integrity
log "Test 3: Testing compressed files..."
find "$BACKUP_DIR" -name "*.gz" -exec gzip -t {} \; || {
  log "ERROR: Corrupted compressed file found"
  exit 1
}
log "âœ… All compressed files valid"

# Test 4: Metadata validation
log "Test 4: Checking metadata..."
if grep -q "backup_type = full-backuped" "$BACKUP_DIR/xtrabackup_checkpoints"; then
  log "âœ… Metadata valid"
else
  log "ERROR: Invalid backup metadata"
  exit 1
fi

log "========================================="
log "Level 1 validation PASSED"
log "========================================="
```

#### Niveau 2 : Restauration partielle (Mensuelle)

**Objectif** : Restaurer une seule base ou table pour valider le processus.

```bash
#!/bin/bash
# level2_partial_restore.sh

BACKUP_FILE="/backups/mariadb/logical/latest.sql.gz"
TEST_DB="restore_test_$(date +%s)"

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a /var/log/restore_test.log
}

log "========================================="
log "Level 2: Partial Restore Test"
log "========================================="

# Docker-based isolated test
log "Creating isolated test environment..."
docker run -d --name restore_test \
  -e MARIADB_ROOT_PASSWORD=test \
  mariadb:11.8

sleep 15

# Extract and restore single database
log "Extracting test database..."
zcat "$BACKUP_FILE" | \
  sed -n "/^-- Current Database: \`myapp\`/,/^-- Current Database:/p" | \
  docker exec -i restore_test mariadb -ptest

# Validation queries
log "Running validation queries..."
USERS_COUNT=$(docker exec restore_test mariadb -ptest myapp -Nse "SELECT COUNT(*) FROM users;" 2>/dev/null || echo "0")
ORDERS_COUNT=$(docker exec restore_test mariadb -ptest myapp -Nse "SELECT COUNT(*) FROM orders;" 2>/dev/null || echo "0")

log "Users count: $USERS_COUNT"
log "Orders count: $ORDERS_COUNT"

# Cleanup
docker rm -f restore_test

if [[ $USERS_COUNT -gt 0 && $ORDERS_COUNT -gt 0 ]]; then
  log "âœ… Level 2 test PASSED"
  exit 0
else
  log "âŒ Level 2 test FAILED"
  exit 1
fi
```

#### Niveau 3 : PITR complet (Trimestrielle)

**Objectif** : Tester le Point-in-Time Recovery complet.

```bash
#!/bin/bash
# level3_pitr_test.sh

set -euo pipefail

FULL_BACKUP="/backups/mariadb/full/20251213"
BINLOG_DIR="/backups/binlogs"
TARGET_TIME="2025-12-13 14:30:00"
TEST_DATADIR="/var/lib/mysql_pitr_test"

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a /var/log/pitr_test.log
}

log "========================================="
log "Level 3: PITR Test"
log "Target time: $TARGET_TIME"
log "========================================="

# Preparation
log "Preparing test environment..."
systemctl stop mariadb-test || true
rm -rf "$TEST_DATADIR"
mkdir -p "$TEST_DATADIR"

# Restore full backup
log "Restoring full backup..."
mariabackup --prepare --target-dir="$FULL_BACKUP"
mariabackup --copy-back --target-dir="$FULL_BACKUP" --datadir="$TEST_DATADIR"
chown -R mysql:mysql "$TEST_DATADIR"

# Start test instance on different port
log "Starting test MariaDB instance..."
mysqld_safe --datadir="$TEST_DATADIR" --port=3307 --socket=/tmp/mysql_test.sock &
MYSQLD_PID=$!
sleep 10

# Apply binary logs
log "Applying binary logs until $TARGET_TIME..."
START_POS=$(cat "$FULL_BACKUP/xtrabackup_binlog_info" | awk '{print $2}')
BINLOGS=$(ls "$BINLOG_DIR"/mariadb-bin.* | sort)

mysqlbinlog \
  --start-position="$START_POS" \
  --stop-datetime="$TARGET_TIME" \
  $BINLOGS | \
  mariadb --socket=/tmp/mysql_test.sock -u root

# Validation
log "Validating restored data..."
LAST_TIMESTAMP=$(mariadb --socket=/tmp/mysql_test.sock -u root -Nse \
  "SELECT MAX(created_at) FROM myapp.orders;")

log "Last order timestamp: $LAST_TIMESTAMP"
log "Target timestamp: $TARGET_TIME"

# Cleanup
kill $MYSQLD_PID
rm -rf "$TEST_DATADIR"

if [[ "$LAST_TIMESTAMP" < "$TARGET_TIME" ]]; then
  log "âœ… Level 3 PITR test PASSED"
  exit 0
else
  log "âŒ Level 3 PITR test FAILED"
  exit 1
fi
```

#### Niveau 4 : Disaster Recovery complet (Annuelle)

**Objectif** : Simuler une perte totale du datacenter.

Couvert dans la section suivante sur les exercices de simulation.

---

## Plan de Reprise d'ActivitÃ© (PRA)

### Structure du PRA

Un PRA complet pour MariaDB comprend les sections suivantes :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Plan de Reprise d'ActivitÃ©              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  1. INFORMATIONS GÃ‰NÃ‰RALES                       â”‚
â”‚     â”œâ”€ PÃ©rimÃ¨tre et objectifs                    â”‚
â”‚     â”œâ”€ RTO/RPO cibles                            â”‚
â”‚     â””â”€ Contacts d'urgence                        â”‚
â”‚                                                  â”‚
â”‚  2. INVENTAIRE DES RESSOURCES                    â”‚
â”‚     â”œâ”€ Serveurs et infrastructure                â”‚
â”‚     â”œâ”€ Backups et emplacements                   â”‚
â”‚     â””â”€ Documentation technique                   â”‚
â”‚                                                  â”‚
â”‚  3. SCÃ‰NARIOS D'INCIDENT                         â”‚
â”‚     â”œâ”€ Matrice de dÃ©cision                       â”‚
â”‚     â”œâ”€ Arbres de dÃ©cision                        â”‚
â”‚     â””â”€ ProcÃ©dures par scÃ©nario                   â”‚
â”‚                                                  â”‚
â”‚  4. PROCÃ‰DURES DE RESTAURATION                   â”‚
â”‚     â”œâ”€ Restauration complÃ¨te                     â”‚
â”‚     â”œâ”€ PITR                                      â”‚
â”‚     â””â”€ Basculement DR                            â”‚
â”‚                                                  â”‚
â”‚  5. COMMUNICATION DE CRISE                       â”‚
â”‚     â”œâ”€ Arbre d'escalade                          â”‚
â”‚     â”œâ”€ Templates de communication                â”‚
â”‚     â””â”€ Stakeholders                              â”‚
â”‚                                                  â”‚
â”‚  6. VALIDATION POST-INCIDENT                     â”‚
â”‚     â”œâ”€ Checklist de validation                   â”‚
â”‚     â”œâ”€ Tests fonctionnels                        â”‚
â”‚     â””â”€ Retour en production                      â”‚
â”‚                                                  â”‚
â”‚  7. TESTS ET EXERCICES                           â”‚
â”‚     â”œâ”€ Planning annuel                           â”‚
â”‚     â”œâ”€ ScÃ©narios de simulation                   â”‚
â”‚     â””â”€ AmÃ©lioration continue                     â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Template PRA

```markdown
# Plan de Reprise d'ActivitÃ© - MariaDB
**Version** : 2.0  
**DerniÃ¨re mise Ã  jour** : 2025-12-13  
**Prochain test** : 2026-01-15  
**Responsable** : John Doe (DBA Lead)

---

## 1. INFORMATIONS GÃ‰NÃ‰RALES

### 1.1 PÃ©rimÃ¨tre
- **Base de donnÃ©es** : Production MariaDB 11.8 (myapp)
- **Taille** : 2 To
- **Transactions/jour** : ~50 millions
- **Utilisateurs** : 500 000 actifs/jour

### 1.2 Objectifs RTO/RPO

| CriticitÃ© | RTO | RPO | Justification |
|-----------|-----|-----|---------------|
| CRITIQUE (transactions) | 2h | 5 min | Perte revenus 50Kâ‚¬/h |
| HAUTE (analytics) | 4h | 1h | Impact business modÃ©rÃ© |
| MOYENNE (archivage) | 24h | 24h | Pas d'impact immÃ©diat |

### 1.3 Contacts d'urgence

| RÃ´le | Nom | Mobile | Email | Escalade |
|------|-----|--------|-------|----------|
| DBA Lead | John Doe | +33 6 XX XX XX XX | john@example.com | - |
| DBA On-call | Jane Smith | +33 6 YY YY YY YY | jane@example.com | 15 min |
| SRE Lead | Bob Wilson | +33 6 ZZ ZZ ZZ ZZ | bob@example.com | 30 min |
| CTO | Alice Brown | +33 6 AA AA AA AA | alice@example.com | 1h |
| Vendor Support | MariaDB Corp | +1-XXX-XXX-XXXX | support@mariadb.com | Critical |

---

## 2. INVENTAIRE DES RESSOURCES

### 2.1 Infrastructure Production

| Composant | Hostname | IP | RÃ´le | Specs |
|-----------|----------|-----|------|-------|
| mariadb-prod-01 | db-prod-01.example.com | 10.0.1.10 | Primary | 64 vCPU, 256GB RAM, 4TB NVMe |
| mariadb-prod-02 | db-prod-02.example.com | 10.0.1.11 | Replica | 64 vCPU, 256GB RAM, 4TB NVMe |
| mariadb-backup | db-backup.example.com | 10.0.1.20 | Backup | 16 vCPU, 64GB RAM, 10TB HDD |

### 2.2 Emplacements des backups

| Type | Emplacement | RÃ©tention | AccÃ¨s |
|------|-------------|-----------|-------|
| Full | /backups/mariadb/full | 30 jours | NAS local + S3 |
| Incremental | /backups/mariadb/inc | 7 jours | NAS local + S3 |
| Binary logs | /backups/binlogs | 7 jours | NAS local + S3 |
| Archivage | s3://backups/archive | 365 jours | S3 Glacier |

### 2.3 Infrastructure DR (Site secondaire)

| Composant | Hostname | IP | Ã‰tat |
|-----------|----------|----|------|
| mariadb-dr-01 | db-dr-01.aws.example.com | 172.16.1.10 | Standby cold |
| mariadb-dr-02 | db-dr-02.aws.example.com | 172.16.1.11 | Standby cold |

---

## 3. SCÃ‰NARIOS D'INCIDENT

### 3.1 Matrice de dÃ©cision

| ScÃ©nario | ProbabilitÃ© | Impact | ProcÃ©dure | RTO cible |
|----------|-------------|--------|-----------|-----------|
| Corruption table unique | Ã‰levÃ©e | Faible | PITR partiel | 1h |
| DELETE sans WHERE | Moyenne | Moyen | PITR complet | 2h |
| Panne disque primary | Moyenne | Ã‰levÃ© | Failover replica | 15 min |
| Corruption base complÃ¨te | Faible | Critique | Full restore + binlogs | 4h |
| Datacenter dÃ©truit | TrÃ¨s faible | Catastrophique | DR site secondaire | 8h |
| Ransomware | Faible | Critique | Restore backup prÃ©-infection | 6h |

### 3.2 Arbre de dÃ©cision

```
Incident dÃ©tectÃ©
    â”‚
    â”œâ”€ Base accessible ? â”€â”€â”€â”€[OUI]â”€â”€â–º Corruption partielle
    â”‚                                      â”‚
    â”‚                                      â”œâ”€ Une table ?
    â”‚                                      â”‚   â””â”€â–º PITR partiel
    â”‚                                      â”‚
    â”‚                                      â””â”€ Plusieurs tables ?
    â”‚                                          â””â”€â–º PITR complet
    â”‚
    â””â”€ Base inaccessible ? â”€â”€[OUI]â”€â”€â–º Panne serveur
                                           â”‚
                                           â”œâ”€ Replica disponible ?
                                           â”‚   â””â”€[OUI]â”€â–º Failover
                                           â”‚
                                           â””â”€[NON]â”€â”€â–º Restauration DR
                                                        â”‚
                                                        â”œâ”€ Backup local OK ?
                                                        â”‚   â””â”€â–º Restore local
                                                        â”‚
                                                        â””â”€ Backup distant ?
                                                            â””â”€â–º Restore S3/DR
```

---

## 4. PROCÃ‰DURES DE RESTAURATION

### 4.1 ProcÃ©dure : Restauration complÃ¨te

**DurÃ©e estimÃ©e** : 4-6 heures (base 2 To)

**Ã‰tapes** :

1. **Activation du PRA** (T+0)
   ```bash
   # DÃ©clarer l'incident
   pagerduty trigger --service mariadb --severity critical \
     --description "Full database restore initiated"
   ```

2. **Provisionnement serveur restauration** (T+30min)
   ```bash
   # DÃ©marrer instance AWS prÃ©configurÃ©e
   aws ec2 start-instances --instance-ids i-xxxxx
   ```

3. **TÃ©lÃ©chargement backup depuis S3** (T+1h)
   ```bash
   aws s3 sync s3://backups/mariadb/full/latest /restore/
   ```

4. **Restauration Mariabackup** (T+3h)
   ```bash
   # Voir section 12.5.1 pour procÃ©dure dÃ©taillÃ©e
   mariabackup --prepare --target-dir=/restore/full
   mariabackup --copy-back --target-dir=/restore/full
   ```

5. **Application binary logs** (T+4h)
   ```bash
   # Voir section 12.5.2 pour PITR
   mysqlbinlog ... | mariadb
   ```

6. **Validation** (T+5h)
   - ExÃ©cuter checklist validation (section 12.5)
   - Tests applicatifs
   - Smoke tests

7. **Basculement production** (T+6h)
   - Update DNS/VIP
   - RedÃ©marrage applications
   - Monitoring intensif

### 4.2 ProcÃ©dure : Failover vers replica

**DurÃ©e estimÃ©e** : 15-30 minutes

**Ã‰tapes** :

1. **VÃ©rification Ã©tat replica** (T+0)
   ```sql
   SHOW SLAVE STATUS\G
   # VÃ©rifier Seconds_Behind_Master < 60
   ```

2. **Promotion replica** (T+5min)
   ```sql
   STOP SLAVE;
   RESET SLAVE ALL;
   SET GLOBAL read_only = 0;
   ```

3. **Basculement applicatif** (T+10min)
   ```bash
   # Update DNS ou VIP
   # RedÃ©marrage applications avec nouvelle config
   ```

4. **Validation** (T+15min)
   - Tests connexion
   - VÃ©rification transactions

---

## 5. COMMUNICATION DE CRISE

### 5.1 Arbre d'escalade

```
Incident dÃ©tectÃ© (DBA On-call)
    â”‚
    â”œâ”€ T+0 : Notification Ã©quipe technique (Slack #incidents)
    â”‚        â””â”€ DBA Lead, SRE Lead
    â”‚
    â”œâ”€ T+15min : Escalade management si non rÃ©solu
    â”‚            â””â”€ CTO, VP Engineering
    â”‚
    â”œâ”€ T+30min : Notification business stakeholders
    â”‚            â””â”€ CEO, VP Product, Customer Success
    â”‚
    â””â”€ T+1h : Communication externe si nÃ©cessaire
                â””â”€ Status page, Email clients
```

### 5.2 Template de communication

**Email aux stakeholders** :

```
Objet: [INCIDENT] Base de donnÃ©es MariaDB - Impact production

Statut: EN COURS
SÃ©vÃ©ritÃ©: CRITIQUE
DÃ©but: 2025-12-13 14:37 UTC
Impact: IndisponibilitÃ© complÃ¨te application

Actions en cours:
- Restauration backup en cours (ETA: 16:00 UTC)
- Ã‰quipe DBA mobilisÃ©e
- Communication client prÃ©parÃ©e

Prochaine mise Ã  jour: 15:30 UTC

Contact: John Doe (DBA Lead) - john@example.com
```

---

## 6. VALIDATION POST-INCIDENT

### 6.1 Checklist de validation technique

- [ ] Base de donnÃ©es dÃ©marrÃ©e sans erreur
- [ ] Toutes les bases prÃ©sentes (SHOW DATABASES)
- [ ] Toutes les tables accessibles
- [ ] Pas d'erreur dans error log
- [ ] CohÃ©rence rÃ©fÃ©rentielle validÃ©e (FK checks)
- [ ] Timestamp des donnÃ©es cohÃ©rent avec objectif RPO
- [ ] ProcÃ©dures stockÃ©es, triggers, events prÃ©sents
- [ ] Utilisateurs et privilÃ¨ges restaurÃ©s
- [ ] RÃ©plication configurÃ©e (si applicable)

### 6.2 Checklist de validation fonctionnelle

- [ ] Connexion application rÃ©ussie
- [ ] Login utilisateur fonctionnel
- [ ] CrÃ©ation de commande test OK
- [ ] Consultation donnÃ©es existantes OK
- [ ] Performance acceptable (temps rÃ©ponse < 100ms)
- [ ] Batch nocturnes validÃ©s
- [ ] Exports/rapports fonctionnels

### 6.3 CritÃ¨res de retour en production

| CritÃ¨re | Seuil | Validation |
|---------|-------|------------|
| Uptime | > 30 min | â–¡ |
| Erreurs logs | 0 ERROR | â–¡ |
| Latency p95 | < 50ms | â–¡ |
| Tests fonctionnels | 100% pass | â–¡ |
| Approbation DBA Lead | Signature | â–¡ |
| Approbation CTO | Signature | â–¡ |

---

## 7. TESTS ET EXERCICES

### 7.1 Planning annuel

| Mois | Type de test | PÃ©rimÃ¨tre | Responsable |
|------|--------------|-----------|-------------|
| Janvier | Level 1 (intÃ©gritÃ©) | Tous backups | AutomatisÃ© |
| FÃ©vrier | Level 2 (partiel) | 1 base | DBA On-call |
| Mars | Level 3 (PITR) | Complet | DBA Lead |
| Avril | Level 1 | Tous backups | AutomatisÃ© |
| Mai | Tabletop exercise | Corruption | Ã‰quipe complÃ¨te |
| Juin | Level 2 | 1 base | DBA On-call |
| Juillet | Level 1 | Tous backups | AutomatisÃ© |
| AoÃ»t | Level 3 (PITR) | Complet | DBA Lead |
| Septembre | Exercice live | Failover | Ã‰quipe + Management |
| Octobre | Level 2 | 1 base | DBA On-call |
| Novembre | Level 1 | Tous backups | AutomatisÃ© |
| DÃ©cembre | Level 4 (DR complet) | Site secondaire | Toute organisation |

### 7.2 Post-mortem template

AprÃ¨s chaque incident (rÃ©el ou simulÃ©), documenter :

```markdown
# Post-Mortem Incident - [DATE]

## RÃ©sumÃ©
- **Incident** : [Description]
- **DurÃ©e** : [X heures]
- **Impact** : [Utilisateurs affectÃ©s, revenus]

## Timeline
- T+0 : [Ã‰vÃ©nement dÃ©clencheur]
- T+15min : [Action 1]
- ...

## Cause racine
[Analyse dÃ©taillÃ©e]

## Ce qui a bien fonctionnÃ©
- [Point positif 1]
- [Point positif 2]

## Ce qui doit Ãªtre amÃ©liorÃ©
- [Point d'amÃ©lioration 1]
- [Point d'amÃ©lioration 2]

## Actions correctives
| Action | Responsable | Ã‰chÃ©ance | Statut |
|--------|-------------|----------|--------|
| [Action 1] | [Nom] | [Date] | [ ] |
```

---

## Exercices de simulation

### Tabletop Exercise (Discussion guidÃ©e)

**DurÃ©e** : 2-3 heures  
**Participants** : DBA, SRE, Devs, Management  
**FrÃ©quence** : Trimestrielle

**DÃ©roulement** :

1. **Briefing** (15 min)
   - Rappel objectifs
   - PrÃ©sentation scÃ©nario

2. **ScÃ©nario** (90 min)
   ```
   "Lundi 14h37 : Alerte monitoring - Primary database inaccessible.
    Cause : Corruption filesystem suite mise Ã  jour kernel.
    Replica en lag de 2h (problÃ¨me rÃ©seau).
    Dernier backup complet : Dimanche 2h.
    Binary logs disponibles jusqu'Ã  14h30.
   
   QUESTION : Quelle procÃ©dure appliquez-vous ?"
   ```

3. **Discussion** (60 min)
   - Chaque participant expose son raisonnement
   - Identification des gaps
   - Validation dÃ©cisions

4. **Debriefing** (30 min)
   - SynthÃ¨se des apprentissages
   - Actions correctives
   - Mise Ã  jour PRA

### Live Exercise (Exercice rÃ©el)

**DurÃ©e** : 1 journÃ©e  
**Participants** : Toute l'Ã©quipe tech + Management  
**FrÃ©quence** : Annuelle

**PrÃ©paration** (J-30) :
- [ ] Planification date (Ã©viter pÃ©riodes critiques)
- [ ] Communication stakeholders
- [ ] Provisionnement infrastructure test
- [ ] PrÃ©paration scÃ©nario dÃ©taillÃ©
- [ ] Briefing participants

**ExÃ©cution** (Jour J) :

```
08:00 - Briefing gÃ©nÃ©ral
08:30 - Injection scÃ©nario (ex: destruction volontaire base test)
08:31 - ChronomÃ¨tre dÃ©marre â†’ Ã‰quipe applique PRA
        â”œâ”€ Communication
        â”œâ”€ Diagnostic
        â”œâ”€ Restauration
        â””â”€ Validation
??:?? - Retour en production dÃ©clarÃ©
        â””â”€ ChronomÃ¨tre arrÃªtÃ© (mesure RTO rÃ©el)
```

**Debriefing** (J+1) :
- Analyse timeline dÃ©taillÃ©e
- Ã‰carts RTO prÃ©vu vs rÃ©el
- DifficultÃ©s rencontrÃ©es
- Mise Ã  jour procÃ©dures
- Actions correctives planifiÃ©es

---

## MÃ©triques et KPI

### Indicateurs de performance du PRA

| KPI | Cible | Mesure | FrÃ©quence |
|-----|-------|--------|-----------|
| Taux de succÃ¨s tests | 100% | Tests rÃ©ussis / Tests totaux | Mensuelle |
| RTO moyen | < 4h | Temps restauration mesurÃ© | Par test |
| RPO rÃ©el | < 15 min | Ã‰cart dernier binlog archivÃ© | Continue |
| Taux de disponibilitÃ© | 99.95% | Uptime / Temps total | Mensuelle |
| MTTR (Mean Time To Repair) | < 2h | Moyenne temps rÃ©paration | Trimestrielle |
| Taux conformitÃ© tests | 100% | Tests effectuÃ©s / Tests planifiÃ©s | Annuelle |

### Dashboard de suivi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PRA Dashboard (Grafana)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  ğŸ“Š Tests de restauration (12 derniers mois)    â”‚
â”‚  â”œâ”€ SuccÃ¨s : 47 / 48 (98%)                      â”‚
â”‚  â””â”€ Ã‰checs : 1 (Binlog manquant - corrigÃ©)      â”‚
â”‚                                                 â”‚
â”‚  â±ï¸ RTO moyen : 3h 22min                        â”‚
â”‚  ğŸ“ˆ Trend : -15% (amÃ©lioration)                 â”‚
â”‚                                                 â”‚
â”‚  ğŸ¯ RPO moyen : 8 minutes                       â”‚
â”‚  ğŸ“ˆ Trend : Stable                              â”‚
â”‚                                                 â”‚
â”‚  âœ… ConformitÃ© tests : 100%                     â”‚
â”‚  (48/48 tests planifiÃ©s effectuÃ©s)              â”‚
â”‚                                                 â”‚
â”‚  ğŸš¨ Incidents rÃ©els (12 mois) : 2               â”‚
â”‚  â”œâ”€ Corruption partielle (PITR 1h10)            â”‚
â”‚  â””â”€ Failover replica (12 min)                   â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AmÃ©lioration continue

### Cycle PDCA (Plan-Do-Check-Act)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cycle d'amÃ©lioration           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  1. PLAN (Planifier)                   â”‚
â”‚     â”œâ”€ Analyser incidents passÃ©s       â”‚
â”‚     â”œâ”€ Identifier amÃ©liorations        â”‚
â”‚     â””â”€ Planifier tests                 â”‚
â”‚          â”‚                             â”‚
â”‚          â–¼                             â”‚
â”‚  2. DO (Faire)                         â”‚
â”‚     â”œâ”€ ExÃ©cuter tests planifiÃ©s        â”‚
â”‚     â”œâ”€ Documenter procÃ©dures           â”‚
â”‚     â””â”€ Former les Ã©quipes              â”‚
â”‚          â”‚                             â”‚
â”‚          â–¼                             â”‚
â”‚  3. CHECK (VÃ©rifier)                   â”‚
â”‚     â”œâ”€ Analyser rÃ©sultats tests        â”‚
â”‚     â”œâ”€ Mesurer RTO/RPO                 â”‚
â”‚     â””â”€ Identifier gaps                 â”‚
â”‚          â”‚                             â”‚
â”‚          â–¼                             â”‚
â”‚  4. ACT (Agir)                         â”‚
â”‚     â”œâ”€ ImplÃ©menter corrections         â”‚
â”‚     â”œâ”€ Mettre Ã  jour PRA               â”‚
â”‚     â””â”€ Partager apprentissages         â”‚
â”‚          â”‚                             â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Retour PLAN  â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Revue annuelle du PRA

**Agenda type** (rÃ©union de 4h) :

1. **Bilan annÃ©e Ã©coulÃ©e** (60 min)
   - Incidents rÃ©els : timeline, impact, rÃ©solution
   - Tests effectuÃ©s : taux de rÃ©ussite, RTO/RPO
   - Ã‰volutions infrastructure

2. **Analyse des Ã©carts** (60 min)
   - RTO/RPO cibles vs rÃ©els
   - ProcÃ©dures obsolÃ¨tes identifiÃ©es
   - Points de blocage rÃ©currents

3. **Mise Ã  jour du PRA** (90 min)
   - Nouvelles procÃ©dures
   - Contacts Ã  jour
   - Infrastructure DR actualisÃ©e
   - ScÃ©narios ajoutÃ©s/modifiÃ©s

4. **Planning annÃ©e suivante** (30 min)
   - Calendrier tests
   - Exercices live
   - Formations Ã©quipe

---

## âœ… Points clÃ©s Ã  retenir

- **Tests obligatoires** : Un backup non testÃ© = Pas de backup (34% n'ont jamais testÃ© !)
- **MÃ©thodologie pyramidale** : 4 niveaux de tests (intÃ©gritÃ© â†’ DR complet)
- **PRA documentÃ©** : Document vivant, mis Ã  jour aprÃ¨s chaque incident/test
- **Exercices rÃ©guliers** : Tabletop trimestriel + Live annuel minimum
- **Communication claire** : Arbre d'escalade, templates, stakeholders identifiÃ©s
- **Validation rigoureuse** : Checklist technique + fonctionnelle avant retour prod
- **MÃ©triques suivies** : RTO/RPO mesurÃ©s, taux succÃ¨s tests, conformitÃ© planning
- **AmÃ©lioration continue** : Post-mortem systÃ©matique, cycle PDCA, revue annuelle
- **Formation Ã©quipes** : Tous doivent connaÃ®tre leur rÃ´le en cas d'incident
- **Infrastructure DR** : Site secondaire, cold standby minimum, procÃ©dures testÃ©es

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation et standards

- [ğŸ“– ITIL Disaster Recovery - AXELOS](https://www.axelos.com/certifications/itil-service-management)
- [ğŸ“– ISO 22301 - Business Continuity Management](https://www.iso.org/iso-22301-business-continuity.html)
- [ğŸ“– NIST Contingency Planning Guide - SP 800-34](https://csrc.nist.gov/publications/detail/sp/800-34/rev-1/final)

### Articles techniques

- [Disaster Recovery Best Practices - MariaDB](https://mariadb.com/resources/blog/disaster-recovery-best-practices/)
- [Testing Your Backups - Percona](https://www.percona.com/blog/testing-mysql-backups/)
- [DR Testing Methodologies - AWS](https://aws.amazon.com/disaster-recovery/)

---

## â¡ï¸ Section suivante

**[12.8 - Sauvegarde cloud-native](./08-sauvegarde-cloud-native.md)** : S3, Azure Blob Storage, Google Cloud Storage, Kubernetes VolumeSnapshots, architectures multi-cloud.

---


â­ï¸ [Sauvegarde cloud-native](/12-sauvegarde-restauration/08-sauvegarde-cloud-native.md)
