ðŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 20.8.1 CDC (Change Data Capture)

> **Niveau** : IntermÃ©diaire Ã  AvancÃ©  
> **DurÃ©e estimÃ©e** : 3 heures  
> **PrÃ©requis** : Chapitre 11 (Administration et Configuration), Chapitre 13 (RÃ©plication), Section 20.8 (Architectures Event-Driven)

## ðŸŽ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- Comprendre les principes et patterns du Change Data Capture (CDC)
- Configurer MariaDB pour exposer les changements via les binary logs
- ImplÃ©menter une solution CDC avec Debezium et Apache Kafka
- Concevoir des pipelines de donnÃ©es temps rÃ©el basÃ©s sur les Ã©vÃ©nements
- Appliquer les bonnes pratiques pour la fiabilitÃ© et les performances
- Identifier les cas d'usage appropriÃ©s pour le CDC

---

## Introduction

Le **Change Data Capture (CDC)** est une technique permettant de **capturer et propager les modifications** apportÃ©es aux donnÃ©es d'une base en temps rÃ©el. Au lieu de synchroniser pÃ©riodiquement des snapshots complets, le CDC **streame les changements incrÃ©mentaux** (INSERT, UPDATE, DELETE) vers des systÃ¨mes consommateurs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHANGE DATA CAPTURE (CDC)                           â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   APPLICATION   â”‚                                                   â”‚
â”‚  â”‚                 â”‚                                                   â”‚
â”‚  â”‚  INSERT/UPDATE  â”‚                                                   â”‚
â”‚  â”‚     DELETE      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚           â”‚                                                            â”‚
â”‚           â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      MariaDB 11.8                               â”‚   â”‚
â”‚  â”‚                                                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚   Tables    â”‚â”€â”€â”€â–¶â”‚           Binary Logs                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚   (InnoDB)  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”        â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ INS â”‚ â”‚ UPD â”‚ â”‚ DEL â”‚ â”‚ INS â”‚ ...    â”‚ â”‚   â”‚
â”‚  â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜        â”‚ â”‚   â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                           â”‚                            â”‚
â”‚                                           â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     CDC CONNECTOR                               â”‚   â”‚
â”‚  â”‚                   (Debezium, Maxwell, etc.)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â–¼              â–¼              â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚    Kafka      â”‚ â”‚  Data Lake    â”‚ â”‚   Search      â”‚                 â”‚
â”‚  â”‚   (Stream)    â”‚ â”‚  (S3/HDFS)    â”‚ â”‚(Elasticsearch)â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Le CDC transforme la base de donnÃ©es en une **source d'Ã©vÃ©nements**, permettant de construire des architectures **event-driven** rÃ©actives et dÃ©couplÃ©es.

---

## Principes fondamentaux

### Pourquoi le CDC ?

Les approches traditionnelles de synchronisation de donnÃ©es prÃ©sentent des limitations significatives :

| Approche | ProblÃ¨me | Impact |
|----------|----------|--------|
| **Batch ETL** | Latence Ã©levÃ©e (heures) | DonnÃ©es obsolÃ¨tes pour les dÃ©cisions |
| **Polling** | Charge sur la source, donnÃ©es manquÃ©es | Performance dÃ©gradÃ©e, incohÃ©rences |
| **Triggers applicatifs** | Couplage fort, complexitÃ© | Maintenance difficile, fragilitÃ© |
| **Double-write** | IncohÃ©rences potentielles | DonnÃ©es divergentes |

Le CDC rÃ©sout ces problÃ¨mes en :

1. **Capturant les changements Ã  la source** (binary logs)
2. **Propageant en quasi temps rÃ©el** (millisecondes Ã  secondes)
3. **Garantissant l'ordre et la complÃ©tude** (sÃ©quence de transactions)
4. **DÃ©couplant les systÃ¨mes** (producteur/consommateur indÃ©pendants)

### Types de CDC

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       TYPES DE CDC                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. LOG-BASED CDC (RecommandÃ©)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Write     â”‚â”€â”€â”€â”€â”€â–¶â”‚ Binary Log  â”‚â”€â”€â”€â”€â”€â–¶â”‚ CDC Reader  â”‚
   â”‚  (INSERT)   â”‚      â”‚  (binlog)   â”‚      â”‚ (Debezium)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   âœ… Impact minimal sur les performances
   âœ… Capture toutes les modifications
   âœ… Ordre transactionnel prÃ©servÃ©
   âœ… Pas de modification du schÃ©ma

2. TRIGGER-BASED CDC
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Write     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Trigger    â”‚â”€â”€â”€â”€â”€â–¶â”‚ Audit Table â”‚
   â”‚  (INSERT)   â”‚      â”‚ (AFTER INS) â”‚      â”‚  (shadow)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   âš ï¸ Overhead sur les Ã©critures
   âš ï¸ Maintenance des triggers
   âœ… Compatible toutes bases

3. QUERY-BASED CDC (Polling)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Table     â”‚â”€â”€â”€â”€â”€â–¶â”‚ SELECT WHEREâ”‚â”€â”€â”€â”€â”€â–¶â”‚  Consumer   â”‚
   â”‚  (source)   â”‚      â”‚ updated_at >â”‚      â”‚             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   âš ï¸ Latence (intervalle de polling)
   âš ï¸ Deletes non capturÃ©s
   âœ… Simple Ã  implÃ©menter
```

Le **Log-based CDC** est l'approche recommandÃ©e pour MariaDB, exploitant les **binary logs** natifs.

### Anatomie d'un Ã©vÃ©nement CDC

Un Ã©vÃ©nement CDC typique contient :

```json
{
  "schema": {...},
  "payload": {
    "before": {                    // Ã‰tat AVANT modification (UPDATE/DELETE)
      "id": 1001,
      "name": "John Doe",
      "email": "john@example.com",
      "status": "active"
    },
    "after": {                     // Ã‰tat APRÃˆS modification (INSERT/UPDATE)
      "id": 1001,
      "name": "John Doe",
      "email": "john.doe@newmail.com",  // Changement
      "status": "active"
    },
    "source": {
      "version": "2.5.0",
      "connector": "mysql",
      "name": "production-db",
      "ts_ms": 1703001234567,      // Timestamp du changement
      "db": "ecommerce",
      "table": "customers",
      "server_id": 1,
      "gtid": "0-1-12345",         // Global Transaction ID
      "file": "mariadb-bin.000042",
      "pos": 12345678
    },
    "op": "u",                     // c=create, u=update, d=delete, r=read(snapshot)
    "ts_ms": 1703001234890,        // Timestamp de processing
    "transaction": {
      "id": "file=mariadb-bin.000042,pos=12345000",
      "total_order": 1,
      "data_collection_order": 1
    }
  }
}
```

---

## Configuration de MariaDB pour le CDC

### PrÃ©requis : Binary Logging

Le CDC log-based nÃ©cessite que les **binary logs** soient activÃ©s et correctement configurÃ©s.

```ini
# /etc/mysql/mariadb.conf.d/50-cdc.cnf

[mysqld]
# ============================================================
# CONFIGURATION DES BINARY LOGS POUR CDC
# ============================================================

# Activation du binary logging
log_bin = /var/log/mysql/mariadb-bin
log_bin_index = /var/log/mysql/mariadb-bin.index

# Format ROW obligatoire pour CDC (capture les valeurs avant/aprÃ¨s)
binlog_format = ROW

# MÃ©tadonnÃ©es complÃ¨tes dans les Ã©vÃ©nements ROW
binlog_row_metadata = FULL  # ðŸ†• MariaDB 10.5+

# Image complÃ¨te pour before/after
binlog_row_image = FULL     # FULL | MINIMAL | NOBLOB

# Identification du serveur (unique dans le cluster)
server_id = 1

# RÃ©tention des logs (ajuster selon l'espace disque et les besoins)
expire_logs_days = 7
# Ou en heures (MariaDB 10.6+)
# binlog_expire_logs_seconds = 604800

# Taille maximale d'un fichier binlog
max_binlog_size = 1G

# Synchronisation pour durabilitÃ©
sync_binlog = 1

# GTID pour tracking prÃ©cis (recommandÃ©)
gtid_strict_mode = ON
gtid_domain_id = 1

# Checksum pour intÃ©gritÃ©
binlog_checksum = CRC32

# ============================================================
# OPTIMISATIONS POUR CDC Ã€ HAUT DÃ‰BIT
# ============================================================

# Cache pour les Ã©vÃ©nements binlog
binlog_cache_size = 1M
max_binlog_cache_size = 4G

# Groupe commit pour performances
binlog_commit_wait_count = 0
binlog_commit_wait_usec = 0

# Compression des Ã©vÃ©nements (MariaDB 10.2+)
# log_bin_compress = ON
# log_bin_compress_min_len = 256
```

### CrÃ©ation d'un utilisateur CDC dÃ©diÃ©

```sql
-- ============================================================
-- UTILISATEUR DÃ‰DIÃ‰ POUR LE CDC
-- ============================================================

-- CrÃ©ation de l'utilisateur
CREATE USER 'cdc_reader'@'%' 
    IDENTIFIED BY 'secure_cdc_password_2025';

-- PrivilÃ¨ges minimaux pour la lecture des binlogs
GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'cdc_reader'@'%';

-- Lecture des mÃ©tadonnÃ©es de schÃ©ma
GRANT SELECT ON *.* TO 'cdc_reader'@'%';

-- Pour Debezium : accÃ¨s aux tables systÃ¨me
GRANT SELECT ON mysql.* TO 'cdc_reader'@'%';

-- ðŸ†• MariaDB 11.8 : PrivilÃ¨ge spÃ©cifique pour SHOW BINLOG EVENTS
GRANT BINLOG MONITOR ON *.* TO 'cdc_reader'@'%';

-- Optionnel : Pour snapshot initial avec LOCK TABLES
-- GRANT LOCK TABLES ON ecommerce.* TO 'cdc_reader'@'%';

-- Application
FLUSH PRIVILEGES;

-- VÃ©rification
SHOW GRANTS FOR 'cdc_reader'@'%';
```

### VÃ©rification de la configuration

```sql
-- ============================================================
-- VÃ‰RIFICATION DE LA CONFIGURATION CDC
-- ============================================================

-- Status des binary logs
SHOW VARIABLES LIKE 'log_bin';
-- +---------------+-------+
-- | Variable_name | Value |
-- +---------------+-------+
-- | log_bin       | ON    |
-- +---------------+-------+

SHOW VARIABLES LIKE 'binlog_format';
-- +---------------+-------+
-- | Variable_name | Value |
-- +---------------+-------+
-- | binlog_format | ROW   |
-- +---------------+-------+

SHOW VARIABLES LIKE 'binlog_row_image';
-- +------------------+-------+
-- | Variable_name    | Value |
-- +------------------+-------+
-- | binlog_row_image | FULL  |
-- +------------------+-------+

-- Liste des fichiers binlog
SHOW BINARY LOGS;
-- +----------------------+-----------+
-- | Log_name             | File_size |
-- +----------------------+-----------+
-- | mariadb-bin.000001   | 123456789 |
-- | mariadb-bin.000002   |  87654321 |
-- +----------------------+-----------+

-- Position actuelle
SHOW MASTER STATUS;
-- +----------------------+----------+--------------+------------------+
-- | File                 | Position | Binlog_Do_DB | Binlog_Ignore_DB |
-- +----------------------+----------+--------------+------------------+
-- | mariadb-bin.000002   | 87654321 |              |                  |
-- +----------------------+----------+--------------+------------------+

-- GTID actuel
SELECT @@gtid_current_pos;
-- +--------------------+
-- | @@gtid_current_pos |
-- +--------------------+
-- | 1-1-12345          |
-- +--------------------+

-- Ã‰vÃ©nements rÃ©cents dans le binlog (debug)
SHOW BINLOG EVENTS IN 'mariadb-bin.000002' LIMIT 10;
```

---

## Architecture CDC avec Debezium

**Debezium** est la solution CDC open-source de rÃ©fÃ©rence, dÃ©veloppÃ©e par Red Hat. Elle fonctionne comme connecteur Kafka Connect et supporte nativement MariaDB/MySQL.

### Architecture de dÃ©ploiement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ARCHITECTURE CDC DEBEZIUM                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application   â”‚     â”‚              KAFKA CLUSTER                     â”‚
â”‚                 â”‚     â”‚                                                â”‚
â”‚  Writes         â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚           â”‚     â”‚  â”‚         Kafka Connect                   â”‚   â”‚
â”‚     â–¼           â”‚     â”‚  â”‚                                         â”‚   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚ â”‚  MariaDB    â”‚â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â–¶â”‚      Debezium MySQL Connector      â”‚ â”‚   â”‚
â”‚ â”‚    11.8     â”‚ â”‚     â”‚  â”‚  â”‚                                    â”‚ â”‚   â”‚
â”‚ â”‚             â”‚ â”‚     â”‚  â”‚  â”‚  â€¢ Reads binary logs               â”‚ â”‚   â”‚
â”‚ â”‚ Binary Logs â”‚ â”‚     â”‚  â”‚  â”‚  â€¢ Parses row events               â”‚ â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚  â”‚  â”‚  â€¢ Publishes to Kafka topics       â”‚ â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                        â”‚                      â”‚                         â”‚
                        â”‚                      â–¼                         â”‚
                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                        â”‚  â”‚           Kafka Topics                  â”‚   â”‚
                        â”‚  â”‚                                         â”‚   â”‚
                        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
                        â”‚  â”‚  â”‚ dbserver.ecommerce.customers     â”‚   â”‚   â”‚
                        â”‚  â”‚  â”‚ dbserver.ecommerce.orders        â”‚   â”‚   â”‚
                        â”‚  â”‚  â”‚ dbserver.ecommerce.products      â”‚   â”‚   â”‚
                        â”‚  â”‚  â”‚ dbserver.ecommerce.order_items   â”‚   â”‚   â”‚
                        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                        â”‚                      â”‚                         â”‚
                        â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                        â”‚       â–¼              â–¼              â–¼          â”‚
                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
                        â”‚  â”‚Consumer â”‚   â”‚Consumer â”‚   â”‚Consumer â”‚       â”‚
                        â”‚  â”‚Analyticsâ”‚   â”‚ Search  â”‚   â”‚  Cache  â”‚       â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©ploiement avec Docker Compose

```yaml
# docker-compose-cdc.yml

version: '3.8'

services:
  # ============================================================
  # ZOOKEEPER (requis pour Kafka)
  # ============================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log

  # ============================================================
  # KAFKA BROKER
  # ============================================================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    volumes:
      - kafka-data:/var/lib/kafka/data

  # ============================================================
  # SCHEMA REGISTRY (pour Avro)
  # ============================================================
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  # ============================================================
  # KAFKA CONNECT + DEBEZIUM
  # ============================================================
  connect:
    image: debezium/connect:2.5
    hostname: connect
    container_name: connect
    depends_on:
      - kafka
      - schema-registry
      - mariadb
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: connect-cluster
      CONFIG_STORAGE_TOPIC: connect-configs
      OFFSET_STORAGE_TOPIC: connect-offsets
      STATUS_STORAGE_TOPIC: connect-status
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'true'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'true'
    volumes:
      - connect-data:/kafka/data

  # ============================================================
  # MARIADB SOURCE
  # ============================================================
  mariadb:
    image: mariadb:11.8
    hostname: mariadb
    container_name: mariadb
    ports:
      - "3306:3306"
    environment:
      MARIADB_ROOT_PASSWORD: rootpassword
      MARIADB_DATABASE: ecommerce
      MARIADB_USER: app_user
      MARIADB_PASSWORD: app_password
    volumes:
      - mariadb-data:/var/lib/mysql
      - ./config/mariadb-cdc.cnf:/etc/mysql/conf.d/cdc.cnf:ro
      - ./init:/docker-entrypoint-initdb.d:ro
    command: >
      --server-id=1
      --log-bin=mariadb-bin
      --binlog-format=ROW
      --binlog-row-image=FULL
      --binlog-row-metadata=FULL
      --gtid-strict-mode=ON

  # ============================================================
  # KAFDROP (UI pour Kafka)
  # ============================================================
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      SCHEMAREGISTRY_CONNECT: http://schema-registry:8081

volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:
  connect-data:
  mariadb-data:
```

### Configuration du connecteur Debezium

```json
// debezium-mariadb-connector.json
{
  "name": "ecommerce-mariadb-connector",
  "config": {
    // ============================================================
    // CONFIGURATION DE BASE
    // ============================================================
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    
    // Identifiant unique du serveur de base de donnÃ©es
    "database.hostname": "mariadb",
    "database.port": "3306",
    "database.user": "cdc_reader",
    "database.password": "secure_cdc_password_2025",
    
    // PrÃ©fixe pour les noms de topics Kafka
    "topic.prefix": "ecommerce",
    
    // Identifiant serveur (doit Ãªtre unique)
    "database.server.id": "184054",
    
    // ============================================================
    // SÃ‰LECTION DES TABLES
    // ============================================================
    
    // Bases de donnÃ©es Ã  capturer (regex)
    "database.include.list": "ecommerce",
    
    // Tables Ã  inclure (regex)
    "table.include.list": "ecommerce.customers,ecommerce.orders,ecommerce.order_items,ecommerce.products",
    
    // Ou exclure certaines tables
    // "table.exclude.list": "ecommerce.audit_logs,ecommerce.sessions",
    
    // Colonnes Ã  exclure (donnÃ©es sensibles)
    "column.exclude.list": "ecommerce.customers.password_hash,ecommerce.customers.ssn",
    
    // ============================================================
    // SNAPSHOT INITIAL
    // ============================================================
    
    // Mode de snapshot: initial, initial_only, schema_only, never
    "snapshot.mode": "initial",
    
    // Verrous pendant le snapshot
    "snapshot.locking.mode": "minimal",
    
    // Taille des lots pour le snapshot
    "snapshot.fetch.size": "10000",
    
    // ============================================================
    // PARSING DES BINLOGS
    // ============================================================
    
    // Inclure les requÃªtes SQL originales (optionnel)
    "include.query": "false",
    
    // Format des dÃ©cimaux
    "decimal.handling.mode": "string",
    
    // Format des temporels
    "time.precision.mode": "adaptive_time_microseconds",
    
    // ============================================================
    // CONFIGURATION GTID
    // ============================================================
    
    // Utiliser GTID pour le tracking de position
    "gtid.source.includes": ".*",
    
    // ============================================================
    // HEARTBEAT (pour Ã©viter les gaps de binlog)
    // ============================================================
    
    "heartbeat.interval.ms": "10000",
    "heartbeat.topics.prefix": "__debezium-heartbeat",
    
    // ============================================================
    // SCHÃ‰MA ET Ã‰VOLUTION
    // ============================================================
    
    // Stockage de l'historique des schÃ©mas
    "schema.history.internal.kafka.bootstrap.servers": "kafka:29092",
    "schema.history.internal.kafka.topic": "schema-changes.ecommerce",
    
    // Propagation des changements de schÃ©ma
    "include.schema.changes": "true",
    
    // ============================================================
    // TRANSFORMATIONS (SMT)
    // ============================================================
    
    // Aplatir la structure Debezium
    "transforms": "unwrap,route",
    
    // ExtractNewRecordState : simplifie la structure
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "rewrite",
    "transforms.unwrap.add.fields": "op,table,source.ts_ms",
    
    // Routing par table
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "$1-$3",
    
    // ============================================================
    // RÃ‰SILIENCE ET PERFORMANCES
    // ============================================================
    
    // Retry en cas d'erreur
    "errors.retry.delay.max.ms": "60000",
    "errors.retry.timeout": "300000",
    
    // Dead letter queue
    "errors.tolerance": "all",
    "errors.deadletterqueue.topic.name": "dlq-ecommerce",
    "errors.deadletterqueue.context.headers.enable": "true",
    
    // Polling des binlogs
    "poll.interval.ms": "100",
    "max.batch.size": "2048",
    "max.queue.size": "8192"
  }
}
```

### DÃ©ploiement du connecteur

```bash
#!/bin/bash
# deploy-connector.sh

CONNECT_HOST="localhost"
CONNECT_PORT="8083"

# VÃ©rifier que Kafka Connect est prÃªt
echo "Waiting for Kafka Connect to be ready..."
until curl -s "http://${CONNECT_HOST}:${CONNECT_PORT}/connectors" > /dev/null 2>&1; do
    echo "Kafka Connect not ready yet..."
    sleep 5
done
echo "Kafka Connect is ready!"

# DÃ©ployer le connecteur
echo "Deploying Debezium MariaDB connector..."
curl -X POST "http://${CONNECT_HOST}:${CONNECT_PORT}/connectors" \
    -H "Content-Type: application/json" \
    -d @debezium-mariadb-connector.json

# VÃ©rifier le statut
sleep 5
echo ""
echo "Connector status:"
curl -s "http://${CONNECT_HOST}:${CONNECT_PORT}/connectors/ecommerce-mariadb-connector/status" | jq .

# Lister les topics crÃ©Ã©s
echo ""
echo "Kafka topics:"
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092 | grep ecommerce
```

---

## Consommation des Ã©vÃ©nements CDC

### Consumer Python avec kafka-python

```python
# cdc_consumer.py - Consommateur d'Ã©vÃ©nements CDC

import json
import logging
from typing import Dict, Any, Callable, Optional
from dataclasses import dataclass
from datetime import datetime
from kafka import KafkaConsumer
from kafka.errors import KafkaError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CDCEvent:
    """ReprÃ©sentation d'un Ã©vÃ©nement CDC."""
    operation: str  # 'c' (create), 'u' (update), 'd' (delete), 'r' (read/snapshot)
    table: str
    database: str
    timestamp: datetime
    before: Optional[Dict[str, Any]]
    after: Optional[Dict[str, Any]]
    source_position: str
    
    @property
    def is_create(self) -> bool:
        return self.operation == 'c'
    
    @property
    def is_update(self) -> bool:
        return self.operation == 'u'
    
    @property
    def is_delete(self) -> bool:
        return self.operation == 'd'
    
    @property
    def primary_key(self) -> Any:
        """Extrait la clÃ© primaire de l'Ã©vÃ©nement."""
        data = self.after if self.after else self.before
        # Supposons que 'id' est la clÃ© primaire
        return data.get('id') if data else None


class CDCEventProcessor:
    """
    Processeur d'Ã©vÃ©nements CDC avec handlers par table.
    """
    
    def __init__(
        self,
        bootstrap_servers: str,
        topic_pattern: str,
        group_id: str,
        auto_offset_reset: str = 'earliest'
    ):
        self.consumer = KafkaConsumer(
            bootstrap_servers=bootstrap_servers,
            group_id=group_id,
            auto_offset_reset=auto_offset_reset,
            enable_auto_commit=False,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None,
            key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None,
        )
        
        # Subscribe au pattern de topics
        self.consumer.subscribe(pattern=topic_pattern)
        
        # Handlers par table
        self._handlers: Dict[str, Callable[[CDCEvent], None]] = {}
        
        # MÃ©triques
        self._events_processed = 0
        self._errors_count = 0
    
    def register_handler(self, table: str, handler: Callable[[CDCEvent], None]):
        """Enregistre un handler pour une table spÃ©cifique."""
        self._handlers[table] = handler
        logger.info(f"Registered handler for table: {table}")
    
    def _parse_event(self, message) -> Optional[CDCEvent]:
        """Parse un message Kafka en Ã©vÃ©nement CDC."""
        try:
            value = message.value
            if not value:
                return None
            
            # Structure Debezium (aprÃ¨s transformation ExtractNewRecordState)
            # Si unwrap est appliquÃ©, la structure est aplatie
            
            payload = value.get('payload', value)
            source = payload.get('source', {})
            
            return CDCEvent(
                operation=payload.get('op', payload.get('__op', 'r')),
                table=source.get('table', payload.get('__table', '')),
                database=source.get('db', ''),
                timestamp=datetime.fromtimestamp(
                    source.get('ts_ms', payload.get('__source_ts_ms', 0)) / 1000
                ),
                before=payload.get('before'),
                after=payload.get('after', payload if 'op' not in payload else None),
                source_position=f"{source.get('file', '')}:{source.get('pos', 0)}"
            )
        except Exception as e:
            logger.error(f"Error parsing CDC event: {e}")
            return None
    
    def process_events(self, batch_size: int = 100, commit_interval: int = 10):
        """
        Boucle principale de traitement des Ã©vÃ©nements.
        """
        logger.info("Starting CDC event processor...")
        batch_count = 0
        
        try:
            for message in self.consumer:
                try:
                    event = self._parse_event(message)
                    if not event:
                        continue
                    
                    # Trouver et exÃ©cuter le handler
                    handler = self._handlers.get(event.table)
                    if handler:
                        handler(event)
                        self._events_processed += 1
                    else:
                        logger.debug(f"No handler for table: {event.table}")
                    
                    batch_count += 1
                    
                    # Commit pÃ©riodique
                    if batch_count >= commit_interval:
                        self.consumer.commit()
                        batch_count = 0
                        logger.debug(f"Committed offsets. Total processed: {self._events_processed}")
                
                except Exception as e:
                    logger.error(f"Error processing message: {e}")
                    self._errors_count += 1
                    # Continue processing other messages
        
        except KeyboardInterrupt:
            logger.info("Shutting down CDC processor...")
        finally:
            self.consumer.commit()
            self.consumer.close()
            logger.info(f"Processed {self._events_processed} events with {self._errors_count} errors")


# ============================================================
# EXEMPLE D'UTILISATION
# ============================================================

def handle_customer_change(event: CDCEvent):
    """Handler pour les changements de la table customers."""
    
    if event.is_create:
        logger.info(f"New customer created: {event.after.get('email')}")
        # Envoyer email de bienvenue, synchroniser avec CRM, etc.
        sync_to_crm(event.after)
    
    elif event.is_update:
        old_email = event.before.get('email') if event.before else None
        new_email = event.after.get('email') if event.after else None
        
        if old_email != new_email:
            logger.info(f"Customer email changed: {old_email} -> {new_email}")
            # Mettre Ã  jour les systÃ¨mes dÃ©pendants
            update_email_in_newsletter(event.primary_key, new_email)
    
    elif event.is_delete:
        logger.info(f"Customer deleted: {event.before.get('email')}")
        # GDPR: Supprimer des systÃ¨mes tiers
        delete_from_analytics(event.primary_key)


def handle_order_change(event: CDCEvent):
    """Handler pour les changements de la table orders."""
    
    if event.is_create:
        order = event.after
        logger.info(f"New order: #{order.get('order_number')} - ${order.get('total_amount')}")
        
        # Notification temps rÃ©el
        send_order_notification(order)
        
        # Mise Ã  jour analytics
        update_revenue_metrics(order)
    
    elif event.is_update:
        old_status = event.before.get('status') if event.before else None
        new_status = event.after.get('status') if event.after else None
        
        if old_status != new_status:
            logger.info(f"Order #{event.after.get('order_number')} status: {old_status} -> {new_status}")
            
            # Actions selon le nouveau statut
            if new_status == 'shipped':
                send_shipping_notification(event.after)
            elif new_status == 'delivered':
                trigger_review_request(event.after)
            elif new_status == 'cancelled':
                process_cancellation(event.after)


def handle_product_change(event: CDCEvent):
    """Handler pour les changements de la table products."""
    
    # Mise Ã  jour de l'index de recherche (Elasticsearch)
    if event.is_delete:
        delete_from_search_index(event.primary_key)
    else:
        upsert_to_search_index(event.after)
    
    # Invalidation du cache
    invalidate_product_cache(event.primary_key)


# Fonctions helper (Ã  implÃ©menter selon vos besoins)
def sync_to_crm(customer): pass
def update_email_in_newsletter(customer_id, email): pass
def delete_from_analytics(customer_id): pass
def send_order_notification(order): pass
def update_revenue_metrics(order): pass
def send_shipping_notification(order): pass
def trigger_review_request(order): pass
def process_cancellation(order): pass
def delete_from_search_index(product_id): pass
def upsert_to_search_index(product): pass
def invalidate_product_cache(product_id): pass


if __name__ == "__main__":
    processor = CDCEventProcessor(
        bootstrap_servers="localhost:9092",
        topic_pattern="ecommerce-.*",
        group_id="cdc-processor-group"
    )
    
    # Enregistrement des handlers
    processor.register_handler("customers", handle_customer_change)
    processor.register_handler("orders", handle_order_change)
    processor.register_handler("products", handle_product_change)
    
    # DÃ©marrage du traitement
    processor.process_events()
```

### Consumer avec traitement transactionnel

```python
# cdc_transactional_consumer.py - Consumer avec garanties transactionnelles

import json
from typing import List, Dict, Any
from contextlib import contextmanager
from kafka import KafkaConsumer, TopicPartition
import mariadb

class TransactionalCDCProcessor:
    """
    Processeur CDC avec garanties exactly-once via transactions.
    
    Utilise le pattern "Outbox" inversÃ© : stocke les offsets Kafka
    dans la mÃªme transaction que les modifications de donnÃ©es.
    """
    
    def __init__(
        self,
        kafka_config: Dict[str, Any],
        db_config: Dict[str, Any],
        topics: List[str]
    ):
        self.consumer = KafkaConsumer(
            *topics,
            **kafka_config,
            enable_auto_commit=False,  # Commit manuel
            isolation_level='read_committed'  # Transactions Kafka
        )
        
        self.db_pool = mariadb.ConnectionPool(
            pool_name="cdc_pool",
            pool_size=5,
            **db_config
        )
        
        # Charger les offsets depuis la base
        self._restore_offsets()
    
    def _restore_offsets(self):
        """Restaure les offsets depuis la table de stockage."""
        conn = self.db_pool.get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT topic, partition_id, offset_value
                FROM kafka_consumer_offsets
                WHERE consumer_group = %s
            """, ("cdc-transactional-group",))
            
            for topic, partition, offset in cursor.fetchall():
                tp = TopicPartition(topic, partition)
                self.consumer.seek(tp, offset + 1)
        finally:
            conn.close()
    
    @contextmanager
    def _transaction(self):
        """Context manager pour transaction avec stockage d'offset."""
        conn = self.db_pool.get_connection()
        try:
            conn.autocommit = False
            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()
    
    def _save_offset(self, conn, topic: str, partition: int, offset: int):
        """Sauvegarde l'offset dans la mÃªme transaction."""
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO kafka_consumer_offsets 
                (consumer_group, topic, partition_id, offset_value, updated_at)
            VALUES (%s, %s, %s, %s, NOW())
            ON DUPLICATE KEY UPDATE
                offset_value = VALUES(offset_value),
                updated_at = VALUES(updated_at)
        """, ("cdc-transactional-group", topic, partition, offset))
    
    def process_batch(self, batch_size: int = 100):
        """
        Traite un batch de messages de maniÃ¨re transactionnelle.
        """
        messages = self.consumer.poll(timeout_ms=1000, max_records=batch_size)
        
        if not messages:
            return 0
        
        processed = 0
        
        with self._transaction() as conn:
            for tp, records in messages.items():
                for record in records:
                    try:
                        event = json.loads(record.value.decode('utf-8'))
                        
                        # Traitement selon la table
                        self._process_event(conn, event)
                        
                        # Sauvegarde de l'offset
                        self._save_offset(
                            conn, 
                            record.topic, 
                            record.partition, 
                            record.offset
                        )
                        
                        processed += 1
                    
                    except Exception as e:
                        # Log et continue (ou re-raise selon la stratÃ©gie)
                        print(f"Error processing record: {e}")
        
        return processed
    
    def _process_event(self, conn, event: Dict[str, Any]):
        """Traite un Ã©vÃ©nement CDC (Ã  personnaliser)."""
        # Exemple: Synchronisation vers une table de rÃ©plication
        payload = event.get('payload', event)
        source = payload.get('source', {})
        table = source.get('table')
        op = payload.get('op')
        
        cursor = conn.cursor()
        
        if table == 'orders' and op in ('c', 'u'):
            order = payload.get('after')
            cursor.execute("""
                INSERT INTO orders_replica 
                    (order_id, order_number, customer_id, total_amount, status, synced_at)
                VALUES (%s, %s, %s, %s, %s, NOW())
                ON DUPLICATE KEY UPDATE
                    order_number = VALUES(order_number),
                    customer_id = VALUES(customer_id),
                    total_amount = VALUES(total_amount),
                    status = VALUES(status),
                    synced_at = VALUES(synced_at)
            """, (
                order['order_id'],
                order['order_number'],
                order.get('customer_id'),
                order['total_amount'],
                order['status']
            ))
        
        elif table == 'orders' and op == 'd':
            order = payload.get('before')
            cursor.execute("""
                DELETE FROM orders_replica WHERE order_id = %s
            """, (order['order_id'],))


# Table pour stockage des offsets
"""
CREATE TABLE kafka_consumer_offsets (
    consumer_group VARCHAR(255) NOT NULL,
    topic VARCHAR(255) NOT NULL,
    partition_id INT NOT NULL,
    offset_value BIGINT NOT NULL,
    updated_at DATETIME(6) NOT NULL,
    
    PRIMARY KEY (consumer_group, topic, partition_id)
) ENGINE=InnoDB;
"""
```

---

## Cas d'usage du CDC

### 1. Synchronisation de cache

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CACHE INVALIDATION VIA CDC                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MariaDB   â”‚â”€â”€â”€â”€â–¶â”‚   Debezium  â”‚â”€â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â”€â–¶â”‚   Redis     â”‚
â”‚  (Source)   â”‚     â”‚  Connector  â”‚     â”‚   Topic     â”‚     â”‚  Invalidatorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                   â”‚
                                                                   â–¼
                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                            â”‚    Redis    â”‚
                                                            â”‚   Cache     â”‚
                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# cache_invalidator.py

import redis
from cdc_consumer import CDCEventProcessor, CDCEvent

class CacheInvalidator:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    def handle_product_change(self, event: CDCEvent):
        """Invalide le cache produit lors de modifications."""
        product_id = event.primary_key
        
        # ClÃ©s de cache Ã  invalider
        keys_to_delete = [
            f"product:{product_id}",
            f"product:{product_id}:details",
            f"product:{product_id}:variants",
        ]
        
        # Invalider les listes qui pourraient contenir ce produit
        if event.after:
            category = event.after.get('category_path', '')
            keys_to_delete.append(f"products:category:{category}")
        
        # Suppression atomique
        if keys_to_delete:
            self.redis.delete(*keys_to_delete)
        
        # Optionnel: PrÃ©-remplir le cache avec les nouvelles donnÃ©es
        if event.after and not event.is_delete:
            self.redis.hset(
                f"product:{product_id}",
                mapping=event.after
            )
            self.redis.expire(f"product:{product_id}", 3600)


# Usage
redis_client = redis.Redis(host='localhost', port=6379, db=0)
invalidator = CacheInvalidator(redis_client)

processor = CDCEventProcessor(
    bootstrap_servers="localhost:9092",
    topic_pattern="ecommerce-products",
    group_id="cache-invalidator"
)
processor.register_handler("products", invalidator.handle_product_change)
processor.process_events()
```

### 2. Synchronisation vers Elasticsearch

```python
# elasticsearch_sync.py

from elasticsearch import Elasticsearch, helpers
from typing import List, Dict, Any
from cdc_consumer import CDCEvent

class ElasticsearchSynchronizer:
    """Synchronise les donnÃ©es CDC vers Elasticsearch."""
    
    def __init__(self, es_hosts: List[str], index_prefix: str = "cdc"):
        self.es = Elasticsearch(es_hosts)
        self.index_prefix = index_prefix
        self._batch: List[Dict[str, Any]] = []
        self._batch_size = 100
    
    def _get_index_name(self, table: str) -> str:
        return f"{self.index_prefix}-{table}"
    
    def handle_event(self, event: CDCEvent):
        """Traite un Ã©vÃ©nement CDC pour Elasticsearch."""
        index_name = self._get_index_name(event.table)
        doc_id = str(event.primary_key)
        
        if event.is_delete:
            action = {
                "_op_type": "delete",
                "_index": index_name,
                "_id": doc_id
            }
        else:
            # Enrichissement du document
            doc = event.after.copy()
            doc['_cdc_timestamp'] = event.timestamp.isoformat()
            doc['_cdc_operation'] = event.operation
            
            action = {
                "_op_type": "index",
                "_index": index_name,
                "_id": doc_id,
                "_source": doc
            }
        
        self._batch.append(action)
        
        # Flush si batch plein
        if len(self._batch) >= self._batch_size:
            self._flush()
    
    def _flush(self):
        """Envoie le batch vers Elasticsearch."""
        if not self._batch:
            return
        
        try:
            success, errors = helpers.bulk(
                self.es,
                self._batch,
                raise_on_error=False,
                raise_on_exception=False
            )
            
            if errors:
                for error in errors:
                    print(f"ES bulk error: {error}")
            
            print(f"Indexed {success} documents")
        
        finally:
            self._batch = []
    
    def close(self):
        """Flush final et fermeture."""
        self._flush()
        self.es.close()
```

### 3. Audit Trail et Compliance

```sql
-- ============================================================
-- TABLE D'AUDIT ALIMENTÃ‰E PAR CDC
-- ============================================================

CREATE TABLE audit_trail (
    audit_id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    
    -- Source de l'Ã©vÃ©nement
    source_database VARCHAR(64) NOT NULL,
    source_table VARCHAR(64) NOT NULL,
    source_position VARCHAR(100),
    
    -- Type d'opÃ©ration
    operation ENUM('INSERT', 'UPDATE', 'DELETE') NOT NULL,
    
    -- ClÃ© de l'enregistrement modifiÃ©
    record_key JSON NOT NULL,
    
    -- DonnÃ©es avant/aprÃ¨s
    data_before JSON,
    data_after JSON,
    
    -- Champs modifiÃ©s (pour UPDATE)
    changed_fields JSON,
    
    -- Contexte
    event_timestamp DATETIME(6) NOT NULL,
    processed_at DATETIME(6) DEFAULT CURRENT_TIMESTAMP(6),
    
    -- Index pour recherche
    INDEX idx_table_time (source_table, event_timestamp),
    INDEX idx_operation (operation),
    INDEX idx_processed (processed_at)
    
) ENGINE=InnoDB
PARTITION BY RANGE (TO_DAYS(event_timestamp)) (
    PARTITION p_2024_01 VALUES LESS THAN (TO_DAYS('2024-02-01')),
    PARTITION p_2024_02 VALUES LESS THAN (TO_DAYS('2024-03-01')),
    PARTITION p_2024_03 VALUES LESS THAN (TO_DAYS('2024-04-01')),
    -- ... ajouter les partitions mensuelles
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

```python
# audit_consumer.py

import json
from cdc_consumer import CDCEvent
import mariadb

class AuditTrailConsumer:
    """Consommateur CDC pour l'audit trail."""
    
    def __init__(self, db_config):
        self.pool = mariadb.ConnectionPool(pool_name="audit", pool_size=3, **db_config)
    
    def handle_event(self, event: CDCEvent):
        """Enregistre l'Ã©vÃ©nement dans l'audit trail."""
        
        # Calculer les champs modifiÃ©s (pour UPDATE)
        changed_fields = None
        if event.is_update and event.before and event.after:
            changed_fields = [
                key for key in event.after.keys()
                if event.before.get(key) != event.after.get(key)
            ]
        
        conn = self.pool.get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO audit_trail (
                    source_database, source_table, source_position,
                    operation, record_key,
                    data_before, data_after, changed_fields,
                    event_timestamp
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                event.database,
                event.table,
                event.source_position,
                {'c': 'INSERT', 'u': 'UPDATE', 'd': 'DELETE'}.get(event.operation),
                json.dumps({'id': event.primary_key}),
                json.dumps(event.before) if event.before else None,
                json.dumps(event.after) if event.after else None,
                json.dumps(changed_fields) if changed_fields else None,
                event.timestamp
            ))
            conn.commit()
        finally:
            conn.close()
```

### 4. RÃ©plication vers Data Warehouse

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CDC VERS DATA WAREHOUSE (ColumnStore)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MariaDB    â”‚â”€â”€â”€â”€â–¶â”‚   Debezium  â”‚â”€â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â”€â–¶â”‚  Spark      â”‚
â”‚  OLTP       â”‚     â”‚  Connector  â”‚     â”‚   Topics    â”‚     â”‚  Streaming  â”‚
â”‚  (InnoDB)   â”‚     â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                   â”‚
                                                                   â–¼
                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                            â”‚  MariaDB    â”‚
                                                            â”‚ ColumnStore â”‚
                                                            â”‚  (OLAP)     â”‚
                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# spark_cdc_to_columnstore.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, schema_of_json
from pyspark.sql.types import StructType, StructField, StringType, LongType

# Initialisation Spark
spark = SparkSession.builder \
    .appName("CDC-to-ColumnStore") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()

# SchÃ©ma des Ã©vÃ©nements CDC (simplifiÃ©)
cdc_schema = StructType([
    StructField("op", StringType()),
    StructField("before", StringType()),
    StructField("after", StringType()),
    StructField("source", StructType([
        StructField("ts_ms", LongType()),
        StructField("table", StringType())
    ]))
])

# Lecture du stream Kafka
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "ecommerce-orders") \
    .option("startingOffsets", "latest") \
    .load()

# Parsing des Ã©vÃ©nements
parsed = df.select(
    from_json(col("value").cast("string"), cdc_schema).alias("cdc")
).select(
    col("cdc.op"),
    col("cdc.source.ts_ms").alias("event_time"),
    col("cdc.after").alias("data")
)

# Filtrer les inserts et updates
changes = parsed.filter(col("op").isin(["c", "u"]))

# Ã‰criture vers MariaDB ColumnStore
def write_to_columnstore(batch_df, batch_id):
    batch_df.write \
        .format("jdbc") \
        .option("url", "jdbc:mariadb://columnstore-host:3306/analytics") \
        .option("dbtable", "orders_facts") \
        .option("user", "analytics_user") \
        .option("password", "password") \
        .option("driver", "org.mariadb.jdbc.Driver") \
        .mode("append") \
        .save()

# DÃ©marrage du stream
query = changes.writeStream \
    .foreachBatch(write_to_columnstore) \
    .outputMode("append") \
    .option("checkpointLocation", "/tmp/cdc-checkpoint") \
    .start()

query.awaitTermination()
```

---

## Bonnes pratiques et considÃ©rations

### Gestion des erreurs et retry

```python
# resilient_cdc_processor.py

import time
from functools import wraps
from typing import Callable, Any

def retry_with_backoff(
    max_retries: int = 3,
    initial_delay: float = 1.0,
    backoff_factor: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """DÃ©corateur pour retry avec backoff exponentiel."""
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            delay = initial_delay
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        time.sleep(delay)
                        delay *= backoff_factor
                    else:
                        raise
            
            raise last_exception
        
        return wrapper
    return decorator


class ResilientEventHandler:
    """Handler d'Ã©vÃ©nements avec rÃ©silience intÃ©grÃ©e."""
    
    def __init__(self, dlq_handler: Callable):
        self.dlq_handler = dlq_handler
        self.error_count = 0
    
    @retry_with_backoff(max_retries=3, initial_delay=0.5)
    def process_with_retry(self, event, handler: Callable):
        """Traite un Ã©vÃ©nement avec retry automatique."""
        return handler(event)
    
    def handle(self, event, handler: Callable):
        """Point d'entrÃ©e avec gestion DLQ."""
        try:
            self.process_with_retry(event, handler)
        except Exception as e:
            self.error_count += 1
            # Envoyer vers Dead Letter Queue
            self.dlq_handler({
                'original_event': event,
                'error': str(e),
                'error_type': type(e).__name__,
                'timestamp': time.time()
            })
```

### Monitoring et mÃ©triques

```python
# cdc_metrics.py

from prometheus_client import Counter, Histogram, Gauge, start_http_server

# MÃ©triques Prometheus
cdc_events_total = Counter(
    'cdc_events_total',
    'Total number of CDC events processed',
    ['table', 'operation']
)

cdc_processing_time = Histogram(
    'cdc_processing_duration_seconds',
    'Time spent processing CDC events',
    ['table']
)

cdc_lag_seconds = Gauge(
    'cdc_lag_seconds',
    'Lag between event timestamp and processing time',
    ['table']
)

cdc_errors_total = Counter(
    'cdc_errors_total',
    'Total number of CDC processing errors',
    ['table', 'error_type']
)


class MetricsWrapper:
    """Wrapper pour ajouter des mÃ©triques aux handlers."""
    
    def __init__(self, handler, table_name: str):
        self.handler = handler
        self.table_name = table_name
    
    def __call__(self, event):
        import time
        from datetime import datetime
        
        start_time = time.time()
        
        try:
            result = self.handler(event)
            
            # MÃ©triques de succÃ¨s
            cdc_events_total.labels(
                table=self.table_name,
                operation=event.operation
            ).inc()
            
            # Lag
            lag = (datetime.now() - event.timestamp).total_seconds()
            cdc_lag_seconds.labels(table=self.table_name).set(lag)
            
            return result
        
        except Exception as e:
            cdc_errors_total.labels(
                table=self.table_name,
                error_type=type(e).__name__
            ).inc()
            raise
        
        finally:
            duration = time.time() - start_time
            cdc_processing_time.labels(table=self.table_name).observe(duration)


# DÃ©marrer le serveur de mÃ©triques Prometheus
start_http_server(8000)
```

### ConsidÃ©rations de performance

```ini
# Configuration MariaDB optimisÃ©e pour CDC Ã  haut dÃ©bit

[mysqld]
# Binary logs
binlog_cache_size = 4M
max_binlog_cache_size = 8G
binlog_stmt_cache_size = 1M

# RÃ©duire la latence d'Ã©criture des binlogs
sync_binlog = 1  # Pour durabilitÃ©
# sync_binlog = 0  # Pour performance maximale (risque de perte)

# Groupe commit (latence vs throughput)
binlog_commit_wait_count = 10
binlog_commit_wait_usec = 10000

# Rotation des binlogs
max_binlog_size = 1G
expire_logs_days = 3  # Ajuster selon la latence du CDC

# Threads pour le binlog
binlog_parallel_replication = optimistic
slave_parallel_threads = 8
```

### Checklist de production

| Aspect | VÃ©rification | Status |
|--------|--------------|--------|
| **Binlog** | Format ROW, image FULL | â˜ |
| **GTID** | ActivÃ© et mode strict | â˜ |
| **Utilisateur** | PrivilÃ¨ges minimaux CDC | â˜ |
| **RÃ©tention** | Suffisante pour rattrapage | â˜ |
| **Monitoring** | Lag, erreurs, throughput | â˜ |
| **DLQ** | Dead Letter Queue configurÃ©e | â˜ |
| **Idempotence** | Handlers idempotents | â˜ |
| **SchÃ©ma** | Historique des Ã©volutions | â˜ |
| **Snapshots** | StratÃ©gie de resync | â˜ |
| **SÃ©curitÃ©** | Chiffrement, authentification | â˜ |

---

## Alternatives Ã  Debezium

### Maxwell's Daemon

```yaml
# maxwell-config.properties
producer=kafka
kafka.bootstrap.servers=kafka:9092
kafka_topic=maxwell

# MariaDB
host=mariadb
port=3306
user=maxwell
password=maxwell_password

# GTID
gtid_mode=true

# Filtrage
filter=exclude: *.*, include: ecommerce.*

# Output format
output_ddl=true
```

### Canal (Alibaba)

```yaml
# canal.properties
canal.instance.master.address=mariadb:3306
canal.instance.dbUsername=canal
canal.instance.dbPassword=canal_password

# Filtering
canal.instance.filter.regex=ecommerce\\..*

# Kafka output
canal.mq.servers=kafka:9092
canal.mq.topic=canal-events
```

### Comparaison

| Feature | Debezium | Maxwell | Canal |
|---------|----------|---------|-------|
| **Maintien** | Red Hat | Zendesk | Alibaba |
| **Kafka Connect** | Natif | Non | Plugin |
| **SchÃ©ma Registry** | Oui | Non | Non |
| **Snapshot initial** | Oui | Oui (bootstrap) | Oui |
| **Format sortie** | Avro/JSON | JSON | Protobuf/JSON |
| **DDL tracking** | Oui | Oui | Oui |
| **Performance** | Haute | Moyenne | Haute |
| **CommunautÃ©** | Large | Moyenne | Large (Chine) |

---

## âœ… Points clÃ©s Ã  retenir

- Le **CDC log-based** est la mÃ©thode recommandÃ©e pour MariaDB, utilisant les **binary logs** natifs
- Le format **ROW** avec **binlog_row_image=FULL** est obligatoire pour capturer les Ã©tats avant/aprÃ¨s
- **Debezium** est la solution CDC de rÃ©fÃ©rence avec intÃ©gration native Kafka Connect
- Les Ã©vÃ©nements CDC contiennent l'opÃ©ration (c/u/d), les donnÃ©es before/after, et les mÃ©tadonnÃ©es source
- Le **GTID** facilite le suivi de position et la reprise aprÃ¨s incident
- Les **handlers** doivent Ãªtre **idempotents** pour gÃ©rer les retraitements
- Le **monitoring** du lag et des erreurs est essentiel en production
- Une **Dead Letter Queue** permet de gÃ©rer les Ã©vÃ©nements non traitÃ©s
- Le CDC permet de nombreux cas d'usage : cache, search, audit, data warehouse, microservices

---

## ðŸ”— Ressources et rÃ©fÃ©rences

- ðŸ“– [MariaDB Binary Log Documentation](https://mariadb.com/kb/en/binary-log/)
- ðŸ“– [Debezium MySQL Connector](https://debezium.io/documentation/reference/stable/connectors/mysql.html)
- ðŸ“– [Debezium on MariaDB](https://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-mariadb-support)
- ðŸ“– [Maxwell's Daemon](https://maxwells-daemon.io/)
- ðŸ“– [Apache Kafka Connect](https://kafka.apache.org/documentation/#connect)
- ðŸ“– [CDC Patterns and Best Practices - Confluent](https://www.confluent.io/blog/cdc-change-data-capture/)

---

## âž¡ï¸ Section suivante

**20.8.2 IntÃ©gration avec Kafka** : Approfondissez l'intÃ©gration entre MariaDB et Apache Kafka, avec les patterns de production/consommation, le streaming analytics, et les architectures event-driven complÃ¨tes.

â­ï¸ [Integration avec Kafka](/20-cas-usage-architectures/08.2-integration-kafka.md)
