üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.9.1 Semantic Search

> **Niveau** : Interm√©diaire √† Avanc√©  
> **Dur√©e estim√©e** : 3-4 heures  
> **Pr√©requis** : Chapitre 18.10 (MariaDB Vector), notions de Machine Learning et embeddings, SQL avanc√©

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre les diff√©rences fondamentales entre recherche lexicale et recherche s√©mantique
- Concevoir une architecture de Semantic Search avec MariaDB 11.8 Vector
- Impl√©menter un pipeline complet de g√©n√©ration d'embeddings et de recherche
- Optimiser les performances des requ√™tes vectorielles avec les index HNSW
- Int√©grer la Semantic Search dans des architectures de production

---

## Introduction

La **Semantic Search** (recherche s√©mantique) repr√©sente un changement de paradigme fondamental dans la fa√ßon dont nous interrogeons les donn√©es. Contrairement √† la recherche traditionnelle bas√©e sur la correspondance exacte de mots-cl√©s, la recherche s√©mantique comprend le **sens** et l'**intention** derri√®re une requ√™te.

Avec l'av√®nement de MariaDB 11.8 LTS et son support natif des vecteurs (MariaDB Vector), il devient possible d'impl√©menter des syst√®mes de recherche s√©mantique performants directement dans votre base de donn√©es relationnelle, sans d√©pendre d'outils externes sp√©cialis√©s comme Elasticsearch, Pinecone ou Milvus.

üÜï **Nouveaut√© MariaDB 11.8** : Le type `VECTOR` et les index `HNSW` permettent d√©sormais d'effectuer des recherches de similarit√© vectorielle √† grande √©chelle, avec des performances optimis√©es gr√¢ce aux instructions SIMD (AVX2, AVX512, ARM NEON).

---

## Recherche lexicale vs recherche s√©mantique

### Le probl√®me de la recherche par mots-cl√©s

La recherche traditionnelle (lexicale) repose sur la correspondance exacte ou partielle de termes. Elle pr√©sente plusieurs limitations majeures :

| Limitation | Exemple |
|------------|---------|
| **Synonymes non reconnus** | "voiture" ne trouve pas "automobile" |
| **Contexte ignor√©** | "python" (langage) vs "python" (serpent) |
| **Reformulations invisibles** | "comment cuisiner des p√¢tes" vs "recette de spaghetti" |
| **Fautes de frappe bloquantes** | "MariaDB" vs "MariDB" |

```sql
-- Recherche lexicale traditionnelle avec LIKE
SELECT id, titre, contenu
FROM articles
WHERE contenu LIKE '%base de donn√©es%'
   OR contenu LIKE '%database%';

-- Probl√®me : Ne trouve pas "SGBD", "syst√®me de gestion", "data store"...
```

### L'approche s√©mantique

La recherche s√©mantique transforme le texte en **vecteurs d'embeddings** ‚Äî des repr√©sentations num√©riques multi-dimensionnelles qui capturent le sens. Deux textes s√©mantiquement proches auront des vecteurs proches dans l'espace vectoriel, m√™me s'ils n'utilisent aucun mot en commun.

```
"MariaDB est un excellent SGBD open source"
    ‚Üì Embedding Model (ex: OpenAI, Sentence-BERT)
    ‚Üì
[0.023, -0.156, 0.892, ..., 0.341]  (1536 dimensions)

"MySQL est une base de donn√©es relationnelle gratuite"
    ‚Üì
[0.019, -0.148, 0.887, ..., 0.338]  (vecteur proche!)
```

üí° **Conseil** : La qualit√© de la recherche s√©mantique d√©pend directement du mod√®le d'embedding choisi. Un mod√®le entra√Æn√© sur des donn√©es similaires √† votre domaine produira de meilleurs r√©sultats.

---

## Architecture d'une solution Semantic Search

### Vue d'ensemble du pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     PIPELINE SEMANTIC SEARCH                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Document ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Chunking   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Embedding Model       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Source  ‚îÇ    ‚îÇ  (d√©coupage) ‚îÇ     ‚îÇ  (OpenAI, HuggingFace)  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                   ‚îÇ                 ‚îÇ
‚îÇ                                                   ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  R√©sultat‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Ranking    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   MariaDB 11.8 Vector   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  class√©  ‚îÇ    ‚îÇ  (reranking) ‚îÇ     ‚îÇ   + Index HNSW          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                   ‚ñ≤                 ‚îÇ
‚îÇ                                                   ‚îÇ                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  Requ√™te ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Embedding  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ  ‚îÇ  Texte   ‚îÇ    ‚îÇ   Requ√™te    ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants cl√©s

1. **Chunking** : D√©coupage des documents en segments de taille appropri√©e (g√©n√©ralement 256-1024 tokens)
2. **Embedding Model** : Mod√®le de transformation texte ‚Üí vecteur (OpenAI `text-embedding-3-small`, Sentence-BERT, etc.)
3. **Vector Store** : MariaDB 11.8 avec colonnes `VECTOR` et index `HNSW`
4. **Similarity Search** : Recherche des K plus proches voisins (KNN)
5. **Reranking** (optionnel) : Reclassement des r√©sultats avec un mod√®le plus pr√©cis

---

## Impl√©mentation avec MariaDB 11.8 Vector

### Sch√©ma de base pour la Semantic Search

```sql
-- Cr√©ation de la base de donn√©es pour le syst√®me de recherche
CREATE DATABASE IF NOT EXISTS semantic_search
    CHARACTER SET utf8mb4
    COLLATE utf8mb4_uca1400_ai_ci;

USE semantic_search;

-- Table des documents sources
CREATE TABLE documents (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    titre VARCHAR(500) NOT NULL,
    contenu LONGTEXT NOT NULL,
    source VARCHAR(255),
    date_creation DATETIME DEFAULT CURRENT_TIMESTAMP,
    date_modification DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    metadata JSON,
    
    -- Index full-text pour recherche hybride
    FULLTEXT INDEX idx_fulltext_contenu (titre, contenu)
) ENGINE=InnoDB;

-- Table des chunks avec embeddings vectoriels
-- üÜï Utilisation du type VECTOR natif MariaDB 11.8
CREATE TABLE document_chunks (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    document_id BIGINT UNSIGNED NOT NULL,
    chunk_index INT UNSIGNED NOT NULL,
    chunk_text TEXT NOT NULL,
    token_count INT UNSIGNED,
    
    -- Vecteur d'embedding (1536 dimensions pour OpenAI ada-002/text-embedding-3)
    embedding VECTOR(1536) NOT NULL,
    
    -- M√©tadonn√©es du chunk
    start_char_offset INT UNSIGNED,
    end_char_offset INT UNSIGNED,
    
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
    
    -- üÜï Index HNSW pour recherche approximative rapide
    VECTOR INDEX idx_hnsw_embedding (embedding)
        WITH (
            DISTANCE = COSINE,  -- Distance cosinus pour similarit√© s√©mantique
            M = 16,             -- Connexions par couche
            EF_CONSTRUCTION = 200  -- Pr√©cision lors de la construction
        )
) ENGINE=InnoDB;

-- Index composite pour filtrage + recherche vectorielle
CREATE INDEX idx_document_chunk ON document_chunks(document_id, chunk_index);
```

üí° **Conseil** : Le param√®tre `M` contr√¥le le compromis m√©moire/pr√©cision de l'index HNSW. Une valeur de 16 offre un bon √©quilibre. Augmentez √† 32-64 pour plus de pr√©cision si vous avez suffisamment de RAM.

### Insertion des embeddings

```sql
-- Insertion d'un document et de ses chunks avec embeddings
-- Les embeddings sont g√©n√©r√©s c√¥t√© application via l'API OpenAI ou un mod√®le local

INSERT INTO documents (titre, contenu, source, metadata)
VALUES (
    'Introduction √† MariaDB 11.8 LTS',
    'MariaDB 11.8 LTS est la derni√®re version √† support long terme...',
    'documentation_interne',
    '{"categorie": "database", "langue": "fr", "auteur": "equipe_data"}'
);

SET @doc_id = LAST_INSERT_ID();

-- Insertion des chunks avec leurs vecteurs
-- Le vecteur est fourni au format texte et converti via VEC_FromText
INSERT INTO document_chunks (document_id, chunk_index, chunk_text, token_count, embedding)
VALUES 
(
    @doc_id,
    0,
    'MariaDB 11.8 LTS est la derni√®re version √† support long terme de MariaDB.',
    15,
    VEC_FromText('[0.023, -0.156, 0.892, ..., 0.341]')  -- 1536 valeurs
),
(
    @doc_id,
    1,
    'Cette version introduit le support natif des vecteurs pour la recherche s√©mantique.',
    14,
    VEC_FromText('[0.045, -0.132, 0.856, ..., 0.298]')
);
```

### Requ√™te de recherche s√©mantique

```sql
-- Recherche s√©mantique : trouver les chunks les plus similaires √† une requ√™te
-- L'embedding de la requ√™te est g√©n√©r√© c√¥t√© application

SET @query_embedding = VEC_FromText('[0.021, -0.149, 0.885, ..., 0.335]');

-- Recherche des 10 chunks les plus similaires (KNN)
SELECT 
    dc.id,
    dc.chunk_text,
    d.titre AS document_titre,
    d.source,
    -- üÜï Fonction de distance cosinus native
    VEC_DISTANCE_COSINE(dc.embedding, @query_embedding) AS distance,
    -- Score de similarit√© (1 - distance pour cosinus)
    (1 - VEC_DISTANCE_COSINE(dc.embedding, @query_embedding)) AS similarite
FROM document_chunks dc
JOIN documents d ON dc.document_id = d.id
ORDER BY distance ASC  -- Plus petite distance = plus similaire
LIMIT 10;
```

‚ö†Ô∏è **Attention** : La distance cosinus retourne une valeur entre 0 (identique) et 2 (oppos√©). Pour obtenir un score de similarit√© intuitif entre 0 et 1, utilisez `1 - distance` ou `(2 - distance) / 2`.

### Recherche avec filtrage par m√©tadonn√©es

```sql
-- Recherche s√©mantique filtr√©e par cat√©gorie et date
SELECT 
    dc.id,
    dc.chunk_text,
    d.titre,
    JSON_UNQUOTE(JSON_EXTRACT(d.metadata, '$.categorie')) AS categorie,
    VEC_DISTANCE_COSINE(dc.embedding, @query_embedding) AS distance
FROM document_chunks dc
JOIN documents d ON dc.document_id = d.id
WHERE 
    -- Filtres relationnels classiques
    JSON_EXTRACT(d.metadata, '$.categorie') = '"database"'
    AND d.date_creation >= '2024-01-01'
ORDER BY distance ASC
LIMIT 10;
```

üí° **Conseil** : L'index HNSW est utilis√© m√™me avec des filtres `WHERE`, mais les performances d√©pendent de la s√©lectivit√©. Pour des filtres tr√®s restrictifs (< 1% des donn√©es), un scan s√©quentiel peut √™tre plus efficace.

---

## Fonctions de distance vectorielle

MariaDB 11.8 propose plusieurs fonctions de distance selon le cas d'usage :

| Fonction | Cas d'usage | Formule |
|----------|-------------|---------|
| `VEC_DISTANCE_COSINE` | Similarit√© s√©mantique (texte) | 1 - cos(Œ∏) |
| `VEC_DISTANCE_EUCLIDEAN` | Distance g√©om√©trique | ‚àöŒ£(a·µ¢ - b·µ¢)¬≤ |

```sql
-- Comparaison des m√©triques de distance
SELECT 
    dc.chunk_text,
    VEC_DISTANCE_COSINE(dc.embedding, @query_embedding) AS dist_cosine,
    VEC_DISTANCE_EUCLIDEAN(dc.embedding, @query_embedding) AS dist_euclidean
FROM document_chunks dc
ORDER BY dist_cosine ASC
LIMIT 5;
```

**Recommandation pour la Semantic Search** : Utilisez syst√©matiquement `VEC_DISTANCE_COSINE` car elle est invariante √† la magnitude des vecteurs et se concentre sur l'orientation (le sens s√©mantique).

---

## Optimisation des performances

### Configuration de l'index HNSW

```sql
-- Index HNSW optimis√© pour la production
ALTER TABLE document_chunks
DROP INDEX idx_hnsw_embedding,
ADD VECTOR INDEX idx_hnsw_embedding (embedding)
    WITH (
        DISTANCE = COSINE,
        M = 32,                 -- Plus de connexions = meilleure pr√©cision
        EF_CONSTRUCTION = 400,  -- Plus √©lev√© = index de meilleure qualit√© (construction plus lente)
        EF_SEARCH = 100         -- Param√®tre de recherche (compromis vitesse/rappel)
    );
```

| Param√®tre | Impact | Valeur recommand√©e |
|-----------|--------|-------------------|
| `M` | M√©moire ‚Üë, Pr√©cision ‚Üë | 16 (petit), 32 (standard), 64 (haute pr√©cision) |
| `EF_CONSTRUCTION` | Temps construction ‚Üë, Qualit√© index ‚Üë | 200-400 |
| `EF_SEARCH` | Temps requ√™te ‚Üë, Rappel ‚Üë | 50-200 selon latence acceptable |

### Monitoring des performances

```sql
-- Analyser l'utilisation de l'index HNSW
EXPLAIN FORMAT=JSON
SELECT dc.chunk_text, VEC_DISTANCE_COSINE(dc.embedding, @query_embedding) AS distance
FROM document_chunks dc
ORDER BY distance ASC
LIMIT 10;

-- Statistiques sur les chunks
SELECT 
    COUNT(*) AS total_chunks,
    AVG(token_count) AS avg_tokens,
    COUNT(DISTINCT document_id) AS total_documents
FROM document_chunks;
```

üÜï **Nouveaut√© MariaDB 11.8** : Le moteur utilise automatiquement les instructions SIMD disponibles (AVX2, AVX512 sur x86, NEON sur ARM) pour acc√©l√©rer les calculs de distance vectorielle.

---

## Architecture de production

### Pattern recommand√© : Service d'embedding d√©coupl√©

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ARCHITECTURE PRODUCTION                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ   ‚îÇ   Client    ‚îÇ          ‚îÇ         API Gateway / Load Balancer ‚îÇ      ‚îÇ
‚îÇ   ‚îÇ  (Web/App)  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ              (nginx / Traefik)      ‚îÇ      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                               ‚îÇ                         ‚îÇ
‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ                     ‚îÇ                         ‚îÇ                     ‚îÇ   ‚îÇ
‚îÇ                     ‚ñº                         ‚ñº                     ‚ñº   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ   ‚îÇ   Search Service    ‚îÇ   ‚îÇ  Embedding Service  ‚îÇ    ‚îÇ Ingestion     ‚îÇ‚îÇ
‚îÇ   ‚îÇ   (FastAPI/Go)      ‚îÇ   ‚îÇ  (Python + OpenAI)  ‚îÇ    ‚îÇ Service       ‚îÇ‚îÇ
‚îÇ   ‚îÇ                     ‚îÇ   ‚îÇ                     ‚îÇ    ‚îÇ (async)       ‚îÇ‚îÇ
‚îÇ   ‚îÇ - Query validation  ‚îÇ   ‚îÇ - Text ‚Üí Vector     ‚îÇ    ‚îÇ               ‚îÇ‚îÇ
‚îÇ   ‚îÇ - Result formatting ‚îÇ   ‚îÇ - Batch processing  ‚îÇ    ‚îÇ - Chunking    ‚îÇ‚îÇ
‚îÇ   ‚îÇ - Caching (Redis)   ‚îÇ   ‚îÇ - Model management  ‚îÇ    ‚îÇ - Embedding   ‚îÇ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ              ‚îÇ                         ‚îÇ                       ‚îÇ        ‚îÇ
‚îÇ              ‚îÇ                         ‚îÇ                       ‚îÇ        ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                        ‚îÇ                                ‚îÇ
‚îÇ                                        ‚ñº                                ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ              ‚îÇ           MariaDB 11.8 LTS Cluster          ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ                                             ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ Primary ‚îÇ  ‚îÇ Replica ‚îÇ  ‚îÇ Replica ‚îÇ      ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ (Write) ‚îÇ  ‚îÇ (Read)  ‚îÇ  ‚îÇ (Read)  ‚îÇ      ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ                                             ‚îÇ            ‚îÇ
‚îÇ              ‚îÇ         + MaxScale Read/Write Split         ‚îÇ            ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Configuration MaxScale pour Semantic Search

```ini
# /etc/maxscale.cnf - Configuration pour workload Semantic Search

[semantic-search-service]
type=service
router=readwritesplit
servers=primary,replica1,replica2
user=maxscale_user
password=encrypted_password

# Les requ√™tes SELECT avec ORDER BY distance sont rout√©es vers les replicas
# Les INSERT d'embeddings vont vers le primary
router_options=slave_selection_criteria=LEAST_BEHIND_MASTER

[semantic-search-listener]
type=listener
service=semantic-search-service
protocol=MariaDBClient
port=4006
```

---

## Cas d'usage r√©els

### 1. Moteur de recherche documentaire d'entreprise

**Contexte** : Une entreprise de 5000 employ√©s souhaite permettre la recherche dans 500 000 documents internes (wikis, proc√©dures, rapports).

**Architecture d√©cisionnelle** :
- **Chunking** : 512 tokens avec overlap de 50 tokens pour pr√©server le contexte
- **Mod√®le d'embedding** : `text-embedding-3-small` (1536 dim) ‚Äî bon rapport qualit√©/co√ªt
- **Index HNSW** : M=32, EF_CONSTRUCTION=300
- **R√©plication** : 1 primary + 2 replicas pour absorber la charge de lecture

```sql
-- Sch√©ma optimis√© pour ce cas d'usage
CREATE TABLE enterprise_documents (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    titre VARCHAR(500) NOT NULL,
    chemin_fichier VARCHAR(1000),
    type_document ENUM('wiki', 'procedure', 'rapport', 'email', 'presentation'),
    departement VARCHAR(100),
    auteur_id INT UNSIGNED,
    date_publication DATE,
    niveau_confidentialite ENUM('public', 'interne', 'confidentiel', 'secret'),
    
    INDEX idx_departement (departement),
    INDEX idx_type (type_document),
    INDEX idx_confidentialite (niveau_confidentialite)
) ENGINE=InnoDB;

CREATE TABLE enterprise_chunks (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    document_id BIGINT UNSIGNED NOT NULL,
    chunk_index INT UNSIGNED NOT NULL,
    chunk_text TEXT NOT NULL,
    embedding VECTOR(1536) NOT NULL,
    
    FOREIGN KEY (document_id) REFERENCES enterprise_documents(id) ON DELETE CASCADE,
    VECTOR INDEX idx_embedding (embedding) WITH (DISTANCE = COSINE, M = 32)
) ENGINE=InnoDB;

-- Requ√™te typique avec filtrage par d√©partement et confidentialit√©
SELECT 
    ec.chunk_text,
    ed.titre,
    ed.chemin_fichier,
    VEC_DISTANCE_COSINE(ec.embedding, @query_vec) AS distance
FROM enterprise_chunks ec
JOIN enterprise_documents ed ON ec.document_id = ed.id
WHERE 
    ed.departement IN ('IT', 'Engineering')
    AND ed.niveau_confidentialite IN ('public', 'interne')
ORDER BY distance ASC
LIMIT 20;
```

### 2. FAQ intelligente / Support client

**Contexte** : Un service client re√ßoit 10 000 tickets/jour et souhaite sugg√©rer automatiquement des r√©ponses bas√©es sur un historique de 1 million de tickets r√©solus.

```sql
-- Sch√©ma pour FAQ s√©mantique
CREATE TABLE support_tickets (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    question TEXT NOT NULL,
    reponse TEXT NOT NULL,
    categorie VARCHAR(100),
    score_satisfaction TINYINT UNSIGNED,  -- 1-5
    date_resolution DATETIME,
    agent_id INT UNSIGNED,
    
    -- Embedding de la question pour recherche
    question_embedding VECTOR(1536) NOT NULL,
    
    VECTOR INDEX idx_question_embedding (question_embedding) 
        WITH (DISTANCE = COSINE, M = 24, EF_SEARCH = 80)
) ENGINE=InnoDB;

-- Trouver les meilleures r√©ponses pour une nouvelle question
-- Filtrer par tickets ayant un bon score de satisfaction
SELECT 
    st.question AS question_similaire,
    st.reponse AS reponse_suggeree,
    st.categorie,
    st.score_satisfaction,
    VEC_DISTANCE_COSINE(st.question_embedding, @nouvelle_question_vec) AS distance
FROM support_tickets st
WHERE 
    st.score_satisfaction >= 4  -- Uniquement les bonnes r√©ponses
    AND st.date_resolution >= DATE_SUB(NOW(), INTERVAL 1 YEAR)  -- R√©centes
ORDER BY distance ASC
LIMIT 5;
```

### 3. Recherche de produits e-commerce

**Contexte** : Un site e-commerce avec 500 000 produits souhaite am√©liorer la pertinence de recherche au-del√† des mots-cl√©s.

```sql
-- Sch√©ma produit avec embedding multi-champs
CREATE TABLE products (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    sku VARCHAR(50) UNIQUE NOT NULL,
    nom VARCHAR(300) NOT NULL,
    description TEXT,
    marque VARCHAR(100),
    categorie_id INT UNSIGNED,
    prix DECIMAL(10,2),
    en_stock BOOLEAN DEFAULT TRUE,
    note_moyenne DECIMAL(3,2),
    
    -- Embedding combin√© (nom + description + attributs)
    product_embedding VECTOR(1536) NOT NULL,
    
    INDEX idx_categorie (categorie_id),
    INDEX idx_prix (prix),
    INDEX idx_stock (en_stock),
    VECTOR INDEX idx_product_search (product_embedding) WITH (DISTANCE = COSINE, M = 32)
) ENGINE=InnoDB;

-- Recherche s√©mantique avec filtres business
SELECT 
    p.sku,
    p.nom,
    p.prix,
    p.note_moyenne,
    VEC_DISTANCE_COSINE(p.product_embedding, @search_vec) AS semantic_distance
FROM products p
WHERE 
    p.en_stock = TRUE
    AND p.prix BETWEEN 50 AND 200
    AND p.categorie_id IN (SELECT id FROM categories WHERE parent_id = 42)
ORDER BY 
    semantic_distance ASC,
    p.note_moyenne DESC  -- D√©partage par popularit√©
LIMIT 24;
```

---

## Recherche hybride : combiner lexical et s√©mantique

La recherche hybride combine les forces de la recherche full-text (pr√©cision sur les termes exacts) et s√©mantique (compr√©hension du sens).

```sql
-- Recherche hybride avec fusion des scores
WITH 
-- Recherche s√©mantique
semantic_results AS (
    SELECT 
        dc.id,
        dc.document_id,
        dc.chunk_text,
        VEC_DISTANCE_COSINE(dc.embedding, @query_embedding) AS semantic_distance,
        -- Normaliser en score 0-1 (plus proche de 1 = meilleur)
        1 - LEAST(VEC_DISTANCE_COSINE(dc.embedding, @query_embedding), 1) AS semantic_score
    FROM document_chunks dc
    ORDER BY semantic_distance ASC
    LIMIT 100
),
-- Recherche full-text (lexicale)
fulltext_results AS (
    SELECT 
        dc.id,
        dc.document_id,
        dc.chunk_text,
        MATCH(dc.chunk_text) AGAINST(@query_text IN NATURAL LANGUAGE MODE) AS ft_score,
        -- Normaliser le score full-text
        MATCH(dc.chunk_text) AGAINST(@query_text IN NATURAL LANGUAGE MODE) / 
            (SELECT MAX(MATCH(chunk_text) AGAINST(@query_text IN NATURAL LANGUAGE MODE)) FROM document_chunks) AS ft_normalized
    FROM document_chunks dc
    WHERE MATCH(dc.chunk_text) AGAINST(@query_text IN NATURAL LANGUAGE MODE)
    LIMIT 100
)
-- Fusion avec pond√©ration (Reciprocal Rank Fusion ou moyenne pond√©r√©e)
SELECT 
    COALESCE(s.id, f.id) AS chunk_id,
    COALESCE(s.chunk_text, f.chunk_text) AS chunk_text,
    s.semantic_score,
    f.ft_normalized AS fulltext_score,
    -- Score hybride : 70% s√©mantique + 30% lexical
    (COALESCE(s.semantic_score, 0) * 0.7 + COALESCE(f.ft_normalized, 0) * 0.3) AS hybrid_score
FROM semantic_results s
FULL OUTER JOIN fulltext_results f ON s.id = f.id
ORDER BY hybrid_score DESC
LIMIT 20;
```

üí° **Conseil** : Ajustez les poids (70/30) selon votre domaine. Pour des requ√™tes tr√®s techniques avec des termes sp√©cifiques, augmentez le poids du full-text. Pour des requ√™tes naturelles en langage courant, favorisez le s√©mantique.

---

## Int√©gration avec les mod√®les d'embedding

### G√©n√©ration d'embeddings c√¥t√© application (Python)

```python
# semantic_search.py - Service d'embedding et recherche
import mariadb
from openai import OpenAI
from typing import List, Dict, Any

class SemanticSearchService:
    def __init__(self, db_config: dict, openai_api_key: str):
        self.conn = mariadb.connect(**db_config)
        self.openai_client = OpenAI(api_key=openai_api_key)
        self.embedding_model = "text-embedding-3-small"
        self.embedding_dim = 1536
    
    def generate_embedding(self, text: str) -> List[float]:
        """G√©n√®re un vecteur d'embedding pour un texte donn√©."""
        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=text
        )
        return response.data[0].embedding
    
    def search(
        self, 
        query: str, 
        limit: int = 10,
        filters: Dict[str, Any] = None
    ) -> List[Dict]:
        """Recherche s√©mantique avec filtres optionnels."""
        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = self.generate_embedding(query)
        
        # Construire la requ√™te SQL
        sql = """
            SELECT 
                dc.id,
                dc.chunk_text,
                d.titre,
                d.source,
                VEC_DISTANCE_COSINE(dc.embedding, VEC_FromText(%s)) AS distance
            FROM document_chunks dc
            JOIN documents d ON dc.document_id = d.id
            WHERE 1=1
        """
        params = [str(query_embedding)]
        
        # Ajouter les filtres dynamiquement
        if filters:
            if 'source' in filters:
                sql += " AND d.source = %s"
                params.append(filters['source'])
            if 'date_min' in filters:
                sql += " AND d.date_creation >= %s"
                params.append(filters['date_min'])
        
        sql += " ORDER BY distance ASC LIMIT %s"
        params.append(limit)
        
        cursor = self.conn.cursor(dictionary=True)
        cursor.execute(sql, params)
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row['id'],
                'text': row['chunk_text'],
                'document_title': row['titre'],
                'source': row['source'],
                'similarity': 1 - row['distance']  # Convertir distance en similarit√©
            })
        
        return results

# Exemple d'utilisation
if __name__ == "__main__":
    service = SemanticSearchService(
        db_config={
            'host': 'localhost',
            'port': 3306,
            'user': 'semantic_user',
            'password': 'secure_password',
            'database': 'semantic_search'
        },
        openai_api_key='sk-...'
    )
    
    results = service.search(
        query="Comment configurer la r√©plication MariaDB ?",
        limit=5,
        filters={'source': 'documentation_interne'}
    )
    
    for r in results:
        print(f"[{r['similarity']:.3f}] {r['document_title']}: {r['text'][:100]}...")
```

### Batch processing pour l'ingestion

```python
# ingestion_service.py - Ingestion en masse avec batching
import mariadb
from openai import OpenAI
from typing import List
import tiktoken

class DocumentIngestionService:
    def __init__(self, db_config: dict, openai_api_key: str):
        self.conn = mariadb.connect(**db_config)
        self.openai_client = OpenAI(api_key=openai_api_key)
        self.tokenizer = tiktoken.encoding_for_model("text-embedding-3-small")
        self.chunk_size = 512  # tokens
        self.chunk_overlap = 50  # tokens
        self.batch_size = 100  # embeddings par batch API
    
    def chunk_text(self, text: str) -> List[str]:
        """D√©coupe un texte en chunks avec overlap."""
        tokens = self.tokenizer.encode(text)
        chunks = []
        
        start = 0
        while start < len(tokens):
            end = min(start + self.chunk_size, len(tokens))
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)
            start += self.chunk_size - self.chunk_overlap
        
        return chunks
    
    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """G√©n√®re les embeddings pour un batch de textes."""
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]
    
    def ingest_document(self, titre: str, contenu: str, source: str, metadata: dict = None):
        """Ing√®re un document complet avec chunking et embedding."""
        cursor = self.conn.cursor()
        
        # Ins√©rer le document
        cursor.execute(
            "INSERT INTO documents (titre, contenu, source, metadata) VALUES (%s, %s, %s, %s)",
            (titre, contenu, source, json.dumps(metadata or {}))
        )
        doc_id = cursor.lastrowid
        
        # Chunker le contenu
        chunks = self.chunk_text(contenu)
        
        # G√©n√©rer les embeddings par batch
        for batch_start in range(0, len(chunks), self.batch_size):
            batch_chunks = chunks[batch_start:batch_start + self.batch_size]
            embeddings = self.generate_embeddings_batch(batch_chunks)
            
            # Ins√©rer les chunks avec embeddings
            for i, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings)):
                cursor.execute(
                    """
                    INSERT INTO document_chunks 
                    (document_id, chunk_index, chunk_text, token_count, embedding)
                    VALUES (%s, %s, %s, %s, VEC_FromText(%s))
                    """,
                    (
                        doc_id,
                        batch_start + i,
                        chunk,
                        len(self.tokenizer.encode(chunk)),
                        str(embedding)
                    )
                )
        
        self.conn.commit()
        return doc_id
```

---

## Bonnes pratiques et pi√®ges √† √©viter

### ‚úÖ Bonnes pratiques

1. **Choisir la bonne taille de chunk** : 256-512 tokens pour des r√©ponses pr√©cises, 512-1024 pour plus de contexte
2. **Utiliser l'overlap** : 10-20% d'overlap entre chunks pour ne pas couper les id√©es
3. **Normaliser les embeddings** : La plupart des mod√®les retournent des vecteurs d√©j√† normalis√©s, v√©rifiez votre mod√®le
4. **Indexer progressivement** : Pour de gros volumes, ins√©rez par batches et reconstruisez l'index p√©riodiquement
5. **Monitorer le recall** : Testez r√©guli√®rement avec des requ√™tes connues pour valider la qualit√©

### ‚ö†Ô∏è Pi√®ges courants

| Pi√®ge | Cons√©quence | Solution |
|-------|-------------|----------|
| Chunks trop grands | Perte de pr√©cision, bruit | R√©duire √† 256-512 tokens |
| Chunks trop petits | Perte de contexte | Augmenter ou ajouter overlap |
| Pas de m√©tadonn√©es | Impossible de filtrer | Toujours stocker source, date, cat√©gorie |
| Un seul embedding par document | Mauvais recall sur longs docs | Chunker syst√©matiquement |
| Index HNSW sous-dimensionn√© | Mauvais recall | Augmenter M et EF_CONSTRUCTION |
| Mod√®le d'embedding inadapt√© | Mauvaise qualit√© s√©mantique | Tester plusieurs mod√®les sur votre domaine |

---

## ‚úÖ Points cl√©s √† retenir

- La **Semantic Search** recherche par **sens** plut√¥t que par mots-cl√©s, gr√¢ce aux embeddings vectoriels
- MariaDB 11.8 introduit le type `VECTOR` et les index `HNSW` pour une recherche vectorielle native et performante
- Le **chunking** (d√©coupage) des documents est crucial : 256-512 tokens avec 10-20% d'overlap
- Utilisez `VEC_DISTANCE_COSINE` pour la similarit√© s√©mantique textuelle
- L'index **HNSW** offre un excellent compromis pr√©cision/performance pour les recherches approximatives (ANN)
- La **recherche hybride** (s√©mantique + full-text) combine le meilleur des deux approches
- Les **m√©tadonn√©es relationnelles** permettent de combiner filtres SQL classiques et recherche vectorielle

---

## üîó Ressources et r√©f√©rences

- [üìñ MariaDB Vector Documentation](https://mariadb.com/kb/en/vector/) ‚Äî Documentation officielle MariaDB Vector
- [üìñ HNSW Index Documentation](https://mariadb.com/kb/en/hnsw-index/) ‚Äî Configuration et tuning des index HNSW
- [üìñ OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings) ‚Äî Guide des embeddings OpenAI
- [üìñ Sentence-BERT](https://www.sbert.net/) ‚Äî Mod√®les d'embedding open source
- [üìñ Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/) ‚Äî Strat√©gies de d√©coupage de documents

---

## ‚û°Ô∏è Section suivante

**20.9.2 Recommendation Engines** : D√©couvrez comment utiliser MariaDB Vector pour construire des syst√®mes de recommandation bas√©s sur la similarit√© entre utilisateurs, produits et comportements ‚Äî une application cl√© de la recherche vectorielle dans l'e-commerce et les plateformes de contenu.

‚è≠Ô∏è [Recommendation Engines](/20-cas-usage-architectures/09.2-recommendation-engines.md)
