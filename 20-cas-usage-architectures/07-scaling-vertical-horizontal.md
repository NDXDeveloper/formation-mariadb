ðŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 20.7 Scaling Vertical vs Horizontal

> **Niveau** : IntermÃ©diaire Ã  AvancÃ©  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : Chapitres 13-14 (RÃ©plication, Haute DisponibilitÃ©), Section 20.1 (OLTP vs OLAP), notions de capacity planning

## ðŸŽ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- Comprendre les diffÃ©rences fondamentales entre scaling vertical et horizontal
- Identifier les indicateurs qui signalent le besoin de scaling
- Choisir la stratÃ©gie de scaling adaptÃ©e selon le contexte
- ImplÃ©menter le scaling horizontal avec read replicas et sharding
- Utiliser Galera Cluster pour le scaling en Ã©criture
- Optimiser les coÃ»ts en combinant les approches de scaling
- Planifier la capacitÃ© et anticiper les besoins de croissance

---

## Introduction

Le **scaling** (mise Ã  l'Ã©chelle) est la capacitÃ© d'un systÃ¨me Ã  gÃ©rer une charge croissante. C'est l'un des dÃ©fis majeurs pour toute application en croissance. Deux approches fondamentales existent : **vertical** (scale-up) et **horizontal** (scale-out).

MariaDB 11.8 LTS offre des mÃ©canismes robustes pour les deux approches : optimisation des ressources pour le scaling vertical, et rÃ©plication/clustering pour le scaling horizontal.

### DÃ©finitions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALING VERTICAL VS HORIZONTAL                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  SCALING VERTICAL (Scale-Up)                                               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                               â”‚
â”‚  Augmenter les ressources d'un serveur unique                              â”‚
â”‚                                                                            â”‚
â”‚  Avant :                     AprÃ¨s :                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚    Serveur      â”‚         â”‚    Serveur      â”‚                           â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                           â”‚
â”‚  â”‚   â”‚ 16 CPU  â”‚   â”‚  â”€â”€â–º    â”‚   â”‚ 64 CPU  â”‚   â”‚                           â”‚
â”‚  â”‚   â”‚ 64 GB   â”‚   â”‚         â”‚   â”‚ 256 GB  â”‚   â”‚                           â”‚
â”‚  â”‚   â”‚ 1 TB SSDâ”‚   â”‚         â”‚   â”‚ 4 TB NVMeâ”‚  â”‚                           â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                            â”‚
â”‚  âœ… Simple : pas de changement d'architecture                              â”‚
â”‚  âœ… Pas de complexitÃ© distribuÃ©e                                           â”‚
â”‚  âŒ Limite physique (plus gros serveur disponible)                         â”‚
â”‚  âŒ Point unique de dÃ©faillance                                            â”‚
â”‚  âŒ CoÃ»t exponentiel aux hautes specs                                      â”‚
â”‚                                                                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                            â”‚
â”‚  SCALING HORIZONTAL (Scale-Out)                                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                            â”‚
â”‚  Ajouter plus de serveurs au cluster                                       â”‚
â”‚                                                                            â”‚
â”‚  Avant :                     AprÃ¨s :                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚    Serveur      â”‚         â”‚Node 1â”‚ â”‚Node 2â”‚ â”‚Node 3â”‚ â”‚Node 4â”‚           â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”€â”€â–º    â”‚      â”‚ â”‚      â”‚ â”‚      â”‚ â”‚      â”‚           â”‚
â”‚  â”‚   â”‚ 16 CPU  â”‚   â”‚         â”‚16 CPUâ”‚ â”‚16 CPUâ”‚ â”‚16 CPUâ”‚ â”‚16 CPUâ”‚           â”‚
â”‚  â”‚   â”‚ 64 GB   â”‚   â”‚         â”‚64 GB â”‚ â”‚64 GB â”‚ â”‚64 GB â”‚ â”‚64 GB â”‚           â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         Total: 64 CPU, 256 GB (distribuÃ©)             â”‚
â”‚                                                                            â”‚
â”‚  âœ… ScalabilitÃ© quasi-illimitÃ©e                                            â”‚
â”‚  âœ… Haute disponibilitÃ© native                                             â”‚
â”‚  âœ… CoÃ»t linÃ©aire                                                          â”‚
â”‚  âŒ ComplexitÃ© de l'architecture distribuÃ©e                                â”‚
â”‚  âŒ Latence rÃ©seau entre nÅ“uds                                             â”‚
â”‚  âŒ CohÃ©rence des donnÃ©es (CAP theorem)                                    â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quand scaler ?

### Indicateurs de besoin de scaling

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- INDICATEURS DE SATURATION - QUAND SCALER ?
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- 1. CPU : Utilisation Ã©levÃ©e
-- Alerte si > 70% sur pÃ©riode prolongÃ©e
SELECT 
    VARIABLE_NAME,
    VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS
WHERE VARIABLE_NAME IN (
    'Threads_running',      -- Threads actifs (vs max_connections)
    'Slow_queries',         -- RequÃªtes lentes
    'Handler_read_rnd_next' -- Full table scans (CPU intensive)
);

-- 2. MÃ©moire : Buffer Pool sous pression
-- Objectif : Hit ratio > 99%
SELECT 
    (1 - (
        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
         WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads') /
        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
         WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests')
    )) * 100 AS buffer_pool_hit_ratio;

-- Si < 99%, le buffer pool est trop petit â†’ Scale vertical (RAM)

-- 3. I/O : Disque saturÃ©
SELECT 
    VARIABLE_NAME,
    VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS
WHERE VARIABLE_NAME IN (
    'Innodb_data_reads',
    'Innodb_data_writes',
    'Innodb_data_pending_reads',
    'Innodb_data_pending_writes'  -- > 0 = I/O saturÃ©
);

-- 4. Connexions : Pool Ã©puisÃ©
SELECT 
    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
     WHERE VARIABLE_NAME = 'Threads_connected') AS current_connections,
    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_VARIABLES 
     WHERE VARIABLE_NAME = 'max_connections') AS max_connections,
    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
     WHERE VARIABLE_NAME = 'Max_used_connections') AS peak_connections,
    ROUND(
        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
         WHERE VARIABLE_NAME = 'Threads_connected') * 100.0 /
        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_VARIABLES 
         WHERE VARIABLE_NAME = 'max_connections')
    , 1) AS connection_usage_pct;

-- Si > 80% â†’ Scale horizontal (read replicas) ou vertical (RAM)

-- 5. RÃ©plication : Lag croissant
-- Si le replica ne suit plus â†’ Scale vertical (replica) ou ajouter replicas
SHOW SLAVE STATUS\G
-- Seconds_Behind_Master croissant = replica sous-dimensionnÃ©

-- 6. Temps de rÃ©ponse dÃ©gradÃ©
SELECT 
    schema_name,
    ROUND(SUM(sum_timer_wait) / SUM(count_star) / 1000000000, 3) AS avg_query_time_ms,
    SUM(count_star) AS total_queries
FROM performance_schema.events_statements_summary_by_digest
WHERE schema_name NOT IN ('mysql', 'information_schema', 'performance_schema')
GROUP BY schema_name
HAVING avg_query_time_ms > 100  -- Alerte si > 100ms moyenne
ORDER BY avg_query_time_ms DESC;
```

### Matrice de dÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MATRICE DE DÃ‰CISION SCALING                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ProblÃ¨me observÃ©          â”‚ Scale Vertical â”‚ Scale Horizontal              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  CPU saturÃ©                â”‚ âœ… Plus cores  â”‚ âœ… Read replicas (reads)      â”‚
â”‚  MÃ©moire insuffisante      â”‚ âœ… Plus RAM    â”‚ âš ï¸ Sharding (si > 1TB)        â”‚
â”‚  I/O disque saturÃ©         â”‚ âœ… NVMe/RAID   â”‚ âœ… Sharding                   â”‚
â”‚  Connexions Ã©puisÃ©es       â”‚ âš ï¸ LimitÃ©     â”‚ âœ… Read replicas + ProxySQL    â”‚
â”‚  Writes saturent           â”‚ âœ… Disque+CPU  â”‚ âœ… Galera / Sharding          â”‚
â”‚  Reads saturent            â”‚ âš ï¸ LimitÃ©     â”‚ âœ… Read replicas               â”‚
â”‚  Latence gÃ©ographique      â”‚ âŒ N/A        â”‚ âœ… Replicas rÃ©gionaux          â”‚
â”‚  Taille base > RAM         â”‚ âš ï¸ CoÃ»teux   â”‚ âœ… Sharding                     â”‚
â”‚  HA requise                â”‚ âŒ SPOF       â”‚ âœ… Cluster/Replicas            â”‚
â”‚                                                                             â”‚
â”‚  LÃ©gende :                                                                  â”‚
â”‚  âœ… Solution recommandÃ©e                                                    â”‚
â”‚  âš ï¸ Possible mais limitÃ©                                                    â”‚
â”‚  âŒ Non applicable                                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Scaling Vertical

### Optimisation des ressources MariaDB

```ini
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION OPTIMISÃ‰E - SCALING VERTICAL
# /etc/mysql/mariadb.conf.d/scaled-vertical.cnf
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[mysqld]
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MÃ‰MOIRE (Exemple : Serveur 256 GB RAM)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Buffer Pool : 70-80% de la RAM pour DB dÃ©diÃ©e
innodb_buffer_pool_size = 180G
innodb_buffer_pool_instances = 16  # 1 instance par 1-2 GB
innodb_buffer_pool_chunk_size = 1G

# Chargement Ã  chaud au dÃ©marrage
innodb_buffer_pool_dump_at_shutdown = ON
innodb_buffer_pool_load_at_startup = ON

# Log Buffer (pour gros batch inserts)
innodb_log_buffer_size = 256M

# ðŸ†• MariaDB 11.8 : ContrÃ´le de l'espace temporaire
max_tmp_space_usage = 16G
max_total_tmp_space_usage = 64G

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CPU (Exemple : 64 cores)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ParallÃ©lisme InnoDB
innodb_read_io_threads = 16
innodb_write_io_threads = 16
innodb_purge_threads = 8

# Thread pool (mieux que thread-per-connection pour > 100 connexions)
thread_handling = pool-of-threads
thread_pool_size = 32  # ~50% des cores
thread_pool_max_threads = 1000
thread_pool_idle_timeout = 60

# Concurrence
innodb_thread_concurrency = 0  # Auto

# ðŸ†• MariaDB 11.8 : Optimisation pour NVMe
optimizer_disk_read_ratio = 0.0002

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STOCKAGE (Exemple : NVMe RAID 10)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# I/O Capacity (ajuster selon benchmark)
innodb_io_capacity = 20000       # NVMe haute performance
innodb_io_capacity_max = 40000

# Flush method (O_DIRECT pour Ã©viter double buffering)
innodb_flush_method = O_DIRECT

# Redo Log (2-4 GB pour gros volumes)
innodb_log_file_size = 4G
innodb_log_files_in_group = 2

# Checkpointing
innodb_max_dirty_pages_pct = 75
innodb_max_dirty_pages_pct_lwm = 10

# Compression (Ã©conomise I/O au prix de CPU)
# innodb_compression_default = ON  # Si CPU disponible

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNEXIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

max_connections = 2000  # AugmentÃ© grÃ¢ce au thread pool
max_user_connections = 1800

# Timeouts pour libÃ©rer les connexions inactives
wait_timeout = 600
interactive_timeout = 3600

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CACHES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Table cache
table_open_cache = 8000
table_definition_cache = 4000

# Query cache (dÃ©sactivÃ© en 11.8, utiliser ProxySQL pour caching)
# query_cache_type = OFF

# Join buffer (par connexion, attention Ã  la RAM totale)
join_buffer_size = 4M
sort_buffer_size = 4M
read_buffer_size = 2M
read_rnd_buffer_size = 2M
```

### Limites du scaling vertical

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LIMITES DU SCALING VERTICAL                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Ressource    â”‚ Limite pratique      â”‚ Limite absolue                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â• â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚  RAM          â”‚ 2-4 TB (coÃ»t)        â”‚ 12 TB (plus gros serveurs)           â”‚
â”‚  CPU          â”‚ 128-256 cores        â”‚ 512 cores (multi-socket)             â”‚
â”‚  Stockage     â”‚ 100 TB SSD (coÃ»t)    â”‚ PÃ©taoctets (SAN)                     â”‚
â”‚  IOPS         â”‚ 1M IOPS NVMe         â”‚ 10M IOPS (NVMe array)                â”‚
â”‚  RÃ©seau       â”‚ 100 Gbps             â”‚ 400 Gbps                             â”‚
â”‚                                                                             â”‚
â”‚  ProblÃ¨mes au-delÃ  des limites pratiques :                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                   â”‚
â”‚                                                                             â”‚
â”‚  ðŸ’° COÃ›T EXPONENTIEL                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Exemple AWS (EC2 + EBS) :                                        â”‚      â”‚
â”‚  â”‚  â€¢ r6g.xlarge  (4 CPU, 32 GB)   : ~$250/mois                      â”‚      â”‚
â”‚  â”‚  â€¢ r6g.4xlarge (16 CPU, 128 GB) : ~$1,000/mois (4x specs = 4x $)  â”‚      â”‚
â”‚  â”‚  â€¢ r6g.16xlarge (64 CPU, 512 GB): ~$4,500/mois (16x specs = 18x $)â”‚      â”‚
â”‚  â”‚                                                                   â”‚      â”‚
â”‚  â”‚  Le coÃ»t croÃ®t plus vite que les specs !                          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â”‚  â±ï¸ DOWNTIME POUR UPGRADE                                                   â”‚
â”‚  â€¢ Migration vers instance plus grande = downtime                           â”‚
â”‚  â€¢ Exceptions : AWS Graviton, Azure Flex (online resize limitÃ©)             â”‚
â”‚                                                                             â”‚
â”‚  ðŸŽ¯ POINT UNIQUE DE DÃ‰FAILLANCE                                             â”‚
â”‚  â€¢ Un seul serveur = si crash, tout tombe                                   â”‚
â”‚  â€¢ MÃªme avec HA, le failover prend du temps                                 â”‚
â”‚                                                                             â”‚
â”‚  ðŸ“ˆ RENDEMENTS DÃ‰CROISSANTS                                                 â”‚
â”‚  â€¢ Locks, contention, latence interne                                       â”‚
â”‚  â€¢ Doubler la RAM â‰  doubler les performances                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Scaling Horizontal

### 1. Read Replicas

La mÃ©thode la plus simple pour scaler les lectures.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE READ REPLICAS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                            Applications                                     â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                    â”‚      MaxScale /     â”‚                                  â”‚
â”‚                    â”‚      ProxySQL       â”‚                                  â”‚
â”‚                    â”‚   (Read/Write Split)â”‚                                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                               â”‚                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚                â”‚                â”‚                            â”‚
â”‚        Writesâ”‚          Reads â”‚                â”‚ Reads                      â”‚
â”‚              â–¼                â–¼                â–¼                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚   Primary   â”‚   â”‚  Replica 1  â”‚   â”‚  Replica 2  â”‚                     â”‚
â”‚     â”‚    (RW)     â”‚â”€â”€â–ºâ”‚    (RO)     â”‚   â”‚    (RO)     â”‚                     â”‚
â”‚     â”‚             â”‚   â”‚             â”‚   â”‚             â”‚                     â”‚
â”‚     â”‚  Writes +   â”‚   â”‚   Reads     â”‚   â”‚   Reads     â”‚                     â”‚
â”‚     â”‚  Reads      â”‚   â”‚   only      â”‚   â”‚   only      â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚            â”‚                â–²                 â–²                             â”‚
â”‚            â”‚                â”‚                 â”‚                             â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                    Async Replication                                        â”‚
â”‚                                                                             â”‚
â”‚  Scaling :                                                                  â”‚
â”‚  â€¢ Ajouter des replicas = multiplier la capacitÃ© de lecture                 â”‚
â”‚  â€¢ Lectures : N replicas â†’ N Ã— capacitÃ©                                     â”‚
â”‚  â€¢ Ã‰critures : 1 primary â†’ pas de scaling                                   â”‚
â”‚                                                                             â”‚
â”‚  Ratio typique : 80-90% reads â†’ 3-5 replicas suffisent                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```yaml
# maxscale-read-replicas.cnf
# Configuration MaxScale pour read/write split

[maxscale]
threads = auto

[primary]
type = server
address = primary.db.internal
port = 3306
protocol = MariaDBBackend

[replica1]
type = server
address = replica1.db.internal
port = 3306
protocol = MariaDBBackend

[replica2]
type = server
address = replica2.db.internal
port = 3306
protocol = MariaDBBackend

[replica3]
type = server
address = replica3.db.internal
port = 3306
protocol = MariaDBBackend

[replication-monitor]
type = monitor
module = mariadbmon
servers = primary, replica1, replica2, replica3
user = maxscale_monitor
password = secure_password
monitor_interval = 2000ms
auto_failover = true
auto_rejoin = true

[rw-split-service]
type = service
router = readwritesplit
servers = primary, replica1, replica2, replica3
user = maxscale_user
password = secure_password

# Politique de routing
master_accept_reads = false           # Writes seulement sur primary
slave_selection_criteria = ADAPTIVE   # Choix intelligent du replica
max_slave_replication_lag = 10s       # Exclure replicas en retard

# Retry sur erreur
transaction_replay = true
transaction_replay_max_size = 10Mi

[rw-split-listener]
type = listener
service = rw-split-service
protocol = MariaDBClient
port = 3306
```

### 2. Galera Cluster (Multi-Primary)

Scaling des Ã©critures avec synchronisation synchrone.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GALERA CLUSTER - MULTI-PRIMARY                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                            Applications                                     â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                    â”‚   Load Balancer     â”‚                                  â”‚
â”‚                    â”‚   (Round-robin)     â”‚                                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                               â”‚                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚                â”‚                â”‚                            â”‚
â”‚              â–¼                â–¼                â–¼                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚   Node 1    â”‚â—„â”€â–ºâ”‚   Node 2    â”‚â—„â”€â–ºâ”‚   Node 3    â”‚                     â”‚
â”‚     â”‚    (RW)     â”‚   â”‚    (RW)     â”‚   â”‚    (RW)     â”‚                     â”‚
â”‚     â”‚             â”‚   â”‚             â”‚   â”‚             â”‚                     â”‚
â”‚     â”‚  Writes +   â”‚   â”‚  Writes +   â”‚   â”‚  Writes +   â”‚                     â”‚
â”‚     â”‚  Reads      â”‚   â”‚  Reads      â”‚   â”‚  Reads      â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚            â–²                â–²                 â–²                             â”‚
â”‚            â”‚                â”‚                 â”‚                             â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                 Synchronous Replication                                     â”‚
â”‚                 (Certification-based)                                       â”‚
â”‚                                                                             â”‚
â”‚  CaractÃ©ristiques :                                                         â”‚
â”‚  â€¢ Tous les nÅ“uds acceptent les Ã©critures                                   â”‚
â”‚  â€¢ RÃ©plication synchrone (pas de perte de donnÃ©es)                          â”‚
â”‚  â€¢ DÃ©tection automatique des conflits                                       â”‚
â”‚                                                                             â”‚
â”‚  Attention :                                                                â”‚
â”‚  â€¢ Latence d'Ã©criture = RTT rÃ©seau (tous les nÅ“uds doivent certifier)       â”‚
â”‚  â€¢ Conflicts possibles (mÃªme row modifiÃ©e simultanÃ©ment)                    â”‚
â”‚  â€¢ Scaling limitÃ© (~5-7 nÅ“uds pour writes intensifs)                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```ini
# /etc/mysql/mariadb.conf.d/galera.cnf
# Configuration Galera Cluster optimisÃ©e pour scaling

[mysqld]
# Galera Provider
wsrep_on = ON
wsrep_provider = /usr/lib/galera/libgalera_smm.so
wsrep_cluster_name = "production-cluster"
wsrep_cluster_address = "gcomm://node1,node2,node3"
wsrep_node_name = "node1"
wsrep_node_address = "10.0.1.10"

# RÃ©plication
wsrep_sst_method = mariabackup
wsrep_sst_auth = sst_user:sst_password

# Performance
wsrep_slave_threads = 8              # ParallÃ©lisme applyer
wsrep_provider_options = "gcache.size=2G; gcs.fc_limit=256"

# TolÃ©rance aux conflits
wsrep_retry_autocommit = 3           # Retry sur deadlock

# InnoDB settings pour Galera
innodb_autoinc_lock_mode = 2         # Requis pour Galera
innodb_flush_log_at_trx_commit = 2   # Performance (Galera garantit durabilitÃ©)

# Binlog pour replicas asynchrones additionnels
log_bin = mysql-bin
binlog_format = ROW
```

### 3. Sharding

Division horizontale des donnÃ©es sur plusieurs serveurs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SHARDING - DIVISION DES DONNÃ‰ES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                            Applications                                     â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                    â”‚    Shard Router     â”‚                                  â”‚
â”‚                    â”‚   (ProxySQL /       â”‚                                  â”‚
â”‚                    â”‚    Application)     â”‚                                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                               â”‚                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚                     â”‚                     â”‚                       â”‚
â”‚         â–¼                     â–¼                     â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Shard 1   â”‚       â”‚   Shard 2   â”‚       â”‚   Shard 3   â”‚                â”‚
â”‚  â”‚             â”‚       â”‚             â”‚       â”‚             â”‚                â”‚
â”‚  â”‚ user_id     â”‚       â”‚ user_id     â”‚       â”‚ user_id     â”‚                â”‚
â”‚  â”‚  1-1M       â”‚       â”‚  1M-2M      â”‚       â”‚  2M-3M      â”‚                â”‚
â”‚  â”‚             â”‚       â”‚             â”‚       â”‚             â”‚                â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                â”‚
â”‚  â”‚ â”‚ Primary â”‚ â”‚       â”‚ â”‚ Primary â”‚ â”‚       â”‚ â”‚ Primary â”‚ â”‚                â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚       â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚       â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚                â”‚
â”‚  â”‚      â”‚      â”‚       â”‚      â”‚      â”‚       â”‚      â”‚      â”‚                â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚       â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚       â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚                â”‚
â”‚  â”‚ â”‚ Replica â”‚ â”‚       â”‚ â”‚ Replica â”‚ â”‚       â”‚ â”‚ Replica â”‚ â”‚                â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                             â”‚
â”‚  StratÃ©gies de sharding :                                                   â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                    â”‚
â”‚                                                                             â”‚
â”‚  1. Range-based : user_id 1-1M â†’ Shard 1, 1M-2M â†’ Shard 2                   â”‚
â”‚     âœ… Simple, requÃªtes de range efficaces                                  â”‚
â”‚     âŒ Hotspots si distribution inÃ©gale                                     â”‚
â”‚                                                                             â”‚
â”‚  2. Hash-based : shard = hash(user_id) % N                                  â”‚
â”‚     âœ… Distribution uniforme                                                â”‚
â”‚     âŒ RequÃªtes de range sur tous les shards                                â”‚
â”‚                                                                             â”‚
â”‚  3. Directory-based : Table de mapping user_id â†’ shard                      â”‚
â”‚     âœ… Flexible, migration facile                                           â”‚
â”‚     âŒ Lookup additionnel                                                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation du sharding

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SHARDING : EXEMPLE D'IMPLÃ‰MENTATION
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Table de routage des shards (sur un serveur de configuration)
CREATE TABLE shard_config (
    shard_id INT PRIMARY KEY,
    shard_name VARCHAR(50) NOT NULL,
    host VARCHAR(255) NOT NULL,
    port INT DEFAULT 3306,
    min_key BIGINT NOT NULL,
    max_key BIGINT NOT NULL,
    status ENUM('active', 'migrating', 'readonly', 'offline') DEFAULT 'active',
    
    INDEX idx_key_range (min_key, max_key)
) ENGINE=InnoDB;

INSERT INTO shard_config VALUES
    (1, 'shard_1', 'shard1.db.internal', 3306, 1, 1000000, 'active'),
    (2, 'shard_2', 'shard2.db.internal', 3306, 1000001, 2000000, 'active'),
    (3, 'shard_3', 'shard3.db.internal', 3306, 2000001, 3000000, 'active');

-- Fonction de routing (cÃ´tÃ© application ou proxy)
DELIMITER //

CREATE FUNCTION get_shard_for_user(p_user_id BIGINT)
RETURNS INT
DETERMINISTIC
READS SQL DATA
BEGIN
    DECLARE v_shard_id INT;
    
    SELECT shard_id INTO v_shard_id
    FROM shard_config
    WHERE p_user_id BETWEEN min_key AND max_key
      AND status = 'active'
    LIMIT 1;
    
    RETURN v_shard_id;
END //

DELIMITER ;

-- Utilisation
SELECT get_shard_for_user(1500000);  -- Retourne 2

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SCHÃ‰MA SUR CHAQUE SHARD
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Chaque shard contient une partie des donnÃ©es
-- La shard_key (user_id) est incluse dans les PKs pour unicitÃ© globale

CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,  -- Unique globalement
    email VARCHAR(255) NOT NULL,
    name VARCHAR(200),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- DonnÃ©es locales au shard
    UNIQUE KEY uk_email (email)
) ENGINE=InnoDB;

CREATE TABLE orders (
    order_id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,  -- Shard key
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    total_amount DECIMAL(12,2),
    
    PRIMARY KEY (order_id, user_id),  -- Include shard key in PK
    INDEX idx_user (user_id, order_date),
    
    FOREIGN KEY (user_id) REFERENCES users(user_id)
) ENGINE=InnoDB;

-- Note: Les FK cross-shard sont impossibles
-- â†’ DÃ©normalisation ou vÃ©rification applicative
```

### Configuration ProxySQL pour sharding

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- PROXYSQL : CONFIGURATION SHARDING
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Connexion Ã  l'admin ProxySQL
-- mysql -u admin -p -h 127.0.0.1 -P 6032

-- DÃ©finir les hostgroups (un par shard)
INSERT INTO mysql_servers (hostgroup_id, hostname, port, weight) VALUES
    (10, 'shard1-primary.db.internal', 3306, 1000),
    (11, 'shard1-replica.db.internal', 3306, 1000),
    (20, 'shard2-primary.db.internal', 3306, 1000),
    (21, 'shard2-replica.db.internal', 3306, 1000),
    (30, 'shard3-primary.db.internal', 3306, 1000),
    (31, 'shard3-replica.db.internal', 3306, 1000);

-- RÃ¨gles de routing basÃ©es sur user_id
-- Utilise une regex pour extraire user_id et router vers le bon shard

-- Shard 1 : user_id 1-1000000
INSERT INTO mysql_query_rules (
    rule_id, active, match_pattern, destination_hostgroup, apply
) VALUES (
    100, 1, 
    'WHERE.*user_id\s*=\s*([1-9][0-9]{0,5}|1000000)\b',
    10, 1
);

-- Shard 2 : user_id 1000001-2000000
INSERT INTO mysql_query_rules (
    rule_id, active, match_pattern, destination_hostgroup, apply
) VALUES (
    200, 1,
    'WHERE.*user_id\s*=\s*(100000[1-9]|10000[1-9][0-9]|1000[1-9][0-9]{2}|100[1-9][0-9]{3}|10[1-9][0-9]{4}|1[1-9][0-9]{5}|2000000)\b',
    20, 1
);

-- Shard 3 : user_id 2000001-3000000
INSERT INTO mysql_query_rules (
    rule_id, active, match_pattern, destination_hostgroup, apply
) VALUES (
    300, 1,
    'WHERE.*user_id\s*=\s*(200000[1-9]|20000[1-9][0-9]|2000[1-9][0-9]{2}|200[1-9][0-9]{3}|20[1-9][0-9]{4}|2[1-9][0-9]{5}|3000000)\b',
    30, 1
);

-- Read/Write split par shard
INSERT INTO mysql_query_rules (
    rule_id, active, match_pattern, destination_hostgroup, apply
) VALUES
    (101, 1, '^SELECT.*FOR UPDATE', 10, 1),  -- Shard 1 writes
    (102, 1, '^SELECT', 11, 1);              -- Shard 1 reads

-- Appliquer les changements
LOAD MYSQL SERVERS TO RUNTIME;
LOAD MYSQL QUERY RULES TO RUNTIME;
SAVE MYSQL SERVERS TO DISK;
SAVE MYSQL QUERY RULES TO DISK;
```

---

## Comparaison des stratÃ©gies

### Comparaison des StratÃ©gies de Scaling

#### Tableau comparatif

| StratÃ©gie | Reads | Writes | ComplexitÃ© | CoÃ»t | HA | Consistency |
|-----------|:-----:|:------:|:----------:|:----:|:--:|:-----------:|
| **Vertical** | â­â­ | â­â­ | â­ | $$$$ | âŒ | âœ… |
| **Read Replicas** | â­â­â­â­ | â­ | â­â­ | $$ | âœ… | âš ï¸ * |
| **Galera Cluster** | â­â­â­ | â­â­â­ | â­â­â­ | $$$ | âœ…âœ… | âœ… |
| **Sharding** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | $$ | âœ… | âœ… |
| **Sharding + Galera** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | $$$ | âœ…âœ… | âœ… |

> \* Eventual consistency avec async replication

---

#### LÃ©gende

##### Performance (â­)
| Symbole | Signification |
|:-------:|---------------|
| â­ | LimitÃ© |
| â­â­ | Basique |
| â­â­â­ | Bon |
| â­â­â­â­ | Excellent |
| â­â­â­â­â­ | IllimitÃ© |

##### CoÃ»t ($)
| Symbole | Signification |
|:-------:|---------------|
| $ | Ã‰conomique |
| $$ | ModÃ©rÃ© |
| $$$ | Ã‰levÃ© |
| $$$$ | TrÃ¨s Ã©levÃ© |

---

#### Recommandations par Use Case

| Use Case | StratÃ©gie recommandÃ©e |
|----------|----------------------|
| ðŸš€ **Startup / MVP** | Vertical *(simple, rapide)* |
| ðŸ“Š **SaaS read-heavy** (80%+ reads) | Read Replicas *(3-5 replicas)* |
| ðŸ›’ **E-commerce transactionnel** | Galera Cluster *(3-5 nodes)* |
| ðŸ“± **Social media / Big data** | Sharding *(par user_id)* |
| ðŸ’° **Finance / High write volume** | Sharding + Galera par shard |
| ðŸŽ® **Gaming global** | Sharding gÃ©ographique + Galera |

---

## Scaling automatique

### Auto-scaling des replicas

```yaml
# kubernetes-mariadb-autoscaling.yaml
# HPA pour pods read replicas MariaDB

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mariadb-replicas-hpa
  namespace: database
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: mariadb-replicas
  
  minReplicas: 2
  maxReplicas: 10
  
  metrics:
    # Scale sur CPU
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    # Scale sur connexions (mÃ©trique custom)
    - type: Pods
      pods:
        metric:
          name: mysql_connections_active
        target:
          type: AverageValue
          averageValue: 100
    
    # Scale sur lag de rÃ©plication
    - type: Pods
      pods:
        metric:
          name: mysql_slave_lag_seconds
        target:
          type: AverageValue
          averageValue: 5  # Scale si lag > 5s
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
```

```bash
#!/bin/bash
# auto-scale-replicas.sh
# Script d'auto-scaling pour environnements non-Kubernetes

METRICS_HOST="prometheus.internal:9090"
MIN_REPLICAS=2
MAX_REPLICAS=10
SCALE_UP_THRESHOLD=70    # CPU %
SCALE_DOWN_THRESHOLD=30  # CPU %

# RÃ©cupÃ©rer le nombre actuel de replicas
current_replicas=$(terraform output -raw replica_count)

# RÃ©cupÃ©rer la charge CPU moyenne
avg_cpu=$(curl -s "$METRICS_HOST/api/v1/query?query=avg(mysql_cpu_usage_percent)" | jq -r '.data.result[0].value[1]')

echo "Current replicas: $current_replicas, Avg CPU: $avg_cpu%"

if (( $(echo "$avg_cpu > $SCALE_UP_THRESHOLD" | bc -l) )); then
    if [ "$current_replicas" -lt "$MAX_REPLICAS" ]; then
        new_count=$((current_replicas + 1))
        echo "Scaling UP to $new_count replicas"
        terraform apply -var="replica_count=$new_count" -auto-approve
    fi
elif (( $(echo "$avg_cpu < $SCALE_DOWN_THRESHOLD" | bc -l) )); then
    if [ "$current_replicas" -gt "$MIN_REPLICAS" ]; then
        new_count=$((current_replicas - 1))
        echo "Scaling DOWN to $new_count replicas"
        terraform apply -var="replica_count=$new_count" -auto-approve
    fi
else
    echo "No scaling needed"
fi
```

---

## CoÃ»ts et optimisation

### Analyse coÃ»t/performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANALYSE COÃ›T / PERFORMANCE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  ScÃ©nario : 10K QPS, 80% reads, 20% writes, 500 GB donnÃ©es                 â”‚
â”‚                                                                            â”‚
â”‚  Option 1 : SCALE VERTICAL                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                â”‚
â”‚  â€¢ 1 Ã— r6g.16xlarge (64 CPU, 512 GB) + 1 TB gp3                            â”‚
â”‚  â€¢ CoÃ»t : ~$5,000/mois                                                     â”‚
â”‚  â€¢ CapacitÃ© : ~15K QPS max                                                 â”‚
â”‚  â€¢ HA : Besoin d'un replica â†’ $10,000/mois                                 â”‚
â”‚                                                                            â”‚
â”‚  Option 2 : SCALE HORIZONTAL (Read Replicas)                               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                              â”‚
â”‚  â€¢ 1 Ã— r6g.4xlarge Primary (writes)                                        â”‚
â”‚  â€¢ 3 Ã— r6g.2xlarge Replicas (reads)                                        â”‚
â”‚  â€¢ CoÃ»t : $1,000 + 3Ã—$500 = $2,500/mois                                    â”‚
â”‚  â€¢ CapacitÃ© : 2K writes + 24K reads = OK                                   â”‚
â”‚  â€¢ HA : Native (failover vers replica)                                     â”‚
â”‚                                                                            â”‚
â”‚  Option 3 : GALERA CLUSTER                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                â”‚
â”‚  â€¢ 3 Ã— r6g.4xlarge (Galera multi-master)                                   â”‚
â”‚  â€¢ CoÃ»t : 3Ã—$1,000 = $3,000/mois                                           â”‚
â”‚  â€¢ CapacitÃ© : ~8K QPS distribuÃ©                                            â”‚
â”‚  â€¢ HA : Native (n'importe quel node peut Ãªtre primary)                     â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Comparaison (10K QPS)                                             â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â”‚  Option        CoÃ»t/mois  CapacitÃ©   HA    ComplexitÃ©              â”‚    â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚    â”‚
â”‚  â”‚  Vertical HA    $10,000    15K QPS   âš ï¸     â­                     â”‚    â”‚
â”‚  â”‚  Replicas       $2,500     26K QPS   âœ…     â­â­                   â”‚    â”‚
â”‚  â”‚  Galera         $3,000     8K QPS    âœ…âœ…    â­â­â­                â”‚    â”‚
â”‚  â”‚                                                                    â”‚    â”‚
â”‚  â”‚  Winner pour ce scÃ©nario : Read Replicas ($2,500)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                            â”‚
â”‚  Conseil : Commencer par vertical, migrer vers horizontal Ã  70% capacitÃ©   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ã‰tude de cas : Migration de scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Ã‰TUDE DE CAS : Ã‰VOLUTION D'UNE STARTUP                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Phase 1 : Lancement (0-10K users)                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                        â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚  Single Node    â”‚  r6g.xlarge (4 CPU, 32 GB)                            â”‚
â”‚  â”‚  MariaDB        â”‚  $250/mois                                            â”‚
â”‚  â”‚                 â”‚  CapacitÃ© : 500 QPS                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚                                                                            â”‚
â”‚  Phase 2 : Croissance (10K-100K users)                                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                     â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚  Scaled Up      â”‚  r6g.4xlarge (16 CPU, 128 GB)                         â”‚
â”‚  â”‚  MariaDB        â”‚  $1,000/mois                                          â”‚
â”‚  â”‚                 â”‚  CapacitÃ© : 3K QPS                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚                                                                            â”‚
â”‚  Phase 3 : Traction (100K-500K users)                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                     â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚    Primary      â”‚â”€â”€â”€â–ºâ”‚    Replica      â”‚                                â”‚
â”‚  â”‚  r6g.4xlarge    â”‚    â”‚  r6g.2xlarge    â”‚                                â”‚
â”‚  â”‚  (Writes)       â”‚    â”‚  (Reads)        â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                            â”‚
â”‚  $1,500/mois, CapacitÃ© : 5K QPS, HA: âœ…                                    â”‚
â”‚                                                                            â”‚
â”‚  Phase 4 : Scale (500K-2M users)                                           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚    Primary      â”‚â”€â”€â”€â–ºâ”‚   Replica 1     â”‚    â”‚   Replica 2     â”‚         â”‚
â”‚  â”‚  r6g.4xlarge    â”‚    â”‚  r6g.2xlarge    â”‚    â”‚  r6g.2xlarge    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                   â”‚                    â”‚                   â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                          â”‚                                 â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                              â”‚      MaxScale         â”‚                     â”‚
â”‚                              â”‚   (R/W Split + LB)    â”‚                     â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                            â”‚
â”‚  $2,500/mois, CapacitÃ© : 15K QPS, HA: âœ…âœ…                                 â”‚
â”‚                                                                            â”‚
â”‚  Phase 5 : Explosion (2M+ users)                                           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                          â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         ProxySQL (Sharding)                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                         â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚       â”‚                          â”‚                          â”‚              â”‚
â”‚       â–¼                          â–¼                          â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Shard 1    â”‚           â”‚  Shard 2    â”‚           â”‚  Shard 3    â”‚       â”‚
â”‚  â”‚  Galera 3   â”‚           â”‚  Galera 3   â”‚           â”‚  Galera 3   â”‚       â”‚
â”‚  â”‚  (US users) â”‚           â”‚  (EU users) â”‚           â”‚  (APAC)     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                            â”‚
â”‚  $12,000/mois, CapacitÃ© : 100K+ QPS, HA: âœ…âœ…âœ…                            â”‚
â”‚                                                                            â”‚
â”‚  LeÃ§ons :                                                                  â”‚
â”‚  â•â•â•â•â•â•â•â•                                                                  â”‚
â”‚  â€¢ Commencer simple (vertical)                                             â”‚
â”‚  â€¢ Ajouter des replicas avant le scaling complexe                          â”‚
â”‚  â€¢ Sharding seulement quand nÃ©cessaire (complexitÃ© ++)                     â”‚
â”‚  â€¢ Chaque transition = refactoring applicatif                              â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Points clÃ©s Ã  retenir

- **Scaling vertical** est simple mais limitÃ© â€” commencer par lÃ , migrer quand on atteint 70% capacitÃ©
- **Read replicas** sont la premiÃ¨re Ã©tape du scaling horizontal â€” efficace pour les workloads read-heavy (80%+ reads)
- **Galera Cluster** permet le scaling des Ã©critures avec consistance forte â€” limitÃ© Ã  ~5-7 nÅ“uds pour performance optimale
- **Sharding** offre un scaling quasi-illimitÃ© mais au prix d'une complexitÃ© significative â€” rÃ©server pour > 1TB ou > 50K QPS
- **MaxScale/ProxySQL** sont essentiels pour le routing intelligent (R/W split, load balancing, sharding)
- **Auto-scaling** permet d'ajuster dynamiquement la capacitÃ© selon la charge
- **Le coÃ»t par QPS** diminue avec le scaling horizontal â€” mais la complexitÃ© opÃ©rationnelle augmente
- ðŸ†• **MariaDB 11.8** avec `optimizer_disk_read_ratio` optimise automatiquement pour le stockage NVMe

---

## ðŸ”— Ressources et rÃ©fÃ©rences

- [ðŸ“– MariaDB Replication](https://mariadb.com/kb/en/replication/)
- [ðŸ“– Galera Cluster Documentation](https://galeracluster.com/library/documentation/)
- [ðŸ“– MaxScale Readwritesplit](https://mariadb.com/kb/en/maxscale-readwritesplit/)
- [ðŸ“– ProxySQL Configuration](https://proxysql.com/documentation/)
- [ðŸ“– Database Sharding Patterns](https://www.citusdata.com/blog/2018/01/10/sharding-in-plain-english/)
- [ðŸ“– Scaling MySQL - O'Reilly](https://www.oreilly.com/library/view/high-performance-mysql/9781449332471/)

---

## âž¡ï¸ Section suivante

[20.8 Architectures Event-Driven](./08-architectures-event-driven.md) : DÃ©couvrez comment intÃ©grer MariaDB dans des architectures Ã©vÃ©nementielles avec CDC, Kafka et Debezium.

â­ï¸ [MariaDB dans les architectures Event-Driven](/20-cas-usage-architectures/08-architectures-event-driven.md)
