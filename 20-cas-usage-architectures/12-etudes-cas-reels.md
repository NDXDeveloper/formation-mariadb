ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 20.12 Ã‰tudes de Cas RÃ©els

> **Niveau** : AvancÃ©  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : Chapitres 13-14, Sections 20.1 Ã  20.11

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- Analyser des architectures MariaDB dÃ©ployÃ©es en production
- Comprendre les dÃ©cisions de conception dans diffÃ©rents contextes mÃ©tier
- Appliquer les patterns Ã©tudiÃ©s Ã  vos propres projets
- Anticiper les dÃ©fis et les solutions dans des scÃ©narios rÃ©els
- Ã‰valuer les compromis architecture/performance/coÃ»t

---

## Introduction

Cette section prÃ©sente des Ã©tudes de cas dÃ©taillÃ©es d'architectures MariaDB dÃ©ployÃ©es en production. Chaque cas illustre l'application pratique des concepts Ã©tudiÃ©s dans ce chapitre, avec les dÃ©cisions de conception, les dÃ©fis rencontrÃ©s et les rÃ©sultats obtenus.

---

## Ã‰tude de cas 1 : Plateforme E-commerce Multi-tenant

### Contexte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ã‰TUDE DE CAS : E-COMMERCE MULTI-TENANT                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Entreprise : ShopCloud (SaaS e-commerce B2B)                               â”‚
â”‚  Secteur : Commerce Ã©lectronique                                            â”‚
â”‚  Taille : 2,500 marchands, 15M produits, 500K commandes/jour                â”‚
â”‚                                                                             â”‚
â”‚  Exigences :                                                                â”‚
â”‚  â€¢ Multi-tenant avec isolation des donnÃ©es                                  â”‚
â”‚  â€¢ Performance < 100ms P95 pour les pages produits                          â”‚
â”‚  â€¢ DisponibilitÃ© 99.95%                                                     â”‚
â”‚  â€¢ ConformitÃ© RGPD (donnÃ©es EU)                                             â”‚
â”‚  â€¢ ScalabilitÃ© : supporter 10x la croissance sur 3 ans                      â”‚
â”‚  â€¢ Analytics temps rÃ©el pour chaque marchand                                â”‚
â”‚                                                                             â”‚
â”‚  Budget : Infrastructure ~$50K/mois                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture retenue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE SHOPCLOUD                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚                              CDN (Cloudflare)                              â”‚
â”‚                                    â”‚                                       â”‚
â”‚                                    â–¼                                       â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                         â”‚   Load Balancer     â”‚                            â”‚
â”‚                         â”‚   (HAProxy)         â”‚                            â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                    â”‚                                       â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚                          â”‚                          â”‚            â”‚
â”‚         â–¼                          â–¼                          â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   API GW    â”‚            â”‚   API GW    â”‚            â”‚   API GW    â”‚     â”‚
â”‚  â”‚   Zone A    â”‚            â”‚   Zone B    â”‚            â”‚   Zone C    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                          â”‚                          â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                    â”‚                                       â”‚
â”‚                                    â–¼                                       â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                         â”‚      MaxScale       â”‚                            â”‚
â”‚                         â”‚   (Router + Cache)  â”‚                            â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                    â”‚                                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚                     â”‚                     â”‚                 â”‚
â”‚              â–¼                     â–¼                     â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Shard 1 (A-H)    â”‚ â”‚  Shard 2 (I-P)    â”‚ â”‚  Shard 3 (Q-Z)    â”‚         â”‚
â”‚  â”‚                   â”‚ â”‚                   â”‚ â”‚                   â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚
â”‚  â”‚  â”‚   Galera    â”‚  â”‚ â”‚  â”‚   Galera    â”‚  â”‚ â”‚  â”‚   Galera    â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   Cluster   â”‚  â”‚ â”‚  â”‚   Cluster   â”‚  â”‚ â”‚  â”‚   Cluster   â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   (3 nodes) â”‚  â”‚ â”‚  â”‚   (3 nodes) â”‚  â”‚ â”‚  â”‚   (3 nodes) â”‚  â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚
â”‚  â”‚                   â”‚ â”‚                   â”‚ â”‚                   â”‚         â”‚
â”‚  â”‚  ~850 tenants     â”‚ â”‚  ~850 tenants     â”‚ â”‚  ~800 tenants     â”‚         â”‚
â”‚  â”‚  ~5M products     â”‚ â”‚  ~5M products     â”‚ â”‚  ~5M products     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                    â”‚                                       â”‚
â”‚                                    â”‚ CDC (Debezium)                        â”‚
â”‚                                    â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         Apache Kafka                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â–¼                   â–¼                   â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   ColumnStore     â”‚ â”‚  Elasticsearch    â”‚ â”‚   Redis           â”‚         â”‚
â”‚  â”‚   (Analytics DW)  â”‚ â”‚  (Product Search) â”‚ â”‚   (Cache)         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©cisions de conception

| DÃ©cision | Justification | Alternative rejetÃ©e |
|----------|---------------|---------------------|
| **Sharding par tenant_id (hash)** | Distribution uniforme, pas de hotspots | Range-based (risque de dÃ©sÃ©quilibre) |
| **Galera par shard** | HA sans failover manuel, lectures locales | Async replication (RPO non acceptable) |
| **Schema partagÃ© + tenant_id** | EfficacitÃ© opÃ©rationnelle, migrations simples | Schema per tenant (explosion des tables) |
| **Row-Level Security** | Isolation forte sans overhead | Application-level only (risque de bugs) |
| **CDC vers ColumnStore** | Analytics sans impact OLTP | ETL batch (dÃ©lai inacceptable) |
| **MaxScale Query Cache** | RÃ©duction 40% charge DB | Redis cache (complexitÃ© invalidation) |

### ImplÃ©mentation du multi-tenant

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SCHÃ‰MA MULTI-TENANT AVEC ROW-LEVEL SECURITY
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Table des tenants (marchands)
CREATE TABLE tenants (
    tenant_id INT PRIMARY KEY AUTO_INCREMENT,
    tenant_code VARCHAR(50) UNIQUE NOT NULL,  -- Pour sharding
    company_name VARCHAR(255) NOT NULL,
    plan ENUM('starter', 'pro', 'enterprise') DEFAULT 'starter',
    settings JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_code (tenant_code)
) ENGINE=InnoDB;

-- Table produits avec tenant isolation
CREATE TABLE products (
    product_id BIGINT AUTO_INCREMENT,
    tenant_id INT NOT NULL,
    sku VARCHAR(100) NOT NULL,
    name VARCHAR(500) NOT NULL,
    description TEXT,
    price DECIMAL(12,2) NOT NULL,
    stock_quantity INT DEFAULT 0,
    category_id INT,
    attributes JSON,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    PRIMARY KEY (product_id, tenant_id),  -- Tenant dans la PK pour le sharding
    UNIQUE KEY uk_tenant_sku (tenant_id, sku),
    INDEX idx_tenant_category (tenant_id, category_id, is_active),
    INDEX idx_tenant_active (tenant_id, is_active, updated_at),
    FULLTEXT INDEX ft_search (name, description)
) ENGINE=InnoDB;

-- Vue sÃ©curisÃ©e avec isolation automatique
CREATE SQL SECURITY DEFINER VIEW v_products AS
SELECT * FROM products
WHERE tenant_id = GET_CURRENT_TENANT_ID();

-- Fonction pour rÃ©cupÃ©rer le tenant courant (stockÃ© en session)
DELIMITER //
CREATE FUNCTION GET_CURRENT_TENANT_ID()
RETURNS INT
DETERMINISTIC
READS SQL DATA
BEGIN
    DECLARE v_tenant_id INT;
    SELECT @current_tenant_id INTO v_tenant_id;
    IF v_tenant_id IS NULL THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Tenant context not set';
    END IF;
    RETURN v_tenant_id;
END //
DELIMITER ;

-- ProcÃ©dure pour dÃ©finir le contexte tenant
DELIMITER //
CREATE PROCEDURE set_tenant_context(IN p_tenant_code VARCHAR(50))
BEGIN
    DECLARE v_tenant_id INT;
    
    SELECT tenant_id INTO v_tenant_id
    FROM tenants WHERE tenant_code = p_tenant_code;
    
    IF v_tenant_id IS NULL THEN
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Invalid tenant';
    END IF;
    
    SET @current_tenant_id = v_tenant_id;
END //
DELIMITER ;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CONFIGURATION MAXSCALE POUR ROUTING PAR TENANT
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Dans MaxScale, routing basÃ© sur le hash du tenant_code
-- Extrait du fichier maxscale.cnf:
/*
[ShardRouter]
type=service
router=schemarouter
servers=shard1,shard2,shard3
user=maxscale_user

[TenantFilter]
type=filter
module=regexfilter
match=tenant_id\s*=\s*(\d+)
replace=/* shard:$1 */

# Mapping tenant -> shard
# tenant_id % 3 = 0 -> shard1
# tenant_id % 3 = 1 -> shard2
# tenant_id % 3 = 2 -> shard3
*/
```

### RÃ©sultats

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RÃ‰SULTATS APRÃˆS 18 MOIS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  MÃ©triques de performance :                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                  â”‚
â”‚  â€¢ Latence P50 : 23ms (objectif : <50ms) âœ…                                 â”‚
â”‚  â€¢ Latence P95 : 67ms (objectif : <100ms) âœ…                                â”‚
â”‚  â€¢ Latence P99 : 142ms                                                      â”‚
â”‚  â€¢ Throughput : 12,000 requÃªtes/seconde soutenu                             â”‚
â”‚  â€¢ DisponibilitÃ© : 99.97% (objectif : 99.95%) âœ…                            â”‚
â”‚                                                                             â”‚
â”‚  ScalabilitÃ© :                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•                                                              â”‚
â”‚  â€¢ Tenants : 2,500 â†’ 4,200 (+68%)                                           â”‚
â”‚  â€¢ Produits : 15M â†’ 28M (+87%)                                              â”‚
â”‚  â€¢ Commandes/jour : 500K â†’ 1.2M (+140%)                                     â”‚
â”‚  â€¢ Sans ajout de shard (headroom restant : ~40%)                            â”‚
â”‚                                                                             â”‚
â”‚  CoÃ»ts :                                                                    â”‚
â”‚  â•â•â•â•â•â•â•                                                                    â”‚
â”‚  â€¢ Infrastructure : $47K/mois (sous budget)                                 â”‚
â”‚  â€¢ CoÃ»t par tenant : $11.20/mois (-35% vs ancienne architecture)            â”‚
â”‚  â€¢ ROI de la migration : 8 mois                                             â”‚
â”‚                                                                             â”‚
â”‚  LeÃ§ons apprises :                                                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                          â”‚
â”‚  âœ… Le sharding par hash Ã©vite les hotspots mais complique les queries      â”‚
â”‚     cross-tenant (analytics rÃ©solues via ColumnStore)                       â”‚
â”‚  âœ… MaxScale Query Cache trÃ¨s efficace pour le catalogue produits           â”‚
â”‚     (hit ratio 72%)                                                         â”‚
â”‚  âš ï¸  La migration de schÃ©ma requiert coordination sur 9 nodes Galera        â”‚
â”‚     â†’ AdoptÃ© pt-online-schema-change                                        â”‚
â”‚  âš ï¸  CDC lag occasionnel sous forte charge â†’ augmentÃ© les workers           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ã‰tude de cas 2 : Fintech - SystÃ¨me de paiement temps rÃ©el

### Contexte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ã‰TUDE DE CAS : FINTECH PAIEMENT                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Entreprise : PayFlow (PSP - Payment Service Provider)                      â”‚
â”‚  Secteur : Services financiers                                              â”‚
â”‚  Volume : 2M transactions/jour, pics Ã  800 TPS                              â”‚
â”‚                                                                             â”‚
â”‚  Exigences rÃ©glementaires :                                                 â”‚
â”‚  â€¢ PCI-DSS Level 1                                                          â”‚
â”‚  â€¢ ConformitÃ© PSD2 (Europe)                                                 â”‚
â”‚  â€¢ RGPD                                                                     â”‚
â”‚  â€¢ Audit trail complet (7 ans de rÃ©tention)                                 â”‚
â”‚                                                                             â”‚
â”‚  Exigences techniques :                                                     â”‚
â”‚  â€¢ Latence < 200ms pour 99% des transactions                                â”‚
â”‚  â€¢ ZÃ©ro perte de donnÃ©es (RPO = 0)                                          â”‚
â”‚  â€¢ RTO < 30 secondes                                                        â”‚
â”‚  â€¢ DisponibilitÃ© 99.999% (5 minutes downtime/an max)                        â”‚
â”‚  â€¢ Multi-rÃ©gion (EU + backup US)                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture retenue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE PAYFLOW                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  RÃ‰GION EU (PRIMARY)                          RÃ‰GION US (DR)               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                           â•â•â•â•â•â•â•â•â•â•â•â•â•â•               â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Async    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Global Load Balancer    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Global Load Balancer  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    Repl     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                        â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              â”‚              â”‚             â”‚           â”‚              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚      MaxScale HA      â”‚  â”‚             â”‚  â”‚    MaxScale HA     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   (Active-Standby)    â”‚  â”‚             â”‚  â”‚   (Standby)        â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚              â”‚              â”‚             â”‚           â”‚              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚    Galera Cluster     â”‚  â”‚             â”‚  â”‚   Galera Cluster   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚      (5 nodes)        â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”‚     (3 nodes)      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                       â”‚  â”‚   Async     â”‚  â”‚                    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”      â”‚  â”‚   GTID      â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ N1  â”‚ â”‚ N2  â”‚      â”‚  â”‚   Repl      â”‚  â”‚  â”‚ N1  â”‚ â”‚ N2  â”‚   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ DC1 â”‚ â”‚ DC1 â”‚      â”‚  â”‚             â”‚  â”‚  â”‚ DC3 â”‚ â”‚ DC3 â”‚   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜      â”‚  â”‚             â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”      â”‚  â”‚             â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ N3  â”‚ â”‚ N4  â”‚      â”‚  â”‚             â”‚  â”‚  â”‚ N3  â”‚           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ DC2 â”‚ â”‚ DC2 â”‚      â”‚  â”‚             â”‚  â”‚  â”‚ DC3 â”‚           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜      â”‚  â”‚             â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”              â”‚  â”‚             â”‚  â”‚                    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ N5  â”‚  Arbitrator  â”‚  â”‚             â”‚  â”‚                    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ DC2 â”‚              â”‚  â”‚             â”‚  â”‚                    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜              â”‚  â”‚             â”‚  â”‚                    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                             â”‚             â”‚                          â”‚  â”‚
â”‚  â”‚  Encryption at rest:        â”‚             â”‚                          â”‚  â”‚
â”‚  â”‚  â€¢ AWS KMS + CloudHSM       â”‚             â”‚                          â”‚  â”‚
â”‚  â”‚  â€¢ TDE (Transparent Data    â”‚             â”‚                          â”‚  â”‚
â”‚  â”‚    Encryption)              â”‚             â”‚                          â”‚  â”‚
â”‚  â”‚                             â”‚             â”‚                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         Event Bus (Kafka)                           â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Topics:                                                            â”‚   â”‚
â”‚  â”‚  â€¢ transactions.initiated (Outbox pattern)                          â”‚   â”‚
â”‚  â”‚  â€¢ transactions.completed                                           â”‚   â”‚
â”‚  â”‚  â€¢ transactions.failed                                              â”‚   â”‚
â”‚  â”‚  â€¢ audit.events (immutable log)                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                â”‚                                                           â”‚
â”‚                â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Audit & Compliance                               â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚   ColumnStore   â”‚  â”‚   S3 Glacier    â”‚  â”‚   Elasticsearch â”‚      â”‚   â”‚
â”‚  â”‚  â”‚   (Analytics)   â”‚  â”‚   (Archive 7y)  â”‚  â”‚   (Search)      â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©cisions de conception critiques

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DÃ‰CISIONS DE CONCEPTION                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. GALERA 5 NODES (pas 3)                                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                  â”‚
â”‚  Justification:                                                             â”‚
â”‚  â€¢ TolÃ©rance Ã  2 pannes simultanÃ©es (quorum = 3)                            â”‚
â”‚  â€¢ Split-brain impossible avec 5 nodes                                      â”‚
â”‚  â€¢ Maintenance rolling sans impact disponibilitÃ©                            â”‚
â”‚                                                                             â”‚
â”‚  CoÃ»t additionnel: +$8K/mois                                                â”‚
â”‚  BÃ©nÃ©fice: 99.999% vs 99.99% disponibilitÃ©                                  â”‚
â”‚                                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                             â”‚
â”‚  2. RÃ‰PLICATION SYNCHRONE INTRA-RÃ‰GION, ASYNC INTER-RÃ‰GION                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•               â”‚
â”‚  Justification:                                                             â”‚
â”‚  â€¢ Synchrone EU: RPO=0 pour les transactions (PCI-DSS)                      â”‚
â”‚  â€¢ Async US: Latence EU-US (~80ms) incompatible avec synchrone              â”‚
â”‚  â€¢ RPO inter-rÃ©gion: ~2 secondes (acceptable pour DR)                       â”‚
â”‚                                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                             â”‚
â”‚  3. OUTBOX PATTERN POUR LES Ã‰VÃ‰NEMENTS                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                      â”‚
â”‚  Justification:                                                             â”‚
â”‚  â€¢ CohÃ©rence transaction + Ã©vÃ©nement garantie                               â”‚
â”‚  â€¢ Pas de dual-write (risque d'incohÃ©rence)                                 â”‚
â”‚  â€¢ Idempotence native (replay possible)                                     â”‚
â”‚                                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                             â”‚
â”‚  4. ENCRYPTION EVERYWHERE                                                   â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                   â”‚
â”‚  ImplÃ©mentation:                                                            â”‚
â”‚  â€¢ TLS 1.3 in-transit (obligatoire)                                         â”‚
â”‚  â€¢ TDE at-rest (InnoDB tablespace encryption)                               â”‚
â”‚  â€¢ Column-level encryption pour PAN/CVV (HSM)                               â”‚
â”‚  â€¢ Key rotation automatique (90 jours)                                      â”‚
â”‚                                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                             â”‚
â”‚  5. IMMUTABLE AUDIT LOG                                                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                   â”‚
â”‚  ImplÃ©mentation:                                                            â”‚
â”‚  â€¢ Table audit_log avec triggers AFTER INSERT/UPDATE/DELETE                 â”‚
â”‚  â€¢ Pas de UPDATE/DELETE autorisÃ© sur audit_log                              â”‚
â”‚  â€¢ Hash chaÃ®nÃ© pour intÃ©gritÃ© (blockchain-like)                             â”‚
â”‚  â€¢ Archivage vers S3 Glacier aprÃ¨s 1 an                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SchÃ©ma critique

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SCHÃ‰MA TRANSACTIONNEL PAYFLOW
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Table des transactions (cÅ“ur du systÃ¨me)
CREATE TABLE transactions (
    transaction_id BINARY(16) PRIMARY KEY,  -- UUID v7 (time-ordered)
    
    -- Identification
    merchant_id INT NOT NULL,
    external_reference VARCHAR(100),
    
    -- Montant
    amount DECIMAL(15,2) NOT NULL,
    currency CHAR(3) NOT NULL,
    
    -- Type et statut
    transaction_type ENUM('payment', 'refund', 'chargeback', 'payout') NOT NULL,
    status ENUM('initiated', 'pending', 'authorized', 'captured', 
                'settled', 'failed', 'cancelled', 'refunded') NOT NULL,
    
    -- DonnÃ©es sensibles (encrypted at column level)
    card_token VARBINARY(512),  -- Tokenized, not actual PAN
    
    -- MÃ©tadonnÃ©es
    metadata JSON,
    
    -- Timestamps prÃ©cis
    initiated_at TIMESTAMP(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    completed_at TIMESTAMP(6),
    
    -- Versioning optimiste
    version INT DEFAULT 1,
    
    -- Index
    INDEX idx_merchant_date (merchant_id, initiated_at),
    INDEX idx_status_date (status, initiated_at),
    INDEX idx_external_ref (merchant_id, external_reference)
) ENGINE=InnoDB
  ROW_FORMAT=COMPRESSED
  ENCRYPTION='Y';

-- Table Outbox pour les Ã©vÃ©nements (pattern Outbox)
CREATE TABLE transaction_outbox (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    
    aggregate_id BINARY(16) NOT NULL,  -- transaction_id
    event_type VARCHAR(100) NOT NULL,
    payload JSON NOT NULL,
    
    created_at TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP(6),
    processed_at TIMESTAMP(6),
    
    INDEX idx_unprocessed (processed_at, created_at)
) ENGINE=InnoDB;

-- Audit log immuable
CREATE TABLE audit_log (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    
    -- DonnÃ©es de l'Ã©vÃ©nement
    table_name VARCHAR(100) NOT NULL,
    record_id VARCHAR(100) NOT NULL,
    action ENUM('INSERT', 'UPDATE', 'DELETE') NOT NULL,
    old_values JSON,
    new_values JSON,
    
    -- Contexte
    user_id VARCHAR(100),
    ip_address VARCHAR(45),
    user_agent VARCHAR(500),
    
    -- Timestamp et intÃ©gritÃ©
    created_at TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP(6),
    previous_hash CHAR(64),
    current_hash CHAR(64) AS (
        SHA2(CONCAT(
            COALESCE(previous_hash, 'GENESIS'),
            table_name,
            record_id,
            action,
            COALESCE(old_values, ''),
            COALESCE(new_values, ''),
            created_at
        ), 256)
    ) STORED,
    
    INDEX idx_table_record (table_name, record_id, created_at),
    INDEX idx_created (created_at)
) ENGINE=InnoDB
  ENCRYPTION='Y';

-- Trigger pour audit automatique
DELIMITER //

CREATE TRIGGER trg_transactions_audit_insert
AFTER INSERT ON transactions
FOR EACH ROW
BEGIN
    DECLARE v_prev_hash CHAR(64);
    
    SELECT current_hash INTO v_prev_hash 
    FROM audit_log ORDER BY id DESC LIMIT 1;
    
    INSERT INTO audit_log (table_name, record_id, action, new_values, 
                          user_id, previous_hash)
    VALUES (
        'transactions',
        HEX(NEW.transaction_id),
        'INSERT',
        JSON_OBJECT(
            'merchant_id', NEW.merchant_id,
            'amount', NEW.amount,
            'currency', NEW.currency,
            'status', NEW.status
        ),
        @current_user_id,
        v_prev_hash
    );
END //

CREATE TRIGGER trg_transactions_audit_update
AFTER UPDATE ON transactions
FOR EACH ROW
BEGIN
    DECLARE v_prev_hash CHAR(64);
    
    SELECT current_hash INTO v_prev_hash 
    FROM audit_log ORDER BY id DESC LIMIT 1;
    
    INSERT INTO audit_log (table_name, record_id, action, old_values, 
                          new_values, user_id, previous_hash)
    VALUES (
        'transactions',
        HEX(NEW.transaction_id),
        'UPDATE',
        JSON_OBJECT('status', OLD.status, 'version', OLD.version),
        JSON_OBJECT('status', NEW.status, 'version', NEW.version),
        @current_user_id,
        v_prev_hash
    );
END //

DELIMITER ;

-- ProcÃ©dure de traitement transactionnel
DELIMITER //

CREATE PROCEDURE process_payment(
    IN p_transaction_id BINARY(16),
    IN p_merchant_id INT,
    IN p_amount DECIMAL(15,2),
    IN p_currency CHAR(3),
    IN p_card_token VARBINARY(512)
)
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        RESIGNAL;
    END;
    
    START TRANSACTION;
    
    -- 1. CrÃ©er la transaction
    INSERT INTO transactions 
        (transaction_id, merchant_id, amount, currency, 
         transaction_type, status, card_token)
    VALUES 
        (p_transaction_id, p_merchant_id, p_amount, p_currency,
         'payment', 'initiated', p_card_token);
    
    -- 2. CrÃ©er l'Ã©vÃ©nement outbox (sera capturÃ© par CDC)
    INSERT INTO transaction_outbox (aggregate_id, event_type, payload)
    VALUES (
        p_transaction_id,
        'PaymentInitiated',
        JSON_OBJECT(
            'transaction_id', HEX(p_transaction_id),
            'merchant_id', p_merchant_id,
            'amount', p_amount,
            'currency', p_currency,
            'timestamp', UNIX_TIMESTAMP(NOW(6)) * 1000
        )
    );
    
    COMMIT;
END //

DELIMITER ;
```

### RÃ©sultats

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RÃ‰SULTATS APRÃˆS 24 MOIS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ConformitÃ© :                                                               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•                                                               â”‚
â”‚  â€¢ Certification PCI-DSS Level 1 obtenue âœ…                                 â”‚
â”‚  â€¢ Audit RGPD passÃ© sans observation majeure âœ…                             â”‚
â”‚  â€¢ AgrÃ©ment PSD2 maintenu âœ…                                                â”‚
â”‚                                                                             â”‚
â”‚  Performance :                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•                                                              â”‚
â”‚  â€¢ Latence P50 : 45ms                                                       â”‚
â”‚  â€¢ Latence P99 : 167ms (objectif : <200ms) âœ…                               â”‚
â”‚  â€¢ Throughput max : 1,200 TPS (objectif : 800 TPS) âœ…                       â”‚
â”‚                                                                             â”‚
â”‚  DisponibilitÃ© :                                                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                            â”‚
â”‚  â€¢ Uptime : 99.9985% (7.9 minutes downtime sur 24 mois)                     â”‚
â”‚  â€¢ Objectif 99.999% quasi-atteint                                           â”‚
â”‚  â€¢ 0 perte de donnÃ©es (RPO = 0) âœ…                                          â”‚
â”‚                                                                             â”‚
â”‚  Incidents majeurs :                                                        â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â”‚
â”‚  â€¢ 1 failover inter-datacenter (3.2 minutes) - panne rÃ©seau DC1             â”‚
â”‚  â€¢ 1 failover US activÃ© en test DR (4.7 minutes)                            â”‚
â”‚  â€¢ 0 incident de sÃ©curitÃ©                                                   â”‚
â”‚                                                                             â”‚
â”‚  Volume traitÃ© :                                                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                            â”‚
â”‚  â€¢ 1.5 milliards de transactions sur 24 mois                                â”‚
â”‚  â€¢ â‚¬47 milliards de volume de paiement                                      â”‚
â”‚  â€¢ Croissance : +85% YoY                                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ã‰tude de cas 3 : MÃ©dia - Plateforme de contenu avec IA

### Contexte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ã‰TUDE DE CAS : MÃ‰DIA & IA                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Entreprise : NewsAI (AgrÃ©gateur de news avec IA)                           â”‚
â”‚  Secteur : MÃ©dia digital                                                    â”‚
â”‚  Volume : 500K articles, 2M utilisateurs, 50M requÃªtes/jour                 â”‚
â”‚                                                                             â”‚
â”‚  FonctionnalitÃ©s IA :                                                       â”‚
â”‚  â€¢ Recherche sÃ©mantique dans les articles                                   â”‚
â”‚  â€¢ Recommandations personnalisÃ©es                                           â”‚
â”‚  â€¢ RÃ©sumÃ© automatique (RAG)                                                 â”‚
â”‚  â€¢ Chatbot Q&A sur l'actualitÃ©                                              â”‚
â”‚  â€¢ DÃ©tection de fake news                                                   â”‚
â”‚                                                                             â”‚
â”‚  Exigences :                                                                â”‚
â”‚  â€¢ Latence recherche < 500ms                                                â”‚
â”‚  â€¢ Recommandations temps rÃ©el                                               â”‚
â”‚  â€¢ Indexation des nouveaux articles < 5 minutes                             â”‚
â”‚  â€¢ CoÃ»t embedding maÃ®trisÃ©                                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture retenue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE NEWSAI                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                            â”‚   Users / Apps  â”‚                             â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                     â”‚                                      â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                            â”‚   API Gateway   â”‚                             â”‚
â”‚                            â”‚   (FastAPI)     â”‚                             â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                     â”‚                                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚         â”‚                           â”‚                           â”‚          â”‚
â”‚         â–¼                           â–¼                           â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Search    â”‚            â”‚   RAG /     â”‚            â”‚   Reco      â”‚     â”‚
â”‚  â”‚   Service   â”‚            â”‚   Chatbot   â”‚            â”‚   Engine    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                          â”‚                          â”‚            â”‚
â”‚         â”‚                          â”‚                          â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                    â”‚                                       â”‚
â”‚                                    â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         LangChain / LlamaIndex                      â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚  Embedding  â”‚  â”‚    LLM      â”‚  â”‚  Retriever  â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  (Cached)   â”‚  â”‚  (Claude)   â”‚  â”‚  (Hybrid)   â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚                   â”‚                   â”‚                     â”‚
â”‚              â–¼                   â–¼                   â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚     MariaDB       â”‚ â”‚      Redis        â”‚ â”‚    Embedding      â”‚         â”‚
â”‚  â”‚   Vector Store    â”‚ â”‚   (Cache + MQ)    â”‚ â”‚    Service        â”‚         â”‚
â”‚  â”‚                   â”‚ â”‚                   â”‚ â”‚                   â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  - Query cache    â”‚ â”‚  Local model ou   â”‚         â”‚ 
â”‚  â”‚  â”‚  articles   â”‚  â”‚ â”‚  - Embedding cacheâ”‚ â”‚  OpenAI API       â”‚         â”‚
â”‚  â”‚  â”‚  500K rows  â”‚  â”‚ â”‚  - Session store  â”‚ â”‚                   â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  - Rate limiting  â”‚ â”‚                   â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚                   â”‚ â”‚                   â”‚         â”‚
â”‚  â”‚  â”‚  embeddings â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â”‚  â”‚  500K Ã— 1536â”‚  â”‚                                                     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                                     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                                     â”‚
â”‚  â”‚  â”‚user_profilesâ”‚  â”‚                                                     â”‚
â”‚  â”‚  â”‚  2M rows    â”‚  â”‚                                                     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                                     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                                                     â”‚
â”‚  â”‚  â”‚interactions â”‚  â”‚                                                     â”‚
â”‚  â”‚  â”‚  100M rows  â”‚  â”‚                                                     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                                     â”‚
â”‚  â”‚                   â”‚                                                     â”‚
â”‚  â”‚  FULLTEXT +       â”‚                                                     â”‚
â”‚  â”‚  Vector Search    â”‚                                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Ingestion Pipeline                               â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  RSS/API â”€â”€â–º Parser â”€â”€â–º Embedding â”€â”€â–º MariaDB â”€â”€â–º Index Update      â”‚   â”‚
â”‚  â”‚             â”‚          (batch)       (INSERT)                       â”‚   â”‚
â”‚  â”‚             â”‚                                                       â”‚   â”‚
â”‚  â”‚             â””â”€â”€â–º Fake News Detection â”€â”€â–º Flag                       â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation du systÃ¨me de recommandation

```python
#!/usr/bin/env python3
"""
newsai_recommendation.py
SystÃ¨me de recommandation hybride pour NewsAI
"""

import numpy as np
import mariadb
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json
import redis
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


@dataclass
class Article:
    id: int
    title: str
    content: str
    category: str
    published_at: datetime
    embedding: Optional[np.ndarray] = None


@dataclass
class Recommendation:
    article: Article
    score: float
    reason: str  # 'similar_content', 'trending', 'collaborative', 'personalized'


class NewsAIRecommendationEngine:
    """
    Moteur de recommandation hybride combinant :
    - Content-based (similaritÃ© vectorielle)
    - Collaborative filtering (comportement utilisateurs similaires)
    - Trending (popularitÃ© rÃ©cente)
    - Personalization (profil utilisateur)
    """
    
    def __init__(self, db_config: Dict, redis_config: Dict):
        self.db_pool = mariadb.ConnectionPool(
            pool_name="reco_pool",
            pool_size=10,
            **db_config
        )
        self.redis = redis.Redis(**redis_config)
        
        # Poids pour le score hybride
        self.weights = {
            'content': 0.35,
            'collaborative': 0.25,
            'trending': 0.20,
            'personalized': 0.20
        }
    
    def get_recommendations(
        self,
        user_id: int,
        k: int = 20,
        exclude_read: bool = True,
        category_filter: Optional[str] = None
    ) -> List[Recommendation]:
        """
        GÃ©nÃ¨re des recommandations personnalisÃ©es pour un utilisateur.
        """
        
        # 1. RÃ©cupÃ©rer le profil utilisateur
        user_profile = self._get_user_profile(user_id)
        
        # 2. Articles dÃ©jÃ  lus (Ã  exclure)
        read_articles = self._get_read_articles(user_id) if exclude_read else set()
        
        # 3. Candidats de chaque source
        content_candidates = self._get_content_based_candidates(
            user_profile, k * 2, read_articles, category_filter
        )
        
        collaborative_candidates = self._get_collaborative_candidates(
            user_id, k * 2, read_articles
        )
        
        trending_candidates = self._get_trending_candidates(
            k * 2, read_articles, category_filter
        )
        
        # 4. Fusion et scoring
        all_candidates = {}
        
        for article, score in content_candidates:
            if article.id not in all_candidates:
                all_candidates[article.id] = {
                    'article': article, 
                    'scores': {'content': 0, 'collaborative': 0, 'trending': 0, 'personalized': 0}
                }
            all_candidates[article.id]['scores']['content'] = score
        
        for article, score in collaborative_candidates:
            if article.id not in all_candidates:
                all_candidates[article.id] = {
                    'article': article,
                    'scores': {'content': 0, 'collaborative': 0, 'trending': 0, 'personalized': 0}
                }
            all_candidates[article.id]['scores']['collaborative'] = score
        
        for article, score in trending_candidates:
            if article.id not in all_candidates:
                all_candidates[article.id] = {
                    'article': article,
                    'scores': {'content': 0, 'collaborative': 0, 'trending': 0, 'personalized': 0}
                }
            all_candidates[article.id]['scores']['trending'] = score
        
        # 5. Calcul du score personnalisÃ©
        for article_id, data in all_candidates.items():
            data['scores']['personalized'] = self._compute_personalization_score(
                data['article'], user_profile
            )
        
        # 6. Score final pondÃ©rÃ©
        recommendations = []
        for article_id, data in all_candidates.items():
            final_score = sum(
                self.weights[source] * score 
                for source, score in data['scores'].items()
            )
            
            # DÃ©terminer la raison principale
            main_reason = max(data['scores'].items(), key=lambda x: x[1])[0]
            
            recommendations.append(Recommendation(
                article=data['article'],
                score=final_score,
                reason=main_reason
            ))
        
        # 7. Tri et diversification
        recommendations.sort(key=lambda x: x.score, reverse=True)
        recommendations = self._diversify(recommendations, k)
        
        return recommendations
    
    def _get_user_profile(self, user_id: int) -> Dict:
        """RÃ©cupÃ¨re ou calcule le profil utilisateur."""
        
        # Check cache
        cache_key = f"user_profile:{user_id}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        conn = self.db_pool.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        try:
            # Embedding moyen des articles lus
            cursor.execute("""
                SELECT e.embedding
                FROM user_interactions ui
                JOIN embeddings e ON ui.article_id = e.article_id
                WHERE ui.user_id = %s
                  AND ui.interaction_type IN ('read', 'like', 'share')
                  AND ui.created_at > DATE_SUB(NOW(), INTERVAL 30 DAY)
                ORDER BY 
                    CASE ui.interaction_type 
                        WHEN 'share' THEN 3
                        WHEN 'like' THEN 2
                        ELSE 1 
                    END DESC
                LIMIT 50
            """, (user_id,))
            
            embeddings = []
            weights = []
            for i, row in enumerate(cursor.fetchall()):
                emb = np.frombuffer(row['embedding'], dtype=np.float32)
                embeddings.append(emb)
                weights.append(1.0 / (i + 1))  # DÃ©croissance temporelle
            
            if embeddings:
                weighted_avg = np.average(embeddings, axis=0, weights=weights)
                preference_embedding = weighted_avg / np.linalg.norm(weighted_avg)
            else:
                preference_embedding = None
            
            # CatÃ©gories prÃ©fÃ©rÃ©es
            cursor.execute("""
                SELECT a.category, COUNT(*) as cnt
                FROM user_interactions ui
                JOIN articles a ON ui.article_id = a.id
                WHERE ui.user_id = %s
                  AND ui.created_at > DATE_SUB(NOW(), INTERVAL 30 DAY)
                GROUP BY a.category
                ORDER BY cnt DESC
            """, (user_id,))
            
            category_prefs = {row['category']: row['cnt'] for row in cursor.fetchall()}
            
            profile = {
                'embedding': preference_embedding.tolist() if preference_embedding is not None else None,
                'categories': category_prefs,
                'computed_at': datetime.now().isoformat()
            }
            
            # Cache for 1 hour
            self.redis.setex(cache_key, 3600, json.dumps(profile))
            
            return profile
            
        finally:
            cursor.close()
            conn.close()
    
    def _get_content_based_candidates(
        self,
        user_profile: Dict,
        k: int,
        exclude: set,
        category: Optional[str]
    ) -> List[Tuple[Article, float]]:
        """Candidats basÃ©s sur la similaritÃ© de contenu."""
        
        if not user_profile.get('embedding'):
            return []
        
        query_embedding = np.array(user_profile['embedding'], dtype=np.float32)
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        
        conn = self.db_pool.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        try:
            where_clause = "a.published_at > DATE_SUB(NOW(), INTERVAL 7 DAY)"
            params = []
            
            if category:
                where_clause += " AND a.category = %s"
                params.append(category)
            
            cursor.execute(f"""
                SELECT a.id, a.title, a.content, a.category, a.published_at, e.embedding
                FROM articles a
                JOIN embeddings e ON a.id = e.article_id
                WHERE {where_clause}
            """, params)
            
            candidates = []
            for row in cursor.fetchall():
                if row['id'] in exclude:
                    continue
                
                doc_embedding = np.frombuffer(row['embedding'], dtype=np.float32)
                doc_norm = doc_embedding / np.linalg.norm(doc_embedding)
                similarity = float(np.dot(query_norm, doc_norm))
                
                article = Article(
                    id=row['id'],
                    title=row['title'],
                    content=row['content'][:500],
                    category=row['category'],
                    published_at=row['published_at']
                )
                candidates.append((article, similarity))
            
            candidates.sort(key=lambda x: x[1], reverse=True)
            return candidates[:k]
            
        finally:
            cursor.close()
            conn.close()
    
    def _get_collaborative_candidates(
        self,
        user_id: int,
        k: int,
        exclude: set
    ) -> List[Tuple[Article, float]]:
        """Candidats basÃ©s sur le comportement d'utilisateurs similaires."""
        
        conn = self.db_pool.get_connection()
        cursor = conn.cursor(dictionary=True)
        
        try:
            # Trouver des utilisateurs similaires
            cursor.execute("""
                WITH my_articles AS (
                    SELECT article_id FROM user_interactions
                    WHERE user_id = %s AND interaction_type = 'read'
                ),
                similar_users AS (
                    SELECT ui.user_id, COUNT(*) as common_reads
                    FROM user_interactions ui
                    JOIN my_articles ma ON ui.article_id = ma.article_id
                    WHERE ui.user_id != %s
                    GROUP BY ui.user_id
                    HAVING common_reads >= 5
                    ORDER BY common_reads DESC
                    LIMIT 50
                )
                SELECT a.id, a.title, a.content, a.category, a.published_at,
                       COUNT(DISTINCT ui.user_id) as recommenders
                FROM similar_users su
                JOIN user_interactions ui ON su.user_id = ui.user_id
                JOIN articles a ON ui.article_id = a.id
                WHERE ui.interaction_type IN ('read', 'like')
                  AND a.published_at > DATE_SUB(NOW(), INTERVAL 7 DAY)
                  AND a.id NOT IN (
                      SELECT article_id FROM user_interactions WHERE user_id = %s
                  )
                GROUP BY a.id
                ORDER BY recommenders DESC
                LIMIT %s
            """, (user_id, user_id, user_id, k))
            
            candidates = []
            for row in cursor.fetchall():
                if row['id'] in exclude:
                    continue
                
                article = Article(
                    id=row['id'],
                    title=row['title'],
                    content=row['content'][:500] if row['content'] else '',
                    category=row['category'],
                    published_at=row['published_at']
                )
                # Normaliser le score (0-1)
                score = min(row['recommenders'] / 10.0, 1.0)
                candidates.append((article, score))
            
            return candidates
            
        finally:
            cursor.close()
            conn.close()
    
    def _get_trending_candidates(
        self,
        k: int,
        exclude: set,
        category: Optional[str]
    ) -> List[Tuple[Article, float]]:
        """Candidats basÃ©s sur la popularitÃ© rÃ©cente."""
        
        # Check cache
        cache_key = f"trending:{category or 'all'}"
        cached = self.redis.get(cache_key)
        
        if cached:
            trending = json.loads(cached)
        else:
            conn = self.db_pool.get_connection()
            cursor = conn.cursor(dictionary=True)
            
            try:
                where_clause = "a.published_at > DATE_SUB(NOW(), INTERVAL 24 HOUR)"
                params = []
                
                if category:
                    where_clause += " AND a.category = %s"
                    params.append(category)
                
                cursor.execute(f"""
                    SELECT a.id, a.title, a.content, a.category, a.published_at,
                           COUNT(*) as views,
                           SUM(CASE WHEN ui.interaction_type = 'share' THEN 5
                                    WHEN ui.interaction_type = 'like' THEN 2
                                    ELSE 1 END) as engagement_score
                    FROM articles a
                    LEFT JOIN user_interactions ui ON a.id = ui.article_id
                        AND ui.created_at > DATE_SUB(NOW(), INTERVAL 6 HOUR)
                    WHERE {where_clause}
                    GROUP BY a.id
                    ORDER BY engagement_score DESC
                    LIMIT 100
                """, params)
                
                trending = [dict(row) for row in cursor.fetchall()]
                
                # Cache for 5 minutes
                self.redis.setex(cache_key, 300, json.dumps(trending, default=str))
                
            finally:
                cursor.close()
                conn.close()
        
        # Normaliser et retourner
        candidates = []
        max_score = max((t['engagement_score'] or 0) for t in trending) if trending else 1
        
        for t in trending[:k]:
            if t['id'] in exclude:
                continue
            
            article = Article(
                id=t['id'],
                title=t['title'],
                content=t['content'][:500] if t['content'] else '',
                category=t['category'],
                published_at=datetime.fromisoformat(t['published_at']) if isinstance(t['published_at'], str) else t['published_at']
            )
            score = (t['engagement_score'] or 0) / max_score if max_score > 0 else 0
            candidates.append((article, score))
        
        return candidates
    
    def _compute_personalization_score(
        self,
        article: Article,
        user_profile: Dict
    ) -> float:
        """Score de personnalisation basÃ© sur les prÃ©fÃ©rences."""
        
        score = 0.0
        
        # Bonus catÃ©gorie prÃ©fÃ©rÃ©e
        categories = user_profile.get('categories', {})
        if categories:
            total = sum(categories.values())
            if article.category in categories:
                score += categories[article.category] / total
        
        # FraÃ®cheur (articles rÃ©cents prÃ©fÃ©rÃ©s)
        hours_old = (datetime.now() - article.published_at).total_seconds() / 3600
        freshness = max(0, 1 - (hours_old / 168))  # DÃ©croissance sur 7 jours
        score += 0.3 * freshness
        
        return min(score, 1.0)
    
    def _diversify(
        self,
        recommendations: List[Recommendation],
        k: int
    ) -> List[Recommendation]:
        """Diversifie les recommandations par catÃ©gorie."""
        
        result = []
        category_counts = {}
        max_per_category = max(2, k // 4)
        
        for rec in recommendations:
            cat = rec.article.category
            if category_counts.get(cat, 0) < max_per_category:
                result.append(rec)
                category_counts[cat] = category_counts.get(cat, 0) + 1
            
            if len(result) >= k:
                break
        
        return result
    
    def _get_read_articles(self, user_id: int) -> set:
        """RÃ©cupÃ¨re les IDs des articles dÃ©jÃ  lus."""
        
        conn = self.db_pool.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT article_id FROM user_interactions
                WHERE user_id = %s AND interaction_type = 'read'
                  AND created_at > DATE_SUB(NOW(), INTERVAL 30 DAY)
            """, (user_id,))
            
            return {row[0] for row in cursor.fetchall()}
            
        finally:
            cursor.close()
            conn.close()
```

### RÃ©sultats

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RÃ‰SULTATS APRÃˆS 12 MOIS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  MÃ©triques d'engagement :                                                   â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                   â”‚
â”‚  â€¢ CTR recommandations : 12.3% (+145% vs baseline alÃ©atoire)                â”‚
â”‚  â€¢ Temps de session moyen : 8.2 min (+67%)                                  â”‚
â”‚  â€¢ Articles lus par session : 4.7 (+89%)                                    â”‚
â”‚  â€¢ Taux de rebond : 34% (-41%)                                              â”‚
â”‚                                                                             â”‚
â”‚  Performance technique :                                                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚  â€¢ Latence recherche sÃ©mantique P50 : 89ms                                  â”‚
â”‚  â€¢ Latence recherche sÃ©mantique P95 : 234ms                                 â”‚
â”‚  â€¢ Latence recommandations P50 : 156ms                                      â”‚
â”‚  â€¢ Indexation nouvel article : 2.3 min (objectif : <5 min) âœ…               â”‚
â”‚                                                                             â”‚
â”‚  CoÃ»ts IA :                                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•                                                                 â”‚
â”‚  â€¢ Embeddings (batch) : $1,200/mois                                         â”‚
â”‚  â€¢ Claude API (RAG) : $3,800/mois                                           â”‚
â”‚  â€¢ Infrastructure : $8,500/mois                                             â”‚
â”‚  â€¢ Total : $13,500/mois (budget : $15K) âœ…                                  â”‚
â”‚                                                                             â”‚
â”‚  Optimisations clÃ©s :                                                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â”‚
â”‚  â€¢ Cache Redis embeddings : rÃ©duction 70% appels API                        â”‚
â”‚  â€¢ Batch embedding (100 articles/appel) : rÃ©duction 85% coÃ»ts               â”‚
â”‚  â€¢ Hybrid search (FT + Vector) : +23% pertinence vs vector seul             â”‚
â”‚  â€¢ User profile caching : rÃ©duction 60% queries DB                          â”‚
â”‚                                                                             â”‚
â”‚  QualitÃ© IA :                                                               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•                                                                â”‚
â”‚  â€¢ PrÃ©cision fake news detection : 94.2%                                    â”‚
â”‚  â€¢ Satisfaction utilisateurs chatbot : 4.1/5                                â”‚
â”‚  â€¢ Pertinence rÃ©sumÃ©s (Ã©valuation humaine) : 4.3/5                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ã‰tude de cas 4 : IoT - TÃ©lÃ©mÃ©trie industrielle

### Contexte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ã‰TUDE DE CAS : IOT INDUSTRIEL                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Entreprise : SmartFactory (Industrie 4.0)                                  â”‚
â”‚  Secteur : Manufacturing / IoT                                              â”‚
â”‚  Volume : 10,000 capteurs, 500M points/jour, 5 ans rÃ©tention                â”‚
â”‚                                                                             â”‚
â”‚  Types de donnÃ©es :                                                         â”‚
â”‚  â€¢ TempÃ©rature, pression, vibration (haute frÃ©quence)                       â”‚
â”‚  â€¢ Ã‰tats machines (on/off, alarmes)                                         â”‚
â”‚  â€¢ Production (compteurs, qualitÃ©)                                          â”‚
â”‚  â€¢ Ã‰nergie (consommation, pics)                                             â”‚
â”‚                                                                             â”‚
â”‚  Exigences :                                                                â”‚
â”‚  â€¢ Ingestion : 50K points/seconde soutenu                                   â”‚
â”‚  â€¢ RequÃªtes agrÃ©gÃ©es < 2 secondes                                           â”‚
â”‚  â€¢ Alertes temps rÃ©el < 5 secondes                                          â”‚
â”‚  â€¢ RÃ©tention : 5 ans avec downsampling                                      â”‚
â”‚  â€¢ CoÃ»t stockage optimisÃ©                                                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture retenue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE SMARTFACTORY                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         EDGE LAYER                                  â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚Sensor 1 â”‚ â”‚Sensor 2 â”‚ â”‚   ...   â”‚  â”€â”€â”€â”€â”€â”€â–ºâ”‚  Edge Gateway   â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  (Aggregation)  â”‚    â”‚   â”‚
â”‚  â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚                 â”‚
â”‚                                                   MQTT / Kafka             â”‚
â”‚                                                          â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         INGESTION LAYER               â”‚             â”‚   â”‚
â”‚  â”‚                                                       â–¼             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚                    Apache Kafka                             â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                             â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  Topics:                                                    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ telemetry.temperature (50K msg/s)                        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ telemetry.vibration (100K msg/s)                         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ events.alarms                                            â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ events.production                                        â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                    â”‚                                â”‚   â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚              â”‚                     â”‚                     â”‚          â”‚   â”‚
â”‚  â”‚              â–¼                     â–¼                     â–¼          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Stream Processor â”‚ â”‚  Stream Processor â”‚ â”‚   Alert Engine    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  (Aggregation)    â”‚ â”‚  (Anomaly Det.)   â”‚ â”‚   (Flink)         â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚            â”‚                     â”‚                     â”‚            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚               â”‚                     â”‚                     â”‚                â”‚
â”‚               â–¼                     â–¼                     â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         STORAGE LAYER                               â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚   HOT TIER      â”‚  â”‚   WARM TIER     â”‚  â”‚   COLD TIER     â”‚      â”‚   â”‚
â”‚  â”‚  â”‚   (7 jours)     â”‚  â”‚   (90 jours)    â”‚  â”‚   (5 ans)       â”‚      â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚      â”‚   â”‚
â”‚  â”‚  â”‚  MariaDB        â”‚  â”‚  ColumnStore    â”‚  â”‚  S3 + Parquet   â”‚      â”‚   â”‚
â”‚  â”‚  â”‚  InnoDB         â”‚  â”‚  (aggregated)   â”‚  â”‚  (archived)     â”‚      â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚      â”‚   â”‚
â”‚  â”‚  â”‚  Raw data       â”‚  â”‚  1-min rollups  â”‚  â”‚  1-hour rollups â”‚      â”‚   â”‚
â”‚  â”‚  â”‚  ~50GB          â”‚  â”‚  ~200GB         â”‚  â”‚  ~2TB           â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â”‚          â”‚                    â”‚                    â”‚                â”‚   â”‚
â”‚  â”‚          â”‚    Downsampling    â”‚     Archiving      â”‚                â”‚   â”‚
â”‚  â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         QUERY LAYER                                 â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚   Real-time     â”‚  â”‚   Historical    â”‚  â”‚   ML/Analytics  â”‚      â”‚   â”‚
â”‚  â”‚  â”‚   Dashboard     â”‚  â”‚   Analysis      â”‚  â”‚   (Python)      â”‚      â”‚   â”‚
â”‚  â”‚  â”‚   (Grafana)     â”‚  â”‚   (Superset)    â”‚  â”‚                 â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SchÃ©ma de stockage tiered

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- HOT TIER : DonnÃ©es brutes (InnoDB, 7 jours)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CREATE TABLE telemetry_raw (
    ts TIMESTAMP(3) NOT NULL,  -- Millisecond precision
    sensor_id INT NOT NULL,
    metric_type ENUM('temperature', 'pressure', 'vibration', 'flow') NOT NULL,
    value DOUBLE NOT NULL,
    quality TINYINT DEFAULT 100,  -- 0-100
    
    PRIMARY KEY (ts, sensor_id, metric_type),
    INDEX idx_sensor_time (sensor_id, ts)
) ENGINE=InnoDB
  PARTITION BY RANGE (UNIX_TIMESTAMP(ts)) (
    PARTITION p_current VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 1 DAY))),
    PARTITION p_day1 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 2 DAY))),
    PARTITION p_day2 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 3 DAY))),
    PARTITION p_day3 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 4 DAY))),
    PARTITION p_day4 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 5 DAY))),
    PARTITION p_day5 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 6 DAY))),
    PARTITION p_day6 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 7 DAY))),
    PARTITION p_future VALUES LESS THAN MAXVALUE
  );

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- WARM TIER : AgrÃ©gations 1 minute (ColumnStore, 90 jours)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CREATE TABLE telemetry_1min (
    ts_bucket TIMESTAMP NOT NULL,  -- Arrondi Ã  la minute
    sensor_id INT NOT NULL,
    metric_type VARCHAR(20) NOT NULL,
    
    -- AgrÃ©gations
    value_min DOUBLE,
    value_max DOUBLE,
    value_avg DOUBLE,
    value_sum DOUBLE,
    value_count INT,
    value_stddev DOUBLE,
    
    -- Percentiles prÃ©-calculÃ©s
    value_p50 DOUBLE,
    value_p95 DOUBLE,
    value_p99 DOUBLE,
    
    -- QualitÃ©
    quality_avg DOUBLE,
    
    PRIMARY KEY (ts_bucket, sensor_id, metric_type)
) ENGINE=Columnstore;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- PROCÃ‰DURE DE DOWNSAMPLING
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DELIMITER //

CREATE PROCEDURE downsample_to_1min(IN p_start_ts TIMESTAMP, IN p_end_ts TIMESTAMP)
BEGIN
    INSERT INTO telemetry_1min
    SELECT 
        DATE_FORMAT(ts, '%Y-%m-%d %H:%i:00') AS ts_bucket,
        sensor_id,
        metric_type,
        MIN(value) AS value_min,
        MAX(value) AS value_max,
        AVG(value) AS value_avg,
        SUM(value) AS value_sum,
        COUNT(*) AS value_count,
        STDDEV(value) AS value_stddev,
        -- Approximation percentiles via bucketing
        NULL AS value_p50,  -- CalculÃ© sÃ©parÃ©ment si nÃ©cessaire
        NULL AS value_p95,
        NULL AS value_p99,
        AVG(quality) AS quality_avg
    FROM telemetry_raw
    WHERE ts BETWEEN p_start_ts AND p_end_ts
    GROUP BY ts_bucket, sensor_id, metric_type
    ON DUPLICATE KEY UPDATE
        value_min = LEAST(value_min, VALUES(value_min)),
        value_max = GREATEST(value_max, VALUES(value_max)),
        value_avg = (value_avg * value_count + VALUES(value_avg) * VALUES(value_count)) 
                    / (value_count + VALUES(value_count)),
        value_sum = value_sum + VALUES(value_sum),
        value_count = value_count + VALUES(value_count);
END //

-- Job de downsampling (exÃ©cutÃ© toutes les minutes)
CREATE EVENT evt_downsample_1min
ON SCHEDULE EVERY 1 MINUTE
DO
BEGIN
    CALL downsample_to_1min(
        DATE_SUB(NOW(), INTERVAL 2 MINUTE),
        DATE_SUB(NOW(), INTERVAL 1 MINUTE)
    );
END //

-- Job de purge (exÃ©cutÃ© quotidiennement)
CREATE EVENT evt_purge_hot_tier
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_DATE + INTERVAL 1 DAY + INTERVAL 3 HOUR  -- 3h du matin
DO
BEGIN
    -- Supprimer les donnÃ©es > 7 jours du hot tier
    ALTER TABLE telemetry_raw 
    DROP PARTITION p_day6,
    REORGANIZE PARTITION p_future INTO (
        PARTITION p_day6 VALUES LESS THAN (UNIX_TIMESTAMP(DATE_ADD(CURDATE(), INTERVAL 7 DAY))),
        PARTITION p_future VALUES LESS THAN MAXVALUE
    );
END //

DELIMITER ;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- REQUÃŠTES OPTIMISÃ‰ES
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Dashboard temps rÃ©el (hot tier)
SELECT 
    DATE_FORMAT(ts, '%Y-%m-%d %H:%i') AS minute,
    AVG(value) AS avg_temp,
    MAX(value) AS max_temp
FROM telemetry_raw
WHERE sensor_id = 1234
  AND metric_type = 'temperature'
  AND ts > DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY minute
ORDER BY minute;

-- Analyse historique (warm tier - ColumnStore)
SELECT 
    DATE(ts_bucket) AS day,
    sensor_id,
    AVG(value_avg) AS daily_avg,
    MAX(value_max) AS daily_max,
    SUM(value_count) AS total_readings
FROM telemetry_1min
WHERE metric_type = 'temperature'
  AND ts_bucket BETWEEN '2024-01-01' AND '2024-03-31'
GROUP BY day, sensor_id
ORDER BY day, sensor_id;

-- DÃ©tection d'anomalies (utilise les agrÃ©gations)
SELECT 
    sensor_id,
    ts_bucket,
    value_avg,
    value_stddev,
    CASE 
        WHEN value_max > (value_avg + 3 * value_stddev) THEN 'HIGH_SPIKE'
        WHEN value_min < (value_avg - 3 * value_stddev) THEN 'LOW_SPIKE'
        ELSE 'NORMAL'
    END AS anomaly_status
FROM telemetry_1min
WHERE ts_bucket > DATE_SUB(NOW(), INTERVAL 24 HOUR)
  AND value_stddev > 0
HAVING anomaly_status != 'NORMAL';
```

### RÃ©sultats

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RÃ‰SULTATS APRÃˆS 18 MOIS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Performance d'ingestion :                                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚  â€¢ Throughput soutenu : 65K points/seconde (objectif : 50K) âœ…              â”‚
â”‚  â€¢ Pic gÃ©rÃ© : 120K points/seconde (burst 2 min)                             â”‚
â”‚  â€¢ Latence ingestion P99 : 45ms                                             â”‚
â”‚                                                                             â”‚
â”‚  Performance des requÃªtes :                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                 â”‚
â”‚  â€¢ Dashboard temps rÃ©el (1h) : 120ms P95                                    â”‚
â”‚  â€¢ Analyse journaliÃ¨re (30 jours) : 0.8s P95                                â”‚
â”‚  â€¢ Analyse mensuelle (1 an) : 3.2s P95                                      â”‚
â”‚  â€¢ RequÃªtes cross-sensor (100 capteurs) : 1.4s P95                          â”‚
â”‚                                                                             â”‚
â”‚  Stockage :                                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•                                                                 â”‚
â”‚  â€¢ Hot tier (InnoDB) : 48GB (7 jours)                                       â”‚
â”‚  â€¢ Warm tier (ColumnStore) : 180GB (90 jours) - compression 12:1            â”‚
â”‚  â€¢ Cold tier (S3 Parquet) : 1.8TB (5 ans) - compression 25:1                â”‚
â”‚  â€¢ CoÃ»t stockage total : $420/mois                                          â”‚
â”‚                                                                             â”‚
â”‚  Alertes :                                                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•                                                                  â”‚
â”‚  â€¢ Latence dÃ©tection anomalie : 2.3s (objectif : <5s) âœ…                    â”‚
â”‚  â€¢ Taux de faux positifs : 0.3%                                             â”‚
â”‚  â€¢ Alertes critiques manquÃ©es : 0                                           â”‚
â”‚                                                                             â”‚
â”‚  ROI Maintenance prÃ©dictive :                                               â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                               â”‚
â”‚  â€¢ RÃ©duction pannes non planifiÃ©es : -67%                                   â”‚
â”‚  â€¢ Ã‰conomies maintenance : $1.2M/an                                         â”‚
â”‚  â€¢ ROI du projet : 340% sur 18 mois                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SynthÃ¨se et recommandations

### Matrice de dÃ©cision par use case

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MATRICE DE DÃ‰CISION ARCHITECTURALE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Use Case              â”‚ Architecture recommandÃ©e                           â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚  SaaS Multi-tenant     â”‚ Sharding + Galera + Schema partagÃ© + RLS           â”‚
â”‚  E-commerce standard   â”‚ Read replicas + MaxScale + CDC pour analytics      â”‚
â”‚  Fintech/Paiement      â”‚ Galera 5+ nodes + Outbox + Audit immutable         â”‚
â”‚  Media/Contenu IA      â”‚ Hybrid search + Vector store + Cache agressif      â”‚
â”‚  IoT/TÃ©lÃ©mÃ©trie        â”‚ Tiered storage + ColumnStore + Partitioning        â”‚
â”‚  Analytics/BI          â”‚ ColumnStore dÃ©diÃ© + CDC depuis OLTP                â”‚
â”‚  Microservices         â”‚ Database per service + Event-driven (Kafka)        â”‚
â”‚  GÃ©o-distribuÃ©         â”‚ Multi-rÃ©gion async + Galera intra-rÃ©gion           â”‚
â”‚                                                                             â”‚
â”‚  Patterns transverses :                                                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚  â€¢ Toujours : MaxScale pour routing + failover                              â”‚
â”‚  â€¢ Souvent : CDC (Debezium) pour dÃ©couplage                                 â”‚
â”‚  â€¢ Si IA : LangChain/LlamaIndex + embeddings dans MariaDB                   â”‚
â”‚  â€¢ Si scale-out : Galera pour HA, sharding pour capacitÃ©                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Checklist avant mise en production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHECKLIST PRÃ‰-PRODUCTION                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â˜ HAUTE DISPONIBILITÃ‰                                                      â”‚
â”‚    â˜ Galera ou rÃ©plication configurÃ©e et testÃ©e                             â”‚
â”‚    â˜ MaxScale ou ProxySQL pour failover automatique                         â”‚
â”‚    â˜ Test de failover effectuÃ© et documentÃ©                                 â”‚
â”‚    â˜ RTO/RPO validÃ©s et conformes aux exigences                             â”‚
â”‚                                                                             â”‚
â”‚  â˜ PERFORMANCE                                                              â”‚
â”‚    â˜ Benchmarks rÃ©alisÃ©s avec donnÃ©es rÃ©alistes                             â”‚
â”‚    â˜ Index optimisÃ©s (EXPLAIN vÃ©rifiÃ©)                                      â”‚
â”‚    â˜ Configuration MariaDB ajustÃ©e (buffer pool, etc.)                      â”‚
â”‚    â˜ Connection pooling configurÃ©                                           â”‚
â”‚                                                                             â”‚
â”‚  â˜ SÃ‰CURITÃ‰                                                                 â”‚
â”‚    â˜ TLS activÃ© pour toutes les connexions                                  â”‚
â”‚    â˜ Utilisateurs avec privilÃ¨ges minimaux                                  â”‚
â”‚    â˜ Encryption at rest si donnÃ©es sensibles                                â”‚
â”‚    â˜ Audit logging activÃ©                                                   â”‚
â”‚                                                                             â”‚
â”‚  â˜ OBSERVABILITÃ‰                                                            â”‚
â”‚    â˜ MÃ©triques Prometheus/Grafana configurÃ©es                               â”‚
â”‚    â˜ Alertes dÃ©finies (lag, connexions, erreurs)                            â”‚
â”‚    â˜ Logs centralisÃ©s (ELK ou Ã©quivalent)                                   â”‚
â”‚    â˜ Slow query log activÃ© et surveillÃ©                                     â”‚
â”‚                                                                             â”‚
â”‚  â˜ BACKUP & RECOVERY                                                        â”‚
â”‚    â˜ Backups automatisÃ©s (mariabackup)                                      â”‚
â”‚    â˜ Test de restauration effectuÃ©                                          â”‚
â”‚    â˜ PITR configurÃ© si nÃ©cessaire                                           â”‚
â”‚    â˜ RÃ©tention conforme aux exigences lÃ©gales                               â”‚
â”‚                                                                             â”‚
â”‚  â˜ SCALABILITÃ‰                                                              â”‚
â”‚    â˜ Plan de scaling documentÃ© (vertical puis horizontal)                   â”‚
â”‚    â˜ Sharding strategy dÃ©finie si > 1TB                                     â”‚
â”‚    â˜ Read replicas prÃªtes si read-heavy                                     â”‚
â”‚    â˜ Headroom suffisant (charge < 70%)                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Points clÃ©s Ã  retenir

- **Le choix d'architecture dÃ©pend du use case** â€” il n'y a pas de solution universelle
- **Galera Cluster** reste le standard pour la HA avec RPO=0 en intra-rÃ©gion
- **Le sharding** n'est nÃ©cessaire que pour les trÃ¨s gros volumes (> 1TB ou > 50K QPS)
- **ColumnStore** est idÃ©al pour l'analytics sans impacter l'OLTP
- **Le pattern Outbox + CDC** garantit la cohÃ©rence dans les architectures event-driven
- **L'IA avec MariaDB** est viable pour < 10M vecteurs avec hybrid search
- **Le tiered storage** optimise les coÃ»ts pour les donnÃ©es temporelles
- **MaxScale** simplifie considÃ©rablement le routing et le failover

---

## ğŸ”— Ressources et rÃ©fÃ©rences

- [ğŸ“– MariaDB Architecture Guides](https://mariadb.com/kb/en/mariadb-architecture/)
- [ğŸ“– Galera Cluster Best Practices](https://galeracluster.com/library/documentation/)
- [ğŸ“– ColumnStore User Guide](https://mariadb.com/kb/en/mariadb-columnstore/)
- [ğŸ“– MaxScale Documentation](https://mariadb.com/kb/en/maxscale/)
- [ğŸ“– Debezium MariaDB Connector](https://debezium.io/documentation/reference/stable/connectors/mariadb.html)

---

## â¡ï¸ Conclusion du chapitre

Ce chapitre a couvert les principaux cas d'usage et architectures pour MariaDB 11.8 LTS :

| Section | Sujet | Points clÃ©s |
|---------|-------|-------------|
| 20.1 | OLTP vs OLAP | InnoDB pour transactions, ColumnStore pour analytics |
| 20.2 | Microservices | Database per service, Saga pattern, API Composition |
| 20.3 | Data Warehousing | ColumnStore, ETL/ELT, modÃ©lisation dimensionnelle |
| 20.4 | Multi-tenant | Schema shared, Row-Level Security, isolation |
| 20.5 | GÃ©o-distribution | Multi-rÃ©gion, latence, consistance |
| 20.6 | Hybrid/Multi-cloud | PortabilitÃ©, vendor lock-in, exit strategy |
| 20.7 | Scaling | Vertical vs horizontal, Galera, sharding |
| 20.8 | Event-driven | CDC, Debezium, Kafka, Outbox pattern |
| 20.9 | IA/RAG | Vector search, embeddings, hybrid search |
| 20.10 | MCP Server | IntÃ©gration Claude, agents SQL |
| 20.11 | Frameworks IA | LangChain, LlamaIndex, Haystack |
| 20.12 | Ã‰tudes de cas | Applications rÃ©elles, dÃ©cisions, rÃ©sultats |


â­ï¸ [Glossaire des Termes Techniques](/annexes/glossaire/README.md)
