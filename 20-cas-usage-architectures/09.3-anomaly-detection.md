üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.9.3 Anomaly Detection

> **Niveau** : Interm√©diaire √† Avanc√©  
> **Dur√©e estim√©e** : 4-5 heures  
> **Pr√©requis** : Chapitre 18.10 (MariaDB Vector), Section 20.9.1 (Semantic Search), notions de statistiques et Machine Learning

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre les diff√©rentes approches de d√©tection d'anomalies (distance-based, density-based, statistical)
- Impl√©menter un syst√®me de d√©tection d'anomalies vectoriel avec MariaDB 11.8
- D√©tecter des fraudes financi√®res, intrusions r√©seau et comportements utilisateurs suspects
- Combiner r√®gles m√©tier et similarit√© vectorielle pour r√©duire les faux positifs
- Construire un pipeline de d√©tection en temps r√©el avec alerting

---

## Introduction

La **d√©tection d'anomalies** (ou d√©tection d'outliers) consiste √† identifier des observations qui s'√©cartent significativement du comportement normal. C'est une probl√©matique critique dans de nombreux domaines : d√©tection de fraude bancaire, intrusion r√©seau, d√©faillance d'√©quipements industriels, ou encore comportements utilisateurs suspects.

Traditionnellement, ces syst√®mes reposaient sur des r√®gles statiques (seuils) ou des mod√®les ML complexes n√©cessitant des infrastructures d√©di√©es. Avec MariaDB 11.8 LTS et son support natif des vecteurs, il devient possible d'impl√©menter des syst√®mes de d√©tection d'anomalies performants bas√©s sur la **distance vectorielle** ‚Äî une observation est anormale si elle est "loin" des observations normales dans l'espace vectoriel.

üÜï **Nouveaut√© MariaDB 11.8** : Le type `VECTOR` et les fonctions `VEC_DISTANCE_*` permettent de calculer efficacement la proximit√© d'une nouvelle observation par rapport √† un ensemble de r√©f√©rence, directement en SQL.

---

## Approches de d√©tection d'anomalies

### Vue d'ensemble des m√©thodes

| M√©thode | Principe | Avantages | Inconv√©nients |
|---------|----------|-----------|---------------|
| **R√®gles statiques** | Seuils pr√©d√©finis (montant > X) | Simple, explicable | Rigide, faux positifs |
| **Statistique** | √âcart √† la moyenne (z-score) | Math√©matiquement fond√© | Suppose distribution normale |
| **Distance-based** | √âloignement des voisins (KNN) | Flexible, multi-dimensionnel | Sensible au bruit |
| **Density-based** | Densit√© locale (LOF, DBSCAN) | D√©tecte anomalies locales | Complexe √† param√©trer |
| **Reconstruction** | Erreur autoencoder | Capture patterns complexes | N√©cessite entra√Ænement |

### L'approche vectorielle avec MariaDB

L'approche vectorielle combine la puissance des embeddings (qui capturent des patterns complexes) avec la simplicit√© des m√©thodes distance-based :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   D√âTECTION D'ANOMALIES VECTORIELLE                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                      ‚îÇ
‚îÇ   ‚îÇ  Observation ‚îÇ                                                      ‚îÇ
‚îÇ   ‚îÇ  (features)  ‚îÇ                                                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ
‚îÇ          ‚îÇ                                                              ‚îÇ
‚îÇ          ‚ñº                                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ   ‚îÇ   Embedding  ‚îÇ      ‚îÇ  R√©f√©rence "Normal"          ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ    Mod√®le    ‚îÇ      ‚îÇ  (centro√Øde ou population)   ‚îÇ                ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ          ‚îÇ                             ‚îÇ                                ‚îÇ
‚îÇ          ‚ñº                             ‚ñº                                ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ   ‚îÇ  VECTOR(N)   ‚îÇ            ‚îÇ  VECTOR(N)   ‚îÇ                          ‚îÇ
‚îÇ   ‚îÇ  observation ‚îÇ            ‚îÇ  r√©f√©rence   ‚îÇ                          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ          ‚îÇ                           ‚îÇ                                  ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îÇ                      ‚îÇ                                                  ‚îÇ
‚îÇ                      ‚ñº                                                  ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ            ‚îÇ VEC_DISTANCE_*  ‚îÇ                                          ‚îÇ
‚îÇ            ‚îÇ   (distance)    ‚îÇ                                          ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îÇ                     ‚îÇ                                                   ‚îÇ
‚îÇ                     ‚ñº                                                   ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ            ‚îÇ  distance >     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ OUI ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ üö® ANOMALIE                ‚îÇ
‚îÇ            ‚îÇ  threshold ?    ‚îÇ                                          ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îÇ                     ‚îÇ                                                   ‚îÇ
‚îÇ                    NON                                                  ‚îÇ
‚îÇ                     ‚îÇ                                                   ‚îÇ
‚îÇ                     ‚ñº                                                   ‚îÇ
‚îÇ               ‚úÖ NORMAL                                                 ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Architecture de base pour la d√©tection d'anomalies

### Sch√©ma de donn√©es

```sql
-- Base de donn√©es pour syst√®me de d√©tection d'anomalies
CREATE DATABASE IF NOT EXISTS anomaly_detection
    CHARACTER SET utf8mb4
    COLLATE utf8mb4_uca1400_ai_ci;

USE anomaly_detection;

-- ============================================
-- TABLES DE R√âF√âRENCE (comportement normal)
-- ============================================

-- Profils de comportement normal par entit√© (user, device, account...)
CREATE TABLE normal_profiles (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    entity_type ENUM('user', 'device', 'account', 'ip', 'merchant') NOT NULL,
    entity_id VARCHAR(100) NOT NULL,
    
    -- üÜï Embedding du comportement normal
    behavior_embedding VECTOR(256) NOT NULL,
    
    -- Statistiques pour calcul dynamique du seuil
    observation_count INT UNSIGNED DEFAULT 0,
    avg_distance_to_self FLOAT,  -- Distance moyenne des observations normales
    std_distance FLOAT,          -- √âcart-type pour calcul du seuil
    
    -- M√©tadonn√©es
    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
    window_days INT UNSIGNED DEFAULT 30,  -- Fen√™tre de calcul
    
    UNIQUE KEY uk_entity (entity_type, entity_id),
    
    VECTOR INDEX idx_behavior (behavior_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 16, EF_CONSTRUCTION = 100)
) ENGINE=InnoDB;

-- Centro√Ødes de clusters (comportements types)
CREATE TABLE behavior_clusters (
    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    cluster_name VARCHAR(100) NOT NULL,
    domain ENUM('transactions', 'network', 'user_behavior', 'iot') NOT NULL,
    
    -- Centro√Øde du cluster
    centroid_embedding VECTOR(256) NOT NULL,
    
    -- Rayon du cluster (distance max normale)
    radius FLOAT NOT NULL,
    
    -- Statistiques
    member_count INT UNSIGNED DEFAULT 0,
    
    VECTOR INDEX idx_centroid (centroid_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 8)
) ENGINE=InnoDB;

-- ============================================
-- TABLES D'√âV√âNEMENTS ET ANOMALIES
-- ============================================

-- √âv√©nements bruts avec leur embedding
CREATE TABLE events (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    entity_type ENUM('user', 'device', 'account', 'ip', 'merchant') NOT NULL,
    entity_id VARCHAR(100) NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    
    -- Donn√©es brutes de l'√©v√©nement
    event_data JSON NOT NULL,
    
    -- üÜï Embedding de l'√©v√©nement
    event_embedding VECTOR(256) NOT NULL,
    
    -- Scores de d√©tection
    distance_to_profile FLOAT,
    anomaly_score FLOAT,  -- 0-1, normalis√©
    is_anomaly BOOLEAN DEFAULT FALSE,
    
    -- Contexte
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    source_ip VARCHAR(45),
    user_agent VARCHAR(500),
    
    INDEX idx_entity_time (entity_type, entity_id, timestamp DESC),
    INDEX idx_anomalies (is_anomaly, timestamp DESC),
    INDEX idx_score (anomaly_score DESC)
) ENGINE=InnoDB;

-- Alertes g√©n√©r√©es
CREATE TABLE anomaly_alerts (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    event_id BIGINT UNSIGNED NOT NULL,
    entity_type ENUM('user', 'device', 'account', 'ip', 'merchant') NOT NULL,
    entity_id VARCHAR(100) NOT NULL,
    
    alert_type ENUM('distance', 'velocity', 'pattern', 'rule', 'combined') NOT NULL,
    severity ENUM('low', 'medium', 'high', 'critical') NOT NULL,
    
    anomaly_score FLOAT NOT NULL,
    explanation JSON,  -- {"reason": "...", "features": [...], "threshold": ...}
    
    -- Workflow
    status ENUM('new', 'investigating', 'confirmed_fraud', 'false_positive', 'resolved') DEFAULT 'new',
    assigned_to VARCHAR(100),
    resolution_notes TEXT,
    
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (event_id) REFERENCES events(id),
    INDEX idx_status_severity (status, severity DESC),
    INDEX idx_entity (entity_type, entity_id)
) ENGINE=InnoDB;
```

---

## D√©tection d'anomalies par distance

### M√©thode 1 : Distance au profil individuel

Chaque entit√© (utilisateur, compte, appareil) a son propre profil de comportement normal.

```sql
-- D√©tecter si un nouvel √©v√©nement est anormal pour une entit√© sp√©cifique

SET @entity_type = 'user';
SET @entity_id = 'user_12345';
SET @new_event_embedding = VEC_FromText('[0.12, -0.45, 0.78, ...]');  -- 256 dim

-- Calculer la distance au profil normal
SELECT 
    np.entity_id,
    VEC_DISTANCE_EUCLIDEAN(@new_event_embedding, np.behavior_embedding) AS distance,
    np.avg_distance_to_self,
    np.std_distance,
    -- Seuil dynamique : moyenne + 3 √©carts-types
    (np.avg_distance_to_self + 3 * np.std_distance) AS threshold,
    -- Est-ce une anomalie ?
    CASE 
        WHEN VEC_DISTANCE_EUCLIDEAN(@new_event_embedding, np.behavior_embedding) 
             > (np.avg_distance_to_self + 3 * np.std_distance)
        THEN TRUE
        ELSE FALSE
    END AS is_anomaly,
    -- Score normalis√© (0 = normal, 1 = tr√®s anormal)
    LEAST(
        (VEC_DISTANCE_EUCLIDEAN(@new_event_embedding, np.behavior_embedding) - np.avg_distance_to_self) 
        / (3 * np.std_distance),
        1.0
    ) AS anomaly_score
FROM normal_profiles np
WHERE np.entity_type = @entity_type
AND np.entity_id = @entity_id;
```

### M√©thode 2 : Distance aux K plus proches voisins (KNN)

Une observation est anormale si elle est loin de ses K voisins les plus proches.

```sql
-- D√©tection par distance moyenne aux K plus proches voisins
-- Une transaction est anormale si sa distance moyenne aux 5 transactions
-- les plus similaires d√©passe un seuil

SET @new_event_embedding = VEC_FromText('[0.12, -0.45, 0.78, ...]');
SET @entity_id = 'user_12345';
SET @k = 5;

WITH 
-- Trouver les K √©v√©nements les plus proches (historique normal)
knn_events AS (
    SELECT 
        e.id,
        VEC_DISTANCE_EUCLIDEAN(e.event_embedding, @new_event_embedding) AS distance
    FROM events e
    WHERE e.entity_id = @entity_id
    AND e.is_anomaly = FALSE  -- Seulement les √©v√©nements normaux
    AND e.timestamp >= DATE_SUB(NOW(), INTERVAL 30 DAY)
    ORDER BY distance ASC
    LIMIT 5  -- @k
),
-- Calculer la distance moyenne aux K voisins
knn_stats AS (
    SELECT 
        AVG(distance) AS avg_knn_distance,
        MAX(distance) AS max_knn_distance,
        STDDEV(distance) AS std_knn_distance
    FROM knn_events
)
SELECT 
    ks.avg_knn_distance,
    ks.max_knn_distance,
    -- Anomalie si distance moyenne > seuil adaptatif
    CASE 
        WHEN ks.avg_knn_distance > 0.5  -- Seuil absolu
        OR ks.avg_knn_distance > ks.max_knn_distance * 1.5  -- Seuil relatif
        THEN TRUE
        ELSE FALSE
    END AS is_anomaly,
    -- Score bas√© sur le ratio
    LEAST(ks.avg_knn_distance / 0.5, 1.0) AS anomaly_score
FROM knn_stats ks;
```

### M√©thode 3 : Distance au centro√Øde de cluster

Utile quand on a identifi√© des "types" de comportements normaux.

```sql
-- V√©rifier si un √©v√©nement appartient √† un cluster connu

SET @new_event_embedding = VEC_FromText('[0.12, -0.45, 0.78, ...]');
SET @domain = 'transactions';

SELECT 
    bc.cluster_name,
    bc.radius AS cluster_radius,
    VEC_DISTANCE_EUCLIDEAN(@new_event_embedding, bc.centroid_embedding) AS distance_to_centroid,
    CASE 
        WHEN VEC_DISTANCE_EUCLIDEAN(@new_event_embedding, bc.centroid_embedding) <= bc.radius
        THEN 'INSIDE_CLUSTER'
        ELSE 'OUTSIDE_CLUSTER'
    END AS cluster_membership,
    -- Marge par rapport au rayon (n√©gatif = √† l'int√©rieur)
    VEC_DISTANCE_EUCLIDEAN(@new_event_embedding, bc.centroid_embedding) - bc.radius AS margin
FROM behavior_clusters bc
WHERE bc.domain = @domain
ORDER BY distance_to_centroid ASC
LIMIT 1;

-- Si aucun cluster n'accepte l'√©v√©nement ‚Üí anomalie potentielle
```

üí° **Conseil** : Combinez plusieurs m√©thodes pour r√©duire les faux positifs. Un √©v√©nement flagg√© par une seule m√©thode peut √™tre un bruit ; flagg√© par plusieurs, c'est probablement une vraie anomalie.

---

## Cas d'usage : D√©tection de fraude financi√®re

### Sch√©ma sp√©cifique pour les transactions

```sql
-- Table des transactions avec features pour d√©tection de fraude
CREATE TABLE financial_transactions (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    transaction_id VARCHAR(64) UNIQUE NOT NULL,
    account_id VARCHAR(50) NOT NULL,
    
    -- Donn√©es transactionnelles
    amount DECIMAL(15,2) NOT NULL,
    currency CHAR(3) DEFAULT 'EUR',
    merchant_id VARCHAR(50),
    merchant_category VARCHAR(50),
    channel ENUM('online', 'pos', 'atm', 'wire', 'mobile') NOT NULL,
    
    -- Contexte g√©ographique
    country_code CHAR(2),
    city VARCHAR(100),
    latitude DECIMAL(10,8),
    longitude DECIMAL(11,8),
    
    -- Contexte temporel
    timestamp DATETIME NOT NULL,
    hour_of_day TINYINT AS (HOUR(timestamp)) STORED,
    day_of_week TINYINT AS (DAYOFWEEK(timestamp)) STORED,
    is_weekend BOOLEAN AS (DAYOFWEEK(timestamp) IN (1, 7)) STORED,
    
    -- Contexte appareil
    device_id VARCHAR(100),
    ip_address VARCHAR(45),
    user_agent_hash VARCHAR(64),
    
    -- üÜï Embedding de la transaction (features combin√©es)
    transaction_embedding VECTOR(128) NOT NULL,
    
    -- Scores de fraude
    fraud_score FLOAT DEFAULT 0,
    fraud_reasons JSON,
    is_flagged BOOLEAN DEFAULT FALSE,
    is_confirmed_fraud BOOLEAN DEFAULT NULL,
    
    INDEX idx_account_time (account_id, timestamp DESC),
    INDEX idx_flagged (is_flagged, timestamp DESC),
    INDEX idx_fraud_score (fraud_score DESC),
    
    VECTOR INDEX idx_tx_embedding (transaction_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 24)
) ENGINE=InnoDB;

-- Profil de d√©penses par compte
CREATE TABLE account_spending_profiles (
    account_id VARCHAR(50) PRIMARY KEY,
    
    -- Embedding du comportement de d√©pense normal
    spending_embedding VECTOR(128) NOT NULL,
    
    -- Statistiques agr√©g√©es
    avg_transaction_amount DECIMAL(10,2),
    std_transaction_amount DECIMAL(10,2),
    max_transaction_amount DECIMAL(10,2),
    avg_daily_transactions FLOAT,
    typical_merchants JSON,  -- Top 10 cat√©gories
    typical_countries JSON,  -- Pays habituels
    typical_hours JSON,      -- Heures actives
    
    -- M√©triques de distance
    avg_distance FLOAT,
    std_distance FLOAT,
    
    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    VECTOR INDEX idx_spending (spending_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 16)
) ENGINE=InnoDB;
```

### D√©tection en temps r√©el

```sql
-- Proc√©dure de scoring de fraude pour une nouvelle transaction
DELIMITER //

CREATE PROCEDURE score_transaction(
    IN p_transaction_id VARCHAR(64),
    OUT p_fraud_score FLOAT,
    OUT p_is_flagged BOOLEAN,
    OUT p_reasons JSON
)
BEGIN
    DECLARE v_account_id VARCHAR(50);
    DECLARE v_amount DECIMAL(15,2);
    DECLARE v_embedding VECTOR(128);
    DECLARE v_distance FLOAT;
    DECLARE v_amount_zscore FLOAT;
    DECLARE v_velocity_score FLOAT;
    DECLARE v_geo_score FLOAT;
    DECLARE v_combined_score FLOAT;
    DECLARE v_reasons JSON DEFAULT '[]';
    
    -- R√©cup√©rer les donn√©es de la transaction
    SELECT account_id, amount, transaction_embedding
    INTO v_account_id, v_amount, v_embedding
    FROM financial_transactions
    WHERE transaction_id = p_transaction_id;
    
    -- ===================
    -- Score 1: Distance au profil (comportement anormal)
    -- ===================
    SELECT 
        VEC_DISTANCE_EUCLIDEAN(v_embedding, asp.spending_embedding),
        CASE 
            WHEN VEC_DISTANCE_EUCLIDEAN(v_embedding, asp.spending_embedding) 
                 > (asp.avg_distance + 2.5 * asp.std_distance)
            THEN LEAST(
                (VEC_DISTANCE_EUCLIDEAN(v_embedding, asp.spending_embedding) - asp.avg_distance) 
                / (3 * asp.std_distance), 
                1.0
            )
            ELSE 0
        END
    INTO v_distance, v_combined_score
    FROM account_spending_profiles asp
    WHERE asp.account_id = v_account_id;
    
    IF v_combined_score > 0.3 THEN
        SET v_reasons = JSON_ARRAY_APPEND(v_reasons, '$', 
            JSON_OBJECT('type', 'behavior_distance', 'score', v_combined_score, 
                        'detail', 'Transaction √©loign√©e du comportement habituel'));
    END IF;
    
    -- ===================
    -- Score 2: Montant anormal (z-score)
    -- ===================
    SELECT 
        CASE 
            WHEN std_transaction_amount > 0 THEN
                (v_amount - avg_transaction_amount) / std_transaction_amount
            ELSE 0
        END
    INTO v_amount_zscore
    FROM account_spending_profiles
    WHERE account_id = v_account_id;
    
    IF v_amount_zscore > 3 THEN
        SET v_combined_score = v_combined_score + LEAST((v_amount_zscore - 3) / 3, 0.3);
        SET v_reasons = JSON_ARRAY_APPEND(v_reasons, '$',
            JSON_OBJECT('type', 'amount_anomaly', 'score', v_amount_zscore,
                        'detail', CONCAT('Montant ', ROUND(v_amount_zscore, 1), ' √©carts-types au-dessus de la moyenne')));
    END IF;
    
    -- ===================
    -- Score 3: V√©locit√© (trop de transactions r√©centes)
    -- ===================
    SELECT 
        CASE 
            WHEN COUNT(*) > asp.avg_daily_transactions * 3 THEN 0.3
            WHEN COUNT(*) > asp.avg_daily_transactions * 2 THEN 0.15
            ELSE 0
        END
    INTO v_velocity_score
    FROM financial_transactions ft
    JOIN account_spending_profiles asp ON ft.account_id = asp.account_id
    WHERE ft.account_id = v_account_id
    AND ft.timestamp >= DATE_SUB(NOW(), INTERVAL 1 HOUR);
    
    IF v_velocity_score > 0 THEN
        SET v_combined_score = v_combined_score + v_velocity_score;
        SET v_reasons = JSON_ARRAY_APPEND(v_reasons, '$',
            JSON_OBJECT('type', 'velocity', 'score', v_velocity_score,
                        'detail', 'Nombre de transactions inhabituellement √©lev√©'));
    END IF;
    
    -- ===================
    -- Score 4: G√©olocalisation impossible
    -- ===================
    -- (Transaction dans un pays diff√©rent de la pr√©c√©dente en moins de 2h = impossible physiquement)
    SELECT 
        CASE 
            WHEN prev.country_code IS NOT NULL 
                 AND prev.country_code != curr.country_code
                 AND TIMESTAMPDIFF(MINUTE, prev.timestamp, curr.timestamp) < 120
            THEN 0.5
            ELSE 0
        END
    INTO v_geo_score
    FROM financial_transactions curr
    LEFT JOIN (
        SELECT country_code, timestamp
        FROM financial_transactions
        WHERE account_id = v_account_id
        AND transaction_id != p_transaction_id
        ORDER BY timestamp DESC
        LIMIT 1
    ) prev ON 1=1
    WHERE curr.transaction_id = p_transaction_id;
    
    IF v_geo_score > 0 THEN
        SET v_combined_score = v_combined_score + v_geo_score;
        SET v_reasons = JSON_ARRAY_APPEND(v_reasons, '$',
            JSON_OBJECT('type', 'impossible_travel', 'score', v_geo_score,
                        'detail', 'D√©placement g√©ographique impossible'));
    END IF;
    
    -- ===================
    -- Score final et flagging
    -- ===================
    SET v_combined_score = LEAST(v_combined_score, 1.0);
    SET p_fraud_score = v_combined_score;
    SET p_is_flagged = (v_combined_score >= 0.5);
    SET p_reasons = v_reasons;
    
    -- Mettre √† jour la transaction
    UPDATE financial_transactions
    SET fraud_score = v_combined_score,
        fraud_reasons = v_reasons,
        is_flagged = p_is_flagged
    WHERE transaction_id = p_transaction_id;
    
    -- Cr√©er une alerte si flagg√©e
    IF p_is_flagged THEN
        INSERT INTO anomaly_alerts (
            event_id, entity_type, entity_id, alert_type, severity,
            anomaly_score, explanation
        )
        SELECT 
            id, 'account', v_account_id, 'combined',
            CASE 
                WHEN v_combined_score >= 0.8 THEN 'critical'
                WHEN v_combined_score >= 0.6 THEN 'high'
                ELSE 'medium'
            END,
            v_combined_score, v_reasons
        FROM financial_transactions
        WHERE transaction_id = p_transaction_id;
    END IF;
    
END //

DELIMITER ;

-- Utilisation
CALL score_transaction('TX_20251215_001', @score, @flagged, @reasons);
SELECT @score AS fraud_score, @flagged AS is_flagged, @reasons AS reasons;
```

### Construction du profil de d√©penses (Python)

```python
# fraud_detection_service.py
import numpy as np
import mariadb
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

class FraudDetectionService:
    
    def __init__(self, db_connection):
        self.conn = db_connection
        self.embedding_dim = 128
    
    def build_transaction_embedding(self, tx_data: Dict) -> np.ndarray:
        """
        Construit un embedding de transaction √† partir des features.
        En production, utiliser un mod√®le entra√Æn√© (autoencoder, etc.)
        Ici, version simplifi√©e avec normalisation des features.
        """
        features = []
        
        # Montant (log-normalis√©)
        features.append(np.log1p(tx_data['amount']) / 10)
        
        # Heure (cyclique)
        hour = tx_data['hour_of_day']
        features.append(np.sin(2 * np.pi * hour / 24))
        features.append(np.cos(2 * np.pi * hour / 24))
        
        # Jour de la semaine (cyclique)
        dow = tx_data['day_of_week']
        features.append(np.sin(2 * np.pi * dow / 7))
        features.append(np.cos(2 * np.pi * dow / 7))
        
        # Channel (one-hot)
        channels = ['online', 'pos', 'atm', 'wire', 'mobile']
        for ch in channels:
            features.append(1.0 if tx_data['channel'] == ch else 0.0)
        
        # Cat√©gorie marchand (hash embedding simplifi√©)
        merchant_hash = hash(tx_data.get('merchant_category', '')) % 50
        merchant_one_hot = [0.0] * 50
        merchant_one_hot[merchant_hash] = 1.0
        features.extend(merchant_one_hot)
        
        # Pays (hash embedding)
        country_hash = hash(tx_data.get('country_code', '')) % 30
        country_one_hot = [0.0] * 30
        country_one_hot[country_hash] = 1.0
        features.extend(country_one_hot)
        
        # Padding pour atteindre embedding_dim
        while len(features) < self.embedding_dim:
            features.append(0.0)
        
        embedding = np.array(features[:self.embedding_dim])
        
        # Normaliser L2
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def update_account_profile(self, account_id: str, lookback_days: int = 30):
        """
        Met √† jour le profil de d√©penses d'un compte bas√© sur son historique.
        """
        cursor = self.conn.cursor(dictionary=True)
        
        # R√©cup√©rer les transactions normales r√©centes
        cursor.execute("""
            SELECT 
                amount, hour_of_day, day_of_week, channel,
                merchant_category, country_code,
                VEC_ToText(transaction_embedding) AS embedding_text
            FROM financial_transactions
            WHERE account_id = %s
            AND is_confirmed_fraud = FALSE OR is_confirmed_fraud IS NULL
            AND timestamp >= DATE_SUB(NOW(), INTERVAL %s DAY)
            ORDER BY timestamp DESC
            LIMIT 1000
        """, (account_id, lookback_days))
        
        transactions = cursor.fetchall()
        
        if len(transactions) < 10:
            return  # Pas assez de donn√©es
        
        embeddings = []
        amounts = []
        
        for tx in transactions:
            embedding = np.array(eval(tx['embedding_text']))
            embeddings.append(embedding)
            amounts.append(float(tx['amount']))
        
        embeddings = np.array(embeddings)
        
        # Calculer le centro√Øde (embedding moyen)
        centroid = np.mean(embeddings, axis=0)
        centroid = centroid / np.linalg.norm(centroid)  # Normaliser
        
        # Calculer les distances au centro√Øde
        distances = np.linalg.norm(embeddings - centroid, axis=1)
        
        # Statistiques
        avg_distance = float(np.mean(distances))
        std_distance = float(np.std(distances))
        
        # Calculer les stats de montant
        avg_amount = float(np.mean(amounts))
        std_amount = float(np.std(amounts))
        max_amount = float(np.max(amounts))
        
        # Mettre √† jour le profil
        cursor.execute("""
            INSERT INTO account_spending_profiles 
            (account_id, spending_embedding, avg_transaction_amount, std_transaction_amount,
             max_transaction_amount, avg_distance, std_distance, last_updated)
            VALUES (%s, VEC_FromText(%s), %s, %s, %s, %s, %s, NOW())
            ON DUPLICATE KEY UPDATE
                spending_embedding = VEC_FromText(%s),
                avg_transaction_amount = %s,
                std_transaction_amount = %s,
                max_transaction_amount = %s,
                avg_distance = %s,
                std_distance = %s,
                last_updated = NOW()
        """, (
            account_id, str(centroid.tolist()),
            avg_amount, std_amount, max_amount, avg_distance, std_distance,
            str(centroid.tolist()),
            avg_amount, std_amount, max_amount, avg_distance, std_distance
        ))
        
        self.conn.commit()
    
    def detect_anomaly(self, tx_data: Dict) -> Tuple[float, bool, List[str]]:
        """
        D√©tecte si une transaction est anormale.
        Retourne (score, is_anomaly, reasons)
        """
        cursor = self.conn.cursor(dictionary=True)
        
        account_id = tx_data['account_id']
        embedding = self.build_transaction_embedding(tx_data)
        
        # R√©cup√©rer le profil
        cursor.execute("""
            SELECT 
                VEC_ToText(spending_embedding) AS profile_embedding,
                avg_transaction_amount, std_transaction_amount,
                avg_distance, std_distance
            FROM account_spending_profiles
            WHERE account_id = %s
        """, (account_id,))
        
        profile = cursor.fetchone()
        
        if not profile:
            return 0.0, False, ["Nouveau compte - pas de profil"]
        
        profile_embedding = np.array(eval(profile['profile_embedding']))
        
        # Calculer la distance
        distance = float(np.linalg.norm(embedding - profile_embedding))
        
        # Calculer le z-score de la distance
        avg_dist = profile['avg_distance']
        std_dist = profile['std_distance']
        
        distance_zscore = (distance - avg_dist) / std_dist if std_dist > 0 else 0
        
        # Score et raisons
        score = 0.0
        reasons = []
        
        # Anomalie de comportement
        if distance_zscore > 2.5:
            behavior_score = min((distance_zscore - 2.5) / 2, 0.4)
            score += behavior_score
            reasons.append(f"Comportement anormal (z={distance_zscore:.1f})")
        
        # Anomalie de montant
        amount = tx_data['amount']
        avg_amount = float(profile['avg_transaction_amount'])
        std_amount = float(profile['std_transaction_amount'])
        
        if std_amount > 0:
            amount_zscore = (amount - avg_amount) / std_amount
            if amount_zscore > 3:
                amount_score = min((amount_zscore - 3) / 3, 0.3)
                score += amount_score
                reasons.append(f"Montant √©lev√© ({amount}‚Ç¨, z={amount_zscore:.1f})")
        
        is_anomaly = score >= 0.5
        
        return score, is_anomaly, reasons
```

---

## Cas d'usage : D√©tection d'intrusion r√©seau

### Sch√©ma pour logs r√©seau

```sql
-- Table des connexions r√©seau
CREATE TABLE network_connections (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    connection_id VARCHAR(64) UNIQUE NOT NULL,
    
    -- Identifiants r√©seau
    src_ip VARCHAR(45) NOT NULL,
    dst_ip VARCHAR(45) NOT NULL,
    src_port INT UNSIGNED,
    dst_port INT UNSIGNED,
    protocol ENUM('tcp', 'udp', 'icmp', 'other') NOT NULL,
    
    -- M√©triques de flux
    bytes_sent BIGINT UNSIGNED,
    bytes_received BIGINT UNSIGNED,
    packets_sent INT UNSIGNED,
    packets_received INT UNSIGNED,
    duration_ms INT UNSIGNED,
    
    -- Flags et √©tats
    tcp_flags VARCHAR(20),
    connection_state VARCHAR(20),
    
    -- Timestamp
    timestamp DATETIME NOT NULL,
    
    -- üÜï Embedding du comportement r√©seau
    flow_embedding VECTOR(64) NOT NULL,
    
    -- Scores de d√©tection
    anomaly_score FLOAT DEFAULT 0,
    threat_type VARCHAR(50),
    is_suspicious BOOLEAN DEFAULT FALSE,
    
    INDEX idx_src_ip (src_ip, timestamp DESC),
    INDEX idx_dst_ip (dst_ip, timestamp DESC),
    INDEX idx_suspicious (is_suspicious, timestamp DESC),
    
    VECTOR INDEX idx_flow (flow_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 16)
) ENGINE=InnoDB;

-- Profils de comportement r√©seau par IP interne
CREATE TABLE network_profiles (
    ip_address VARCHAR(45) PRIMARY KEY,
    profile_type ENUM('server', 'workstation', 'iot_device', 'unknown') DEFAULT 'unknown',
    
    -- Embedding du comportement normal
    behavior_embedding VECTOR(64) NOT NULL,
    
    -- Statistiques
    avg_connections_per_hour FLOAT,
    typical_dst_ports JSON,      -- Ports de destination habituels
    typical_dst_ips JSON,        -- IPs de destination habituelles
    avg_bytes_per_connection FLOAT,
    
    avg_distance FLOAT,
    std_distance FLOAT,
    
    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    VECTOR INDEX idx_network_behavior (behavior_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 12)
) ENGINE=InnoDB;

-- Table des menaces connues (signatures)
CREATE TABLE threat_signatures (
    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    threat_name VARCHAR(100) NOT NULL,
    threat_category ENUM('malware', 'dos', 'scan', 'exfiltration', 'c2', 'lateral') NOT NULL,
    severity ENUM('low', 'medium', 'high', 'critical') NOT NULL,
    
    -- Embedding de la signature de menace
    signature_embedding VECTOR(64) NOT NULL,
    signature_radius FLOAT NOT NULL,  -- Rayon de d√©tection
    
    description TEXT,
    mitre_attack_id VARCHAR(20),  -- Ex: T1071
    
    VECTOR INDEX idx_threat_sig (signature_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 8)
) ENGINE=InnoDB;
```

### D√©tection multi-m√©thodes

```sql
-- D√©tection d'anomalies r√©seau combinant plusieurs approches

SET @connection_id = 'CONN_20251215_123456';

WITH 
-- R√©cup√©rer les donn√©es de la connexion
connection_data AS (
    SELECT 
        nc.*,
        VEC_ToText(nc.flow_embedding) AS embedding_text
    FROM network_connections nc
    WHERE nc.connection_id = @connection_id
),
-- Score 1: Distance au profil IP source
profile_distance AS (
    SELECT 
        cd.connection_id,
        VEC_DISTANCE_EUCLIDEAN(cd.flow_embedding, np.behavior_embedding) AS distance,
        np.avg_distance,
        np.std_distance,
        CASE 
            WHEN np.std_distance > 0 THEN
                (VEC_DISTANCE_EUCLIDEAN(cd.flow_embedding, np.behavior_embedding) - np.avg_distance) 
                / np.std_distance
            ELSE 0
        END AS zscore
    FROM connection_data cd
    LEFT JOIN network_profiles np ON cd.src_ip = np.ip_address
),
-- Score 2: Correspondance avec signatures de menaces connues
threat_match AS (
    SELECT 
        cd.connection_id,
        ts.threat_name,
        ts.threat_category,
        ts.severity,
        VEC_DISTANCE_EUCLIDEAN(cd.flow_embedding, ts.signature_embedding) AS distance,
        ts.signature_radius,
        CASE 
            WHEN VEC_DISTANCE_EUCLIDEAN(cd.flow_embedding, ts.signature_embedding) <= ts.signature_radius
            THEN TRUE
            ELSE FALSE
        END AS matches_threat
    FROM connection_data cd
    CROSS JOIN threat_signatures ts
    ORDER BY distance ASC
    LIMIT 1
),
-- Score 3: Comportement inhabituel (port scanning, etc.)
behavior_check AS (
    SELECT 
        cd.connection_id,
        -- Scan de ports (beaucoup de connexions vers des ports diff√©rents)
        (SELECT COUNT(DISTINCT dst_port) 
         FROM network_connections 
         WHERE src_ip = cd.src_ip 
         AND timestamp >= DATE_SUB(cd.timestamp, INTERVAL 5 MINUTE)
        ) AS unique_ports_5min,
        -- Exfiltration (gros volume sortant)
        cd.bytes_sent,
        -- Connexion vers IP suspecte (pas dans l'historique)
        CASE 
            WHEN cd.dst_ip NOT IN (
                SELECT DISTINCT dst_ip 
                FROM network_connections 
                WHERE src_ip = cd.src_ip 
                AND timestamp < cd.timestamp
                LIMIT 1000
            ) THEN TRUE
            ELSE FALSE
        END AS new_destination
    FROM connection_data cd
)
-- Score final combin√©
SELECT 
    cd.connection_id,
    cd.src_ip,
    cd.dst_ip,
    cd.dst_port,
    
    -- Scores individuels
    COALESCE(pd.zscore, 0) AS profile_zscore,
    COALESCE(tm.threat_name, 'none') AS matched_threat,
    tm.matches_threat,
    bc.unique_ports_5min,
    bc.new_destination,
    
    -- Score combin√©
    (
        -- Score de distance au profil
        LEAST(GREATEST(COALESCE(pd.zscore, 0) - 2, 0) / 3, 0.3) +
        -- Score de menace connue
        CASE WHEN tm.matches_threat THEN 0.5 ELSE 0 END +
        -- Score de scan de ports
        CASE WHEN bc.unique_ports_5min > 50 THEN 0.2 
             WHEN bc.unique_ports_5min > 20 THEN 0.1 
             ELSE 0 END +
        -- Score nouvelle destination
        CASE WHEN bc.new_destination THEN 0.1 ELSE 0 END
    ) AS anomaly_score,
    
    -- Classification
    CASE 
        WHEN tm.matches_threat THEN tm.threat_category
        WHEN bc.unique_ports_5min > 50 THEN 'scan'
        WHEN cd.bytes_sent > 100000000 AND bc.new_destination THEN 'exfiltration'
        WHEN COALESCE(pd.zscore, 0) > 4 THEN 'unknown_anomaly'
        ELSE NULL
    END AS threat_classification

FROM connection_data cd
LEFT JOIN profile_distance pd ON cd.connection_id = pd.connection_id
LEFT JOIN threat_match tm ON cd.connection_id = tm.connection_id
LEFT JOIN behavior_check bc ON cd.connection_id = bc.connection_id;
```

---

## Cas d'usage : Monitoring d'√©quipements IoT

### Sch√©ma pour capteurs IoT

```sql
-- Mesures de capteurs IoT
CREATE TABLE iot_measurements (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    device_id VARCHAR(50) NOT NULL,
    device_type ENUM('temperature', 'pressure', 'vibration', 'flow', 'power') NOT NULL,
    
    -- Valeur mesur√©e
    value FLOAT NOT NULL,
    unit VARCHAR(20),
    
    -- Contexte
    timestamp DATETIME NOT NULL,
    location VARCHAR(100),
    
    -- üÜï Embedding de la mesure (valeur + contexte temporel + tendance)
    measurement_embedding VECTOR(32) NOT NULL,
    
    -- D√©tection
    anomaly_score FLOAT DEFAULT 0,
    is_anomaly BOOLEAN DEFAULT FALSE,
    anomaly_type VARCHAR(50),
    
    INDEX idx_device_time (device_id, timestamp DESC),
    INDEX idx_anomalies (is_anomaly, device_type, timestamp DESC),
    
    VECTOR INDEX idx_measurement (measurement_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 12)
) ENGINE=InnoDB;

-- Profils de capteurs (comportement normal)
CREATE TABLE iot_device_profiles (
    device_id VARCHAR(50) PRIMARY KEY,
    device_type ENUM('temperature', 'pressure', 'vibration', 'flow', 'power') NOT NULL,
    
    -- Valeurs normales
    min_normal FLOAT,
    max_normal FLOAT,
    avg_value FLOAT,
    std_value FLOAT,
    
    -- Embedding du comportement normal
    normal_embedding VECTOR(32) NOT NULL,
    avg_distance FLOAT,
    std_distance FLOAT,
    
    -- Seuils adaptatifs
    warning_threshold FLOAT,  -- distance
    critical_threshold FLOAT,
    
    last_calibration DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    VECTOR INDEX idx_normal (normal_embedding)
        WITH (DISTANCE = EUCLIDEAN, M = 8)
) ENGINE=InnoDB;
```

### D√©tection de d√©faillance pr√©coce

```sql
-- D√©tecter une d√©rive progressive (early warning de panne)

SET @device_id = 'SENSOR_TEMP_001';

WITH 
-- Mesures des derni√®res 24h
recent_measurements AS (
    SELECT 
        im.id,
        im.value,
        im.timestamp,
        im.measurement_embedding,
        ROW_NUMBER() OVER (ORDER BY im.timestamp DESC) AS rn
    FROM iot_measurements im
    WHERE im.device_id = @device_id
    AND im.timestamp >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
),
-- Calculer la tendance de distance au profil normal
distance_trend AS (
    SELECT 
        rm.rn,
        rm.timestamp,
        rm.value,
        VEC_DISTANCE_EUCLIDEAN(rm.measurement_embedding, dp.normal_embedding) AS distance,
        dp.avg_distance,
        dp.std_distance,
        dp.warning_threshold,
        dp.critical_threshold
    FROM recent_measurements rm
    CROSS JOIN iot_device_profiles dp
    WHERE dp.device_id = @device_id
),
-- D√©tecter la tendance (r√©gression lin√©aire simplifi√©e)
trend_analysis AS (
    SELECT 
        AVG(distance) AS avg_recent_distance,
        -- Pente approximative : diff√©rence entre premi√®re et derni√®re moiti√©
        (SELECT AVG(distance) FROM distance_trend WHERE rn <= (SELECT MAX(rn)/2 FROM distance_trend)) 
        - (SELECT AVG(distance) FROM distance_trend WHERE rn > (SELECT MAX(rn)/2 FROM distance_trend)) AS distance_trend,
        MIN(warning_threshold) AS warning_threshold,
        MIN(critical_threshold) AS critical_threshold,
        MIN(avg_distance) AS baseline_distance,
        MIN(std_distance) AS baseline_std
    FROM distance_trend
)
SELECT 
    @device_id AS device_id,
    ta.avg_recent_distance,
    ta.baseline_distance,
    ta.distance_trend,
    
    -- Ratio par rapport √† la baseline
    ta.avg_recent_distance / NULLIF(ta.baseline_distance, 0) AS distance_ratio,
    
    -- √âtat du capteur
    CASE 
        WHEN ta.avg_recent_distance > ta.critical_threshold THEN 'CRITICAL'
        WHEN ta.avg_recent_distance > ta.warning_threshold THEN 'WARNING'
        WHEN ta.distance_trend < -0.1 THEN 'DEGRADING'  -- Tendance √† s'√©loigner
        ELSE 'NORMAL'
    END AS device_status,
    
    -- Pr√©diction de panne (si tendance continue)
    CASE 
        WHEN ta.distance_trend < -0.05 THEN
            CONCAT('Panne estim√©e dans ', 
                   ROUND((ta.critical_threshold - ta.avg_recent_distance) / ABS(ta.distance_trend), 0),
                   ' heures')
        ELSE 'Pas de panne pr√©visible'
    END AS failure_prediction

FROM trend_analysis ta;
```

---

## Pipeline de d√©tection en temps r√©el

### Architecture streaming

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PIPELINE D√âTECTION TEMPS R√âEL                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                    ‚îÇ
‚îÇ   ‚îÇ   Sources    ‚îÇ                                                    ‚îÇ
‚îÇ   ‚îÇ  (Kafka,     ‚îÇ                                                    ‚îÇ
‚îÇ   ‚îÇ   API, IoT)  ‚îÇ                                                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                    ‚îÇ
‚îÇ          ‚îÇ                                                            ‚îÇ
‚îÇ          ‚ñº                                                            ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ   ‚îÇ              Stream Processor (Flink / Kafka Streams)     ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ Enrichment ‚îÇ‚îÄ‚ñ∂‚îÇ Embedding  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Anomaly Scoring     ‚îÇ ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ (context)  ‚îÇ  ‚îÇ Generation ‚îÇ   ‚îÇ (r√®gles + ML)       ‚îÇ ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                  ‚îÇ                    ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ                    ‚îÇ                                      ‚îÇ           ‚îÇ
‚îÇ                    ‚ñº                                      ‚ñº           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ  MariaDB 11.8 Vector    ‚îÇ          ‚îÇ     Alert System         ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                         ‚îÇ          ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ events               ‚îÇ          ‚îÇ  ‚îÇ score >= critical   ‚îÇ ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ profiles             ‚îÇ          ‚îÇ  ‚îÇ ‚Üí PagerDuty         ‚îÇ ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ anomaly_alerts       ‚îÇ          ‚îÇ  ‚îÇ score >= high       ‚îÇ ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ VECTOR INDEX (HNSW)  ‚îÇ          ‚îÇ  ‚îÇ ‚Üí Slack             ‚îÇ ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                         ‚îÇ          ‚îÇ  ‚îÇ score >= medium     ‚îÇ ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ  ‚îÇ ‚Üí Dashboard         ‚îÇ ‚îÇ   ‚îÇ
‚îÇ               ‚îÇ                        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ               ‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚ñº                                                       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                         ‚îÇ
‚îÇ   ‚îÇ  Analytics Dashboard    ‚îÇ                                         ‚îÇ
‚îÇ   ‚îÇ  (Grafana / Superset)   ‚îÇ                                         ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                         ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Impl√©mentation du scoring service

```python
# anomaly_scoring_service.py
import mariadb
import numpy as np
from kafka import KafkaConsumer, KafkaProducer
import json
from typing import Dict, Tuple, List
from datetime import datetime

class AnomalyScoringService:
    
    def __init__(self, db_config: Dict, kafka_config: Dict):
        self.conn = mariadb.connect(**db_config)
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_config['bootstrap_servers'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        # Cache des profils (refresh p√©riodique)
        self.profile_cache = {}
        self.cache_ttl = 300  # 5 minutes
        self.last_cache_refresh = {}
    
    def get_profile(self, entity_type: str, entity_id: str) -> Dict:
        """R√©cup√®re le profil avec cache."""
        cache_key = f"{entity_type}:{entity_id}"
        now = datetime.now().timestamp()
        
        if (cache_key in self.profile_cache and 
            now - self.last_cache_refresh.get(cache_key, 0) < self.cache_ttl):
            return self.profile_cache[cache_key]
        
        cursor = self.conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                VEC_ToText(behavior_embedding) AS embedding,
                avg_distance_to_self,
                std_distance
            FROM normal_profiles
            WHERE entity_type = %s AND entity_id = %s
        """, (entity_type, entity_id))
        
        result = cursor.fetchone()
        
        if result:
            profile = {
                'embedding': np.array(eval(result['embedding'])),
                'avg_distance': result['avg_distance_to_self'],
                'std_distance': result['std_distance']
            }
            self.profile_cache[cache_key] = profile
            self.last_cache_refresh[cache_key] = now
            return profile
        
        return None
    
    def score_event(self, event: Dict) -> Tuple[float, bool, List[str]]:
        """
        Score un √©v√©nement en temps r√©el.
        Retourne (score, is_anomaly, reasons)
        """
        entity_type = event['entity_type']
        entity_id = event['entity_id']
        event_embedding = np.array(event['embedding'])
        
        reasons = []
        score = 0.0
        
        # 1. Score de distance au profil
        profile = self.get_profile(entity_type, entity_id)
        
        if profile:
            distance = float(np.linalg.norm(event_embedding - profile['embedding']))
            
            if profile['std_distance'] and profile['std_distance'] > 0:
                zscore = (distance - profile['avg_distance']) / profile['std_distance']
                
                if zscore > 3:
                    distance_score = min((zscore - 3) / 3, 0.5)
                    score += distance_score
                    reasons.append(f"Distance anormale (z={zscore:.2f})")
        else:
            # Nouveau profil - comparer aux clusters
            score += self._score_against_clusters(event_embedding, entity_type)
            if score > 0:
                reasons.append("Comportement non reconnu (nouveau profil)")
        
        # 2. R√®gles m√©tier sp√©cifiques
        rule_score, rule_reasons = self._apply_business_rules(event)
        score += rule_score
        reasons.extend(rule_reasons)
        
        # 3. V√©rifier les patterns connus (signatures)
        sig_score, sig_name = self._check_signatures(event_embedding, entity_type)
        if sig_score > 0:
            score += sig_score
            reasons.append(f"Signature d√©tect√©e: {sig_name}")
        
        # Score final
        score = min(score, 1.0)
        is_anomaly = score >= 0.5
        
        return score, is_anomaly, reasons
    
    def _score_against_clusters(self, embedding: np.ndarray, entity_type: str) -> float:
        """Score bas√© sur la distance aux clusters connus."""
        cursor = self.conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                cluster_name,
                VEC_ToText(centroid_embedding) AS centroid,
                radius
            FROM behavior_clusters
            WHERE domain = %s
        """, (entity_type,))
        
        min_margin = float('inf')
        
        for cluster in cursor.fetchall():
            centroid = np.array(eval(cluster['centroid']))
            distance = float(np.linalg.norm(embedding - centroid))
            margin = distance - cluster['radius']
            min_margin = min(min_margin, margin)
        
        if min_margin == float('inf'):
            return 0.0
        
        # Si en dehors de tous les clusters
        if min_margin > 0:
            return min(min_margin / 0.5, 0.4)  # Score max 0.4
        
        return 0.0
    
    def _apply_business_rules(self, event: Dict) -> Tuple[float, List[str]]:
        """Applique les r√®gles m√©tier."""
        score = 0.0
        reasons = []
        
        event_data = event.get('data', {})
        
        # Exemple: montant de transaction
        if 'amount' in event_data:
            amount = event_data['amount']
            if amount > 10000:
                score += 0.2
                reasons.append(f"Montant √©lev√©: {amount}‚Ç¨")
            if amount > 50000:
                score += 0.3
                reasons.append("Montant tr√®s √©lev√© (>50k‚Ç¨)")
        
        # Exemple: heure inhabituelle
        if 'hour' in event_data:
            hour = event_data['hour']
            if hour >= 2 and hour <= 5:
                score += 0.15
                reasons.append("Activit√© nocturne inhabituelle")
        
        return score, reasons
    
    def _check_signatures(self, embedding: np.ndarray, entity_type: str) -> Tuple[float, str]:
        """V√©rifie contre les signatures de menaces connues."""
        cursor = self.conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                threat_name,
                VEC_ToText(signature_embedding) AS sig_embedding,
                signature_radius,
                severity
            FROM threat_signatures
            WHERE domain = %s OR domain IS NULL
        """, (entity_type,))
        
        for sig in cursor.fetchall():
            sig_embedding = np.array(eval(sig['sig_embedding']))
            distance = float(np.linalg.norm(embedding - sig_embedding))
            
            if distance <= sig['signature_radius']:
                severity_scores = {'low': 0.3, 'medium': 0.5, 'high': 0.7, 'critical': 0.9}
                return severity_scores.get(sig['severity'], 0.5), sig['threat_name']
        
        return 0.0, ""
    
    def process_stream(self, topic: str, alert_topic: str):
        """Traitement continu des √©v√©nements."""
        consumer = KafkaConsumer(
            topic,
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda v: json.loads(v.decode('utf-8')),
            group_id='anomaly-detection'
        )
        
        for message in consumer:
            event = message.value
            
            try:
                score, is_anomaly, reasons = self.score_event(event)
                
                # Enrichir l'√©v√©nement
                event['anomaly_score'] = score
                event['is_anomaly'] = is_anomaly
                event['anomaly_reasons'] = reasons
                event['scored_at'] = datetime.now().isoformat()
                
                # Persister dans MariaDB
                self._persist_event(event)
                
                # Envoyer alerte si n√©cessaire
                if is_anomaly:
                    self._send_alert(event, alert_topic)
                    
            except Exception as e:
                print(f"Erreur traitement √©v√©nement: {e}")
    
    def _persist_event(self, event: Dict):
        """Persiste l'√©v√©nement scor√©."""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO events 
            (entity_type, entity_id, event_type, event_data, event_embedding,
             distance_to_profile, anomaly_score, is_anomaly, timestamp)
            VALUES (%s, %s, %s, %s, VEC_FromText(%s), %s, %s, %s, NOW())
        """, (
            event['entity_type'],
            event['entity_id'],
            event.get('event_type', 'unknown'),
            json.dumps(event.get('data', {})),
            str(event['embedding']),
            event.get('distance'),
            event['anomaly_score'],
            event['is_anomaly']
        ))
        self.conn.commit()
    
    def _send_alert(self, event: Dict, topic: str):
        """Envoie une alerte."""
        severity = 'medium'
        if event['anomaly_score'] >= 0.8:
            severity = 'critical'
        elif event['anomaly_score'] >= 0.65:
            severity = 'high'
        
        alert = {
            'entity_type': event['entity_type'],
            'entity_id': event['entity_id'],
            'anomaly_score': event['anomaly_score'],
            'severity': severity,
            'reasons': event['anomaly_reasons'],
            'timestamp': event['scored_at']
        }
        
        self.producer.send(topic, alert)
        self.producer.flush()
```

---

## R√©duction des faux positifs

### Strat√©gies de calibration

```sql
-- Analyser les faux positifs pour ajuster les seuils
WITH 
labeled_alerts AS (
    SELECT 
        aa.id,
        aa.anomaly_score,
        aa.alert_type,
        aa.severity,
        aa.status,
        CASE 
            WHEN aa.status = 'false_positive' THEN 0
            WHEN aa.status = 'confirmed_fraud' THEN 1
            ELSE NULL
        END AS is_true_positive
    FROM anomaly_alerts aa
    WHERE aa.status IN ('false_positive', 'confirmed_fraud')
    AND aa.created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY)
),
-- Calculer les m√©triques par seuil
threshold_analysis AS (
    SELECT 
        ROUND(anomaly_score, 1) AS score_bucket,
        COUNT(*) AS total_alerts,
        SUM(is_true_positive) AS true_positives,
        SUM(1 - is_true_positive) AS false_positives,
        AVG(is_true_positive) AS precision
    FROM labeled_alerts
    WHERE is_true_positive IS NOT NULL
    GROUP BY ROUND(anomaly_score, 1)
)
SELECT 
    score_bucket,
    total_alerts,
    true_positives,
    false_positives,
    ROUND(precision * 100, 1) AS precision_pct,
    -- Taux de faux positifs cumul√© si on utilise ce seuil
    (SELECT SUM(false_positives) 
     FROM threshold_analysis t2 
     WHERE t2.score_bucket >= ta.score_bucket) AS cumulative_fp,
    -- Rappel si on utilise ce seuil
    (SELECT SUM(true_positives) 
     FROM threshold_analysis t2 
     WHERE t2.score_bucket >= ta.score_bucket) 
    / (SELECT SUM(true_positives) FROM threshold_analysis) * 100 AS recall_pct
FROM threshold_analysis ta
ORDER BY score_bucket DESC;
```

### Whitelist et exceptions

```sql
-- Table des exceptions (comportements valid√©s manuellement)
CREATE TABLE anomaly_exceptions (
    id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    entity_type ENUM('user', 'device', 'account', 'ip', 'merchant') NOT NULL,
    entity_id VARCHAR(100),  -- NULL = tous
    exception_type ENUM('whitelist', 'known_pattern', 'scheduled', 'temporary') NOT NULL,
    
    -- Pattern √† ignorer (embedding ou r√®gle)
    pattern_embedding VECTOR(256),
    pattern_radius FLOAT,  -- Distance max pour match
    rule_expression TEXT,  -- Expression SQL optionnelle
    
    reason TEXT NOT NULL,
    approved_by VARCHAR(100),
    
    valid_from DATETIME DEFAULT CURRENT_TIMESTAMP,
    valid_until DATETIME,  -- NULL = permanent
    
    is_active BOOLEAN DEFAULT TRUE,
    
    INDEX idx_entity (entity_type, entity_id),
    VECTOR INDEX idx_pattern (pattern_embedding) WITH (DISTANCE = EUCLIDEAN, M = 8)
) ENGINE=InnoDB;

-- V√©rifier si un √©v√©nement match une exception
SELECT 
    ae.id AS exception_id,
    ae.exception_type,
    ae.reason,
    VEC_DISTANCE_EUCLIDEAN(@event_embedding, ae.pattern_embedding) AS distance
FROM anomaly_exceptions ae
WHERE 
    ae.is_active = TRUE
    AND (ae.entity_type = @entity_type)
    AND (ae.entity_id IS NULL OR ae.entity_id = @entity_id)
    AND (ae.valid_until IS NULL OR ae.valid_until > NOW())
    AND VEC_DISTANCE_EUCLIDEAN(@event_embedding, ae.pattern_embedding) <= ae.pattern_radius
LIMIT 1;

-- Si r√©sultat trouv√© ‚Üí ne pas alerter
```

---

## M√©triques et monitoring

### Dashboard de performance

```sql
-- M√©triques op√©rationnelles du syst√®me de d√©tection
SELECT 
    DATE(e.timestamp) AS date,
    
    -- Volume
    COUNT(*) AS total_events,
    SUM(CASE WHEN e.is_anomaly THEN 1 ELSE 0 END) AS anomalies_detected,
    ROUND(SUM(CASE WHEN e.is_anomaly THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS anomaly_rate_pct,
    
    -- Par s√©v√©rit√© (via les alertes)
    SUM(CASE WHEN aa.severity = 'critical' THEN 1 ELSE 0 END) AS critical_alerts,
    SUM(CASE WHEN aa.severity = 'high' THEN 1 ELSE 0 END) AS high_alerts,
    SUM(CASE WHEN aa.severity = 'medium' THEN 1 ELSE 0 END) AS medium_alerts,
    
    -- R√©solution
    SUM(CASE WHEN aa.status = 'confirmed_fraud' THEN 1 ELSE 0 END) AS confirmed_frauds,
    SUM(CASE WHEN aa.status = 'false_positive' THEN 1 ELSE 0 END) AS false_positives,
    
    -- Pr√©cision (si assez de donn√©es label√©es)
    ROUND(
        SUM(CASE WHEN aa.status = 'confirmed_fraud' THEN 1 ELSE 0 END) * 100.0 /
        NULLIF(SUM(CASE WHEN aa.status IN ('confirmed_fraud', 'false_positive') THEN 1 ELSE 0 END), 0),
        1
    ) AS precision_pct,
    
    -- Scores moyens
    ROUND(AVG(e.anomaly_score), 3) AS avg_anomaly_score,
    ROUND(AVG(CASE WHEN e.is_anomaly THEN e.anomaly_score END), 3) AS avg_flagged_score

FROM events e
LEFT JOIN anomaly_alerts aa ON e.id = aa.event_id
WHERE e.timestamp >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY DATE(e.timestamp)
ORDER BY date DESC;
```

---

## ‚úÖ Points cl√©s √† retenir

- La **d√©tection d'anomalies vectorielle** identifie les observations "loin" du comportement normal dans l'espace des embeddings
- MariaDB 11.8 permet de calculer les distances (`VEC_DISTANCE_*`) directement en SQL sans outil externe
- Les **profils d'entit√©s** (utilisateur, compte, appareil) servent de r√©f√©rence pour le comportement normal
- Combinez plusieurs m√©thodes : **distance au profil**, **KNN**, **signatures de menaces**, **r√®gles m√©tier**
- Le **seuil dynamique** (moyenne + N √©carts-types) s'adapte automatiquement √† chaque entit√©
- La **calibration** bas√©e sur les faux positifs labell√©s permet d'optimiser les seuils
- Les **exceptions et whitelists** r√©duisent les alertes pour les patterns valid√©s manuellement
- Le **pipeline temps r√©el** (Kafka + scoring service + MariaDB) permet une d√©tection en continu

---

## üîó Ressources et r√©f√©rences

- [üìñ MariaDB Vector Documentation](https://mariadb.com/kb/en/vector/) ‚Äî Documentation officielle MariaDB Vector
- [üìñ Anomaly Detection: A Survey](https://dl.acm.org/doi/10.1145/1541880.1541882) ‚Äî ACM Computing Surveys
- [üìñ Isolation Forest Paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) ‚Äî Algorithme de r√©f√©rence
- [üìñ LOF: Local Outlier Factor](https://dl.acm.org/doi/10.1145/342009.335388) ‚Äî D√©tection bas√©e densit√©
- [üìñ MITRE ATT&CK Framework](https://attack.mitre.org/) ‚Äî Taxonomie des menaces cyber

---

## ‚û°Ô∏è Section suivante

**20.9.4 Hybrid Search** : D√©couvrez comment combiner recherche vectorielle s√©mantique et recherche full-text traditionnelle pour obtenir le meilleur des deux mondes ‚Äî pr√©cision sur les termes exacts et compr√©hension du sens.

‚è≠Ô∏è [Hybrid Search (vecteurs + SQL relationnel)](/20-cas-usage-architectures/09.4-hybrid-search.md)
