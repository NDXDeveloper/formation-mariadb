üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.8.2 Int√©gration avec Apache Kafka

> **Niveau** : Interm√©diaire √† Avanc√©  
> **Dur√©e estim√©e** : 3 heures  
> **Pr√©requis** : Section 20.8.1 (CDC), Chapitre 13 (R√©plication), notions de syst√®mes distribu√©s

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :

- Comprendre l'architecture Apache Kafka et ses concepts cl√©s
- Impl√©menter des patterns d'int√©gration MariaDB-Kafka bidirectionnels
- Concevoir des architectures event-driven avec MariaDB comme source et destination
- Utiliser le pattern Transactional Outbox pour des garanties exactly-once
- Mettre en place du streaming analytics avec Kafka Streams et ksqlDB
- Appliquer les bonnes pratiques pour la r√©silience et les performances

---

## Introduction

**Apache Kafka** est une plateforme de streaming distribu√© qui permet de publier, stocker et traiter des flux d'√©v√©nements en temps r√©el. L'int√©gration entre MariaDB et Kafka cr√©e un pont entre le monde **transactionnel** (OLTP) et le monde **√©v√©nementiel** (Event-Driven Architecture).

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 MARIADB + KAFKA : ARCHITECTURE HYBRIDE                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

          MONDE TRANSACTIONNEL              MONDE √âV√âNEMENTIEL
          
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         ‚îÇ           ‚îÇ                                 ‚îÇ
‚îÇ       MariaDB           ‚îÇ           ‚îÇ         Apache Kafka            ‚îÇ
‚îÇ                         ‚îÇ           ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ           ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ   Applications  ‚îÇ    ‚îÇ           ‚îÇ    ‚îÇ      Topics         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ   CRUD          ‚îÇ    ‚îÇ    CDC    ‚îÇ    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ‚îÇ orders.events ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Transactions   ‚îÇ    ‚îÇ  Debezium ‚îÇ    ‚îÇ  ‚îÇ users.events  ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ACID           ‚îÇ    ‚îÇ           ‚îÇ    ‚îÇ  ‚îÇ inventory.upd ‚îÇ  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ           ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ      ‚îÇ
‚îÇ                         ‚îÇ           ‚îÇ    ‚îÇ                     ‚îÇ      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ           ‚îÇ    ‚îÇ  Partitions         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ   Tables        ‚îÇ    ‚îÇ  Sink     ‚îÇ    ‚îÇ  R√©plication        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ   InnoDB        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  R√©tention          ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ Connector ‚îÇ    ‚îÇ                     ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ           ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                         ‚îÇ           ‚îÇ                                 ‚îÇ
‚îÇ  ‚Ä¢ Consistance forte    ‚îÇ           ‚îÇ    ‚Ä¢ Scalabilit√© horizontale    ‚îÇ
‚îÇ  ‚Ä¢ Requ√™tes complexes   ‚îÇ           ‚îÇ    ‚Ä¢ D√©couplage temporel        ‚îÇ
‚îÇ  ‚Ä¢ Int√©grit√© r√©f√©rent.  ‚îÇ           ‚îÇ    ‚Ä¢ Replay des √©v√©nements      ‚îÇ
‚îÇ                         ‚îÇ           ‚îÇ    ‚Ä¢ Traitement temps r√©el      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Cette int√©gration permet de combiner les forces des deux syst√®mes : la **fiabilit√© transactionnelle** de MariaDB et la **scalabilit√© √©v√©nementielle** de Kafka.

---

## Concepts fondamentaux de Kafka

### Architecture Kafka

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ARCHITECTURE APACHE KAFKA                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          KAFKA CLUSTER                                  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  Broker 1   ‚îÇ   ‚îÇ  Broker 2   ‚îÇ   ‚îÇ  Broker 3   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îÇTopic A  ‚îÇ ‚îÇ   ‚îÇ ‚îÇTopic A  ‚îÇ ‚îÇ   ‚îÇ ‚îÇTopic A  ‚îÇ ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îÇPart 0   ‚îÇ ‚îÇ   ‚îÇ ‚îÇPart 1   ‚îÇ ‚îÇ   ‚îÇ ‚îÇPart 2   ‚îÇ ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îÇ(Leader) ‚îÇ ‚îÇ   ‚îÇ ‚îÇ(Leader) ‚îÇ ‚îÇ   ‚îÇ ‚îÇ(Leader) ‚îÇ ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îÇTopic A  ‚îÇ ‚îÇ   ‚îÇ ‚îÇTopic A  ‚îÇ ‚îÇ   ‚îÇ ‚îÇTopic A  ‚îÇ ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îÇPart 1   ‚îÇ ‚îÇ   ‚îÇ ‚îÇPart 2   ‚îÇ ‚îÇ   ‚îÇ ‚îÇPart 0   ‚îÇ ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îÇ(Replica)‚îÇ ‚îÇ   ‚îÇ ‚îÇ(Replica)‚îÇ ‚îÇ   ‚îÇ ‚îÇ(Replica)‚îÇ ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                    ZooKeeper / KRaft                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ           (Coordination, m√©tadonn√©es, √©lection leader)            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PRODUCTEURS                                              CONSOMMATEURS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇProducer ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇConsumer ‚îÇ
‚îÇ   A     ‚îÇ       Topic A, Partition 0                   ‚îÇ Group 1 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇProducer ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇConsumer ‚îÇ
‚îÇ   B     ‚îÇ       Topic A, Partition 1                   ‚îÇ Group 1 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                         ‚îÇConsumer ‚îÇ
                                                         ‚îÇ Group 2 ‚îÇ
                                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Concepts cl√©s

| Concept | Description | Analogie MariaDB |
|---------|-------------|------------------|
| **Topic** | Canal de messages cat√©goris√© | Table |
| **Partition** | Subdivision ordonn√©e d'un topic | Shard/Partition |
| **Offset** | Position d'un message dans une partition | Auto-increment ID |
| **Consumer Group** | Groupe de consommateurs partageant la charge | Connection pool |
| **Broker** | Serveur Kafka | Instance MariaDB |
| **Replication Factor** | Nombre de copies d'une partition | R√©plication |

### Garanties de livraison

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GARANTIES DE LIVRAISON KAFKA                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

AT-MOST-ONCE (Au plus une fois)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Producer ‚îÄ‚îÄ‚ñ∂ Send ‚îÄ‚îÄ‚ñ∂ [Kafka] ‚îÄ‚îÄ‚ñ∂ Consumer                              ‚îÇ
‚îÇ              ‚îÇ                                                          ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ Pas de retry en cas d'√©chec                            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ ‚ö†Ô∏è Risque: Messages perdus                                              ‚îÇ
‚îÇ ‚úÖ Avantage: Performance maximale                                       ‚îÇ
‚îÇ üéØ Usage: M√©triques, logs non critiques                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

AT-LEAST-ONCE (Au moins une fois)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Producer ‚îÄ‚îÄ‚ñ∂ Send ‚îÄ‚îÄ‚ñ∂ [Kafka] ‚îÄ‚îÄ‚ñ∂ ACK ‚îÄ‚îÄ‚ñ∂ Consumer                      ‚îÇ
‚îÇ              ‚îÇ                    ‚îÇ                                     ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ Retry si pas ACK ‚îò                                     ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ ‚ö†Ô∏è Risque: Messages dupliqu√©s                                           ‚îÇ
‚îÇ ‚úÖ Avantage: Pas de perte                                               ‚îÇ
‚îÇ üéØ Usage: La plupart des cas (avec idempotence c√¥t√© consumer)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

EXACTLY-ONCE (Exactement une fois)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Producer ‚îÄ‚îÄ‚ñ∂ [Transaction] ‚îÄ‚îÄ‚ñ∂ [Kafka] ‚îÄ‚îÄ‚ñ∂ [Transaction] ‚îÄ‚îÄ‚ñ∂ Consumer   ‚îÇ
‚îÇ              ‚îÇ                             ‚îÇ                            ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ Idempotent Producer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                  + Transactions Kafka                                   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ ‚ö†Ô∏è Co√ªt: Latence et complexit√©                                          ‚îÇ
‚îÇ ‚úÖ Avantage: Garantie forte                                             ‚îÇ
‚îÇ üéØ Usage: Donn√©es financi√®res, syst√®mes critiques                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Patterns d'int√©gration MariaDB-Kafka

### Pattern 1 : Source CDC (MariaDB ‚Üí Kafka)

Le pattern le plus courant utilise **Debezium** pour capturer les changements MariaDB et les publier dans Kafka (voir Section 20.8.1).

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PATTERN: CDC SOURCE                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MariaDB    ‚îÇ     ‚îÇ   Debezium   ‚îÇ     ‚îÇ    Kafka     ‚îÇ     ‚îÇ Consumer‚îÇ
‚îÇ              ‚îÇ     ‚îÇ  Connector   ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ         ‚îÇ
‚îÇ Binary Logs  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ              ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Topics     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Service ‚îÇ
‚îÇ              ‚îÇ     ‚îÇ  (Source)    ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                                                              ‚îÇ
     ‚îÇ  INSERT INTO orders...                                       ‚îÇ
     ‚îÇ  UPDATE customers...                                         ‚ñº
     ‚îÇ  DELETE products...                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                                                  ‚îÇ ‚Ä¢ Analytics     ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Search Index  ‚îÇ
                                                        ‚îÇ ‚Ä¢ Cache Update  ‚îÇ
                                                        ‚îÇ ‚Ä¢ Notifications ‚îÇ
                                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pattern 2 : Sink Connector (Kafka ‚Üí MariaDB)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PATTERN: JDBC SINK                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Producer   ‚îÇ     ‚îÇ    Kafka     ‚îÇ     ‚îÇ    JDBC      ‚îÇ     ‚îÇ MariaDB ‚îÇ
‚îÇ   Service    ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ    Sink      ‚îÇ     ‚îÇ         ‚îÇ
‚îÇ              ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Topics     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Connector   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Tables  ‚îÇ
‚îÇ              ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Configuration du JDBC Sink Connector :**

```json
{
  "name": "mariadb-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "3",
    
    // Connexion MariaDB
    "connection.url": "jdbc:mariadb://mariadb:3306/analytics",
    "connection.user": "sink_user",
    "connection.password": "${file:/secrets/sink-password.txt}",
    
    // Topics √† consommer
    "topics": "events.orders,events.customers,events.products",
    
    // Mapping topics ‚Üí tables
    "table.name.format": "${topic}",
    
    // Mode d'insertion
    "insert.mode": "upsert",
    "pk.mode": "record_key",
    "pk.fields": "id",
    
    // Auto-cr√©ation des tables
    "auto.create": "true",
    "auto.evolve": "true",
    
    // Gestion des erreurs
    "errors.tolerance": "all",
    "errors.deadletterqueue.topic.name": "dlq.sink.mariadb",
    "errors.deadletterqueue.context.headers.enable": "true",
    
    // Batch pour performance
    "batch.size": "1000",
    
    // Transformations
    "transforms": "unwrap,route",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "events\\.(.*)",
    "transforms.route.replacement": "$1_events"
  }
}
```

### Pattern 3 : Transactional Outbox

Le pattern **Transactional Outbox** garantit la coh√©rence entre les modifications de donn√©es et la publication d'√©v√©nements.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PATTERN: TRANSACTIONAL OUTBOX                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                          TRANSACTION UNIQUE
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                         ‚îÇ
‚îÇ   Application                        MariaDB                            ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ  BEGIN TRANSACTION              ‚îÇ                               ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ  INSERT INTO orders (...)       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ‚îÇ      orders         ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ  ‚îÇ  (table m√©tier)     ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ  INSERT INTO outbox_events (...)‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ‚îÇ   outbox_events     ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ  ‚îÇ  (table outbox)     ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ  COMMIT                         ‚îÇ                               ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                               ‚îÇ
‚îÇ       ‚îÇ                                 ‚îÇ                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                 ‚îÇ
        ‚îÇ                                 ‚îÇ
        ‚îÇ                                 ‚ñº
        ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ              Debezium CDC                       ‚îÇ
        ‚îÇ               ‚îÇ                                                 ‚îÇ
        ‚îÇ               ‚îÇ  Lit les √©v√©nements de outbox_events            ‚îÇ
        ‚îÇ               ‚îÇ  Publie vers Kafka avec Outbox Event Router     ‚îÇ
        ‚îÇ               ‚îÇ  Garantit exactly-once via CDC                  ‚îÇ
        ‚îÇ               ‚îÇ                                                 ‚îÇ
        ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                      ‚îÇ
        ‚îÇ                                      ‚ñº
        ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ                   Kafka                         ‚îÇ
        ‚îÇ               ‚îÇ                                                 ‚îÇ
        ‚îÇ               ‚îÇ  Topic: events.orders                           ‚îÇ
        ‚îÇ               ‚îÇ  √âv√©nement: OrderCreated                        ‚îÇ
        ‚îÇ               ‚îÇ                                                 ‚îÇ
        ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Impl√©mentation SQL :**

```sql
-- ============================================================
-- TABLE OUTBOX POUR √âV√âNEMENTS
-- ============================================================

CREATE TABLE outbox_events (
    id BINARY(16) PRIMARY KEY DEFAULT (UUID_TO_BIN(UUID())),
    
    -- Type d'agr√©gat (Order, Customer, Product, etc.)
    aggregate_type VARCHAR(100) NOT NULL,
    
    -- Identifiant de l'agr√©gat (devient la cl√© Kafka)
    aggregate_id VARCHAR(255) NOT NULL,
    
    -- Type d'√©v√©nement (OrderCreated, OrderShipped, etc.)
    event_type VARCHAR(100) NOT NULL,
    
    -- Payload de l'√©v√©nement (JSON)
    payload JSON NOT NULL,
    
    -- M√©tadonn√©es additionnelles
    metadata JSON DEFAULT NULL,
    
    -- Timestamp de cr√©ation
    created_at DATETIME(6) DEFAULT CURRENT_TIMESTAMP(6),
    
    -- Index pour le CDC et la purge
    INDEX idx_created (created_at),
    INDEX idx_aggregate (aggregate_type, aggregate_id)
    
) ENGINE=InnoDB;

-- ============================================================
-- PROC√âDURE DE CR√âATION DE COMMANDE AVEC OUTBOX
-- ============================================================

DELIMITER //

CREATE PROCEDURE create_order_with_event(
    IN p_customer_id BIGINT,
    IN p_items JSON,
    IN p_shipping_address JSON,
    OUT p_order_id BIGINT,
    OUT p_order_number VARCHAR(32)
)
BEGIN
    DECLARE v_subtotal DECIMAL(14,2) DEFAULT 0;
    DECLARE v_order_uuid VARCHAR(36);
    
    -- Calcul du sous-total depuis les items
    SELECT COALESCE(SUM(
        JSON_EXTRACT(item.value, '$.quantity') * 
        JSON_EXTRACT(item.value, '$.unit_price')
    ), 0)
    INTO v_subtotal
    FROM JSON_TABLE(p_items, '$[*]' COLUMNS (value JSON PATH '$')) AS item;
    
    -- G√©n√©ration du num√©ro de commande
    SET p_order_number = CONCAT('ORD-', DATE_FORMAT(NOW(), '%Y%m%d'), '-', 
                                 LPAD(FLOOR(RAND() * 100000), 5, '0'));
    SET v_order_uuid = UUID();
    
    -- Transaction atomique : cr√©ation commande + √©v√©nement outbox
    START TRANSACTION;
    
    -- 1. Cr√©ation de la commande
    INSERT INTO orders (
        order_uuid, order_number, customer_id, 
        status, subtotal, shipping_address, 
        ordered_at, created_at
    ) VALUES (
        v_order_uuid, p_order_number, p_customer_id,
        'pending', v_subtotal, p_shipping_address,
        NOW(6), NOW(6)
    );
    
    SET p_order_id = LAST_INSERT_ID();
    
    -- 2. Insertion des lignes de commande
    INSERT INTO order_items (order_id, product_id, quantity, unit_price, product_snapshot)
    SELECT 
        p_order_id,
        JSON_UNQUOTE(JSON_EXTRACT(item.value, '$.product_id')),
        JSON_EXTRACT(item.value, '$.quantity'),
        JSON_EXTRACT(item.value, '$.unit_price'),
        (SELECT JSON_OBJECT(
            'name', p.name,
            'sku', p.sku,
            'price', p.price
        ) FROM products p 
        WHERE p.product_id = JSON_EXTRACT(item.value, '$.product_id'))
    FROM JSON_TABLE(p_items, '$[*]' COLUMNS (value JSON PATH '$')) AS item;
    
    -- 3. Publication de l'√©v√©nement via Outbox
    INSERT INTO outbox_events (
        aggregate_type,
        aggregate_id,
        event_type,
        payload,
        metadata
    ) VALUES (
        'Order',
        v_order_uuid,
        'OrderCreated',
        JSON_OBJECT(
            'orderId', v_order_uuid,
            'orderNumber', p_order_number,
            'customerId', p_customer_id,
            'items', p_items,
            'subtotal', v_subtotal,
            'shippingAddress', p_shipping_address,
            'status', 'pending',
            'createdAt', NOW(6)
        ),
        JSON_OBJECT(
            'source', 'order-service',
            'correlationId', UUID(),
            'timestamp', UNIX_TIMESTAMP(NOW(6)) * 1000
        )
    );
    
    COMMIT;
    
END //

DELIMITER ;

-- ============================================================
-- EXEMPLE D'UTILISATION
-- ============================================================

CALL create_order_with_event(
    12345,  -- customer_id
    '[
        {"product_id": 101, "quantity": 2, "unit_price": 29.99},
        {"product_id": 205, "quantity": 1, "unit_price": 149.99}
    ]',
    '{"street": "123 Main St", "city": "Paris", "zip": "75001", "country": "FR"}',
    @order_id,
    @order_number
);

SELECT @order_id, @order_number;
```

**Configuration Debezium Outbox Router :**

```json
{
  "name": "mariadb-outbox-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "mariadb",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "debezium",
    "topic.prefix": "app",
    "database.server.id": "184054",
    
    // Capturer uniquement la table outbox
    "table.include.list": "ecommerce.outbox_events",
    
    // Transformation Outbox
    "transforms": "outbox",
    "transforms.outbox.type": "io.debezium.transforms.outbox.EventRouter",
    
    // Mapping des champs
    "transforms.outbox.table.field.event.id": "id",
    "transforms.outbox.table.field.event.key": "aggregate_id",
    "transforms.outbox.table.field.event.type": "event_type",
    "transforms.outbox.table.field.event.payload": "payload",
    "transforms.outbox.table.field.event.timestamp": "created_at",
    
    // Routing du topic bas√© sur l'agr√©gat
    "transforms.outbox.route.by.field": "aggregate_type",
    "transforms.outbox.route.topic.replacement": "events.${routedByValue}",
    
    // Headers additionnels
    "transforms.outbox.table.fields.additional.placement": "event_type:header:eventType,metadata:header:metadata",
    
    // Suppression automatique apr√®s publication
    "transforms.outbox.table.expand.json.payload": "true",
    
    // Schema history
    "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
    "schema.history.internal.kafka.topic": "schema-changes.outbox"
  }
}
```

### Pattern 4 : Event Sourcing

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PATTERN: EVENT SOURCING                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                         ‚îÇ
‚îÇ   Command                Event Store               Read Model           ‚îÇ
‚îÇ                          (Kafka)                   (MariaDB)            ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ CreateOrd‚îÇ     ‚îÇ                    ‚îÇ     ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   er     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  OrderCreated      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  orders_view         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  OrderItemAdded    ‚îÇ     ‚îÇ  (projection)        ‚îÇ   ‚îÇ
‚îÇ                   ‚îÇ  OrderItemAdded    ‚îÇ     ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  PaymentReceived   ‚îÇ     ‚îÇ  customers_view      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Ship     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  OrderShipped      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (projection)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Order  ‚îÇ     ‚îÇ  OrderDelivered    ‚îÇ     ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ                    ‚îÇ     ‚îÇ  inventory_view      ‚îÇ   ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  (projection)        ‚îÇ   ‚îÇ
‚îÇ                            ‚îÇ                 ‚îÇ                      ‚îÇ   ‚îÇ
‚îÇ                            ‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                            ‚îÇ                          ‚îÇ                 ‚îÇ
‚îÇ                            ‚îÇ    Replay                ‚ñ≤                 ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

EVENT STORE (Kafka Topics)               READ MODELS (MariaDB)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Append-only                            ‚Ä¢ Optimis√© pour les lectures
‚Ä¢ Source de v√©rit√©                       ‚Ä¢ D√©normalis√©
‚Ä¢ Tous les √©v√©nements                    ‚Ä¢ Reconstruit depuis events
‚Ä¢ Immuable                               ‚Ä¢ Mise √† jour en temps r√©el
```

**Impl√©mentation du Read Model :**

```sql
-- ============================================================
-- READ MODEL : PROJECTION DES COMMANDES
-- ============================================================

CREATE TABLE orders_projection (
    order_id VARCHAR(36) PRIMARY KEY,
    order_number VARCHAR(32) NOT NULL,
    customer_id BIGINT UNSIGNED NOT NULL,
    
    -- Donn√©es client d√©normalis√©es
    customer_name VARCHAR(255),
    customer_email VARCHAR(255),
    
    -- √âtat actuel
    status VARCHAR(50) NOT NULL,
    payment_status VARCHAR(50) DEFAULT 'pending',
    fulfillment_status VARCHAR(50) DEFAULT 'unfulfilled',
    
    -- Montants
    subtotal DECIMAL(14,2) NOT NULL DEFAULT 0,
    tax_amount DECIMAL(14,2) NOT NULL DEFAULT 0,
    shipping_amount DECIMAL(14,2) NOT NULL DEFAULT 0,
    total_amount DECIMAL(14,2) NOT NULL DEFAULT 0,
    
    -- Items (snapshot JSON pour queries simples)
    items_summary JSON,
    items_count INT UNSIGNED DEFAULT 0,
    
    -- Addresses
    shipping_address JSON,
    billing_address JSON,
    
    -- Timestamps des √©v√©nements importants
    ordered_at DATETIME(6),
    paid_at DATETIME(6),
    shipped_at DATETIME(6),
    delivered_at DATETIME(6),
    cancelled_at DATETIME(6),
    
    -- M√©tadonn√©es de projection
    last_event_id VARCHAR(36),
    last_event_type VARCHAR(100),
    projection_version BIGINT UNSIGNED DEFAULT 0,
    projected_at DATETIME(6) DEFAULT CURRENT_TIMESTAMP(6) 
                ON UPDATE CURRENT_TIMESTAMP(6),
    
    -- Index pour les requ√™tes courantes
    INDEX idx_customer (customer_id),
    INDEX idx_status (status),
    INDEX idx_ordered (ordered_at DESC),
    INDEX idx_projection (last_event_id)
    
) ENGINE=InnoDB;

-- Vue pour le dashboard
CREATE OR REPLACE VIEW v_orders_dashboard AS
SELECT 
    DATE(ordered_at) AS order_date,
    status,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_revenue,
    AVG(total_amount) AS avg_order_value
FROM orders_projection
WHERE ordered_at >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
GROUP BY DATE(ordered_at), status
ORDER BY order_date DESC, status;
```

**Consumer Python pour la projection :**

```python
# order_projection_consumer.py

import json
from datetime import datetime
from typing import Dict, Any, Optional
from kafka import KafkaConsumer
import mariadb

class OrderProjectionConsumer:
    """
    Consommateur Kafka qui maintient la projection des commandes dans MariaDB.
    """
    
    EVENT_HANDLERS = {
        'OrderCreated': '_handle_order_created',
        'OrderItemAdded': '_handle_item_added',
        'OrderItemRemoved': '_handle_item_removed',
        'PaymentReceived': '_handle_payment_received',
        'OrderShipped': '_handle_order_shipped',
        'OrderDelivered': '_handle_order_delivered',
        'OrderCancelled': '_handle_order_cancelled',
    }
    
    def __init__(self, kafka_config: Dict, db_config: Dict):
        self.consumer = KafkaConsumer(
            'events.Order',
            bootstrap_servers=kafka_config['bootstrap_servers'],
            group_id='order-projection-service',
            auto_offset_reset='earliest',
            enable_auto_commit=False,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        self.db_pool = mariadb.ConnectionPool(
            pool_name="projection",
            pool_size=5,
            **db_config
        )
    
    def run(self):
        """Boucle principale de consommation."""
        print("Starting Order Projection Consumer...")
        
        for message in self.consumer:
            try:
                self._process_event(message)
                self.consumer.commit()
            except Exception as e:
                print(f"Error processing event: {e}")
                # En production: DLQ, alerting, etc.
    
    def _process_event(self, message):
        """Traite un √©v√©nement et met √† jour la projection."""
        event_type = message.headers.get('eventType', [b'Unknown'])[0].decode()
        event_data = message.value
        event_id = message.key.decode() if message.key else None
        
        handler_name = self.EVENT_HANDLERS.get(event_type)
        if not handler_name:
            print(f"Unknown event type: {event_type}")
            return
        
        handler = getattr(self, handler_name)
        
        conn = self.db_pool.get_connection()
        try:
            handler(conn, event_data, event_id, event_type)
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()
    
    def _handle_order_created(self, conn, event: Dict, event_id: str, event_type: str):
        """Cr√©e une nouvelle entr√©e de projection."""
        cursor = conn.cursor()
        
        # R√©cup√©rer les infos client
        cursor.execute(
            "SELECT first_name, last_name, email FROM customers WHERE customer_id = %s",
            (event['customerId'],)
        )
        customer = cursor.fetchone()
        customer_name = f"{customer[0]} {customer[1]}" if customer else "Unknown"
        customer_email = customer[2] if customer else None
        
        cursor.execute("""
            INSERT INTO orders_projection (
                order_id, order_number, customer_id,
                customer_name, customer_email,
                status, subtotal, total_amount,
                items_summary, shipping_address,
                ordered_at, last_event_id, last_event_type
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
        """, (
            event['orderId'],
            event['orderNumber'],
            event['customerId'],
            customer_name,
            customer_email,
            event.get('status', 'pending'),
            event.get('subtotal', 0),
            event.get('subtotal', 0),  # total = subtotal initialement
            json.dumps(event.get('items', [])),
            json.dumps(event.get('shippingAddress')),
            event.get('createdAt'),
            event_id,
            event_type
        ))
    
    def _handle_payment_received(self, conn, event: Dict, event_id: str, event_type: str):
        """Met √† jour le statut de paiement."""
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE orders_projection
            SET payment_status = 'paid',
                paid_at = %s,
                last_event_id = %s,
                last_event_type = %s,
                projection_version = projection_version + 1
            WHERE order_id = %s
        """, (
            event.get('paidAt'),
            event_id,
            event_type,
            event['orderId']
        ))
    
    def _handle_order_shipped(self, conn, event: Dict, event_id: str, event_type: str):
        """Met √† jour le statut d'exp√©dition."""
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE orders_projection
            SET status = 'shipped',
                fulfillment_status = 'shipped',
                shipped_at = %s,
                last_event_id = %s,
                last_event_type = %s,
                projection_version = projection_version + 1
            WHERE order_id = %s
        """, (
            event.get('shippedAt'),
            event_id,
            event_type,
            event['orderId']
        ))
    
    def _handle_order_delivered(self, conn, event: Dict, event_id: str, event_type: str):
        """Marque la commande comme livr√©e."""
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE orders_projection
            SET status = 'delivered',
                fulfillment_status = 'fulfilled',
                delivered_at = %s,
                last_event_id = %s,
                last_event_type = %s,
                projection_version = projection_version + 1
            WHERE order_id = %s
        """, (
            event.get('deliveredAt'),
            event_id,
            event_type,
            event['orderId']
        ))
    
    def _handle_order_cancelled(self, conn, event: Dict, event_id: str, event_type: str):
        """Annule la commande."""
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE orders_projection
            SET status = 'cancelled',
                cancelled_at = %s,
                last_event_id = %s,
                last_event_type = %s,
                projection_version = projection_version + 1
            WHERE order_id = %s
        """, (
            event.get('cancelledAt'),
            event_id,
            event_type,
            event['orderId']
        ))
    
    # Impl√©mentation des autres handlers...
    def _handle_item_added(self, conn, event, event_id, event_type):
        pass  # √Ä impl√©menter
    
    def _handle_item_removed(self, conn, event, event_id, event_type):
        pass  # √Ä impl√©menter


if __name__ == "__main__":
    consumer = OrderProjectionConsumer(
        kafka_config={'bootstrap_servers': 'localhost:9092'},
        db_config={
            'host': 'localhost',
            'port': 3306,
            'user': 'projection_user',
            'password': 'password',
            'database': 'ecommerce'
        }
    )
    consumer.run()
```

---

## Streaming Analytics avec ksqlDB

**ksqlDB** permet d'√©crire des requ√™tes SQL sur les streams Kafka pour de l'analytics temps r√©el.

### Architecture ksqlDB

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    STREAMING ANALYTICS PIPELINE                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MariaDB  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Debezium ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Kafka   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ksqlDB  ‚îÇ
‚îÇ          ‚îÇ     ‚îÇ   CDC    ‚îÇ     ‚îÇ  Topics  ‚îÇ     ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                      ‚îÇ                                 ‚îÇ
                      ‚ñº                                 ‚ñº
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ   Stream    ‚îÇ                   ‚îÇ   Table     ‚îÇ
               ‚îÇ (√©v√©nements)‚îÇ                   ‚îÇ (√©tat)      ‚îÇ
               ‚îÇ             ‚îÇ                   ‚îÇ             ‚îÇ
               ‚îÇ orders_strm ‚îÇ                   ‚îÇ order_stats ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                 ‚îÇ   Sink      ‚îÇ
                                                 ‚îÇ Connector   ‚îÇ
                                                 ‚îÇ             ‚îÇ
                                                 ‚îÇ ‚Üí MariaDB   ‚îÇ
                                                 ‚îÇ ‚Üí Dashboard ‚îÇ
                                                 ‚îÇ ‚Üí Alerting  ‚îÇ
                                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Requ√™tes ksqlDB

```sql
-- ============================================================
-- KSQLDB : STREAMING ANALYTICS SUR √âV√âNEMENTS CDC
-- ============================================================

-- Cr√©ation d'un stream depuis le topic CDC
CREATE STREAM orders_stream (
    order_id VARCHAR KEY,
    order_number VARCHAR,
    customer_id BIGINT,
    status VARCHAR,
    subtotal DECIMAL(14,2),
    tax_amount DECIMAL(14,2),
    total_amount DECIMAL(14,2),
    ordered_at TIMESTAMP,
    __op VARCHAR,          -- Op√©ration CDC (c/u/d)
    __source_ts_ms BIGINT  -- Timestamp source
) WITH (
    KAFKA_TOPIC = 'ecommerce.ecommerce.orders',
    VALUE_FORMAT = 'AVRO',
    TIMESTAMP = '__source_ts_ms'
);

-- Stream filtr√© des nouvelles commandes uniquement
CREATE STREAM new_orders AS
SELECT *
FROM orders_stream
WHERE __op = 'c'
EMIT CHANGES;

-- ============================================================
-- AGR√âGATIONS EN TEMPS R√âEL
-- ============================================================

-- Statistiques par minute (fen√™tre tumbling)
CREATE TABLE orders_per_minute AS
SELECT
    TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss') AS window_start,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_revenue,
    AVG(total_amount) AS avg_order_value
FROM orders_stream
WHERE __op = 'c'
WINDOW TUMBLING (SIZE 1 MINUTE)
GROUP BY 1
EMIT CHANGES;

-- Statistiques par heure avec fen√™tre glissante
CREATE TABLE orders_hourly_rolling AS
SELECT
    TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss') AS window_start,
    COUNT(*) AS order_count,
    SUM(total_amount) AS revenue,
    COUNT_DISTINCT(customer_id) AS unique_customers
FROM orders_stream
WHERE __op = 'c'
WINDOW HOPPING (SIZE 1 HOUR, ADVANCE BY 5 MINUTES)
GROUP BY 1
EMIT CHANGES;

-- ============================================================
-- D√âTECTION D'ANOMALIES
-- ============================================================

-- Commandes de valeur √©lev√©e (alerte)
CREATE STREAM high_value_orders AS
SELECT
    order_id,
    order_number,
    customer_id,
    total_amount,
    ordered_at
FROM orders_stream
WHERE __op = 'c' 
  AND total_amount > 1000
EMIT CHANGES;

-- Clients avec multiples commandes rapides (fraud detection)
CREATE TABLE rapid_orders_by_customer AS
SELECT
    customer_id,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_spent
FROM orders_stream
WHERE __op = 'c'
WINDOW TUMBLING (SIZE 5 MINUTES)
GROUP BY customer_id
HAVING COUNT(*) > 3
EMIT CHANGES;

-- ============================================================
-- JOINTURES STREAM-TABLE
-- ============================================================

-- Cr√©er une table des clients depuis un topic compact√©
CREATE TABLE customers_table (
    customer_id BIGINT PRIMARY KEY,
    first_name VARCHAR,
    last_name VARCHAR,
    email VARCHAR,
    tier VARCHAR
) WITH (
    KAFKA_TOPIC = 'ecommerce.ecommerce.customers',
    VALUE_FORMAT = 'AVRO'
);

-- Enrichir les commandes avec les donn√©es client
CREATE STREAM enriched_orders AS
SELECT
    o.order_id,
    o.order_number,
    o.total_amount,
    c.first_name,
    c.last_name,
    c.email,
    c.tier AS customer_tier,
    o.ordered_at
FROM orders_stream o
LEFT JOIN customers_table c ON o.customer_id = c.customer_id
WHERE o.__op = 'c'
EMIT CHANGES;

-- ============================================================
-- EXPORT VERS MARIADB (via Kafka Connect)
-- ============================================================

-- Les r√©sultats sont publi√©s vers des topics Kafka
-- qu'un JDBC Sink Connector peut √©crire vers MariaDB

-- Topic: ORDERS_PER_MINUTE ‚Üí Table: analytics.orders_per_minute
-- Topic: HIGH_VALUE_ORDERS ‚Üí Table: alerts.high_value_orders
```

### D√©ploiement ksqlDB

```yaml
# docker-compose-ksqldb.yml

version: '3.8'

services:
  ksqldb-server:
    image: confluentinc/ksqldb-server:0.29.0
    hostname: ksqldb-server
    container_name: ksqldb-server
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8088:8088"
    environment:
      KSQL_BOOTSTRAP_SERVERS: kafka:29092
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
      
      # Persistance des requ√™tes
      KSQL_KSQL_SERVICE_ID: ksqldb-analytics
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 1
      
      # Performance
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 4
      KSQL_KSQL_STREAMS_CACHE_MAX_BYTES_BUFFERING: 104857600

  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.29.0
    container_name: ksqldb-cli
    depends_on:
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true
```

---

## Production d'√©v√©nements depuis l'application

### Producer Python asynchrone

```python
# kafka_event_producer.py

import json
import asyncio
from typing import Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from uuid import uuid4
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError

@dataclass
class DomainEvent:
    """Classe de base pour les √©v√©nements du domaine."""
    event_id: str
    event_type: str
    aggregate_type: str
    aggregate_id: str
    payload: Dict[str, Any]
    metadata: Dict[str, Any]
    timestamp: str
    
    @classmethod
    def create(
        cls,
        event_type: str,
        aggregate_type: str,
        aggregate_id: str,
        payload: Dict[str, Any],
        correlation_id: Optional[str] = None,
        causation_id: Optional[str] = None
    ) -> 'DomainEvent':
        return cls(
            event_id=str(uuid4()),
            event_type=event_type,
            aggregate_type=aggregate_type,
            aggregate_id=aggregate_id,
            payload=payload,
            metadata={
                'correlation_id': correlation_id or str(uuid4()),
                'causation_id': causation_id,
                'source': 'order-service',
                'version': '1.0'
            },
            timestamp=datetime.utcnow().isoformat() + 'Z'
        )
    
    def to_kafka_message(self) -> tuple:
        """Retourne (key, value, headers) pour Kafka."""
        key = self.aggregate_id.encode('utf-8')
        value = json.dumps({
            'eventId': self.event_id,
            'eventType': self.event_type,
            'aggregateType': self.aggregate_type,
            'aggregateId': self.aggregate_id,
            'payload': self.payload,
            'metadata': self.metadata,
            'timestamp': self.timestamp
        }).encode('utf-8')
        headers = [
            ('eventType', self.event_type.encode()),
            ('aggregateType', self.aggregate_type.encode()),
            ('correlationId', self.metadata['correlation_id'].encode())
        ]
        return key, value, headers


class EventPublisher:
    """
    Publie des √©v√©nements vers Kafka de mani√®re asynchrone.
    """
    
    def __init__(
        self,
        bootstrap_servers: str,
        topic_prefix: str = 'events'
    ):
        self.bootstrap_servers = bootstrap_servers
        self.topic_prefix = topic_prefix
        self.producer: Optional[AIOKafkaProducer] = None
    
    async def start(self):
        """Initialise la connexion Kafka."""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            # Garanties de livraison
            acks='all',  # Attendre tous les replicas
            enable_idempotence=True,  # Exactly-once semantics
            
            # Retry
            retries=5,
            retry_backoff_ms=100,
            
            # Performance
            linger_ms=5,
            batch_size=16384,
            compression_type='lz4',
            
            # S√©rialisation
            key_serializer=lambda k: k,
            value_serializer=lambda v: v
        )
        await self.producer.start()
    
    async def stop(self):
        """Ferme la connexion Kafka."""
        if self.producer:
            await self.producer.stop()
    
    def _get_topic(self, aggregate_type: str) -> str:
        """D√©termine le topic bas√© sur le type d'agr√©gat."""
        return f"{self.topic_prefix}.{aggregate_type.lower()}"
    
    async def publish(self, event: DomainEvent) -> bool:
        """
        Publie un √©v√©nement vers Kafka.
        
        Returns:
            True si publi√© avec succ√®s
        """
        if not self.producer:
            raise RuntimeError("Producer not started")
        
        topic = self._get_topic(event.aggregate_type)
        key, value, headers = event.to_kafka_message()
        
        try:
            # Envoi asynchrone avec callback
            result = await self.producer.send_and_wait(
                topic,
                value=value,
                key=key,
                headers=headers
            )
            
            print(f"Event {event.event_id} published to {topic} "
                  f"partition={result.partition} offset={result.offset}")
            return True
            
        except KafkaError as e:
            print(f"Failed to publish event {event.event_id}: {e}")
            raise
    
    async def publish_batch(self, events: list[DomainEvent]) -> int:
        """
        Publie un batch d'√©v√©nements.
        
        Returns:
            Nombre d'√©v√©nements publi√©s avec succ√®s
        """
        if not self.producer:
            raise RuntimeError("Producer not started")
        
        tasks = [self.publish(event) for event in events]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = sum(1 for r in results if r is True)
        return success_count


# ============================================================
# EXEMPLE D'UTILISATION AVEC FASTAPI
# ============================================================

from fastapi import FastAPI, HTTPException, Depends
from contextlib import asynccontextmanager
import mariadb

# Configuration
KAFKA_SERVERS = "localhost:9092"
DB_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'app_user',
    'password': 'password',
    'database': 'ecommerce'
}

# Singleton du publisher
event_publisher: Optional[EventPublisher] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global event_publisher
    event_publisher = EventPublisher(KAFKA_SERVERS)
    await event_publisher.start()
    yield
    await event_publisher.stop()

app = FastAPI(lifespan=lifespan)


@app.post("/api/orders")
async def create_order(order_data: dict):
    """
    Cr√©e une commande et publie l'√©v√©nement.
    
    Pattern: Database-First avec publication d'√©v√©nement
    """
    conn = mariadb.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    try:
        # 1. Ins√©rer dans MariaDB
        cursor.execute("""
            INSERT INTO orders (customer_id, status, subtotal, created_at)
            VALUES (%s, %s, %s, NOW())
        """, (
            order_data['customer_id'],
            'pending',
            order_data['subtotal']
        ))
        
        order_id = cursor.lastrowid
        order_uuid = str(uuid4())
        
        # Mettre √† jour avec l'UUID
        cursor.execute(
            "UPDATE orders SET order_uuid = %s WHERE order_id = %s",
            (order_uuid, order_id)
        )
        
        conn.commit()
        
        # 2. Publier l'√©v√©nement vers Kafka
        event = DomainEvent.create(
            event_type='OrderCreated',
            aggregate_type='Order',
            aggregate_id=order_uuid,
            payload={
                'orderId': order_uuid,
                'customerId': order_data['customer_id'],
                'subtotal': float(order_data['subtotal']),
                'status': 'pending'
            }
        )
        
        await event_publisher.publish(event)
        
        return {
            'order_id': order_uuid,
            'status': 'created',
            'message': 'Order created and event published'
        }
        
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cursor.close()
        conn.close()
```

### Producer transactionnel avec Outbox

```python
# transactional_outbox_producer.py

import json
from typing import Dict, Any, List
from datetime import datetime
from uuid import uuid4
import mariadb

class TransactionalEventPublisher:
    """
    Publie des √©v√©nements via le pattern Transactional Outbox.
    
    Garantit l'atomicit√© entre les modifications de donn√©es
    et la publication d'√©v√©nements.
    """
    
    def __init__(self, db_config: Dict[str, Any]):
        self.pool = mariadb.ConnectionPool(
            pool_name="outbox",
            pool_size=10,
            **db_config
        )
    
    def execute_with_events(
        self,
        operations: callable,
        events: List[Dict[str, Any]]
    ) -> Any:
        """
        Ex√©cute des op√©rations DB et ins√®re les √©v√©nements
        dans une transaction unique.
        
        Args:
            operations: Fonction recevant (cursor) et effectuant les op√©rations
            events: Liste d'√©v√©nements √† publier
            
        Returns:
            R√©sultat de la fonction operations
        """
        conn = self.pool.get_connection()
        cursor = conn.cursor()
        
        try:
            conn.begin()
            
            # 1. Ex√©cuter les op√©rations m√©tier
            result = operations(cursor)
            
            # 2. Ins√©rer les √©v√©nements dans l'outbox
            for event in events:
                cursor.execute("""
                    INSERT INTO outbox_events (
                        id, aggregate_type, aggregate_id,
                        event_type, payload, metadata, created_at
                    ) VALUES (
                        UUID_TO_BIN(UUID()), %s, %s, %s, %s, %s, NOW(6)
                    )
                """, (
                    event['aggregate_type'],
                    event['aggregate_id'],
                    event['event_type'],
                    json.dumps(event['payload']),
                    json.dumps(event.get('metadata', {}))
                ))
            
            conn.commit()
            return result
            
        except Exception as e:
            conn.rollback()
            raise
        finally:
            cursor.close()
            conn.close()


# Exemple d'utilisation
def create_order_with_outbox(
    publisher: TransactionalEventPublisher,
    customer_id: int,
    items: List[Dict],
    shipping_address: Dict
) -> str:
    """Cr√©e une commande avec publication d'√©v√©nement garantie."""
    
    order_uuid = str(uuid4())
    order_number = f"ORD-{datetime.now().strftime('%Y%m%d')}-{order_uuid[:8].upper()}"
    subtotal = sum(item['quantity'] * item['unit_price'] for item in items)
    
    def db_operations(cursor):
        # Cr√©er la commande
        cursor.execute("""
            INSERT INTO orders (
                order_uuid, order_number, customer_id,
                status, subtotal, shipping_address, ordered_at
            ) VALUES (%s, %s, %s, %s, %s, %s, NOW(6))
        """, (
            order_uuid, order_number, customer_id,
            'pending', subtotal, json.dumps(shipping_address)
        ))
        
        order_id = cursor.lastrowid
        
        # Cr√©er les lignes
        for item in items:
            cursor.execute("""
                INSERT INTO order_items (
                    order_id, product_id, quantity, unit_price
                ) VALUES (%s, %s, %s, %s)
            """, (
                order_id, item['product_id'],
                item['quantity'], item['unit_price']
            ))
        
        return order_id
    
    # √âv√©nement √† publier
    events = [{
        'aggregate_type': 'Order',
        'aggregate_id': order_uuid,
        'event_type': 'OrderCreated',
        'payload': {
            'orderId': order_uuid,
            'orderNumber': order_number,
            'customerId': customer_id,
            'items': items,
            'subtotal': subtotal,
            'shippingAddress': shipping_address,
            'status': 'pending'
        },
        'metadata': {
            'correlation_id': str(uuid4()),
            'source': 'order-service'
        }
    }]
    
    # Ex√©cution atomique
    publisher.execute_with_events(db_operations, events)
    
    return order_uuid
```

---

## Bonnes pratiques

### Configuration Kafka optimis√©e pour CDC

```properties
# server.properties - Configuration Kafka pour CDC

# R√©tention pour rejouer les √©v√©nements
log.retention.hours=168  # 7 jours
log.retention.bytes=-1   # Pas de limite de taille

# Compaction pour les topics de type "table"
log.cleanup.policy=compact,delete
log.cleaner.min.compaction.lag.ms=3600000  # 1 heure avant compaction

# R√©plication pour la durabilit√©
default.replication.factor=3
min.insync.replicas=2

# Performance
num.partitions=12
num.io.threads=8
num.network.threads=3

# Compression
compression.type=lz4

# Transactions (pour exactly-once)
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
```

### Sch√©ma de topics recommand√©

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONVENTION DE NOMMAGE DES TOPICS                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

CDC Topics (Debezium)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
{prefix}.{database}.{table}
    ‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ Nom de la table
    ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Nom de la base
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Pr√©fixe du connecteur (topic.prefix)

Exemples:
‚Ä¢ prod-ecommerce.ecommerce.orders
‚Ä¢ prod-ecommerce.ecommerce.customers
‚Ä¢ prod-ecommerce.ecommerce.products

Domain Events Topics (Application)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
events.{aggregate-type}
         ‚îî‚îÄ‚îÄ Type d'agr√©gat (pascalCase ‚Üí kebab-case)

Exemples:
‚Ä¢ events.order
‚Ä¢ events.customer
‚Ä¢ events.inventory

Internal Topics
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ connect-configs        : Configuration des connecteurs
‚Ä¢ connect-offsets        : Offsets des connecteurs
‚Ä¢ connect-status         : Status des connecteurs
‚Ä¢ schema-changes.*       : Historique des sch√©mas Debezium
‚Ä¢ __debezium-heartbeat.* : Heartbeats

Dead Letter Queues
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
dlq.{source-topic}
dlq.{connector-name}

Exemples:
‚Ä¢ dlq.events.order
‚Ä¢ dlq.mariadb-connector
```

### Monitoring de l'int√©gration

```sql
-- ============================================================
-- TABLES DE MONITORING POUR L'INT√âGRATION KAFKA
-- ============================================================

-- Suivi des √©v√©nements publi√©s
CREATE TABLE event_publication_log (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    event_id VARCHAR(36) NOT NULL,
    aggregate_type VARCHAR(100) NOT NULL,
    aggregate_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    
    -- Statut de publication
    status ENUM('pending', 'published', 'failed') DEFAULT 'pending',
    
    -- M√©tadonn√©es Kafka
    kafka_topic VARCHAR(255),
    kafka_partition INT,
    kafka_offset BIGINT,
    
    -- Timing
    created_at DATETIME(6) DEFAULT CURRENT_TIMESTAMP(6),
    published_at DATETIME(6),
    
    -- Erreurs
    retry_count INT UNSIGNED DEFAULT 0,
    last_error TEXT,
    
    INDEX idx_status (status),
    INDEX idx_aggregate (aggregate_type, aggregate_id),
    INDEX idx_created (created_at)
) ENGINE=InnoDB;

-- Vue de monitoring
CREATE OR REPLACE VIEW v_event_publication_stats AS
SELECT 
    DATE(created_at) AS date,
    aggregate_type,
    status,
    COUNT(*) AS event_count,
    AVG(TIMESTAMPDIFF(MICROSECOND, created_at, published_at)) / 1000 AS avg_latency_ms,
    MAX(retry_count) AS max_retries
FROM event_publication_log
WHERE created_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
GROUP BY DATE(created_at), aggregate_type, status;

-- Alertes sur les √©checs
CREATE OR REPLACE VIEW v_event_publication_alerts AS
SELECT 
    aggregate_type,
    event_type,
    COUNT(*) AS failed_count,
    MIN(created_at) AS oldest_failure,
    MAX(last_error) AS sample_error
FROM event_publication_log
WHERE status = 'failed'
  AND created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY aggregate_type, event_type
HAVING failed_count > 10;
```

### Checklist de d√©ploiement

| Cat√©gorie | V√©rification | Status |
|-----------|--------------|--------|
| **Kafka** | Cluster avec 3+ brokers | ‚òê |
| | Replication factor ‚â• 3 | ‚òê |
| | min.insync.replicas = 2 | ‚òê |
| | SSL/TLS activ√© | ‚òê |
| | ACLs configur√©es | ‚òê |
| **MariaDB** | Binary logs ROW format | ‚òê |
| | GTID activ√© | ‚òê |
| | Utilisateur CDC d√©di√© | ‚òê |
| | Table Outbox cr√©√©e | ‚òê |
| **Debezium** | Heartbeat configur√© | ‚òê |
| | DLQ activ√©e | ‚òê |
| | M√©triques expos√©es | ‚òê |
| | Snapshot mode appropri√© | ‚òê |
| **Application** | Idempotence des consumers | ‚òê |
| | Retry avec backoff | ‚òê |
| | Monitoring lag | ‚òê |
| | Alertes configur√©es | ‚òê |

---

## Architectures de r√©f√©rence

### E-commerce temps r√©el

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ARCHITECTURE E-COMMERCE EVENT-DRIVEN                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ    API Gateway      ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ                         ‚îÇ                         ‚îÇ
                     ‚ñº                         ‚ñº                         ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Order Service  ‚îÇ     ‚îÇ Payment Service ‚îÇ     ‚îÇInventory Service‚îÇ
            ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ  MariaDB  ‚îÇ  ‚îÇ     ‚îÇ  ‚îÇ  MariaDB  ‚îÇ  ‚îÇ     ‚îÇ  ‚îÇ  MariaDB  ‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ  (orders) ‚îÇ  ‚îÇ     ‚îÇ  ‚îÇ(payments) ‚îÇ  ‚îÇ     ‚îÇ  ‚îÇ(inventory)‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ                       ‚îÇ                       ‚îÇ
                     ‚îÇ CDC                   ‚îÇ CDC                   ‚îÇ CDC
                     ‚ñº                       ‚ñº                       ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                         KAFKA CLUSTER                           ‚îÇ
            ‚îÇ                                                                 ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
            ‚îÇ  ‚îÇevents.order  ‚îÇ  ‚îÇevents.payment‚îÇ  ‚îÇevents.invent.‚îÇ           ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
            ‚îÇ                                                                 ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                     ‚îÇ                     ‚îÇ
                    ‚ñº                     ‚ñº                     ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   Notification  ‚îÇ   ‚îÇ    Analytics    ‚îÇ   ‚îÇ  Search/Cache   ‚îÇ
           ‚îÇ     Service     ‚îÇ   ‚îÇ    Pipeline     ‚îÇ   ‚îÇ    Sync         ‚îÇ
           ‚îÇ                 ‚îÇ   ‚îÇ                 ‚îÇ   ‚îÇ                 ‚îÇ
           ‚îÇ  Email/SMS/Push ‚îÇ   ‚îÇ ksqlDB ‚Üí DWH    ‚îÇ   ‚îÇ ES/Redis Sync   ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### CQRS avec Event Sourcing

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ARCHITECTURE CQRS + EVENT SOURCING                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

WRITE SIDE (Commands)                    READ SIDE (Queries)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Command API    ‚îÇ                      ‚îÇ   Query API     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                        ‚îÇ
         ‚ñº                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Command Handler ‚îÇ                      ‚îÇ  Query Handler  ‚îÇ
‚îÇ                 ‚îÇ                      ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Validation    ‚îÇ                      ‚îÇ ‚Ä¢ Read from     ‚îÇ
‚îÇ ‚Ä¢ Business logic‚îÇ                      ‚îÇ   projections   ‚îÇ
‚îÇ ‚Ä¢ Emit events   ‚îÇ                      ‚îÇ ‚Ä¢ Caching       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                        ‚îÇ
         ‚ñº                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    MariaDB      ‚îÇ                      ‚îÇ    MariaDB      ‚îÇ
‚îÇ  (Outbox +      ‚îÇ                      ‚îÇ  (Projections)  ‚îÇ
‚îÇ   Aggregates)   ‚îÇ                      ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                        ‚îÇ
         ‚îÇ CDC                            Event   ‚îÇ
         ‚ñº                               Handlers ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      KAFKA                              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  events.order    events.customer    events.inventory    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ              SOURCE OF TRUTH                            ‚îÇ
‚îÇ         (Immutable Event Log)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ Points cl√©s √† retenir

- **Kafka** est la plateforme de streaming standard pour les architectures event-driven
- Le **CDC avec Debezium** est le pattern recommand√© pour MariaDB ‚Üí Kafka
- Le **JDBC Sink Connector** permet de synchroniser Kafka ‚Üí MariaDB
- Le pattern **Transactional Outbox** garantit l'atomicit√© donn√©es + √©v√©nements
- **ksqlDB** permet l'analytics temps r√©el directement sur les streams Kafka
- Les **garanties de livraison** (at-least-once, exactly-once) doivent √™tre choisies selon le cas d'usage
- L'**idempotence** des consumers est essentielle pour la r√©silience
- Le **monitoring du lag** est critique pour d√©tecter les probl√®mes de performance
- Les **topics compact√©s** sont utiles pour les donn√©es de type "table" (dernier √©tat)
- La **convention de nommage** des topics facilite l'organisation et la gouvernance

---

## üîó Ressources et r√©f√©rences

- üìñ [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- üìñ [Kafka Connect](https://kafka.apache.org/documentation/#connect)
- üìñ [Confluent JDBC Connector](https://docs.confluent.io/kafka-connectors/jdbc/current/)
- üìñ [ksqlDB Documentation](https://docs.ksqldb.io/)
- üìñ [Debezium Outbox Event Router](https://debezium.io/documentation/reference/stable/transformations/outbox-event-router.html)
- üìñ [Designing Event-Driven Systems - Ben Stopford](https://www.confluent.io/designing-event-driven-systems/)
- üìñ [CQRS Pattern - Microsoft](https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs)

---

## ‚û°Ô∏è Section suivante

**20.8.3 Debezium Connector** : Approfondissez la configuration et l'op√©ration du connecteur Debezium pour MariaDB, avec les modes de snapshot, les transformations SMT, la gestion des sch√©mas et le monitoring en production.

‚è≠Ô∏è [Debezium connector](/20-cas-usage-architectures/08.3-debezium.md)
