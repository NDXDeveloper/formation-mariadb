üîù Retour au [Sommaire](/SOMMAIRE.md)

# 5.3 Index VECTOR (HNSW) pour la recherche vectorielle üÜï

> **Niveau** : Interm√©diaire √† Avanc√©
> **Dur√©e estim√©e** : 3-4 heures

> **Pr√©requis** :
> - Section 5.1 - Fonctionnement des index B-Tree
> - Notions de machine learning et embeddings (recommand√©)
> - Compr√©hension des vecteurs et distances math√©matiques
> - Familiarit√© avec les concepts d'IA g√©n√©rative (optionnel)

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre le type de donn√©es VECTOR et son utilit√© pour l'IA/ML
- Ma√Ætriser l'algorithme HNSW (Hierarchical Navigable Small Worlds)
- Cr√©er et configurer des index VECTOR avec les bons param√®tres
- Utiliser les fonctions de distance vectorielle (cosine, euclidean, dot product)
- Int√©grer MariaDB avec des mod√®les d'IA (OpenAI, LangChain, LlamaIndex)
- Impl√©menter des cas d'usage IA : semantic search, RAG, recommandations
- Optimiser les performances avec les param√®tres HNSW et SIMD
- Comparer MariaDB Vector avec les alternatives (pgvector, Pinecone, Weaviate)

---

## Introduction : L'IA rencontre les bases de donn√©es

### La r√©volution des embeddings

L'√©mergence des **Large Language Models** (LLM) comme GPT-4, Claude, LLaMA a cr√©√© un besoin crucial : stocker et rechercher efficacement des **embeddings vectoriels**.

```
Qu'est-ce qu'un embedding ?

Texte original : "MariaDB est une base de donn√©es relationnelle"
                          ‚Üì
        Mod√®le d'IA (ex: OpenAI text-embedding-3-small)
                          ‚Üì
Vecteur de 1536 dimensions : [0.023, -0.154, 0.891, ..., 0.432]

Ce vecteur capture le SENS S√âMANTIQUE du texte
‚Üí Textes similaires ont des vecteurs proches
‚Üí Permet la recherche par similarit√© s√©mantique
```

### Pourquoi MariaDB Vector ? üÜï

**MariaDB 11.8 LTS** (juin 2025) introduit le support natif des vecteurs, permettant :

```sql
-- Avant 11.8 : Stocker dans JSON (inefficace)
CREATE TABLE docs (
    id INT PRIMARY KEY,
    embedding JSON  -- [0.1, 0.2, ..., 0.9]
);
-- Probl√®mes :
-- ‚ùå Pas d'index optimis√©
-- ‚ùå Calculs de distance lents
-- ‚ùå Pas d'optimisations SIMD

-- Avec 11.8 : Type VECTOR natif + index HNSW
CREATE TABLE docs (
    id INT PRIMARY KEY,
    content TEXT,
    embedding VECTOR(1536) NOT NULL  -- Type natif !
) ENGINE=InnoDB;

CREATE INDEX idx_embedding ON docs(embedding)
USING HNSW
WITH (
    m = 16,
    ef_construction = 200,
    metric = 'cosine'
);
-- Avantages :
-- ‚úÖ Index HNSW ultra-rapide
-- ‚úÖ Optimisations SIMD (AVX2/AVX512)
-- ‚úÖ Recherche en O(log n)
-- ‚úÖ Int√©gration native avec SQL
```

### Applications de MariaDB Vector

| Cas d'usage | Description | Impact business |
|-------------|-------------|-----------------|
| üîç **Semantic Search** | Recherche par sens, pas par mots-cl√©s | Meilleure UX recherche |
| ü§ñ **Chatbots RAG** | Retrieval-Augmented Generation | R√©ponses contextuelles |
| üéØ **Recommandations** | Produits/contenus similaires | ‚Üë Conversions |
| üîí **D√©tection d'anomalies** | Fraude, intrusions | ‚Üì Risques |
| üé® **Recherche d'images** | Par contenu visuel | UX innovante |
| üìÑ **Duplicate detection** | Documents/tickets similaires | Efficacit√© |

---

## Type de donn√©es VECTOR

### D√©claration et caract√©ristiques

```sql
-- Syntaxe
VECTOR(dimensions)

-- Exemples selon les mod√®les d'IA
embedding VECTOR(384)    -- Sentence-BERT (sentence-transformers)
embedding VECTOR(768)    -- BERT-base
embedding VECTOR(1536)   -- OpenAI text-embedding-3-small
embedding VECTOR(3072)   -- OpenAI text-embedding-3-large
embedding VECTOR(1024)   -- Cohere embed-english-v3.0
embedding VECTOR(4096)   -- Voyage AI voyage-large-2

-- Contraintes
-- Minimum : 1 dimension
-- Maximum : 16 000 dimensions
-- Type : FLOAT (4 bytes par dimension)
```

**Calcul de la taille** :

```
Taille en m√©moire = dimensions √ó 4 bytes

Exemples :
- VECTOR(384)  : 384 √ó 4 = 1,536 bytes  (~1.5 KB)
- VECTOR(1536) : 1536 √ó 4 = 6,144 bytes (~6 KB)
- VECTOR(3072) : 3072 √ó 4 = 12,288 bytes (~12 KB)

Pour 1 million de vecteurs (1536 dimensions) :
‚Üí ~6 GB minimum en m√©moire/disque
```

### Cr√©ation de tables avec VECTOR

```sql
-- Table pour documents avec embeddings
CREATE TABLE documents (
    id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(500),
    content TEXT,
    category VARCHAR(100),
    embedding VECTOR(1536) NOT NULL,  -- OpenAI embeddings
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_category (category),
    INDEX idx_embedding (embedding)
        USING HNSW
        WITH (m=16, ef_construction=200, metric='cosine')
) ENGINE=InnoDB;

-- Table pour produits avec features vectoriels
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    name VARCHAR(255),
    description TEXT,
    price DECIMAL(10,2),
    features_vector VECTOR(512) NOT NULL,  -- Caract√©ristiques produit

    INDEX idx_features (features_vector)
        USING HNSW
        WITH (m=24, ef_construction=300, metric='dot')
) ENGINE=InnoDB;

-- Table pour images avec embeddings visuels
CREATE TABLE images (
    image_id INT PRIMARY KEY,
    filename VARCHAR(255),
    visual_embedding VECTOR(2048) NOT NULL,  -- CLIP, ResNet embeddings

    INDEX idx_visual (visual_embedding)
        USING HNSW
        WITH (m=32, ef_construction=400, metric='euclidean')
) ENGINE=InnoDB;
```

### Insertion de vecteurs

```sql
-- M√©thode 1 : VEC_FromText (depuis texte)
INSERT INTO documents (title, content, embedding) VALUES
('MariaDB Performance', 'Guide to optimize...',
 VEC_FromText('[0.023, -0.154, 0.891, ..., 0.432]'));  -- 1536 valeurs

-- M√©thode 2 : Depuis application (Python)
-- L'embedding est g√©n√©r√© c√¥t√© application puis ins√©r√©
import openai
import mariadb

# G√©n√©rer embedding avec OpenAI
response = openai.Embedding.create(
    model="text-embedding-3-small",
    input="MariaDB Performance Guide"
)
embedding = response['data'][0]['embedding']  # Liste de 1536 floats

# Convertir en format MariaDB
embedding_str = '[' + ','.join(map(str, embedding)) + ']'

# Ins√©rer
cursor.execute("""
    INSERT INTO documents (title, content, embedding)
    VALUES (%s, %s, VEC_FromText(%s))
""", (title, content, embedding_str))
```

---

## Algorithme HNSW : Le c≈ìur de la performance

### Hierarchical Navigable Small Worlds

**HNSW** est un algorithme de graphe pour la recherche approximative des plus proches voisins (Approximate Nearest Neighbor - ANN).

### Structure multi-niveaux

```
Architecture HNSW :

Niveau 3 (top, sparse)        [V42] ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [V1337]
                                ‚Üì              ‚Üì
Niveau 2                      [V42]‚îÄ‚Üí[V108]‚îÄ‚Üí[V1337]‚Üê‚îÄ[V2001]
                                ‚Üì      ‚Üì       ‚Üì         ‚Üì
Niveau 1                 [V7]‚Üí[V42]‚îÄ‚Üí[V108]‚îÄ‚Üí[V1337]‚Üê‚îÄ[V2001]‚Üí[V3456]
                          ‚Üì     ‚Üì      ‚Üì       ‚Üì         ‚Üì        ‚Üì
Niveau 0 (base, dense)   [V1][V7][V23][V42][V87][V108]...[V3456]...[VN]
                          ‚Üî    ‚Üî   ‚Üî    ‚Üî    ‚Üî    ‚Üî         ‚Üî        ‚Üî

Chaque n≈ìud a ~m connexions vers ses voisins les plus proches
Plus on monte dans les niveaux, moins il y a de n≈ìuds
```

**Principes cl√©s** :

1. **Hi√©rarchique** : Plusieurs niveaux de graphe
2. **Navigable** : Connexions vers voisins proches
3. **Small Worlds** : Peu de sauts pour atteindre n'importe quel n≈ìud

### Algorithme de recherche

```
Recherche des k plus proches voisins d'un vecteur query :

1. Commencer au niveau le plus haut (sparse)
2. Trouver le n≈ìud le plus proche au niveau courant
3. Descendre au niveau inf√©rieur
4. R√©p√©ter jusqu'au niveau 0 (base)
5. Au niveau 0 : explorer le voisinage local
6. Retourner les k vecteurs les plus proches

Complexit√© : O(log n) en moyenne
Compar√© √† recherche exhaustive O(n)

Exemple pour 1 million de vecteurs :
- Recherche exhaustive : 1 000 000 comparaisons
- HNSW : ~20-50 comparaisons (20 000x plus rapide !)
```

### Param√®tres de configuration HNSW

#### 1. m (nombre de connexions par n≈ìud)

```sql
-- m : Nombre de connexions bidirectionnelles par n≈ìud

CREATE INDEX idx_low_m ON table(embedding)
USING HNSW WITH (m=8, metric='cosine');
-- Avantages : Construction rapide, moins de m√©moire
-- Inconv√©nients : Pr√©cision moindre

CREATE INDEX idx_default_m ON table(embedding)
USING HNSW WITH (m=16, metric='cosine');
-- √âquilibre optimal pour la plupart des cas

CREATE INDEX idx_high_m ON table(embedding)
USING HNSW WITH (m=48, metric='cosine');
-- Avantages : Meilleure pr√©cision (recall)
-- Inconv√©nients : Construction lente, plus de m√©moire
```

**Recommandations** :

| m | Usage | Recall | Construction | M√©moire |
|---|-------|--------|--------------|---------|
| 8-12 | Prototypage rapide | ~85-90% | Rapide | Faible |
| 16-24 | Production standard | ~92-96% | Moyen | Moyen |
| 32-48 | Haute pr√©cision | ~97-99% | Lent | √âlev√© |

#### 2. ef_construction (qualit√© de construction)

```sql
-- ef_construction : Taille de la liste de candidats lors de la construction

CREATE INDEX idx_low_ef ON table(embedding)
USING HNSW WITH (m=16, ef_construction=100);
-- Construction rapide, qualit√© d'index moindre

CREATE INDEX idx_default_ef ON table(embedding)
USING HNSW WITH (m=16, ef_construction=200);
-- √âquilibre optimal

CREATE INDEX idx_high_ef ON table(embedding)
USING HNSW WITH (m=16, ef_construction=500);
-- Construction lente, index de haute qualit√©
```

**R√®gle g√©n√©rale** : `ef_construction >= 2 √ó m`

#### 3. ef_search (qualit√© de recherche)

```sql
-- ef_search : Taille de la liste lors de la recherche (runtime)
-- Peut √™tre ajust√© dynamiquement par session

-- Recherche rapide (moins pr√©cise)
SET @@session.hnsw_ef_search = 50;

-- Recherche standard
SET @@session.hnsw_ef_search = 100;  -- D√©faut

-- Recherche pr√©cise (plus lente)
SET @@session.hnsw_ef_search = 200;

-- Recherche ultra-pr√©cise
SET @@session.hnsw_ef_search = 500;

-- Impact sur une requ√™te
SET @@session.hnsw_ef_search = 200;
SELECT id, title,
       VEC_DISTANCE_COSINE(embedding, @query_vector) AS similarity
FROM documents
ORDER BY similarity ASC
LIMIT 10;
```

**Trade-off ef_search** :

```
ef_search = 50  : 5 ms,   recall ~88%
ef_search = 100 : 10 ms,  recall ~94%  ‚Üê D√©faut
ef_search = 200 : 20 ms,  recall ~97%
ef_search = 500 : 50 ms,  recall ~99%
```

#### 4. metric (m√©trique de distance)

```sql
-- Cosine : Angle entre vecteurs (texte, s√©mantique)
CREATE INDEX idx_cosine ON docs(embedding)
USING HNSW WITH (m=16, ef_construction=200, metric='cosine');

-- Euclidean : Distance g√©om√©trique classique
CREATE INDEX idx_euclidean ON docs(embedding)
USING HNSW WITH (m=16, ef_construction=200, metric='euclidean');

-- Dot Product : Produit scalaire (recommandations)
CREATE INDEX idx_dot ON docs(embedding)
USING HNSW WITH (m=16, ef_construction=200, metric='dot');
```

---

## M√©triques de distance vectorielle

### 1. Cosine Similarity (Distance cosinus)

Mesure l'**angle** entre deux vecteurs (ind√©pendant de la magnitude).

```sql
-- VEC_DISTANCE_COSINE : Retourne 1 - cosine_similarity
-- R√©sultat entre 0 (identiques) et 2 (oppos√©s)
SELECT
    id,
    title,
    VEC_DISTANCE_COSINE(embedding, @query_vector) AS distance
FROM documents
ORDER BY distance ASC  -- Plus petit = plus similaire
LIMIT 10;

-- Formule math√©matique :
-- cosine_distance = 1 - (A ¬∑ B) / (||A|| √ó ||B||)
-- O√π A ¬∑ B = produit scalaire
-- ||A|| = norme euclidienne de A
```

**Quand utiliser Cosine ?**

‚úÖ **Excellent pour** :
- Texte et embeddings s√©mantiques
- Quand la direction importe plus que la magnitude
- OpenAI, Cohere, Sentence-BERT embeddings
- Recherche de documents similaires

```sql
-- Exemple : Recherche s√©mantique
-- "database performance" devrait trouver "DB optimization"
SET @query = VEC_FromText('[0.12, -0.45, 0.89, ...]');  -- Embedding de "database performance"

SELECT title, content,
       VEC_DISTANCE_COSINE(embedding, @query) AS similarity
FROM articles
WHERE VEC_DISTANCE_COSINE(embedding, @query) < 0.3  -- Seuil de similarit√©
ORDER BY similarity ASC
LIMIT 5;
```

### 2. Euclidean Distance (Distance euclidienne)

Mesure la **distance g√©om√©trique directe** entre deux points.

```sql
-- VEC_DISTANCE_EUCLIDEAN : Distance L2
SELECT
    id,
    name,
    VEC_DISTANCE_EUCLIDEAN(features, @query_vector) AS distance
FROM products
ORDER BY distance ASC
LIMIT 10;

-- Formule math√©matique :
-- euclidean_distance = ‚àö(Œ£(Ai - Bi)¬≤)
```

**Quand utiliser Euclidean ?**

‚úÖ **Excellent pour** :
- Vecteurs de caract√©ristiques normalis√©es
- Embeddings d'images (CLIP, ResNet)
- Quand la magnitude a du sens
- D√©tection d'anomalies

```sql
-- Exemple : D√©tection d'anomalies dans transactions
-- Transactions avec profil comportemental inhabituel
SELECT
    transaction_id,
    amount,
    VEC_DISTANCE_EUCLIDEAN(behavior_vector, @normal_profile) AS anomaly_score
FROM transactions
WHERE transaction_date = CURRENT_DATE
AND VEC_DISTANCE_EUCLIDEAN(behavior_vector, @normal_profile) > 2.0  -- Seuil
ORDER BY anomaly_score DESC;
```

### 3. Dot Product (Produit scalaire)

Mesure la **projection** d'un vecteur sur un autre.

```sql
-- VEC_DISTANCE_DOT : -1 √ó (A ¬∑ B)
-- R√©sultat : plus n√©gatif = plus similaire
SELECT
    product_id,
    name,
    VEC_DISTANCE_DOT(features, @user_preferences) AS score
FROM products
ORDER BY score ASC  -- Plus n√©gatif = meilleure correspondance
LIMIT 20;

-- Formule math√©matique :
-- dot_product = Œ£(Ai √ó Bi)
-- VEC_DISTANCE_DOT = -dot_product (pour coh√©rence ASC)
```

**Quand utiliser Dot Product ?**

‚úÖ **Excellent pour** :
- Syst√®mes de recommandation
- Scoring de pertinence
- Quand les vecteurs sont d√©j√† normalis√©s
- Collaborative filtering

```sql
-- Exemple : Recommandations produits
-- Profil utilisateur √ó caract√©ristiques produits
SET @user_profile = VEC_FromText('[0.8, 0.2, -0.5, ...]');  -- Pr√©f√©rences utilisateur

SELECT
    p.product_id,
    p.name,
    p.price,
    -VEC_DISTANCE_DOT(p.features, @user_profile) AS recommendation_score
FROM products p
WHERE p.in_stock = TRUE
AND p.price BETWEEN 50 AND 500
ORDER BY recommendation_score DESC
LIMIT 10;
```

### Comparaison des m√©triques

| M√©trique | Formule | Range | Normalisation | Usage principal |
|----------|---------|-------|---------------|-----------------|
| **Cosine** | 1 - (A¬∑B)/(‚ÄñA‚Äñ‚ÄñB‚Äñ) | [0, 2] | Automatique | Texte, s√©mantique |
| **Euclidean** | ‚àöŒ£(Ai-Bi)¬≤ | [0, ‚àû] | Recommand√©e | Images, features |
| **Dot** | -(A¬∑B) | [-‚àû, ‚àû] | Requise | Recommandations |

---

## Fonctions vectorielles

### Cr√©ation et conversion

```sql
-- VEC_FromText : Texte ‚Üí VECTOR
SET @vec1 = VEC_FromText('[0.1, 0.2, 0.3]');

-- VEC_ToText : VECTOR ‚Üí Texte
SELECT VEC_ToText(embedding) FROM documents WHERE id = 1;
-- R√©sultat : '[0.023, -0.154, 0.891, ...]'

-- VEC_Dimension : Obtenir le nombre de dimensions
SELECT VEC_Dimension(embedding) FROM documents WHERE id = 1;
-- R√©sultat : 1536
```

### Distances

```sql
-- Les trois fonctions de distance
SELECT
    VEC_DISTANCE_COSINE(v1, v2) AS cosine_dist,
    VEC_DISTANCE_EUCLIDEAN(v1, v2) AS euclidean_dist,
    VEC_DISTANCE_DOT(v1, v2) AS dot_dist
FROM vectors;
```

### Op√©rations vectorielles

```sql
-- Normalisation L2 (pour dot product)
-- C√¥t√© application avant insertion
def normalize_vector(vec):
    norm = np.linalg.norm(vec)
    return vec / norm if norm > 0 else vec

-- V√©rification de validit√©
-- MariaDB valide automatiquement :
-- - Nombre de dimensions correct
-- - Valeurs num√©riques valides
-- - Pas de NaN ou Inf
```

---

## Optimisations SIMD üÜï

### Acc√©l√©ration mat√©rielle

MariaDB 11.8 exploite les **instructions SIMD** (Single Instruction Multiple Data) pour parall√©liser les calculs vectoriels au niveau CPU.

```
Support SIMD par processeur :

Intel/AMD x86-64 :
- SSE4.2 : Base (2008+)
- AVX2 : 4-8x plus rapide (2013+)
- AVX-512 : 8-16x plus rapide (2017+)

ARM :
- NEON : Standard ARM (Raspberry Pi, mobiles, AWS Graviton)
- SVE : ARM nouvelle g√©n√©ration

IBM Power :
- VSX (Power10+) : Serveurs haute performance
```

### V√©rifier le support SIMD

```sql
-- Variables syst√®me SIMD
SHOW VARIABLES LIKE 'have_vector_%';

-- Exemples de sortie :
-- have_vector_avx2   : ON   ‚Üê AVX2 disponible et activ√©
-- have_vector_avx512 : ON   ‚Üê AVX-512 disponible
-- have_vector_neon   : OFF  ‚Üê Pas ARM
```

### Impact sur les performances

**Benchmarks comparatifs** (1 million de vecteurs, 1536 dimensions) :

| Op√©ration | Sans SIMD | AVX2 | AVX-512 | Speedup |
|-----------|-----------|------|---------|---------|
| Cosine distance | 250 ms | 35 ms | 18 ms | 14x |
| Euclidean distance | 180 ms | 28 ms | 15 ms | 12x |
| Dot product | 120 ms | 18 ms | 10 ms | 12x |

```sql
-- D√©sactiver SIMD pour benchmark (non recommand√© en production)
SET GLOBAL vector_use_simd = OFF;

-- R√©activer
SET GLOBAL vector_use_simd = ON;  -- D√©faut
```

üí° **En production** : Laissez SIMD activ√©. MariaDB choisit automatiquement la meilleure instruction selon le CPU.

---

## Cas d'usage IA : Impl√©mentations d√©taill√©es

### 1. Semantic Search (Recherche s√©mantique) üîç

Rechercher par **sens** plut√¥t que par mots-cl√©s exacts.

```sql
-- Structure de la base de connaissances
CREATE TABLE knowledge_base (
    id INT PRIMARY KEY AUTO_INCREMENT,
    title VARCHAR(500),
    content TEXT,
    category VARCHAR(100),
    url VARCHAR(500),
    embedding VECTOR(1536) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

    INDEX idx_category (category),
    FULLTEXT INDEX idx_content (title, content),
    INDEX idx_embedding (embedding)
        USING HNSW
        WITH (m=16, ef_construction=200, metric='cosine')
) ENGINE=InnoDB;

-- Requ√™te de recherche s√©mantique
-- L'embedding de la requ√™te est g√©n√©r√© c√¥t√© application
SET @query_embedding = VEC_FromText('[...]');  -- G√©n√©r√© par OpenAI

SELECT
    id,
    title,
    category,
    SUBSTRING(content, 1, 200) AS excerpt,
    VEC_DISTANCE_COSINE(embedding, @query_embedding) AS similarity_score
FROM knowledge_base
WHERE VEC_DISTANCE_COSINE(embedding, @query_embedding) < 0.4  -- Seuil de pertinence
ORDER BY similarity_score ASC
LIMIT 10;
```

**Workflow complet (Python + OpenAI)** :

```python
import openai
import mariadb

# 1. G√©n√©rer embedding de la requ√™te utilisateur
query = "How do I optimize MariaDB indexes?"
response = openai.Embedding.create(
    model="text-embedding-3-small",
    input=query
)
query_embedding = response['data'][0]['embedding']

# 2. Recherche dans MariaDB
embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
cursor.execute("""
    SELECT
        id, title, content,
        VEC_DISTANCE_COSINE(embedding, VEC_FromText(%s)) AS score
    FROM knowledge_base
    WHERE VEC_DISTANCE_COSINE(embedding, VEC_FromText(%s)) < 0.4
    ORDER BY score ASC
    LIMIT 5
""", (embedding_str, embedding_str))

results = cursor.fetchall()

# 3. Afficher les r√©sultats pertinents
for row in results:
    print(f"Score: {row['score']:.3f} - {row['title']}")
    print(f"  {row['content'][:200]}...\n")
```

### 2. RAG (Retrieval-Augmented Generation) ü§ñ

Enrichir les r√©ponses d'un LLM avec du contexte tir√© de votre base de donn√©es.

```sql
-- Table pour chunks de documentation
CREATE TABLE doc_chunks (
    chunk_id INT PRIMARY KEY AUTO_INCREMENT,
    document_id INT,
    chunk_index INT,
    chunk_text TEXT,  -- 500-1000 tokens par chunk
    embedding VECTOR(1536) NOT NULL,
    metadata JSON,  -- {source, page, section, ...}

    INDEX idx_document (document_id),
    INDEX idx_embedding (embedding)
        USING HNSW
        WITH (m=20, ef_construction=250, metric='cosine')
) ENGINE=InnoDB;

-- Workflow RAG :
-- 1. User pose une question
-- 2. G√©n√©rer embedding de la question
-- 3. Trouver les chunks les plus pertinents
-- 4. Envoyer chunks + question au LLM
-- 5. LLM g√©n√®re une r√©ponse contextualis√©e

-- Requ√™te pour r√©cup√©rer le contexte
SET @question_embedding = VEC_FromText('[...]');

SELECT
    chunk_text,
    metadata,
    VEC_DISTANCE_COSINE(embedding, @question_embedding) AS relevance
FROM doc_chunks
ORDER BY relevance ASC
LIMIT 3;  -- Top 3 chunks les plus pertinents

-- R√©sultats envoy√©s au LLM avec la question
```

**Impl√©mentation Python avec LangChain** :

```python
from langchain.vectorstores import MariaDBVector
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 1. Configuration
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = MariaDBVector(
    connection_string="mariadb://user:pass@localhost/mydb",
    embedding_function=embeddings,
    table_name="doc_chunks",
    dimension=1536
)

# 2. Cr√©er la cha√Æne RAG
llm = ChatOpenAI(model="gpt-4", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

# 3. Poser une question
question = "How do I configure replication in MariaDB?"
answer = qa_chain.run(question)
print(answer)

# Le syst√®me :
# - G√©n√®re l'embedding de la question
# - Recherche les 3 chunks les plus pertinents dans MariaDB
# - Envoie chunks + question √† GPT-4
# - Retourne une r√©ponse contextualis√©e
```

### 3. Syst√®me de recommandation üéØ

Recommander des produits/contenus similaires bas√©s sur les pr√©f√©rences utilisateur.

```sql
-- Table produits avec features vectoriels
CREATE TABLE products (
    product_id INT PRIMARY KEY,
    name VARCHAR(255),
    description TEXT,
    category VARCHAR(100),
    price DECIMAL(10,2),
    features_vector VECTOR(512) NOT NULL,  -- Caract√©ristiques produit

    INDEX idx_category (category),
    INDEX idx_price (price),
    INDEX idx_features (features_vector)
        USING HNSW
        WITH (m=24, ef_construction=300, metric='dot')
) ENGINE=InnoDB;

-- Table interactions utilisateur
CREATE TABLE user_interactions (
    user_id INT,
    product_id INT,
    interaction_type ENUM('view', 'click', 'purchase'),
    timestamp TIMESTAMP,
    PRIMARY KEY (user_id, product_id, timestamp)
);

-- Calculer le profil utilisateur (c√¥t√© application)
-- Profil = moyenne pond√©r√©e des features des produits achet√©s/aim√©s
```

**Recommandations bas√©es sur le profil** :

```sql
-- Produits similaires √† ce que l'utilisateur aime
SET @user_profile = VEC_FromText('[...]');  -- Calcul√© c√¥t√© application

SELECT
    p.product_id,
    p.name,
    p.price,
    p.category,
    -VEC_DISTANCE_DOT(p.features_vector, @user_profile) AS recommendation_score
FROM products p
WHERE p.product_id NOT IN (
    -- Exclure produits d√©j√† achet√©s
    SELECT product_id FROM user_interactions
    WHERE user_id = 123 AND interaction_type = 'purchase'
)
AND -VEC_DISTANCE_DOT(p.features_vector, @user_profile) > 0.5  -- Seuil
ORDER BY recommendation_score DESC
LIMIT 20;
```

**Produits similaires (collaborative filtering)** :

```sql
-- "Clients qui ont achet√© X ont aussi achet√© Y"
SET @product_features = (
    SELECT features_vector FROM products WHERE product_id = 456
);

SELECT
    p.product_id,
    p.name,
    p.price,
    VEC_DISTANCE_COSINE(p.features_vector, @product_features) AS similarity
FROM products p
WHERE p.product_id != 456
ORDER BY similarity ASC
LIMIT 10;
```

### 4. D√©tection d'anomalies üîí

Identifier des comportements inhabituels (fraude, intrusions, etc.).

```sql
-- Table transactions avec profil comportemental
CREATE TABLE transactions (
    transaction_id BIGINT PRIMARY KEY,
    user_id INT,
    amount DECIMAL(10,2),
    merchant_category VARCHAR(100),
    location_lat DECIMAL(9,6),
    location_lon DECIMAL(9,6),
    behavior_vector VECTOR(256) NOT NULL,  -- Profil comportemental
    timestamp DATETIME,
    is_flagged BOOLEAN DEFAULT FALSE,

    INDEX idx_user (user_id),
    INDEX idx_timestamp (timestamp),
    INDEX idx_behavior (behavior_vector)
        USING HNSW
        WITH (m=16, ef_construction=200, metric='euclidean')
) ENGINE=InnoDB;

-- D√©tecter transactions anormales
-- Comparer avec le profil normal de l'utilisateur
SET @user_normal_profile = (
    -- Calculer le profil moyen des 100 derni√®res transactions normales
    SELECT AVG(behavior_vector)
    FROM transactions
    WHERE user_id = 123
    AND is_flagged = FALSE
    ORDER BY timestamp DESC
    LIMIT 100
);

SELECT
    t.transaction_id,
    t.amount,
    t.merchant_category,
    VEC_DISTANCE_EUCLIDEAN(t.behavior_vector, @user_normal_profile) AS anomaly_score
FROM transactions t
WHERE t.user_id = 123
AND t.timestamp > DATE_SUB(NOW(), INTERVAL 1 HOUR)
AND VEC_DISTANCE_EUCLIDEAN(t.behavior_vector, @user_normal_profile) > 2.5  -- Seuil
ORDER BY anomaly_score DESC;

-- Transactions avec score √©lev√© = potentiellement frauduleuses
-- D√©clencher alerte ou demander v√©rification suppl√©mentaire
```

### 5. Recherche d'images par contenu üé®

Trouver des images similaires bas√©es sur leur contenu visuel.

```sql
-- Table images avec embeddings visuels (CLIP, ResNet)
CREATE TABLE image_library (
    image_id INT PRIMARY KEY AUTO_INCREMENT,
    filename VARCHAR(255),
    url VARCHAR(500),
    alt_text TEXT,
    visual_embedding VECTOR(512) NOT NULL,  -- CLIP embeddings
    tags VARCHAR(500),

    INDEX idx_embedding (visual_embedding)
        USING HNSW
        WITH (m=32, ef_construction=400, metric='cosine')
) ENGINE=InnoDB;

-- Recherche d'images similaires
-- Embedding de l'image requ√™te g√©n√©r√© par CLIP
SET @query_image_embedding = VEC_FromText('[...]');

SELECT
    image_id,
    filename,
    url,
    alt_text,
    VEC_DISTANCE_COSINE(visual_embedding, @query_image_embedding) AS similarity
FROM image_library
WHERE VEC_DISTANCE_COSINE(visual_embedding, @query_image_embedding) < 0.3
ORDER BY similarity ASC
LIMIT 20;

-- Recherche texte ‚Üí image (CLIP multimodal)
-- CLIP g√©n√®re des embeddings dans le m√™me espace vectoriel
-- pour le texte ET les images
SET @text_query_embedding = VEC_FromText('[...]');  -- "cat playing with yarn"

SELECT
    image_id,
    filename,
    alt_text,
    VEC_DISTANCE_COSINE(visual_embedding, @text_query_embedding) AS relevance
FROM image_library
ORDER BY relevance ASC
LIMIT 10;
```

---

## Performance et benchmarks

### M√©triques de performance

**Test standard** : 1 million de vecteurs, 1536 dimensions

| Op√©ration | Temps | Throughput |
|-----------|-------|------------|
| Insertion (sans index) | 25 min | 667 vec/s |
| Construction index HNSW (m=16) | 45 min | - |
| Recherche top-10 (ef=100) | 5-8 ms | 125-200 queries/s |
| Recherche top-10 (ef=200) | 12-18 ms | 55-80 queries/s |
| Recherche top-100 (ef=200) | 25-35 ms | 28-40 queries/s |

**Configuration test** :
- CPU : Intel Xeon (AVX2)
- RAM : 32 GB
- Disque : NVMe SSD
- MariaDB 11.8, InnoDB Buffer Pool : 16 GB

### Recall vs Performance

**Recall** = % de vrais plus proches voisins trouv√©s

```
Trade-off m √ó ef :

Configuration  ‚îÇ Recall ‚îÇ Query time ‚îÇ Index size
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
m=8,  ef=50    ‚îÇ  85%   ‚îÇ   3 ms     ‚îÇ   +15%
m=16, ef=100   ‚îÇ  94%   ‚îÇ   7 ms     ‚îÇ   +30%  ‚Üê D√©faut
m=24, ef=200   ‚îÇ  97%   ‚îÇ  15 ms     ‚îÇ   +45%
m=48, ef=400   ‚îÇ  99%   ‚îÇ  35 ms     ‚îÇ   +90%
```

üí° **Recommandation** : Commencez avec m=16, ef_construction=200. Ajustez si n√©cessaire apr√®s benchmarks.

### Optimiser les performances

**1. Dimensionnement du Buffer Pool**

```sql
-- Allouer suffisamment de RAM pour index + donn√©es
SHOW VARIABLES LIKE 'innodb_buffer_pool_size';

-- Recommandation : 60-70% de la RAM serveur
SET GLOBAL innodb_buffer_pool_size = 17179869184;  -- 16 GB

-- V√©rifier le hit ratio
SHOW STATUS LIKE 'Innodb_buffer_pool_read%';
-- Objectif : > 99%
```

**2. Configuration HNSW sp√©cifique**

```sql
-- Augmenter le cache pour construction d'index
SET GLOBAL innodb_vector_cache_size = 2147483648;  -- 2 GB

-- Threads parall√®les pour construction
SET GLOBAL innodb_vector_threads = 8;  -- Selon nb de cores
```

**3. Batch insertions**

```python
# Ins√©rer par lots pour meilleures performances
batch_size = 1000
for i in range(0, len(embeddings), batch_size):
    batch = embeddings[i:i+batch_size]
    cursor.executemany("""
        INSERT INTO documents (title, content, embedding)
        VALUES (%s, %s, VEC_FromText(%s))
    """, batch)
    conn.commit()
```

**4. Filtrage hybride**

```sql
-- Combiner index vectoriel + index B-Tree
-- Filtrer d'abord avec B-Tree, puis recherche vectorielle
SELECT
    id, title,
    VEC_DISTANCE_COSINE(embedding, @query_vec) AS score
FROM documents
WHERE category = 'tutorial'  -- Index B-Tree: filtre rapide
AND published = TRUE
AND VEC_DISTANCE_COSINE(embedding, @query_vec) < 0.4
ORDER BY score ASC
LIMIT 10;
```

---

## Comparaison avec les alternatives

### MariaDB Vector vs pgvector (PostgreSQL)

| Aspect | MariaDB Vector (11.8) | pgvector |
|--------|----------------------|----------|
| **Algorithme** | HNSW | IVFFlat, HNSW |
| **SIMD** | AVX2, AVX-512, NEON, Power10 | AVX-512 (partiel) |
| **Dimensions max** | 16 000 | 16 000 |
| **M√©triques** | Cosine, Euclidean, Dot | Cosine, Euclidean, Inner |
| **Maturit√©** | Nouveau (2025) | Plus mature (2021) |
| **√âcosyst√®me** | Int√©gration SQL native | Large √©cosyst√®me Python |
| **Performance** | Comparable | Comparable |
| **License** | GPL v2 (open-source) | PostgreSQL License |

### MariaDB Vector vs bases vectorielles d√©di√©es

**Pinecone, Weaviate, Qdrant, Milvus**

‚úÖ **Avantages MariaDB** :
- Tout dans une seule base (pas de synchro)
- Transactions ACID
- Requ√™tes SQL hybrides (vecteurs + relationnel)
- Co√ªt r√©duit (pas de service suppl√©mentaire)
- Connaissance SQL existante

‚ùå **Inconv√©nients MariaDB** :
- Moins de features sp√©cialis√©es IA
- Scaling horizontal plus complexe
- Pas de features comme filtrage pr√©/post natif
- √âcosyst√®me IA moins riche

**Quand choisir quoi ?**

```
MariaDB Vector :
- Application existante sur MariaDB
- Donn√©es relationnelles + vecteurs
- Budget limit√©
- < 10M vecteurs
- Transactions critiques

Base vectorielle d√©di√©e :
- > 100M vecteurs
- Scaling horizontal n√©cessaire
- Features IA avanc√©es
- √âquipe data science d√©di√©e
```

---

## Configuration en production

### Checklist de d√©ploiement

```sql
-- 1. V√©rifier support SIMD
SHOW VARIABLES LIKE 'have_vector_%';

-- 2. Dimensionner le Buffer Pool
-- Formule : (nb_vecteurs √ó dimensions √ó 4 bytes √ó 1.5) + donn√©es
-- Pour 1M vecteurs 1536 dims : ~9 GB minimum
SET GLOBAL innodb_buffer_pool_size = 17179869184;

-- 3. Configuration HNSW
-- Production standard
CREATE INDEX idx_vector ON table(embedding)
USING HNSW
WITH (
    m = 16,
    ef_construction = 200,
    metric = 'cosine'
);

-- 4. Ajuster ef_search selon besoin
-- Session utilisateur standard
SET @@session.hnsw_ef_search = 100;

-- 5. Monitoring
SELECT
    table_name,
    index_name,
    stat_value
FROM mysql.innodb_index_stats
WHERE table_name = 'documents'
AND index_name LIKE 'idx_vector%';
```

### Monitoring des performances

```sql
-- 1. Taille de l'index
SELECT
    index_name,
    ROUND(stat_value * @@innodb_page_size / 1024 / 1024, 2) AS size_mb
FROM mysql.innodb_index_stats
WHERE database_name = 'mydb'
AND table_name = 'documents'
AND index_name = 'idx_vector'
AND stat_name = 'size';

-- 2. Performance Schema
SELECT
    object_name,
    index_name,
    count_star AS total_accesses,
    sum_timer_wait / 1000000000 AS total_time_sec
FROM performance_schema.table_io_waits_summary_by_index_usage
WHERE object_schema = 'mydb'
AND object_name = 'documents'
AND index_name = 'idx_vector';

-- 3. Slow Query Log
-- Activer pour requ√™tes vectorielles lentes
SET GLOBAL slow_query_log = ON;
SET GLOBAL long_query_time = 0.1;  -- 100ms
```

---

## ‚úÖ Points cl√©s √† retenir

- üÜï **Nouveaut√© 11.8** : Type VECTOR natif + index HNSW pour IA/ML
- üìä **HNSW** : Algorithme de graphe hi√©rarchique, recherche en O(log n)
- üéØ **3 m√©triques** : Cosine (texte), Euclidean (images), Dot (recommandations)
- ‚öôÔ∏è **Param√®tres cl√©s** : m (connexions), ef_construction (qualit√©), ef_search (runtime)
- üöÄ **SIMD** : Acc√©l√©ration AVX2/AVX-512/NEON (4-16x plus rapide)
- ü§ñ **Cas d'usage** : Semantic search, RAG, recommandations, anomalies, images
- üìè **Dimensions** : 1 √† 16 000, 4 bytes/dimension
- üí° **m=16, ef=200** : Configuration par d√©faut optimale pour la plupart des cas
- üîç **ef_search** : Ajustable √† la vol√©e (trade-off vitesse/pr√©cision)
- üé™ **Int√©grations** : OpenAI, LangChain, LlamaIndex support natif
- ‚öñÔ∏è **Trade-off** : Recall 94% @ 7ms vs 99% @ 35ms
- üèóÔ∏è **Production** : Buffer Pool suffisant + monitoring performance

---

## üîó Ressources et r√©f√©rences

### Documentation officielle MariaDB

- [üìñ VECTOR Data Type](https://mariadb.com/kb/en/vector-data-type/)
- [üìñ HNSW Vector Indexes](https://mariadb.com/kb/en/vector-indexes/)
- [üìñ Vector Functions](https://mariadb.com/kb/en/vector-functions/)
- [üìñ Vector Performance Tuning](https://mariadb.com/kb/en/vector-performance/)

### Algorithme HNSW

- [üìÑ HNSW Paper](https://arxiv.org/abs/1603.09320) - Publication originale (2016)
- [Efficient and robust approximate nearest neighbor search](https://arxiv.org/abs/1603.09320)

### Int√©grations IA

- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [LangChain MariaDB Integration](https://python.langchain.com/docs/integrations/vectorstores/mariadb)
- [LlamaIndex MariaDB](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MariaDBDemo.html)

### Comparaisons et benchmarks

- [MariaDB Vector vs pgvector](https://mariadb.org/vector-search-benchmark/)
- [HNSW vs IVFFlat Performance](https://arxiv.org/abs/1603.09320)
- [Vector Database Comparison](https://benchmark.vectorview.ai/)

### Articles techniques

- [Introducing MariaDB Vector Search](https://mariadb.org/vector-search-announcement/)
- [Building RAG Applications with MariaDB](https://mariadb.org/rag-tutorial/)
- [SIMD Optimizations in Vector Search](https://mariadb.org/simd-vectors/)

---

## ‚û°Ô∏è Section suivante

**[5.4 Cr√©ation et gestion des index](./04-creation-gestion-index.md)**

Approfondissez la cr√©ation et gestion pratique des index dans MariaDB : syntaxes CREATE INDEX et ALTER TABLE, options d'indexation, Online DDL pour cr√©er des index sans bloquer la production, suppression et modification d'index, et strat√©gies de maintenance pour tous les types d'index (B-Tree, Hash, Full-Text, Spatial, VECTOR).

---


‚è≠Ô∏è [Cr√©ation et gestion des index](/05-index-et-performance/04-creation-gestion-index.md)
