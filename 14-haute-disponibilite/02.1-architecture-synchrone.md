ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.2.1 Architecture synchrone multi-master

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : MaÃ®trise de la rÃ©plication asynchrone (section 13), concepts ACID, rÃ©seaux distribuÃ©s

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- Comprendre les fondamentaux de la rÃ©plication synchrone et ses implications architecturales
- MaÃ®triser les principes du multi-master avec Galera Cluster
- Concevoir et dimensionner une architecture Galera pour la production
- Identifier les compromis CAP et choisir les configurations appropriÃ©es
- Anticiper les comportements en cas de partition rÃ©seau ou dÃ©faillance de nÅ“uds

---

## Introduction

L'architecture synchrone multi-master de MariaDB Galera Cluster reprÃ©sente un changement de paradigme par rapport Ã  la rÃ©plication asynchrone traditionnelle Master-Slave. PlutÃ´t que de propager les modifications aprÃ¨s validation sur un nÅ“ud primaire, Galera implÃ©mente une **rÃ©plication synchrone virtuelle** oÃ¹ chaque transaction est certifiÃ©e par l'ensemble du cluster avant d'Ãªtre validÃ©e localement.

Cette approche rÃ©sout plusieurs dÃ©fis critiques des architectures haute disponibilitÃ© :
- **ZÃ©ro perte de donnÃ©es** lors d'un basculement (RPO = 0)
- **CohÃ©rence forte** entre tous les nÅ“uds
- **Lecture et Ã©criture** possibles sur tous les nÅ“uds
- **Failover automatique** sans intervention manuelle

Cependant, elle introduit Ã©galement des contraintes et des compromis qui doivent Ãªtre parfaitement compris pour concevoir des systÃ¨mes fiables et performants.

---

## Fondamentaux de la rÃ©plication synchrone

### RÃ©plication synchrone vs asynchrone : Les diffÃ©rences critiques

**RÃ©plication asynchrone (Master-Slave traditionnel) :**
```
Client â†’ Master (COMMIT local) â†’ Binlog â†’ Replica (applique)
         â†“
      RÃ©ponse immÃ©diate au client
```

**RÃ©plication synchrone (Galera) :**
```
Client â†’ NÅ“ud â†’ Certification cluster â†’ Tous les nÅ“uds reÃ§oivent
                â†“ (succÃ¨s)
             COMMIT local
                â†“
             RÃ©ponse au client
```

### Le concept de "virtually synchronous replication"

Galera implÃ©mente une **rÃ©plication synchrone virtuelle**, ce qui signifie :

1. **Phase de certification** : Synchrone entre tous les nÅ“uds
2. **Phase d'application** : Asynchrone aprÃ¨s certification

```
Transaction T1 sur NÅ“ud A
â”œâ”€ DÃ©but transaction (local)
â”œâ”€ ExÃ©cution requÃªtes (local)
â”œâ”€ COMMIT dÃ©clenchÃ©
â”‚  â”œâ”€ GÃ©nÃ©ration write-set
â”‚  â”œâ”€ Broadcast Ã  tous les nÅ“uds â†’ [SYNCHRONE]
â”‚  â”œâ”€ Certification sur chaque nÅ“ud â†’ [SYNCHRONE]
â”‚  â”‚  â”œâ”€ VÃ©rification conflits
â”‚  â”‚  â””â”€ Vote (succÃ¨s/Ã©chec)
â”‚  â”œâ”€ RÃ©ception rÃ©ponses â†’ [SYNCHRONE]
â”‚  â””â”€ Si majoritÃ© OK :
â”‚     â”œâ”€ COMMIT local â†’ [SYNCHRONE]
â”‚     â””â”€ Application sur autres nÅ“uds â†’ [ASYNCHRONE]
â””â”€ RÃ©ponse au client
```

ğŸ’¡ **Point clÃ©** : Le client reÃ§oit la confirmation seulement aprÃ¨s certification par le cluster, garantissant que la transaction sera visible partout (cohÃ©rence forte).

---

## Architecture multi-master : Principes et implications

### Topologie multi-master active-active

Contrairement aux architectures Master-Slave oÃ¹ un seul nÅ“ud accepte les Ã©critures, Galera permet des Ã©critures simultanÃ©es sur tous les nÅ“uds :

```
        Application Layer
         /      |      \
        /       |       \
    NÅ“ud A   NÅ“ud B   NÅ“ud C
       \       |       /
        \      |      /
      Galera Replication
    (Group Communication)
```

**CaractÃ©ristiques :**
- Tous les nÅ“uds sont Ã©gaux (pas de Master/Slave)
- Chaque nÅ“ud peut servir lectures ET Ã©critures
- Load balancing naturel sur tous les nÅ“uds
- Pas de Single Point of Failure (SPOF)

### Les dÃ©fis du multi-master

#### 1. Conflits de certification

Deux transactions concurrentes modifiant les mÃªmes lignes sur des nÅ“uds diffÃ©rents :

```sql
-- Sur NÅ“ud A (T1)
START TRANSACTION;
UPDATE accounts SET balance = balance - 100 WHERE id = 42;
COMMIT; -- Envoi pour certification

-- SimultanÃ©ment sur NÅ“ud B (T2)
START TRANSACTION;
UPDATE accounts SET balance = balance + 50 WHERE id = 42;
COMMIT; -- Envoi pour certification
```

**RÃ©solution :**
- Galera dÃ©tecte le conflit via les write-sets
- La transaction arrivÃ©e en premier en certification est validÃ©e
- La seconde est **rejetÃ©e** avec `DEADLOCK` error
- L'application doit implÃ©menter un **retry logic**

âš ï¸ **Attention** : Le taux de conflits augmente avec :
- Le nombre de nÅ“uds Ã©crivant simultanÃ©ment
- La concentration des Ã©critures sur les mÃªmes lignes (hot spots)
- La latence rÃ©seau inter-nÅ“uds

#### 2. Flow control

Pour Ã©viter qu'un nÅ“ud lent ne ralentisse tout le cluster, Galera implÃ©mente un mÃ©canisme de **flow control** :

```ini
# my.cnf - Configuration flow control
[galera]
# Limite de transactions en attente d'application
wsrep_slave_threads = 4

# Seuils de flow control
wsrep_provider_options = "gcs.fc_limit=100;gcs.fc_factor=0.8;gcs.fc_master_slave=YES"
```

**Fonctionnement :**
- Si un nÅ“ud accumule trop de transactions non appliquÃ©es (queue)
- Il envoie un signal de **flow control** au cluster
- Les autres nÅ“uds ralentissent temporairement leurs certifications
- Ã‰vite le dÃ©crochage total du nÅ“ud lent

ğŸ’¡ **Conseil production** : Surveiller `wsrep_local_recv_queue` et `wsrep_flow_control_paused` pour dÃ©tecter les nÅ“uds lents.

---

## Protocole de certification : Le cÅ“ur de Galera

### Write-sets et identification des conflits

Chaque transaction gÃ©nÃ¨re un **write-set** contenant :

```
Write-Set Structure:
â”œâ”€ GTID (Global Transaction ID)
â”œâ”€ ClÃ©s primaires modifiÃ©es
â”œâ”€ Checksums des lignes
â”œâ”€ Metadata (timestamp, nÅ“ud origine)
â””â”€ Statements/Rows (selon binlog_format)
```

**Algorithme de certification simplifiÃ© :**

```python
def certify_transaction(write_set, certification_index):
    """
    Pseudo-code du processus de certification
    """
    conflicts = []
    
    # VÃ©rifier chaque clÃ© modifiÃ©e
    for key in write_set.keys:
        # Chercher transactions certifiÃ©es touchant cette clÃ©
        # depuis le dernier seqno connu du nÅ“ud origine
        overlapping_txs = certification_index.get_range(
            key, 
            from_seqno=write_set.origin_last_seen_seqno,
            to_seqno=current_global_seqno
        )
        
        if overlapping_txs:
            conflicts.append(key)
    
    if conflicts:
        return CERTIFICATION_FAILED
    else:
        # Ajouter au certification index
        certification_index.add(write_set)
        return CERTIFICATION_PASSED
```

### Isolation et cohÃ©rence

**Garanties ACID dans Galera :**

- âœ… **Atomicity** : Transaction appliquÃ©e entiÃ¨rement ou pas du tout
- âœ… **Consistency** : Contraintes vÃ©rifiÃ©es sur tous les nÅ“uds
- âœ… **Isolation** : Snapshot isolation (REPEATABLE READ par dÃ©faut)
- âœ… **Durability** : Persistance garantie aprÃ¨s certification

**Niveau d'isolation :**

```sql
-- Galera supporte REPEATABLE READ et READ COMMITTED
-- READ UNCOMMITTED et SERIALIZABLE ne sont pas recommandÃ©s

-- Configuration recommandÃ©e
SET GLOBAL transaction_isolation = 'REPEATABLE-READ';

-- VÃ©rifier le niveau
SELECT @@transaction_isolation;
```

âš ï¸ **Attention** : Galera applique une **certification based** isolation, diffÃ©rente de l'isolation 2PL traditionnelle. Cela peut entraÃ®ner des rejets de transactions mÃªme sans deadlock InnoDB classique.

---

## Dimensionnement et topologies de production

### Nombre de nÅ“uds optimal

**Formule du quorum :**
```
Quorum = floor(N/2) + 1
```

**Configurations courantes :**

| NÅ“uds | Quorum | TolÃ¨re pannes | Usage recommandÃ© |
|-------|--------|---------------|------------------|
| 3 | 2 | 1 nÅ“ud | Production standard |
| 5 | 3 | 2 nÅ“uds | Haute criticitÃ© |
| 7 | 4 | 3 nÅ“uds | Multi-DC (rare) |

ğŸ’¡ **Recommandation** : **3 nÅ“uds** est le sweet spot pour la plupart des cas :
- TolÃ©rance Ã  une panne
- Overhead de certification acceptable
- Latence de certification raisonnable

âš ï¸ **Ã‰viter les nombres pairs** : 4 nÅ“uds ne tolÃ¨rent toujours qu'une seule panne (quorum = 3), autant rester Ã  3 nÅ“uds et Ã©conomiser les ressources.

### Architectures gÃ©o-distribuÃ©es

#### Configuration mono-datacenter (recommandÃ©e)

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Datacenter Primaire   â”‚
        â”‚                         â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚   â”‚Node Aâ”‚  â”‚Node Bâ”‚    â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”‚        â”‚        â”‚       â”‚
        â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
        â”‚            â”‚            â”‚
        â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”         â”‚
        â”‚        â”‚Node Câ”‚         â”‚
        â”‚        â””â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
Latence rÃ©seau : < 1ms
Bande passante : 10 Gbps+
```

**Configuration rÃ©seau :**

```ini
# my.cnf - Optimisation LAN
[galera]
wsrep_provider_options = "
    evs.send_window=512;
    evs.user_send_window=256;
    evs.keepalive_period=PT1S;
    evs.inactive_timeout=PT10S;
    evs.suspect_timeout=PT5S
"
```

#### Configuration multi-datacenter (avancÃ©e)

```
    DC1 (Primary)           DC2 (Backup)
    â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚Node Aâ”‚ â”‚Node Bâ”‚      â”‚Node Dâ”‚
    â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”˜
         â”‚       â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”
            â”‚Node Câ”‚ (Arbitrator)
            â””â”€â”€â”€â”€â”€â”€â”˜
            
Latence WAN : 5-50ms
Segment sÃ©parÃ© (gmcast.segment)
```

**Configuration WAN-optimized :**

```ini
# NÅ“uds DC1
[galera]
wsrep_provider_options = "
    gmcast.segment=0;
    evs.send_window=1024;
    evs.user_send_window=512;
    evs.keepalive_period=PT3S;
    evs.inactive_timeout=PT30S;
    evs.suspect_timeout=PT15S;
    evs.auto_evict=10
"

# NÅ“uds DC2
[galera]
wsrep_provider_options = "
    gmcast.segment=1;
    evs.send_window=1024;
    evs.user_send_window=512;
    evs.keepalive_period=PT3S;
    evs.inactive_timeout=PT30S;
    evs.suspect_timeout=PT15S;
    evs.auto_evict=10
"
```

ğŸ’¡ **Point critique** : Dans un setup multi-DC, les latences WAN impactent directement le temps de certification. Une latence de 20ms ajoute au minimum 40ms (RTT) Ã  chaque COMMIT.

**Impact sur les performances :**

```
Latence certification = max(Latence rÃ©seau entre nÅ“uds) * 2 + overhead certification

Exemple :
- LAN (< 1ms) : ~2ms de certification
- WAN (20ms) : ~42ms de certification
- Intercontinental (100ms) : ~202ms de certification
```

---

## Configurations de production optimisÃ©es

### Configuration complÃ¨te pour cluster 3 nÅ“uds

**NÅ“ud 1 (galera-node-1) :**

```ini
# /etc/mysql/mariadb.conf.d/60-galera.cnf

[mysqld]
#
# Galera Cluster Configuration
#
binlog_format = ROW
default_storage_engine = InnoDB
innodb_autoinc_lock_mode = 2
bind-address = 0.0.0.0

# Galera Provider Configuration
wsrep_on = ON
wsrep_provider = /usr/lib/galera/libgalera_smm.so

# Cluster connection
wsrep_cluster_address = "gcomm://10.0.1.11,10.0.1.12,10.0.1.13"
wsrep_cluster_name = "production_cluster"

# Node configuration
wsrep_node_address = "10.0.1.11"
wsrep_node_name = "galera-node-1"

# SST (State Snapshot Transfer) method
wsrep_sst_method = mariabackup
wsrep_sst_auth = "sst_user:SecurePassword123!"

# Replication threads
wsrep_slave_threads = 4

# Provider options
wsrep_provider_options = "
    gcache.size=2G;
    gcache.page_size=1G;
    gcs.fc_limit=256;
    gcs.fc_factor=0.9;
    evs.keepalive_period=PT1S;
    evs.suspect_timeout=PT10S;
    evs.inactive_timeout=PT30S;
    evs.install_timeout=PT15S;
    evs.send_window=1024;
    evs.user_send_window=512
"

# InnoDB configuration for Galera
innodb_buffer_pool_size = 16G
innodb_log_file_size = 512M
innodb_flush_log_at_trx_commit = 0  # Safe in Galera (certification garantit durabilitÃ©)
innodb_file_per_table = ON

# Binary logging (optionnel pour PITR)
log_bin = /var/log/mysql/mariadb-bin
log_bin_index = /var/log/mysql/mariadb-bin.index
expire_logs_days = 7
```

**NÅ“uds 2 et 3** : Configuration identique, seuls ces paramÃ¨tres changent :

```ini
# NÅ“ud 2
wsrep_node_address = "10.0.1.12"
wsrep_node_name = "galera-node-2"

# NÅ“ud 3
wsrep_node_address = "10.0.1.13"
wsrep_node_name = "galera-node-3"
```

### CrÃ©ation de l'utilisateur SST

```sql
-- Sur le premier nÅ“ud uniquement
CREATE USER 'sst_user'@'localhost' IDENTIFIED BY 'SecurePassword123!';
GRANT RELOAD, PROCESS, LOCK TABLES, REPLICATION CLIENT ON *.* 
    TO 'sst_user'@'localhost';
FLUSH PRIVILEGES;
```

### ParamÃ¨tres critiques expliquÃ©s

**`gcache.size`** : Cache des write-sets pour IST (Incremental State Transfer)

```ini
# Dimensionnement recommandÃ©
gcache.size = 2G  # Minimum
gcache.size = 10G # Environnement haute activitÃ©

# Calcul : taux_ecriture (MB/s) * temps_indisponibilite_max (secondes)
# Exemple : 10 MB/s * 600s = 6GB minimum
```

**`wsrep_slave_threads`** : Nombre de threads d'application parallÃ¨les

```ini
# Formule : nombre de CPU cores / 2
wsrep_slave_threads = 4  # Serveur 8 cores
wsrep_slave_threads = 8  # Serveur 16 cores

# Maximum efficace rarement > 16
```

**`innodb_autoinc_lock_mode`** : Obligatoire = 2 pour Galera

```sql
-- Mode 2 (interleaved) requis pour rÃ©plication multi-master
-- Ã‰vite les deadlocks sur AUTO_INCREMENT

-- VÃ©rifier
SHOW VARIABLES LIKE 'innodb_autoinc_lock_mode';
-- Doit retourner: 2
```

âš ï¸ **Attention** : `innodb_autoinc_lock_mode = 2` peut gÃ©nÃ©rer des gaps dans les AUTO_INCREMENT lors d'un failover. Si votre application dÃ©pend de sÃ©quences continues strictes, utiliser des SEQUENCES explicites.

---

## Performance et optimisations

### ParallÃ©lisation des apply threads

ğŸ†• **MariaDB 11.8** amÃ©liore le parallÃ©lisme d'application :

```sql
-- VÃ©rifier le parallÃ©lisme actuel
SHOW STATUS LIKE 'wsrep_cert_deps_distance';
-- Plus la valeur est Ã©levÃ©e, plus de parallÃ©lisme est possible

-- Monitorer l'efficacitÃ©
SHOW STATUS LIKE 'wsrep_apply_oool';  -- Out-of-order local commits
SHOW STATUS LIKE 'wsrep_apply_oooe';  -- Out-of-order external commits
```

### Optimisation des write-sets

**RÃ©duction de la taille des write-sets :**

```sql
-- PrÃ©fÃ©rer les transactions courtes
-- MAUVAIS : Transaction longue
START TRANSACTION;
UPDATE large_table SET status = 'processed' WHERE date < NOW();
-- (potentiellement millions de lignes)
COMMIT;

-- BON : Batches de 1000 lignes
DELIMITER $$
CREATE PROCEDURE process_by_batch()
BEGIN
    DECLARE rows_affected INT DEFAULT 1000;
    
    WHILE rows_affected = 1000 DO
        START TRANSACTION;
        UPDATE large_table 
        SET status = 'processed' 
        WHERE date < NOW() 
        AND status = 'pending'
        LIMIT 1000;
        
        SET rows_affected = ROW_COUNT();
        COMMIT;
        
        -- Pause pour Ã©viter de saturer la certification
        DO SLEEP(0.01);
    END WHILE;
END$$
DELIMITER ;
```

### Monitoring des mÃ©triques critiques

```sql
-- Script de monitoring Galera
SELECT 
    VARIABLE_NAME,
    VARIABLE_VALUE
FROM information_schema.GLOBAL_STATUS
WHERE VARIABLE_NAME IN (
    'wsrep_cluster_size',           -- Nombre de nÅ“uds
    'wsrep_cluster_status',         -- Statut cluster (Primary)
    'wsrep_connected',              -- ConnectÃ© au cluster
    'wsrep_ready',                  -- PrÃªt Ã  servir requÃªtes
    'wsrep_local_state_comment',    -- Ã‰tat du nÅ“ud (Synced)
    'wsrep_cert_deps_distance',     -- ParallÃ©lisme possible
    'wsrep_local_recv_queue',       -- Queue de rÃ©ception
    'wsrep_local_send_queue',       -- Queue d'envoi
    'wsrep_flow_control_paused',    -- % temps en flow control
    'wsrep_local_bf_aborts'         -- Transactions avortÃ©es
)
ORDER BY VARIABLE_NAME;
```

**Valeurs attendues en production saine :**

```
wsrep_cluster_size = 3              âœ… Tous les nÅ“uds prÃ©sents
wsrep_cluster_status = Primary      âœ… Cluster opÃ©rationnel
wsrep_connected = ON                âœ… Connexion active
wsrep_ready = ON                    âœ… PrÃªt Ã  servir
wsrep_local_state_comment = Synced  âœ… NÅ“ud synchronisÃ©
wsrep_local_recv_queue < 10         âœ… Pas de retard d'application
wsrep_flow_control_paused < 0.05    âœ… Moins de 5% en flow control
wsrep_local_bf_aborts < 10/s        âœ… Peu de conflits de certification
```

---

## ConsidÃ©rations CAP et garanties

### Positionnement dans le thÃ©orÃ¨me CAP

Galera Cluster opÃ¨re dans le **CP** (Consistency + Partition tolerance) du thÃ©orÃ¨me CAP :

```
CAP Triangle:
      C (Consistency)
      /\
     /  \
    /    \
   /  CP  \
  /  GALERA\
 /          \
/______________\
P              A
(Partition)    (Availability)
```

**En pratique :**
- âœ… **Consistency** : Forte (linearizable aprÃ¨s certification)
- âœ… **Partition tolerance** : Cluster continue si quorum maintenu
- âš ï¸ **Availability** : SacrifiÃ©e en cas de partition sans quorum

### Comportement en cas de partition rÃ©seau

**ScÃ©nario : Split-brain potentiel**

```
Avant partition :           AprÃ¨s partition :
    [A] [B] [C]            [A] [B]    |    [C]
     \___|___/              \_|_/     |    isolÃ©
       Cluster              Quorum:2  |    Quorum:1
                            âœ… Continue    âŒ Non-Primary
```

**RÃ©solution automatique :**

```sql
-- Sur nÅ“uds A et B (quorum maintenu)
SHOW STATUS LIKE 'wsrep_cluster_status';
-- PRIMARY : Continue Ã  accepter Ã©critures

-- Sur nÅ“ud C (isolÃ©)
SHOW STATUS LIKE 'wsrep_cluster_status';
-- NON-PRIMARY : Refuse Ã©critures, accepte lectures

-- VÃ©rifier l'Ã©tat
SHOW STATUS LIKE 'wsrep_ready';
-- ON si Primary, OFF si Non-Primary pour Ã©critures
```

ğŸ’¡ **Garantie** : Impossible d'avoir deux partitions actives simultanÃ©ment (pas de split-brain rÃ©el grÃ¢ce au quorum).

### Configuration pour tolÃ©rance aux partitions

```ini
# Eviction automatique des nÅ“uds suspectÃ©s
wsrep_provider_options = "
    evs.auto_evict=10;              # Eviction aprÃ¨s 10 pÃ©riodes
    evs.suspect_timeout=PT5S;       # Timeout de suspicion
    pc.weight=1                      # Poids du nÅ“ud (pour arbitrage)
"
```

**ScÃ©nario avancÃ© : Arbitrator node**

```
     [A] â†â†’ [B]        Arbitrator
       â†“      â†“             [G]
       â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       
En cas de partition A|B :
- A vote (1) + Arbitrator (1) = 2 votes
- B vote (1) = 1 vote
- A forme le Primary avec Arbitrator
```

Configuration arbitrator :

```bash
# Installation garbd (Galera Arbitrator Daemon)
apt-get install galera-arbitrator-4

# Lancement
garbd \
    --group=production_cluster \
    --address="gcomm://10.0.1.11,10.0.1.12,10.0.1.13" \
    --options="gmcast.listen_addr=tcp://0.0.0.0:4444"
```

---

## Limitations et considÃ©rations architecturales

### Limites intrinsÃ¨ques de Galera

**1. Pas de Foreign Keys en CASCADE distribuÃ©es**

```sql
-- ATTENTION : Risque de certification failure
-- Table parent
CREATE TABLE orders (
    id INT PRIMARY KEY,
    customer_id INT,
    total DECIMAL(10,2)
);

-- Table enfant avec FK CASCADE
CREATE TABLE order_items (
    id INT PRIMARY KEY,
    order_id INT,
    product_id INT,
    FOREIGN KEY (order_id) REFERENCES orders(id) ON DELETE CASCADE
);

-- Transaction sur NÅ“ud A
DELETE FROM orders WHERE id = 100;  -- Cascade sur order_items

-- Transaction simultanÃ©e sur NÅ“ud B
UPDATE order_items SET quantity = 5 WHERE order_id = 100;

-- RÃ©sultat : Conflit de certification, une transaction est rejetÃ©e
```

ğŸ’¡ **Solution** : GÃ©rer les cascades au niveau applicatif ou utiliser des triggers.

**2. Pas de LOCK TABLES global**

```sql
-- LOCK TABLES ne lock QUE sur le nÅ“ud local
LOCK TABLES accounts WRITE;

-- Un autre nÅ“ud peut toujours Ã©crire sur accounts
-- Risque de conflit de certification au COMMIT

-- Solution : Utiliser des transactions explicites
START TRANSACTION;
SELECT ... FOR UPDATE;  -- Row-level locks repliquÃ©s
-- ... opÃ©rations
COMMIT;
```

**3. Transactions DDL bloquent l'ensemble du cluster**

```sql
-- ALTER TABLE bloque TOUT le cluster pendant l'exÃ©cution
ALTER TABLE large_table ADD COLUMN new_col VARCHAR(100);

-- Impact : Tous les nÅ“uds appliquent le DDL en TOI (Total Order Isolation)
-- DurÃ©e : Temps d'exÃ©cution sur le nÅ“ud le plus lent
```

ğŸ†• **MariaDB 11.8 : Optimistic ALTER TABLE**
```ini
# Permet certains ALTER TABLE en NBO (Non-Blocking Operation)
wsrep_OSU_method = NBO  # Au lieu de TOI

# BÃ©nÃ©fice : RÃ©duction du blocage pour certaines opÃ©rations
```

âš ï¸ **Attention** : NBO est encore experimental pour certaines opÃ©rations. Tester en staging.

**4. Limite sur la taille des transactions**

```ini
# Taille maximale d'un write-set (dÃ©faut 2GB)
wsrep_max_ws_size = 2147483647

# Transaction excÃ©dant cette taille sera rejetÃ©e
# ERROR: 1180 (HY000): wsrep_max_ws_size exceeded
```

ğŸ’¡ **Solution** : DÃ©couper les grosses transactions en batches.

### Workloads dÃ©conseillÃ©s

**âŒ Non recommandÃ© pour Galera :**

1. **Ã‰critures sur colonnes AUTO_INCREMENT massivement concurrentes**
```sql
-- Pattern problÃ©matique
INSERT INTO sequences (id) VALUES (NULL);  -- AUTO_INCREMENT
-- GÃ©nÃ¨re des conflits de certification si massif
```

2. **TrÃ¨s grosses transactions analytiques**
```sql
-- Transaction de 100GB de donnÃ©es
START TRANSACTION;
INSERT INTO data_warehouse SELECT * FROM external_source;  -- Ã‰norme volume
COMMIT;
-- Saturera wsrep_max_ws_size et le rÃ©seau
```

3. **Hot spots avec conflits frÃ©quents**
```sql
-- Compteur global mis Ã  jour constamment
UPDATE global_counter SET value = value + 1;
-- Taux de rejet trÃ¨s Ã©levÃ© en multi-master
```

âœ… **Alternatives** :
- Utiliser des sÃ©quences avec plages rÃ©servÃ©es par nÅ“ud
- ExÃ©cuter les analytiques sur un nÅ“ud dÃ©diÃ© (lecture seule)
- Sharding des hot spots (compteurs par rÃ©gion, par client, etc.)

---

## Cas d'usage optimaux pour l'architecture synchrone

### ScÃ©narios idÃ©aux

**1. Applications haute disponibilitÃ© avec lectures distribuÃ©es**
```
E-commerce, SaaS, API backends
- Ã‰critures modÃ©rÃ©es et distribuÃ©es
- Lectures massivement scalables sur tous les nÅ“uds
- TolÃ©rance zÃ©ro perte de donnÃ©es (RPO = 0)
```

**2. Applications multi-rÃ©gions avec cohÃ©rence forte**
```
SystÃ¨mes financiers, rÃ©servations, inventaires
- Besoin de cohÃ©rence immÃ©diate
- Failover automatique transparent
- Audit trail complet et cohÃ©rent
```

**3. Modernisation d'anciennes architectures Master-Slave**
```
Migration vers HA sans refonte applicative majeure
- BÃ©nÃ©fice immÃ©diat du multi-master
- Ã‰limination du SPOF Master
- Simplification des failovers
```

### Patterns de dÃ©ploiement recommandÃ©s

**Pattern 1 : Active-Active avec rÃ©partition gÃ©ographique**

```
        LoadBalancer (MaxScale/ProxySQL)
               /     |     \
              /      |      \
        NÅ“ud A   NÅ“ud B   NÅ“ud C
         (EU)     (EU)     (US)
         
Ã‰critures EU â†’ NÅ“uds A/B (latence minimale)
Ã‰critures US â†’ NÅ“ud C
Lectures : Tous les nÅ“uds
```

**Pattern 2 : Active-Passive avec nÅ“ud analytics**

```
        LoadBalancer
         /        \
    NÅ“ud A      NÅ“ud B      NÅ“ud C
   (Write)     (Write)     (Async Read-Only)
                                â†“
                        Reporting/Analytics
```

Configuration NÅ“ud C :

```ini
# NÅ“ud dÃ©diÃ© analytics (pas de certification)
wsrep_provider_options = "
    repl.causal_read_timeout=PT90S;
    gmcast.segment=2
"

# Activer wsrep_sync_wait seulement pour reports
# SET SESSION wsrep_sync_wait = 1;  -- Avant lecture
```

---

## âœ… Points clÃ©s Ã  retenir

- **RÃ©plication synchrone virtuelle** : Certification synchrone, application asynchrone pour optimiser performance et cohÃ©rence
- **Multi-master actif-actif** : Tous les nÅ“uds Ã©gaux, Ã©critures et lectures partout, pas de SPOF
- **Certification-based replication** : DÃ©tection dÃ©terministe des conflits via write-sets, transaction rejetÃ©e en cas de conflit
- **Quorum obligatoire** : floor(N/2)+1 nÅ“uds pour maintenir le cluster en Primary (Ã©vite split-brain)
- **Configuration 3 nÅ“uds** : Sweet spot production (tolÃ¨re 1 panne, overhead raisonnable)
- **Latence rÃ©seau critique** : Impact direct sur temps de certification, privilÃ©gier LAN (<1ms) ou optimiser pour WAN
- **Flow control** : MÃ©canisme protÃ©geant le cluster contre les nÅ“uds lents, surveiller `wsrep_flow_control_paused`
- **Limitations** : Pas de FK CASCADE distribuÃ©es fiables, DDL bloque tout le cluster (TOI), limite taille transactions
- **Cas d'usage optimaux** : HA avec lectures distribuÃ©es, cohÃ©rence forte, failover automatique, RPO=0
- **Monitoring essentiel** : `wsrep_cluster_status`, `wsrep_ready`, `wsrep_local_state_comment`, queues, flow control

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle MariaDB
- [ğŸ“– Galera Cluster Official Documentation](https://mariadb.com/kb/en/galera-cluster/)
- [ğŸ“– Galera System Variables](https://mariadb.com/kb/en/galera-cluster-system-variables/)
- [ğŸ“– Galera Status Variables](https://mariadb.com/kb/en/galera-cluster-status-variables/)
- [ğŸ“– State Snapshot Transfer (SST)](https://mariadb.com/kb/en/introduction-to-state-snapshot-transfers-ssts/)

### Documentation Galera (Codership)
- [Galera Cluster Documentation](https://galeracluster.com/library/documentation/)
- [Write-Set Replication](https://galeracluster.com/library/documentation/tech-desc-introduction.html)
- [Certification Based Replication](https://galeracluster.com/library/documentation/certification-based-replication.html)

### Articles et guides techniques
- [Galera Cluster: Avoiding Pitfalls](https://severalnines.com/resources/database-management-tutorials/mysql-load-balancing-haproxy-tutorial/)
- [Understanding Galera Replication](https://www.percona.com/blog/understanding-galera-replication/)
- [Multi-Master Replication Challenges](https://www.highgo.ca/2020/01/31/understanding-multi-master-replication/)

### Outils de monitoring
- [ClusterControl for Galera](https://severalnines.com/product/clustercontrol)
- [Galera Manager](https://galeracluster.com/galera-mgr/)
- [Prometheus mysqld_exporter](https://github.com/prometheus/mysqld_exporter)

---

## â¡ï¸ Section suivante

**14.2.2 Certification-based replication** : Approfondissement du protocole de certification, gestion des write-sets, algorithmes de dÃ©tection de conflits, et optimisations pour rÃ©duire les taux de rejets de transactions.

Nous y explorerons en dÃ©tail le fonctionnement interne du moteur de certification, les stratÃ©gies d'optimisation des write-sets, et les techniques avancÃ©es pour minimiser les conflits dans des environnements multi-master haute concurrence.

---


â­ï¸ [Certification-based replication](/14-haute-disponibilite/02.2-certification-based.md)
