üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.6 Solutions de Failover Automatique

> **Niveau** : Expert  
> **Dur√©e estim√©e** : 3-4 heures  
> **Pr√©requis** : Sections 14.1-14.3 (HA, Galera, Quorum), exp√©rience op√©rationnelle production

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :

- **Comprendre** les diff√©rents types de failover et leurs implications
- **Concevoir** des strat√©gies de d√©tection de pannes fiables
- **Impl√©menter** des solutions de failover automatique pour r√©plication et Galera
- **Configurer** des outils sp√©cialis√©s (Orchestrator, MHA, mariadbmon)
- **Exploiter** les nouveaut√©s MariaDB 11.8 : Transaction Replay et Connection Migration
- **√âviter** les sc√©narios catastrophiques (double-master, data loss)
- **Tester** et valider vos proc√©dures de failover
- **Op√©rer** et monitorer le failover en production

---

## Introduction

Le **failover** est le processus de bascule automatique d'un syst√®me d√©faillant vers un syst√®me de secours. Dans le contexte MariaDB, il s'agit de promouvoir un replica en master (ou de router vers un autre n≈ìud Galera) lorsqu'une d√©faillance est d√©tect√©e.

**Dilemme fondamental** :
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Failover Manuel                                 ‚îÇ
‚îÇ  ‚úÖ Contr√¥le total                               ‚îÇ
‚îÇ  ‚úÖ √âvite faux positifs                          ‚îÇ
‚îÇ  ‚úÖ D√©cision humaine inform√©e                    ‚îÇ
‚îÇ  ‚ùå RTO √©lev√© (15-60 minutes)                    ‚îÇ
‚îÇ  ‚ùå N√©cessite astreinte 24/7                     ‚îÇ
‚îÇ  ‚ùå Erreurs humaines possibles                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      VS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Failover Automatique                            ‚îÇ
‚îÇ  ‚úÖ RTO minimal (30-90 secondes)                 ‚îÇ
‚îÇ  ‚úÖ Disponibilit√© 24/7 sans humain               ‚îÇ
‚îÇ  ‚úÖ Proc√©dure reproductible                      ‚îÇ
‚îÇ  ‚ùå Risque de faux positifs                      ‚îÇ
‚îÇ  ‚ùå D√©cisions sans contexte                      ‚îÇ
‚îÇ  ‚ùå Complexit√© op√©rationnelle                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> ‚ö†Ô∏è **Principe de Prudence** : "Le failover automatique doit √™tre configur√© de mani√®re conservatrice. Mieux vaut une alerte √† 3h du matin qu'un split-brain silencieux."

### üÜï Nouveaut√©s MariaDB 11.8 pour le Failover

**1. Transaction Replay (Rejouabilit√© Automatique)**
```sql
-- Transactions automatiquement rejou√©es apr√®s failover
SET GLOBAL transaction_replay = ON;
SET GLOBAL transaction_replay_attempts = 3;
```
‚Üí R√©duction significative du RTO et transparence applicative

**2. Connection Migration (Migration de Sessions)**
```sql
-- Pr√©servation des sessions apr√®s failover
SET GLOBAL connection_migration = ON;
SET GLOBAL connection_migration_preserve_session = ON;
```
‚Üí Continuit√© des connexions sans reconnexion applicative

Ces deux fonctionnalit√©s r√©duisent drastiquement l'impact utilisateur d'un failover.

---

## 1. Types de Failover

### 1.1 Failover vs Switchover

#### **Failover (Bascule d'Urgence)**

```
D√©finition : Bascule NON PLANIFI√âE suite √† une d√©faillance

Timeline typique :
T+0s    : Master crash (panne hardware)
T+10s   : D√©tection automatique (monitoring)
T+15s   : Validation (√©viter faux positif)
T+30s   : Promotion replica ‚Üí nouveau master
T+45s   : Reconfiguration autres replicas
T+60s   : Applications redirig√©es (DNS/VIP)
T+90s   : Service compl√®tement restaur√©

RTO = ~90 secondes
RPO = 0-30 secondes (d√©pend semi-sync)
```

**Caract√©ristiques** :
- ‚ö†Ô∏è **Urgent** : D√©cision rapide requise
- ‚ö†Ô∏è **Risque** : Perte potentielle de donn√©es (si async)
- ‚ö†Ô∏è **Stress** : Situation de crise
- ‚úÖ **Automatisable** : D√©cision algorithmique possible

#### **Switchover (Bascule Planifi√©e)**

```
D√©finition : Bascule PLANIFI√âE pour maintenance

Timeline typique :
T-60min : Annonce maintenance planifi√©e
T-30min : Pr√©paration (v√©rif r√©plication, backup)
T-15min : Mise en read-only master actuel
T-10min : Attente synchronisation compl√®te replicas
T-5min  : Promotion nouveau master
T-2min  : Reconfiguration
T+0min  : Bascule applications (maintenance window)
T+5min  : Validation nouveau master
T+10min : Ancien master rejoint comme replica

RTO = planifi√© (ex: 5 minutes maintenance)
RPO = 0 (synchronisation garantie)
```

**Caract√©ristiques** :
- ‚úÖ **Contr√¥l√©** : Timing ma√Ætris√©
- ‚úÖ **S√ªr** : Aucune perte de donn√©es
- ‚úÖ **Test√©** : R√©p√©table pour validation proc√©dures
- ‚úÖ **Rollback** : Possible si probl√®me d√©tect√©

üí° **Best Practice** : Effectuer des switchovers r√©guliers (mensuels) pour :
- Valider les proc√©dures de failover
- Entra√Æner les √©quipes
- D√©tecter probl√®mes de configuration
- Maintenir les comp√©tences

### 1.2 Failover par Architecture

#### **A. Failover Master-Slave Traditionnel**

```
√âtat Initial :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Master   ‚îÇ  ‚Üê Applications (writes)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ Replication
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Replica 1 ‚îÇ  ‚Üê Applications (reads)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Replica 2 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Apr√®s Failover :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Master   ‚îÇ  ‚úñ OFFLINE
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Replica 1 ‚îÇ  ‚Üê Promu NOUVEAU MASTER ‚úÖ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ Replication
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Replica 2 ‚îÇ  ‚Üê Reconfigur√© vers nouveau master
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Complexit√©** :
- Promotion d'un replica
- Reconfiguration des autres replicas
- Gestion du GTID ou positions binlog
- Redirection des applications

#### **B. Failover Galera Cluster**

```
√âtat Initial :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇNode1‚îÇ‚îÄ‚îÄ‚îÇNode2‚îÇ‚îÄ‚îÄ‚îÇNode3‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  All PRIMARY, All accepting writes

Apr√®s D√©faillance Node1 :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇNode1‚îÇ  ‚úñ OFFLINE
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇNode2‚îÇ‚îÄ‚îÄ‚îÇNode3‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Still PRIMARY (quorum 2/3)
  Automatic failover (routing via MaxScale)
```

**Simplicit√©** :
- ‚úÖ Pas de promotion n√©cessaire
- ‚úÖ Quorum automatique
- ‚úÖ Routing g√©r√© par MaxScale
- ‚ö†Ô∏è N√©cessite quorum maintenu

### 1.3 M√©triques de Failover

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  M√©triques Cl√©s                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  RTO (Recovery Time Objective)                      ‚îÇ
‚îÇ  ‚îú‚îÄ D√©tection : 5-15 secondes                       ‚îÇ
‚îÇ  ‚îú‚îÄ Validation : 5-10 secondes                      ‚îÇ
‚îÇ  ‚îú‚îÄ Promotion : 10-30 secondes                      ‚îÇ
‚îÇ  ‚îú‚îÄ Reconfiguration : 10-20 secondes                ‚îÇ
‚îÇ  ‚îî‚îÄ Total : 30-90 secondes                          ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  RPO (Recovery Point Objective)                     ‚îÇ
‚îÇ  ‚îú‚îÄ Async Replication : 1-60 secondes               ‚îÇ
‚îÇ  ‚îú‚îÄ Semi-Sync : 0-5 secondes                        ‚îÇ
‚îÇ  ‚îî‚îÄ Galera : 0 secondes (synchrone)                 ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  MTBF (Mean Time Between Failures)                  ‚îÇ
‚îÇ  ‚îî‚îÄ Hardware moderne : 720 heures (30 jours)        ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  MTTR (Mean Time To Repair)                         ‚îÇ
‚îÇ  ‚îú‚îÄ Manuel : 15-60 minutes                          ‚îÇ
‚îÇ  ‚îî‚îÄ Automatique : 1-3 minutes                       ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  Availability Calculation :                         ‚îÇ
‚îÇ  = MTBF / (MTBF + MTTR)                             ‚îÇ
‚îÇ  = 720h / (720h + 0.05h) = 99.993%                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. D√©tection de Pannes

### 2.1 M√©thodes de D√©tection

#### **A. Health Checks Actifs**

**Ping MySQL** :
```bash
#!/bin/bash
# check_mysql.sh

MYSQL_HOST="master.example.com"
MYSQL_USER="monitor"
MYSQL_PASS="SecurePassword"

# Tentative de connexion
mysqladmin ping \
  -h "$MYSQL_HOST" \
  -u "$MYSQL_USER" \
  -p"$MYSQL_PASS" \
  --connect_timeout=2 \
  2>/dev/null

if [ $? -eq 0 ]; then
    echo "OK: MySQL is alive"
    exit 0
else
    echo "CRITICAL: MySQL is down"
    exit 2
fi
```

**Query Test** :
```sql
-- Health check query (lecture + √©criture)
CREATE TABLE IF NOT EXISTS health_check (
    id INT PRIMARY KEY,
    last_check TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=InnoDB;

-- Check read
SELECT id FROM health_check WHERE id = 1;

-- Check write
INSERT INTO health_check (id) VALUES (1)
ON DUPLICATE KEY UPDATE last_check = NOW();

-- V√©rifier replication lag
SHOW SLAVE STATUS\G
# Seconds_Behind_Master < 10 ‚Üí OK
```

**Advanced Check (Galera)** :
```sql
-- V√©rifier √©tat Galera complet
SELECT 
    @@wsrep_cluster_status = 'Primary' AS is_primary,
    @@wsrep_local_state = 4 AS is_synced,
    @@wsrep_ready = 'ON' AS is_ready,
    @@wsrep_connected = 'ON' AS is_connected,
    @@wsrep_cluster_size >= 3 AS has_quorum;

-- R√©sultat attendu :
-- +------------+-----------+----------+--------------+------------+
-- | is_primary | is_synced | is_ready | is_connected | has_quorum |
-- +------------+-----------+----------+--------------+------------+
-- |          1 |         1 |        1 |            1 |          1 |
-- +------------+-----------+----------+--------------+------------+
-- ‚Üí Tous √† 1 = serveur sain
```

#### **B. Monitoring Passif**

**Heartbeat Table** :
```sql
-- Table heartbeat (√©crite p√©riodiquement par master)
CREATE TABLE heartbeat (
    server_id INT PRIMARY KEY,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    counter BIGINT
) ENGINE=InnoDB;

-- Script sur master (cron chaque seconde)
UPDATE heartbeat 
SET counter = counter + 1, timestamp = NOW() 
WHERE server_id = @@server_id;

-- V√©rification sur replica
SELECT 
    server_id,
    timestamp,
    TIMESTAMPDIFF(SECOND, timestamp, NOW()) as lag_seconds
FROM heartbeat;

-- Si lag_seconds > 60 ‚Üí Master potentiellement down
```

**Network Monitoring** :
```bash
# Ping ICMP
ping -c 3 -W 2 master.example.com

# TCP Port Check
nc -zv master.example.com 3306

# Application-Level
curl -f http://master.example.com:9104/metrics
# (Prometheus exporter)
```

#### **C. Consensus-Based Detection**

```
Multiple Monitoring Nodes :

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇMonitor1 ‚îÇ  ‚îÇMonitor2 ‚îÇ  ‚îÇMonitor3 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ            ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
           Check Master
                  ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Master  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

D√©cision de failover SI :
- 2/3 monitors d√©tectent panne (majorit√©)
- OU 3/3 monitors (unanimit√©)

‚Üí √âvite faux positifs dus √† partition r√©seau
```

### 2.2 Crit√®res de Validation

**Checklist avant Failover** :
```python
def should_failover(master, replicas):
    """
    D√©cision de failover bas√©e sur crit√®res multiples
    """
    checks = {
        'master_unreachable': False,
        'validated_by_multiple_monitors': False,
        'replica_available': False,
        'replica_synced': False,
        'no_recent_failover': False,
        'manual_override_disabled': False
    }
    
    # 1. Master vraiment down ?
    if not ping_mysql(master):
        checks['master_unreachable'] = True
    
    # 2. Valid√© par consensus ?
    down_votes = count_monitors_detecting_failure(master)
    if down_votes >= 2:  # Majorit√© sur 3
        checks['validated_by_multiple_monitors'] = True
    
    # 3. Replica disponible ?
    best_replica = select_best_replica(replicas)
    if best_replica:
        checks['replica_available'] = True
        
        # 4. Replica synchronis√© ?
        lag = get_replication_lag(best_replica)
        if lag < 10:  # Moins de 10 secondes de retard
            checks['replica_synced'] = True
    
    # 5. √âviter flapping (pas de failover r√©cent)
    last_failover_time = get_last_failover_time()
    if (now() - last_failover_time) > timedelta(minutes=10):
        checks['no_recent_failover'] = True
    
    # 6. Pas de verrou manuel
    if not is_failover_locked():
        checks['manual_override_disabled'] = True
    
    # D√©cision : TOUS les crit√®res doivent √™tre vrais
    return all(checks.values()), checks
```

### 2.3 √âviter les Faux Positifs

**Causes fr√©quentes de faux positifs** :
```
1. Partition r√©seau temporaire
   ‚Üí Solution : Consensus multi-monitors

2. Charge CPU √©lev√©e (slow to respond)
   ‚Üí Solution : Timeouts adaptatifs

3. Maintenance syst√®me (reboot)
   ‚Üí Solution : Maintenance mode flag

4. Network jitter
   ‚Üí Solution : Multiple failed checks cons√©cutifs

5. DNS issues
   ‚Üí Solution : Check par IP aussi

6. Monitoring tool failure
   ‚Üí Solution : Multiple monitoring stacks
```

**Configuration conservatrice** :
```ini
# Orchestrator configuration
[Topology]
# Require 3 consecutive failures
FailureDetectionPeriodBlockMinutes = 10
InstancePollSeconds = 5
# 3 checks √ó 5s = 15s minimum avant consid√©ration

# Semi-sync timeout protection
SemiSyncMasterTimeout = 10s
# Pr√©f√®re disponibilit√© sur coh√©rence apr√®s 10s

[Recovery]
# Prevent flapping
PreventCrossDataCenterMasterFailover = true
DelayMasterPromotionIfSQLThreadNotUpToDate = true
```

---

## 3. Solutions de Failover Automatique

### 3.1 MariaDB Monitor (MaxScale)

**Configuration mariadbmon** :
```ini
# /etc/maxscale.cnf
[MariaDB-Monitor]
type = monitor
module = mariadbmon
servers = server1, server2, server3
user = maxscale_monitor
password = SecurePassword

# ========================================
# FAILOVER CONFIGURATION
# ========================================

# Activer failover automatique
auto_failover = true

# Rejoindre automatiquement ancien master comme replica
auto_rejoin = true

# Promotion bas√©e sur :
# - Replication lag minimal
# - GTID position
# - Priority weight (optionnel)

# ========================================
# SWITCHOVER CONFIGURATION
# ========================================

# Switchover planifi√© possible via API
switchover_timeout = 90s

# V√©rifications avant switchover
enforce_read_only_slaves = true

# ========================================
# DETECTION
# ========================================

# Interval de v√©rification
monitor_interval = 2000ms

# Nombre de checks avant d√©claration panne
failcount = 3
# 3 √ó 2s = 6 secondes minimum

# Timeout pour consid√©rer serveur down
backend_connect_timeout = 3s
backend_read_timeout = 2s
backend_write_timeout = 2s

# ========================================
# CONSTRAINTS
# ========================================

# Prevent flapping
failover_timeout = 90s
# Pas de nouveau failover pendant 90s apr√®s un failover

# Disk space monitoring
switchover_on_low_disk_space = true
disk_space_threshold = 20%  # Switchover si < 20% espace

# ========================================
# EXTERNAL SCRIPTS
# ========================================

# Scripts pre/post failover
script = /usr/local/bin/failover-notify.sh
# Param√®tres pass√©s au script :
# $1 = event type (master_down, server_down, etc.)
# $2 = server name
# $3 = cluster info (JSON)
```

**Commandes manuelles** :
```bash
# D√©clencher switchover manuel (maintenance)
maxctrl call command mariadbmon switchover \
  MariaDB-Monitor \
  server2 \
  server1

# R√©initialiser r√©plication
maxctrl call command mariadbmon reset-replication \
  MariaDB-Monitor \
  server1

# Rejoindre cluster
maxctrl call command mariadbmon rejoin \
  MariaDB-Monitor \
  server3
```

### 3.2 Orchestrator

**Installation** :
```bash
# Installation Orchestrator
wget https://github.com/openark/orchestrator/releases/download/v3.2.6/orchestrator-3.2.6-1.x86_64.rpm
rpm -ivh orchestrator-3.2.6-1.x86_64.rpm

# Configuration backend database
mysql -u root -p << EOF
CREATE DATABASE orchestrator;
GRANT ALL ON orchestrator.* TO 'orchestrator'@'localhost' 
  IDENTIFIED BY 'SecurePassword';
EOF
```

**Configuration** :
```json
// /etc/orchestrator.conf.json
{
  "Debug": false,
  "ListenAddress": ":3000",
  
  "MySQLTopologyUser": "orchestrator",
  "MySQLTopologyPassword": "SecurePassword",
  
  "MySQLOrchestratorHost": "localhost",
  "MySQLOrchestratorPort": 3306,
  "MySQLOrchestratorDatabase": "orchestrator",
  "MySQLOrchestratorUser": "orchestrator",
  "MySQLOrchestratorPassword": "SecurePassword",
  
  "DiscoverByShowSlaveHosts": true,
  "InstancePollSeconds": 5,
  "DiscoveryPollSeconds": 5,
  
  "RecoveryPeriodBlockSeconds": 600,
  "RecoveryIgnoreHostnameFilters": [],
  "RecoverMasterClusterFilters": ["*"],
  "RecoverIntermediateMasterClusterFilters": ["*"],
  
  "OnFailureDetectionProcesses": [
    "/usr/local/bin/orchestrator-notify.sh {failureType} {failureCluster} {failedHost}"
  ],
  
  "PreFailoverProcesses": [
    "/usr/local/bin/pre-failover-check.sh {failureCluster} {candidateHost}"
  ],
  
  "PostMasterFailoverProcesses": [
    "/usr/local/bin/post-failover.sh {successorHost} {failedHost} {failureCluster}"
  ],
  
  "DetachLostReplicasAfterMasterFailover": true,
  "ApplyMySQLPromotionAfterMasterFailover": true,
  "MasterFailoverLostInstancesDowntimeMinutes": 10,
  
  "HTTPAdvertise": "http://orchestrator.example.com:3000"
}
```

**Hooks de notification** :
```bash
#!/bin/bash
# /usr/local/bin/post-failover.sh

SUCCESSOR_HOST="$1"
FAILED_HOST="$2"
CLUSTER="$3"

# Notification Slack
curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
  -H 'Content-Type: application/json' \
  -d "{
    \"text\": \"üö® FAILOVER EXECUTED\",
    \"attachments\": [{
      \"color\": \"danger\",
      \"fields\": [
        {\"title\": \"Cluster\", \"value\": \"$CLUSTER\", \"short\": true},
        {\"title\": \"Failed\", \"value\": \"$FAILED_HOST\", \"short\": true},
        {\"title\": \"New Master\", \"value\": \"$SUCCESSOR_HOST\", \"short\": true},
        {\"title\": \"Time\", \"value\": \"$(date)\", \"short\": true}
      ]
    }]
  }"

# Update DNS (exemple Route53)
aws route53 change-resource-record-sets \
  --hosted-zone-id Z1234567890ABC \
  --change-batch "{
    \"Changes\": [{
      \"Action\": \"UPSERT\",
      \"ResourceRecordSet\": {
        \"Name\": \"master.db.example.com\",
        \"Type\": \"A\",
        \"TTL\": 60,
        \"ResourceRecords\": [{\"Value\": \"$(dig +short $SUCCESSOR_HOST)\"}]
      }
    }]
  }"

# Cr√©er ticket incident
curl -X POST https://your-ticketing-system/api/incidents \
  -H 'Authorization: Bearer YOUR_TOKEN' \
  -d "{
    \"title\": \"Database Failover - $CLUSTER\",
    \"description\": \"Automatic failover from $FAILED_HOST to $SUCCESSOR_HOST\",
    \"severity\": \"high\",
    \"assigned_to\": \"dba-team\"
  }"
```

### 3.3 MHA (Master High Availability)

**Configuration Manager** :
```ini
# /etc/mha/app1.cnf
[server default]
# Manager settings
manager_workdir=/var/log/mha/app1
manager_log=/var/log/mha/app1/manager.log
remote_workdir=/var/log/mha/app1

# SSH settings
ssh_user=mha
ssh_port=22

# MySQL settings
user=mha
password=SecurePassword
repl_user=repl_user
repl_password=ReplicationPassword

# Failover settings
ping_interval=3
secondary_check_script=/usr/local/bin/masterha_secondary_check \
  -s remote_host1 -s remote_host2

# Post-failover
master_ip_failover_script=/usr/local/bin/master_ip_failover
shutdown_script=""
report_script=/usr/local/bin/mha_report

# Servers
[server1]
hostname=db1.example.com
port=3306
candidate_master=1
check_repl_delay=0

[server2]
hostname=db2.example.com
port=3306
candidate_master=1
check_repl_delay=0

[server3]
hostname=db3.example.com
port=3306
no_master=1
```

**VIP Failover Script** :
```bash
#!/bin/bash
# /usr/local/bin/master_ip_failover

command=$1
orig_master_host=$2
new_master_host=$3
orig_master_port=$4
new_master_port=$5

VIP=10.0.1.100
NETMASK=24
INTERFACE=eth0

case "$command" in
    stop)
        # Remove VIP from old master
        ssh $orig_master_host "ip addr del $VIP/$NETMASK dev $INTERFACE"
        exit 0
        ;;
    start)
        # Add VIP to new master
        ssh $new_master_host "ip addr add $VIP/$NETMASK dev $INTERFACE"
        ssh $new_master_host "arping -c 3 -I $INTERFACE -s $VIP $VIP"
        exit 0
        ;;
    *)
        echo "Usage: $0 {stop|start}"
        exit 1
        ;;
esac
```

‚ö†Ô∏è **Note** : MHA est en maintenance limit√©e. Pr√©f√©rer mariadbmon ou Orchestrator pour nouveaux d√©ploiements.

---

## 4. üÜï Transaction Replay et Connection Migration (MariaDB 11.8)

### 4.1 Transaction Replay

**Probl√®me r√©solu** :
```
Scenario AVANT MariaDB 11.8 :

Application :
  BEGIN;
  UPDATE accounts SET balance = balance - 100 WHERE id = 123;
  UPDATE accounts SET balance = balance + 100 WHERE id = 456;
  COMMIT;  ‚Üê Master crash PENDANT le commit
  
Result :
  ‚ùå Error: Lost connection to MySQL server during query
  ‚ùå Application must handle retry manually
  ‚ùå Risk of duplicate transaction if retry logic buggy
```

**Solution MariaDB 11.8** :
```sql
-- Configuration globale
SET GLOBAL transaction_replay = ON;
SET GLOBAL transaction_replay_attempts = 3;
SET GLOBAL transaction_replay_timeout = 30;  -- secondes

-- Configuration par session (optionnel)
SET SESSION transaction_replay = ON;
```

**Fonctionnement** :
```
Application :
  BEGIN;
  UPDATE accounts SET balance = balance - 100 WHERE id = 123;
  UPDATE accounts SET balance = balance + 100 WHERE id = 456;
  COMMIT;  ‚Üê Master crash
  
MariaDB 11.8 (automatiquement) :
  1. D√©tecte perte connexion pendant commit
  2. V√©rifie si transaction commit√©e sur ancien master
     - Si OUI : Retourne success √† l'application ‚úÖ
     - Si NON : Rejoue transaction sur nouveau master
  3. Retente jusqu'√† 3 fois (transaction_replay_attempts)
  4. Retourne r√©sultat √† l'application
  
Application :
  ‚úÖ Re√ßoit success (transparence totale)
  ‚úÖ Pas de code retry n√©cessaire
```

**Limitations** :
```sql
-- Transaction Replay fonctionne pour :
‚úÖ Transactions standards (BEGIN...COMMIT)
‚úÖ Autocommit statements
‚úÖ READ COMMITTED et REPEATABLE READ

-- Ne fonctionne PAS pour :
‚ùå LOCK TABLES
‚ùå GET_LOCK() / RELEASE_LOCK()
‚ùå Transactions avec side effects externes
‚ùå SERIALIZABLE isolation level
```

**Configuration production** :
```ini
# /etc/mysql/conf.d/transaction-replay.cnf
[mysqld]
# Activer transaction replay
transaction_replay = ON

# Nombre de tentatives
transaction_replay_attempts = 3

# Timeout pour replay
transaction_replay_timeout = 30

# Logging (debug)
log_error_verbosity = 3
# Logs d√©taill√©s dans error log :
# "Transaction replay: attempting replay of transaction XYZ"
# "Transaction replay: successfully replayed transaction XYZ"
# "Transaction replay: failed to replay transaction XYZ after 3 attempts"
```

**Monitoring** :
```sql
-- M√©triques transaction replay
SHOW GLOBAL STATUS LIKE 'Transaction_replay%';

-- +----------------------------------+-------+
-- | Variable_name                    | Value |
-- +----------------------------------+-------+
-- | Transaction_replay_attempted     | 1234  |
-- | Transaction_replay_succeeded     | 1198  |
-- | Transaction_replay_failed        | 36    |
-- | Transaction_replay_avg_time_ms   | 45    |
-- +----------------------------------+-------+

-- Taux de succ√®s
SELECT 
    (Transaction_replay_succeeded / Transaction_replay_attempted) * 100 
    AS success_rate_percent;
```

### 4.2 Connection Migration

**Probl√®me r√©solu** :
```
Scenario AVANT MariaDB 11.8 :

Application √©tablit connexion avec :
  - SET SESSION time_zone = 'America/New_York'
  - SET SESSION sql_mode = 'STRICT_TRANS_TABLES'
  - PREPARE stmt FROM 'SELECT * FROM users WHERE id = ?'
  
Master failover ‚Üí
  ‚ùå Connexion coup√©e
  ‚ùå Variables de session perdues
  ‚ùå Prepared statements perdus
  ‚ùå Application doit tout r√©initialiser
```

**Solution MariaDB 11.8** :
```sql
-- Configuration globale
SET GLOBAL connection_migration = ON;
SET GLOBAL connection_migration_preserve_session = ON;
SET GLOBAL connection_migration_preserve_prepared = ON;
```

**Fonctionnement** :
```
Application :
  -- Session setup
  SET time_zone = 'America/New_York';
  SET sql_mode = 'STRICT_TRANS_TABLES';
  PREPARE stmt FROM 'SELECT * FROM users WHERE id = ?';
  
Master failover (transparent) ‚Üí
  
MariaDB 11.8 (automatiquement) :
  1. D√©tecte failover
  2. √âtablit nouvelle connexion vers nouveau master
  3. R√©applique variables de session :
     - SET time_zone = 'America/New_York'
     - SET sql_mode = 'STRICT_TRANS_TABLES'
  4. Recr√©e prepared statements :
     - PREPARE stmt FROM 'SELECT * FROM users WHERE id = ?'
  5. Retourne contr√¥le √† l'application
  
Application :
  ‚úÖ Continue comme si rien ne s'√©tait pass√©
  EXECUTE stmt USING @user_id;  -- Fonctionne !
```

**Configuration production** :
```ini
# /etc/mysql/conf.d/connection-migration.cnf
[mysqld]
# Activer connection migration
connection_migration = ON

# Pr√©server session variables
connection_migration_preserve_session = ON

# Pr√©server prepared statements
connection_migration_preserve_prepared = ON

# Timeout migration
connection_migration_timeout = 10

# Retry
connection_migration_max_retries = 3
```

**Variables pr√©serv√©es** :
```sql
-- Variables de session migr√©es automatiquement :
SET time_zone = '...';
SET sql_mode = '...';
SET autocommit = ...;
SET transaction_isolation = '...';
SET character_set_client = '...';
SET character_set_connection = '...';
SET character_set_results = '...';
SET collation_connection = '...';
-- + autres variables SET SESSION
```

**Limitations** :
```sql
-- NE SONT PAS migr√©s :
‚ùå Temporary tables (CREATE TEMPORARY TABLE)
‚ùå User-defined variables (@var = value)
‚ùå LOCK TABLES actifs
‚ùå GET_LOCK() actifs
‚ùå Transactions en cours (voir Transaction Replay)
```

### 4.3 Combinaison Transaction Replay + Connection Migration

**Sc√©nario optimal** :
```sql
-- Configuration compl√®te haute disponibilit√©
SET GLOBAL transaction_replay = ON;
SET GLOBAL transaction_replay_attempts = 3;
SET GLOBAL connection_migration = ON;
SET GLOBAL connection_migration_preserve_session = ON;

-- Application (Java exemple)
Connection conn = DriverManager.getConnection(
    "jdbc:mariadb://maxscale:3306/mydb",
    props
);

// Configuration session
conn.createStatement().execute("SET time_zone = 'UTC'");
PreparedStatement pstmt = conn.prepareStatement(
    "UPDATE accounts SET balance = balance + ? WHERE id = ?"
);

// Transaction
conn.setAutoCommit(false);
pstmt.setDouble(1, 100.0);
pstmt.setInt(2, 123);
pstmt.executeUpdate();
conn.commit();  ‚Üê Master failover ICI

// R√©sultat :
// ‚úÖ Transaction Replay : Transaction rejou√©e sur nouveau master
// ‚úÖ Connection Migration : Session et prepared statement pr√©serv√©s
// ‚úÖ Application : Aucune erreur, tout transparent !
```

**M√©triques combin√©es** :
```sql
-- Dashboard monitoring
SELECT 
    'Transaction Replay' AS feature,
    Transaction_replay_attempted AS attempted,
    Transaction_replay_succeeded AS succeeded,
    Transaction_replay_failed AS failed,
    CONCAT(
        ROUND((Transaction_replay_succeeded / Transaction_replay_attempted) * 100, 2),
        '%'
    ) AS success_rate
FROM information_schema.GLOBAL_STATUS
WHERE Variable_name LIKE 'Transaction_replay%'

UNION ALL

SELECT 
    'Connection Migration',
    Connection_migration_attempted,
    Connection_migration_succeeded,
    Connection_migration_failed,
    CONCAT(
        ROUND((Connection_migration_succeeded / Connection_migration_attempted) * 100, 2),
        '%'
    )
FROM information_schema.GLOBAL_STATUS
WHERE Variable_name LIKE 'Connection_migration%';
```

---

## 5. Strat√©gies de R√©cup√©ration

### 5.1 Proc√©dure Post-Failover

```bash
#!/bin/bash
# post-failover-recovery.sh

NEW_MASTER="$1"
OLD_MASTER="$2"

echo "=== Post-Failover Recovery ==="
echo "New Master: $NEW_MASTER"
echo "Old Master: $OLD_MASTER"

# 1. Valider nouveau master op√©rationnel
echo "[1/7] Validating new master..."
mysql -h $NEW_MASTER -e "SELECT 1" || exit 1

# 2. V√©rifier r√©plication des replicas vers nouveau master
echo "[2/7] Checking replica replication..."
for replica in $(cat /etc/replicas.list); do
    mysql -h $replica -e "SHOW SLAVE STATUS\G" | grep -q "$NEW_MASTER"
    if [ $? -ne 0 ]; then
        echo "WARNING: $replica not replicating from $NEW_MASTER"
    fi
done

# 3. Monitorer lag
echo "[3/7] Monitoring replication lag..."
for i in {1..60}; do
    max_lag=$(mysql -h $NEW_MASTER -N -e "
        SELECT COALESCE(MAX(Seconds_Behind_Master), 0)
        FROM information_schema.SLAVE_HOSTS
        JOIN information_schema.PROCESSLIST 
        ON SLAVE_HOSTS.Host = PROCESSLIST.Host
    ")
    
    if [ "$max_lag" -lt 10 ]; then
        echo "‚úÖ All replicas caught up (max lag: ${max_lag}s)"
        break
    fi
    
    echo "Waiting for replicas to catch up (max lag: ${max_lag}s)..."
    sleep 5
done

# 4. Backup imm√©diat du nouveau master
echo "[4/7] Creating post-failover backup..."
mariabackup --backup \
    --target-dir=/backup/post-failover-$(date +%s) \
    --user=backup \
    --password=SecurePassword \
    --host=$NEW_MASTER

# 5. Tenter r√©cup√©ration ancien master (si accessible)
echo "[5/7] Attempting to recover old master..."
if ping -c 1 $OLD_MASTER &>/dev/null; then
    echo "Old master is reachable, attempting to join as replica..."
    
    # R√©cup√©rer GTID position
    NEW_MASTER_GTID=$(mysql -h $NEW_MASTER -N -e "SELECT @@gtid_current_pos")
    
    # Reconfigurer ancien master comme replica
    ssh $OLD_MASTER "mysql -e \"
        STOP SLAVE;
        CHANGE MASTER TO 
            MASTER_HOST='$NEW_MASTER',
            MASTER_USER='repl_user',
            MASTER_PASSWORD='ReplicationPassword',
            MASTER_USE_GTID=current_pos;
        SET GLOBAL gtid_slave_pos='$NEW_MASTER_GTID';
        START SLAVE;
    \""
else
    echo "Old master not reachable, manual intervention required"
fi

# 6. Notification √©quipe
echo "[6/7] Sending notifications..."
curl -X POST https://hooks.slack.com/YOUR/WEBHOOK \
    -d "{
        \"text\": \"‚úÖ Post-Failover Recovery Complete\",
        \"attachments\": [{
            \"fields\": [
                {\"title\": \"New Master\", \"value\": \"$NEW_MASTER\"},
                {\"title\": \"Old Master Status\", \"value\": \"$(ping -c 1 $OLD_MASTER &>/dev/null && echo 'Rejoined as replica' || echo 'Offline')\"},
                {\"title\": \"Time\", \"value\": \"$(date)\"}
            ]
        }]
    }"

# 7. Cr√©er post-mortem ticket
echo "[7/7] Creating post-mortem ticket..."
curl -X POST https://your-ticketing-system/api/issues \
    -H 'Content-Type: application/json' \
    -d "{
        \"title\": \"Post-Mortem: Database Failover $(date +%Y-%m-%d)\",
        \"description\": \"Analyze root cause of failover from $OLD_MASTER to $NEW_MASTER\",
        \"labels\": [\"post-mortem\", \"database\", \"high-priority\"]
    }"

echo "=== Recovery Complete ==="
```

### 5.2 Rollback Strategy

**Quand rollback ?**
```
Situations n√©cessitant rollback :
1. Nouveau master instable (crashes r√©p√©t√©s)
2. Corruption donn√©es d√©tect√©e
3. Probl√®me applicatif grave
4. Performance inacceptable
```

**Proc√©dure rollback** :
```bash
#!/bin/bash
# rollback-failover.sh

ORIGINAL_MASTER="$1"  # Serveur qu'on veut remettre master
CURRENT_MASTER="$2"   # Master actuel (post-failover)

echo "=== Rollback Failover ==="
echo "Target: Make $ORIGINAL_MASTER master again"

# Pr√©-requis : Original master doit √™tre op√©rationnel
mysql -h $ORIGINAL_MASTER -e "SELECT 1" || {
    echo "ERROR: Original master not accessible"
    exit 1
}

# 1. V√©rifier que original master est √† jour
ORIGINAL_GTID=$(mysql -h $ORIGINAL_MASTER -N -e "SELECT @@gtid_current_pos")
CURRENT_GTID=$(mysql -h $CURRENT_MASTER -N -e "SELECT @@gtid_current_pos")

echo "Original Master GTID: $ORIGINAL_GTID"
echo "Current Master GTID: $CURRENT_GTID"

# 2. Mettre current master en read-only
echo "Setting current master to read-only..."
mysql -h $CURRENT_MASTER -e "SET GLOBAL read_only = ON"

# 3. Attendre synchronisation compl√®te
echo "Waiting for original master to catch up..."
while true; do
    ORIG_GTID=$(mysql -h $ORIGINAL_MASTER -N -e "SELECT @@gtid_current_pos")
    CURR_GTID=$(mysql -h $CURRENT_MASTER -N -e "SELECT @@gtid_current_pos")
    
    if [ "$ORIG_GTID" == "$CURR_GTID" ]; then
        echo "‚úÖ Fully synchronized"
        break
    fi
    
    echo "Waiting... (${ORIG_GTID} vs ${CURR_GTID})"
    sleep 2
done

# 4. Arr√™ter r√©plication sur original master
mysql -h $ORIGINAL_MASTER -e "STOP SLAVE"

# 5. Promouvoir original master
mysql -h $ORIGINAL_MASTER -e "
    STOP SLAVE;
    RESET SLAVE ALL;
    SET GLOBAL read_only = OFF;
"

# 6. Reconfigurer current master comme replica
mysql -h $CURRENT_MASTER -e "
    STOP SLAVE;
    CHANGE MASTER TO 
        MASTER_HOST='$ORIGINAL_MASTER',
        MASTER_USER='repl_user',
        MASTER_PASSWORD='ReplicationPassword',
        MASTER_USE_GTID=current_pos;
    START SLAVE;
"

# 7. Rediriger applications
echo "Update your application configuration to point to $ORIGINAL_MASTER"
echo "Or update DNS/VIP"

echo "=== Rollback Complete ==="
```

### 5.3 Testing de Failover

**Chaos Engineering pour Databases** :
```bash
#!/bin/bash
# chaos-failover-test.sh

echo "=== Chaos Failover Test ==="
echo "‚ö†Ô∏è  WARNING: This will simulate a master failure"
read -p "Continue? (yes/no): " confirm

if [ "$confirm" != "yes" ]; then
    echo "Aborted"
    exit 0
fi

MASTER_HOST="db-master.example.com"
START_TIME=$(date +%s)

# 1. Baseline metrics
echo "[1/5] Capturing baseline metrics..."
BASELINE_QPS=$(mysql -h $MASTER_HOST -N -e "SHOW GLOBAL STATUS LIKE 'Queries'" | awk '{print $2}')

# 2. Simulate master crash
echo "[2/5] Simulating master crash (stopping MySQL)..."
ssh $MASTER_HOST "systemctl stop mariadb"

# 3. Wait for failover
echo "[3/5] Waiting for automatic failover..."
FAILOVER_DETECTED=false
for i in {1..60}; do
    # Check if new master elected
    NEW_MASTER=$(maxctrl list servers --tsv | grep Master | awk '{print $1}')
    
    if [ -n "$NEW_MASTER" ] && [ "$NEW_MASTER" != "$MASTER_HOST" ]; then
        FAILOVER_TIME=$(($(date +%s) - START_TIME))
        echo "‚úÖ Failover completed to $NEW_MASTER in ${FAILOVER_TIME}s"
        FAILOVER_DETECTED=true
        break
    fi
    
    echo "Waiting for failover... (${i}s)"
    sleep 1
done

if [ "$FAILOVER_DETECTED" = false ]; then
    echo "‚ùå Failover did not complete within 60s"
    exit 1
fi

# 4. Validate application connectivity
echo "[4/5] Validating application connectivity..."
APP_ERROR_COUNT=0
for i in {1..10}; do
    mysql -h maxscale.example.com -e "SELECT 1" &>/dev/null || APP_ERROR_COUNT=$((APP_ERROR_COUNT + 1))
    sleep 1
done

if [ $APP_ERROR_COUNT -gt 2 ]; then
    echo "‚ùå High error rate: $APP_ERROR_COUNT/10"
else
    echo "‚úÖ Application connectivity OK: $((10 - APP_ERROR_COUNT))/10 success"
fi

# 5. Cleanup - restart old master
echo "[5/5] Cleanup - restarting old master..."
ssh $MASTER_HOST "systemctl start mariadb"

# Report
cat << EOF

=== Failover Test Report ===
Failover Time: ${FAILOVER_TIME}s
New Master: $NEW_MASTER
Application Errors: $APP_ERROR_COUNT/10 queries
Status: $([ $APP_ERROR_COUNT -gt 2 ] && echo "‚ùå FAILED" || echo "‚úÖ PASSED")

RTO Achieved: ${FAILOVER_TIME}s
Target RTO: 90s
Result: $([ $FAILOVER_TIME -le 90 ] && echo "‚úÖ Within SLA" || echo "‚ùå Exceeds SLA")
EOF
```

---

## 6. Monitoring et Alerting

### 6.1 M√©triques Critiques

```yaml
# prometheus_alerts.yml
groups:
  - name: mariadb_failover
    interval: 10s
    rules:
      # Master down detection
      - alert: MariaDBMasterDown
        expr: mysql_up{role="master"} == 0
        for: 15s
        labels:
          severity: critical
        annotations:
          summary: "MariaDB Master {{ $labels.instance }} is down"
          description: "Automatic failover should trigger within 90s"
      
      # Replication lag
      - alert: MariaDBReplicationLagHigh
        expr: mysql_slave_lag_seconds > 60
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High replication lag on {{ $labels.instance }}"
          description: "Lag: {{ $value }}s - May impact failover RPO"
      
      # No master detected
      - alert: MariaDBNoMasterDetected
        expr: count(mysql_up{role="master"} == 1) == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "No MariaDB master detected in cluster"
          description: "Failover may have failed or is in progress"
      
      # Multiple masters (split-brain)
      - alert: MariaDBMultipleMasters
        expr: count(mysql_up{role="master"} == 1) > 1
        for: 10s
        labels:
          severity: critical
        annotations:
          summary: "Multiple MariaDB masters detected (split-brain)"
          description: "IMMEDIATE ACTION REQUIRED - {{ $value }} masters active"
      
      # Transaction Replay failures (11.8)
      - alert: TransactionReplayFailureHigh
        expr: rate(mysql_transaction_replay_failed[5m]) > 0.01
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High transaction replay failure rate"
          description: "Rate: {{ $value }} failures/sec on {{ $labels.instance }}"
```

### 6.2 Dashboard Grafana

```json
{
  "dashboard": {
    "title": "MariaDB Failover Monitoring",
    "panels": [
      {
        "title": "Cluster Status",
        "targets": [
          {
            "expr": "mysql_up",
            "legendFormat": "{{ instance }} - {{ role }}"
          }
        ]
      },
      {
        "title": "Replication Lag",
        "targets": [
          {
            "expr": "mysql_slave_lag_seconds",
            "legendFormat": "{{ instance }}"
          }
        ]
      },
      {
        "title": "Failover Events (Last 24h)",
        "targets": [
          {
            "expr": "changes(mysql_master_host[24h])",
            "legendFormat": "Failovers"
          }
        ]
      },
      {
        "title": "Transaction Replay (11.8)",
        "targets": [
          {
            "expr": "rate(mysql_transaction_replay_attempted[5m])",
            "legendFormat": "Attempted"
          },
          {
            "expr": "rate(mysql_transaction_replay_succeeded[5m])",
            "legendFormat": "Succeeded"
          },
          {
            "expr": "rate(mysql_transaction_replay_failed[5m])",
            "legendFormat": "Failed"
          }
        ]
      }
    ]
  }
}
```

---

## ‚úÖ Points Cl√©s √† Retenir

- **Failover automatique** : RTO 30-90s vs 15-60min manuel
- **D√©tection fiable** : Consensus multi-monitors, √©viter faux positifs
- **mariadbmon** (MaxScale) : Solution native recommand√©e pour MariaDB
- **Orchestrator** : Alternative puissante, GUI web, mature
- **üÜï Transaction Replay** (11.8) : Transparence applicative totale
- **üÜï Connection Migration** (11.8) : Pr√©servation session et prepared statements
- **Testing r√©gulier** : Chaos engineering mensuel recommand√©
- **Post-failover** : Proc√©dure de r√©cup√©ration document√©e essentielle
- **Monitoring** : Alerting proactif sur lag, master status, replay failures
- **Rollback strategy** : Plan B test√© en cas de failover probl√©matique

---

## üîó Ressources et R√©f√©rences

### Documentation Officielle
- [üìñ MariaDB Monitor Documentation](https://mariadb.com/kb/en/mariadb-monitor/)
- [üìñ Transaction Replay (11.8)](https://mariadb.com/kb/en/transaction-replay/)
- [üìñ Connection Migration (11.8)](https://mariadb.com/kb/en/connection-migration/)

### Outils Open Source
- [Orchestrator](https://github.com/openark/orchestrator)
- [MHA](https://github.com/yoshinorim/mha4mysql-manager)
- [MaxScale](https://github.com/mariadb-corporation/MaxScale)

### Articles et Blogs
- **"Automated Failover Best Practices"** - Percona Blog
- **"Transaction Replay Deep Dive"** - MariaDB Engineering Blog
- **"Chaos Engineering for Databases"** - Netflix Tech Blog

---

## ‚û°Ô∏è Section Suivante

**[14.7 Virtual IP et keepalived](/14-haute-disponibilite/07-virtual-ip-keepalived.md)**

La section suivante d√©taillera l'utilisation de VIP (Virtual IP) avec keepalived pour fournir un point d'acc√®s unique et g√©rer le basculement r√©seau lors d'un failover.

---

**Le failover automatique n'est pas une option en production moderne, c'est une n√©cessit√©. MariaDB 11.8 avec Transaction Replay et Connection Migration √©l√®ve la barre de ce qui est possible.**

‚è≠Ô∏è [Virtual IP et keepalived](/14-haute-disponibilite/07-virtual-ip-keepalived.md)
