üîù Retour au [Sommaire](/SOMMAIRE.md)

# 14.5.1 Workload Capture

> **Niveau** : Expert  
> **Dur√©e estim√©e** : 3-4 heures  
> **Pr√©requis** : MaxScale (14.4), Administration syst√®me, Analyse de performance

## üéØ Objectifs d'apprentissage

√Ä l'issue de cette section, vous serez capable de :
- Comprendre la fonctionnalit√© Workload Capture de MaxScale 25.01
- Configurer la capture de workload en production
- Filtrer et √©chantillonner les requ√™tes captur√©es
- Anonymiser les donn√©es sensibles dans les captures
- G√©rer le stockage et l'archivage des workloads
- Optimiser la capture pour minimiser l'impact performance
- Analyser les workloads captur√©s
- Pr√©parer des workloads pour replay et tests

---

## Introduction

Le **Workload Capture** est une fonctionnalit√© majeure introduite dans **MaxScale 25.01** qui permet d'enregistrer l'int√©gralit√© du trafic SQL passant par MaxScale. Cette capture peut ensuite √™tre rejou√©e (Workload Replay, section 14.5.2) pour valider des upgrades, tester de nouvelles configurations ou effectuer des benchmarks avec un trafic r√©aliste.

```
Workflow complet:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRODUCTION                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                               ‚îÇ
‚îÇ  ‚îÇ   App    ‚îÇ ‚îÄ‚îÄ‚Üí MaxScale (Capture ON) ‚îÄ‚îÄ‚Üí MariaDB 11.8    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚Üì                                     ‚îÇ
‚îÇ                  workload.log                               ‚îÇ
‚îÇ                  (100% requ√™tes)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ Copy
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TEST/STAGING                             ‚îÇ
‚îÇ  Workload Replay                                            ‚îÇ
‚îÇ  workload.log ‚îÄ‚îÄ‚Üí MaxScale ‚îÄ‚îÄ‚Üí MariaDB 12.0                 ‚îÇ
‚îÇ                                    ‚Üì                        ‚îÇ
‚îÇ                           Comparaison r√©sultats             ‚îÇ
‚îÇ                           Analyse performance               ‚îÇ
‚îÇ                           Validation upgrade                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Cas d'usage:
‚úÖ Validation upgrade MariaDB (11.8 ‚Üí 12.0)
‚úÖ Test configuration MaxScale (comparer 2 configs)
‚úÖ Benchmark infrastructure (vieux vs nouveaux serveurs)
‚úÖ Analyse workload r√©el (patterns, hot queries)
‚úÖ Reproduction bugs production
‚úÖ Capacity planning (projection charge future)
```

---

## Cas d'usage du Workload Capture

### 1. Validation d'upgrade MariaDB

**Sc√©nario :** Upgrade MariaDB 11.8 LTS ‚Üí 12.0

```
Processus:
1. Capturer 24h de trafic production (MariaDB 11.8)
2. Rejouer sur environnement test (MariaDB 12.0)
3. Comparer r√©sultats et performances
4. Identifier incompatibilit√©s avant mise en production

B√©n√©fices:
- D√©tection incompatibilit√©s SQL
- Validation performances (regressions?)
- Test avec charge r√©elle (pas synthetic)
- Confiance √©lev√©e pour upgrade
```

**Exemple concret :**

```bash
# Production (MariaDB 11.8)
# Capture pendant 24h un jour ouvrable type
maxscale-capture --duration 24h --output prod_workload_20241215.log

# Test (MariaDB 12.0)
# Replay du workload captur√©
maxscale-replay --input prod_workload_20241215.log \
                --target test-db-12.0:3306 \
                --compare-results \
                --report upgrade_validation_report.html

# Analyse du rapport:
# - 99.8% requ√™tes: r√©sultats identiques ‚úÖ
# - 0.2% requ√™tes: diff√©rences d√©tect√©es ‚ö†Ô∏è
#   ‚Üí Investigation n√©cessaire avant upgrade prod
```

### 2. Test de nouvelles configurations

**Sc√©nario :** Comparer 2 configurations MaxScale

```
Configuration A (actuelle):
- readwritesplit
- max_slave_connections = 1
- causal_reads = false

Configuration B (candidate):
- readwritesplit
- max_slave_connections = 100%
- causal_reads = true

Question: Configuration B am√©liore-t-elle les performances?
```

**M√©thodologie :**

```bash
# 1. Capturer workload avec Config A (production)
# Capture pendant heures de pointe
maxscale-capture --duration 2h --output workload_peak_hours.log

# 2. Rejouer avec Config A (baseline)
maxscale-replay --input workload_peak_hours.log \
                --target maxscale-config-a:3306 \
                --report config_a_results.json

# 3. Rejouer avec Config B (candidate)
maxscale-replay --input workload_peak_hours.log \
                --target maxscale-config-b:3306 \
                --report config_b_results.json

# 4. Comparer r√©sultats
maxscale-compare config_a_results.json config_b_results.json

# R√©sultats exemple:
# Config A: Latence moyenne 15ms, P95 45ms
# Config B: Latence moyenne 12ms (-20%), P95 38ms (-15%) ‚úÖ
# ‚Üí Config B am√©liore les performances!
```

### 3. Benchmark infrastructure

**Sc√©nario :** Valider migration vers nouveaux serveurs

```
Infrastructure actuelle:
- 3√ó serveurs: 16 cores, 64GB RAM, SSD SATA

Infrastructure candidate:
- 3√ó serveurs: 32 cores, 128GB RAM, NVMe SSD

Question: Am√©lioration performance justifie co√ªt?
```

**Approche :**

```bash
# 1. Capturer workload repr√©sentatif (semaine compl√®te)
maxscale-capture --duration 168h --sample-rate 10% \
                 --output workload_week_sample.log

# 2. Rejouer sur infrastructure actuelle
maxscale-replay --input workload_week_sample.log \
                --target old-infra:3306 \
                --collect-metrics \
                --report old_infra_perf.json

# 3. Rejouer sur infrastructure candidate
maxscale-replay --input workload_week_sample.log \
                --target new-infra:3306 \
                --collect-metrics \
                --report new_infra_perf.json

# 4. Analyse ROI
# Old: P95 latency 50ms, Max QPS 5,000
# New: P95 latency 20ms (-60%), Max QPS 12,000 (+140%)
# ‚Üí ROI justifi√© si besoin de scaling
```

### 4. Analyse et optimisation workload

**Sc√©nario :** Identifier requ√™tes probl√©matiques

```bash
# Capturer workload avec m√©triques d√©taill√©es
maxscale-capture --duration 1h \
                 --include-metrics \
                 --output workload_analysis.log

# Analyser le workload
maxscale-analyze workload_analysis.log

# Rapport g√©n√©r√©:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Workload Analysis Report                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Top 10 slowest queries:                                     ‚îÇ
‚îÇ 1. SELECT * FROM orders WHERE status IN (...)  (avg 850ms)  ‚îÇ
‚îÇ 2. UPDATE inventory SET qty = ...             (avg 420ms)   ‚îÇ
‚îÇ 3. SELECT COUNT(*) FROM logs WHERE ...        (avg 380ms)   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ Top 10 most frequent queries:                               ‚îÇ
‚îÇ 1. SELECT * FROM products WHERE id = ?  (15,234 calls)      ‚îÇ
‚îÇ 2. INSERT INTO sessions (...)           (12,891 calls)      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ Recommendations:                                            ‚îÇ
‚îÇ - Add index on orders.status (improve query #1)             ‚îÇ
‚îÇ - Optimize inventory updates (batching?)                    ‚îÇ
‚îÇ - Add summary table for logs stats                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Configuration Workload Capture

### Configuration de base

**MaxScale configuration (/etc/maxscale.cnf) :**

```ini
# /etc/maxscale.cnf - Workload Capture Configuration

[maxscale]
threads = auto

#
# === Servers ===
#
[server1]
type = server
address = 10.0.1.11
port = 3306
protocol = MariaDBBackend

[server2]
type = server
address = 10.0.1.12
port = 3306
protocol = MariaDBBackend

[server3]
type = server
address = 10.0.1.13
port = 3306
protocol = MariaDBBackend

#
# === Monitor ===
#
[MariaDB-Monitor]
type = monitor
module = mariadbmon
servers = server1, server2, server3
user = maxscale_monitor
password = Monitor_P@ssw0rd_2024!
monitor_interval = 2000ms

#
# === Service avec Workload Capture ===
#
[Capture-Service]
type = service
router = readwritesplit
servers = server1, server2, server3

user = maxscale_router
password = Router_P@ssw0rd_2024!

# üÜï Workload Capture (MaxScale 25.01)
workload_capture = true

# Fichier de sortie (support variables)
workload_capture_file = /var/lib/maxscale/workloads/workload_$(date +%Y%m%d_%H%M%S).log

# Format de capture
workload_capture_format = json
# Options: json (recommand√©), binary, text

# Dur√©e de capture (0 = infini)
workload_capture_duration = 3600s  # 1 heure

# √âchantillonnage (% de requ√™tes √† capturer)
workload_capture_sample_rate = 100  # 100% = tout capturer
# Pour production busy: 10-50% peut suffire

# === Filtres de capture ===

# Types de requ√™tes √† capturer
workload_capture_query_types = SELECT,INSERT,UPDATE,DELETE
# Exclure: DDL (CREATE, ALTER, DROP) si pas pertinent

# Utilisateurs √† capturer (vide = tous)
workload_capture_users = app_user,readonly_user
# Exclure utilisateurs techniques (monitoring, backup)

# Bases de donn√©es √† capturer
workload_capture_databases = production_db,analytics_db
# Exclure: mysql, information_schema, performance_schema

# === Filtres de performance ===

# Dur√©e minimale pour capturer (ignore requ√™tes rapides)
workload_capture_min_duration = 10ms
# Capturer seulement requ√™tes >10ms

# Taille maximale requ√™te
workload_capture_max_query_size = 65536  # 64KB
# Tronquer requ√™tes tr√®s longues

# === Metadata et contexte ===

# Inclure metadata d√©taill√©e
workload_capture_include_metadata = true
# Ajoute: timestamp, user, database, source_ip, session_id

# Inclure r√©sultats requ√™tes (pour comparaison)
workload_capture_include_results = true
# Capture nombre de lignes retourn√©es, rows affected

# Inclure m√©triques performance
workload_capture_include_metrics = true
# Ajoute: execution_time, rows_examined, tmp_tables_used

# === Limites de s√©curit√© ===

# Rotation automatique si fichier trop gros
workload_capture_max_file_size = 10GB
workload_capture_rotate_on_size = true

# Buffer m√©moire
workload_capture_buffer_size = 100MB
workload_capture_flush_interval = 1s

#
# === Listener ===
#
[Capture-Listener]
type = listener
service = Capture-Service
protocol = MariaDBClient
port = 3306
address = 0.0.0.0
```

### Activation/D√©sactivation dynamique

**Via maxctrl (sans restart MaxScale) :**

```bash
# Activer capture
maxctrl alter service Capture-Service workload_capture true

# D√©sactiver capture
maxctrl alter service Capture-Service workload_capture false

# Changer fichier de sortie
maxctrl alter service Capture-Service \
    workload_capture_file "/var/lib/maxscale/workloads/new_capture.log"

# Changer sample rate (r√©duire impact)
maxctrl alter service Capture-Service workload_capture_sample_rate 50

# V√©rifier √©tat capture
maxctrl show service Capture-Service | grep workload_capture
```

### Capture programm√©e (cron)

**Automatiser captures r√©guli√®res :**

```bash
#!/bin/bash
# /usr/local/bin/maxscale-scheduled-capture.sh
# Capture automatique quotidienne heures de pointe

CAPTURE_DIR="/var/lib/maxscale/workloads"
CAPTURE_FILE="$CAPTURE_DIR/daily_peak_$(date +%Y%m%d).log"

# V√©rifier que c'est un jour ouvrable (Lun-Ven)
if [ $(date +%u) -le 5 ]; then
    # Activer capture
    maxctrl alter service Capture-Service workload_capture true
    maxctrl alter service Capture-Service workload_capture_file "$CAPTURE_FILE"
    
    echo "$(date): Workload capture started: $CAPTURE_FILE" >> /var/log/maxscale/capture_schedule.log
    
    # Capturer pendant 2 heures (heures de pointe: 10h-12h)
    sleep 7200
    
    # D√©sactiver capture
    maxctrl alter service Capture-Service workload_capture false
    
    echo "$(date): Workload capture completed: $CAPTURE_FILE" >> /var/log/maxscale/capture_schedule.log
    
    # Compresser
    gzip "$CAPTURE_FILE"
    
    # Archiver (S3, NAS, etc.)
    aws s3 cp "$CAPTURE_FILE.gz" s3://backups/workloads/ || true
    
    # Cleanup vieux fichiers (garder 30 jours)
    find "$CAPTURE_DIR" -name "*.log.gz" -mtime +30 -delete
fi
```

**Crontab :**

```cron
# /etc/cron.d/maxscale-capture
# Capture quotidienne 10h-12h (heures de pointe)
0 10 * * 1-5 root /usr/local/bin/maxscale-scheduled-capture.sh
```

---

## Format de fichier captur√©

### Format JSON (recommand√©)

**Structure d'un enregistrement :**

```json
{
  "version": "1.0",
  "capture_metadata": {
    "start_time": "2024-12-15T10:00:00.000Z",
    "end_time": "2024-12-15T12:00:00.000Z",
    "maxscale_version": "25.01.0",
    "sample_rate": 100,
    "total_queries": 1245678
  },
  "queries": [
    {
      "query_id": 1,
      "timestamp": "2024-12-15T10:00:00.123Z",
      "session_id": "abc123def456",
      "user": "app_user",
      "database": "production_db",
      "source_ip": "10.0.2.50",
      "query_type": "SELECT",
      "query": "SELECT id, name, email FROM users WHERE id = ?",
      "parameters": [123],
      "execution_time_ms": 12.5,
      "rows_returned": 1,
      "rows_examined": 1,
      "server": "server1",
      "result_checksum": "a1b2c3d4e5f6",
      "metadata": {
        "tmp_tables": 0,
        "filesort": false,
        "index_used": "PRIMARY"
      }
    },
    {
      "query_id": 2,
      "timestamp": "2024-12-15T10:00:00.456Z",
      "session_id": "abc123def456",
      "user": "app_user",
      "database": "production_db",
      "source_ip": "10.0.2.50",
      "query_type": "UPDATE",
      "query": "UPDATE users SET last_login = ? WHERE id = ?",
      "parameters": ["2024-12-15 10:00:00", 123],
      "execution_time_ms": 8.3,
      "rows_affected": 1,
      "server": "server1"
    },
    {
      "query_id": 3,
      "timestamp": "2024-12-15T10:00:01.789Z",
      "session_id": "xyz789abc123",
      "user": "readonly_user",
      "database": "analytics_db",
      "source_ip": "10.0.2.51",
      "query_type": "SELECT",
      "query": "SELECT COUNT(*) FROM page_views WHERE date >= ?",
      "parameters": ["2024-12-01"],
      "execution_time_ms": 850.2,
      "rows_returned": 1,
      "rows_examined": 1245678,
      "server": "server2",
      "metadata": {
        "tmp_tables": 1,
        "filesort": true,
        "full_scan": true
      }
    }
  ]
}
```

### Format binaire (haute performance)

**Pour workloads tr√®s intenses (>10K QPS) :**

```ini
[Capture-Service]
# Format binaire (plus compact, plus rapide)
workload_capture_format = binary

# Fichier binaire
workload_capture_file = /var/lib/maxscale/workloads/workload.bin
```

**Conversion binaire ‚Üí JSON :**

```bash
# Outil de conversion MaxScale
maxscale-convert-workload \
    --input workload.bin \
    --output workload.json \
    --format json
```

---

## Filtrage et √©chantillonnage

### √âchantillonnage intelligent

**Strat√©gies d'√©chantillonnage :**

```ini
# === Strat√©gie 1: √âchantillonnage uniforme ===
[Uniform-Sampling-Service]
workload_capture = true
workload_capture_sample_rate = 10  # 10% des requ√™tes
# Avantage: Simple, pr√©visible
# Inconv√©nient: Peut manquer requ√™tes rares mais importantes

# === Strat√©gie 2: √âchantillonnage par requ√™tes lentes ===
[Slow-Query-Sampling-Service]
workload_capture = true
workload_capture_sample_rate = 100  # Tout capturer
workload_capture_min_duration = 100ms  # Seulement >100ms
# Avantage: Focus sur requ√™tes probl√©matiques
# Inconv√©nient: Ignore requ√™tes rapides mais fr√©quentes

# === Strat√©gie 3: √âchantillonnage adaptatif ===
[Adaptive-Sampling-Service]
workload_capture = true
workload_capture_sample_rate = 50  # Baseline 50%

# Script externe ajuste dynamiquement
# Heures creuses: 10%
# Heures normales: 50%
# Heures de pointe: 100%
```

**Script √©chantillonnage adaptatif :**

```bash
#!/bin/bash
# /usr/local/bin/adaptive-sampling.sh
# Ajuste sample rate selon charge

while true; do
    # R√©cup√©rer QPS actuel
    QPS=$(maxctrl show service Capture-Service | grep "Queries per second" | awk '{print $4}')
    
    # Ajuster sample rate
    if [ "$QPS" -lt 1000 ]; then
        # Charge faible: 100%
        RATE=100
    elif [ "$QPS" -lt 5000 ]; then
        # Charge moyenne: 50%
        RATE=50
    else
        # Charge √©lev√©e: 10%
        RATE=10
    fi
    
    # Appliquer
    maxctrl alter service Capture-Service workload_capture_sample_rate $RATE
    
    echo "$(date): QPS=$QPS, Sample rate adjusted to $RATE%" >> /var/log/maxscale/adaptive_sampling.log
    
    # V√©rifier toutes les minutes
    sleep 60
done
```

### Filtrage par patterns

**Exclure requ√™tes non pertinentes :**

```ini
# Configuration avec regex filter AVANT capture
[Exclude-Filter]
type = filter
module = regexfilter
rules = /etc/maxscale.d/capture_exclude_rules.txt

[Filtered-Capture-Service]
type = service
router = readwritesplit
servers = server1, server2, server3
filters = Exclude-Filter  # Appliqu√© avant capture

workload_capture = true
# Seulement requ√™tes passant le filtre sont captur√©es
```

**R√®gles d'exclusion :**

```regex
# /etc/maxscale.d/capture_exclude_rules.txt
# Exclure requ√™tes de la capture

# Monitoring queries (non repr√©sentatives)
SELECT\s+1                                                  exclude
SHOW\s+STATUS                                               exclude
SHOW\s+SLAVE\s+STATUS                                       exclude

# Healthcheck queries
SELECT\s+@@.*                                               exclude
SELECT\s+VERSION\(\)                                        exclude

# Session setup (r√©p√©titif)
SET\s+NAMES.*                                               exclude
SET\s+SESSION.*                                             exclude

# Tout le reste: inclure
.*                                                          include
```

---

## Anonymisation et s√©curit√©

### Anonymisation des donn√©es sensibles

**Probl√©matique :** Les captures contiennent potentiellement des donn√©es sensibles.

```sql
-- Requ√™te captur√©e (contient PII):
SELECT * FROM users WHERE email = 'john.doe@example.com' AND ssn = '123-45-6789';

-- Risque si fichier de capture compromis!
```

**Solution : Anonymisation automatique**

```ini
[Anonymized-Capture-Service]
type = service
router = readwritesplit
servers = server1, server2, server3

workload_capture = true
workload_capture_file = /var/lib/maxscale/workloads/workload_anon.log

# üÜï Anonymisation (MaxScale 25.01)
workload_capture_anonymize = true

# Patterns √† anonymiser
workload_capture_anonymize_patterns = email,phone,ssn,credit_card,password

# M√©thode d'anonymisation
workload_capture_anonymize_method = hash
# Options: hash (SHA256), mask (****), fake (g√©n√®re fake data)
```

**R√©sultat anonymis√© :**

```json
{
  "query": "SELECT * FROM users WHERE email = ? AND ssn = ?",
  "parameters": [
    "hash:a1b2c3d4e5f6...",  // email hash√©
    "hash:f6e5d4c3b2a1..."   // ssn hash√©
  ],
  "anonymized": true
}
```

**Script d'anonymisation post-capture :**

```python
#!/usr/bin/env python3
# /usr/local/bin/anonymize-workload.py
# Anonymiser workload captur√©

import json
import re
import hashlib
from faker import Faker

fake = Faker()

def anonymize_query(query, parameters):
    """
    Anonymise une requ√™te et ses param√®tres
    """
    # Patterns sensibles
    patterns = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
        'credit_card': r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b',
    }
    
    anonymized_query = query
    anonymized_params = []
    
    # Anonymiser param√®tres
    for param in parameters:
        if isinstance(param, str):
            # V√©rifier si param matche pattern sensible
            for pattern_name, pattern_regex in patterns.items():
                if re.match(pattern_regex, param):
                    # Anonymiser selon type
                    if pattern_name == 'email':
                        param = fake.email()
                    elif pattern_name == 'phone':
                        param = fake.phone_number()
                    elif pattern_name == 'ssn':
                        param = fake.ssn()
                    elif pattern_name == 'credit_card':
                        param = fake.credit_card_number()
                    break
        
        anonymized_params.append(param)
    
    return anonymized_query, anonymized_params

def anonymize_workload_file(input_file, output_file):
    """
    Anonymise un fichier de workload complet
    """
    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
        workload = json.load(f_in)
        
        # Anonymiser chaque requ√™te
        for query_entry in workload['queries']:
            query = query_entry.get('query', '')
            params = query_entry.get('parameters', [])
            
            anon_query, anon_params = anonymize_query(query, params)
            
            query_entry['query'] = anon_query
            query_entry['parameters'] = anon_params
            query_entry['anonymized'] = True
        
        # √âcrire fichier anonymis√©
        json.dump(workload, f_out, indent=2)

# Utilisation
if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print("Usage: anonymize-workload.py <input> <output>")
        sys.exit(1)
    
    anonymize_workload_file(sys.argv[1], sys.argv[2])
    print(f"Workload anonymized: {sys.argv[2]}")
```

### Chiffrement des captures

**Chiffrer fichiers de capture au repos :**

```bash
#!/bin/bash
# /usr/local/bin/encrypt-workload.sh
# Chiffrer workload avec GPG

WORKLOAD_FILE=$1
ENCRYPTED_FILE="${WORKLOAD_FILE}.gpg"
GPG_RECIPIENT="dba@example.com"

# Chiffrer
gpg --encrypt --recipient "$GPG_RECIPIENT" --output "$ENCRYPTED_FILE" "$WORKLOAD_FILE"

# Supprimer original
shred -u "$WORKLOAD_FILE"

echo "Workload encrypted: $ENCRYPTED_FILE"
```

---

## Optimisation de performance

### Impact de la capture sur production

**Benchmark impact :**

```
Test environment:
- MariaDB 11.8 Galera (3 n≈ìuds)
- MaxScale 25.01
- Workload: 5,000 QPS (70% SELECT, 30% WRITE)

Sans capture:
- Latence P50: 8ms
- Latence P95: 25ms
- Latence P99: 45ms

Avec capture (100%, format JSON):
- Latence P50: 10ms (+25%)
- Latence P95: 30ms (+20%)
- Latence P99: 55ms (+22%)

Avec capture (100%, format binaire):
- Latence P50: 9ms (+12%)
- Latence P95: 27ms (+8%)
- Latence P99: 48ms (+7%)

Avec capture (50%, format binaire):
- Latence P50: 8.5ms (+6%)
- Latence P95: 26ms (+4%)
- Latence P99: 46ms (+2%)

Conclusion: Impact acceptable si format binaire + √©chantillonnage
```

### Configuration optimis√©e pour production

```ini
[Production-Optimized-Capture]
type = service
router = readwritesplit
servers = server1, server2, server3

# Capture avec impact minimal
workload_capture = true
workload_capture_format = binary  # Plus rapide que JSON
workload_capture_sample_rate = 25  # 25% √©chantillonnage

# Filtres performance
workload_capture_min_duration = 50ms  # Ignore requ√™tes rapides
workload_capture_query_types = SELECT,INSERT,UPDATE,DELETE  # Exclure DDL

# Buffer et flush optimis√©s
workload_capture_buffer_size = 256MB  # Buffer large
workload_capture_flush_interval = 5s  # Flush moins fr√©quent

# Rotation automatique
workload_capture_max_file_size = 5GB
workload_capture_rotate_on_size = true

# Compression √† la vol√©e (si support CPU)
workload_capture_compress = true
workload_capture_compression_level = 3  # 1-9 (3 = bon compromis)
```

### Monitoring de la capture

```bash
# M√©triques Prometheus pour capture
maxscale_workload_capture_queries_total
maxscale_workload_capture_bytes_written
maxscale_workload_capture_buffer_usage_percent
maxscale_workload_capture_flush_duration_ms

# Alertes recommand√©es
alert: WorkloadCaptureBufferFull
expr: maxscale_workload_capture_buffer_usage_percent > 90
for: 5m

alert: WorkloadCaptureSlowFlush
expr: maxscale_workload_capture_flush_duration_ms > 1000
for: 5m
```

---

## Gestion du stockage

### Sizing du stockage

**Calcul approximatif :**

```
Formule:
Taille fichier = QPS √ó taille_moyenne_requ√™te √ó dur√©e_capture √ó (1 / sample_rate)

Exemple:
- QPS: 1,000
- Taille moyenne requ√™te (JSON): 500 bytes
- Dur√©e: 1 heure = 3,600 secondes
- Sample rate: 100%

Taille = 1,000 √ó 500 √ó 3,600 √ó 1.0 = 1,800,000,000 bytes ‚âà 1.8 GB

Avec compression gzip (ratio ~5:1):
1.8 GB / 5 ‚âà 360 MB

Recommandation stockage:
- 1 semaine de captures (7 jours √ó 2h pointe √ó 360MB) = ~5 GB
- Avec marge s√©curit√© √ó2 = 10 GB
```

### Strat√©gie de r√©tention

**Script de gestion automatique :**

```bash
#!/bin/bash
# /usr/local/bin/workload-retention-policy.sh
# Politique de r√©tention des workloads

WORKLOAD_DIR="/var/lib/maxscale/workloads"
ARCHIVE_DIR="/var/lib/maxscale/workloads/archive"
S3_BUCKET="s3://backups/maxscale-workloads"

# Cr√©er r√©pertoires si n√©cessaire
mkdir -p "$ARCHIVE_DIR"

# === Compression ===
# Compresser fichiers de plus de 1 jour non compress√©s
find "$WORKLOAD_DIR" -name "*.log" -mtime +1 -exec gzip {} \;

# === Archivage local ===
# D√©placer fichiers de plus de 7 jours vers archive
find "$WORKLOAD_DIR" -name "*.log.gz" -mtime +7 -exec mv {} "$ARCHIVE_DIR" \;

# === Archivage distant (S3) ===
# Uploader archives vers S3
aws s3 sync "$ARCHIVE_DIR" "$S3_BUCKET" --storage-class STANDARD_IA

# === Cleanup ===
# Supprimer fichiers locaux de plus de 30 jours
find "$ARCHIVE_DIR" -name "*.log.gz" -mtime +30 -delete

# === Reporting ===
CURRENT_SIZE=$(du -sh "$WORKLOAD_DIR" | cut -f1)
echo "$(date): Workload storage: $CURRENT_SIZE" >> /var/log/maxscale/retention.log
```

**Crontab :**

```cron
# /etc/cron.d/workload-retention
# Ex√©cuter tous les jours √† 2h du matin
0 2 * * * root /usr/local/bin/workload-retention-policy.sh
```

---

## Analyse des workloads captur√©s

### Outil d'analyse int√©gr√©

```bash
# Analyse statistique d'un workload
maxscale-analyze-workload \
    --input workload_20241215.log.gz \
    --output analysis_report.html \
    --format html

# Rapport g√©n√©r√©:
```

**Contenu du rapport HTML :**

```html
<!DOCTYPE html>
<html>
<head>
    <title>Workload Analysis Report</title>
</head>
<body>
    <h1>Workload Analysis Report</h1>
    <p>File: workload_20241215.log.gz</p>
    <p>Period: 2024-12-15 10:00:00 - 12:00:00 (2 hours)</p>
    
    <h2>Summary Statistics</h2>
    <table>
        <tr><td>Total queries:</td><td>1,245,678</td></tr>
        <tr><td>QPS (avg):</td><td>173</td></tr>
        <tr><td>QPS (peak):</td><td>542</td></tr>
        <tr><td>Unique queries:</td><td>3,456</td></tr>
    </table>
    
    <h2>Query Type Distribution</h2>
    <ul>
        <li>SELECT: 870,974 (70%)</li>
        <li>INSERT: 248,935 (20%)</li>
        <li>UPDATE: 99,574 (8%)</li>
        <li>DELETE: 24,893 (2%)</li>
    </ul>
    
    <h2>Top 10 Slowest Queries</h2>
    <table>
        <tr>
            <th>Query</th>
            <th>Avg Time</th>
            <th>Count</th>
        </tr>
        <tr>
            <td>SELECT * FROM orders WHERE...</td>
            <td>850ms</td>
            <td>234</td>
        </tr>
        <!-- ... -->
    </table>
    
    <h2>Top 10 Most Frequent Queries</h2>
    <!-- ... -->
    
    <h2>Performance Recommendations</h2>
    <ul>
        <li>Add index on orders.status (improve 234 queries by ~80%)</li>
        <li>Consider partitioning logs table (reduce scan time)</li>
        <li>Cache result of summary query (called 15,234 times)</li>
    </ul>
</body>
</html>
```

### Script d'analyse custom

```python
#!/usr/bin/env python3
# /usr/local/bin/analyze-workload-custom.py
# Analyse personnalis√©e de workload

import json
import gzip
from collections import defaultdict, Counter
from datetime import datetime

class WorkloadAnalyzer:
    def __init__(self, workload_file):
        self.workload_file = workload_file
        self.queries = []
        self.load_workload()
    
    def load_workload(self):
        """Charger workload depuis fichier"""
        opener = gzip.open if self.workload_file.endswith('.gz') else open
        with opener(self.workload_file, 'rt') as f:
            data = json.load(f)
            self.queries = data.get('queries', [])
    
    def analyze_query_patterns(self):
        """Analyser patterns de requ√™tes"""
        patterns = defaultdict(list)
        
        for query in self.queries:
            # Normaliser requ√™te (remplacer valeurs par ?)
            normalized = self.normalize_query(query['query'])
            patterns[normalized].append(query)
        
        # Statistiques par pattern
        stats = {}
        for pattern, queries in patterns.items():
            stats[pattern] = {
                'count': len(queries),
                'avg_time': sum(q.get('execution_time_ms', 0) for q in queries) / len(queries),
                'total_time': sum(q.get('execution_time_ms', 0) for q in queries),
            }
        
        # Trier par temps total
        sorted_stats = sorted(stats.items(), key=lambda x: x[1]['total_time'], reverse=True)
        
        print("Top 10 Query Patterns by Total Time:")
        for pattern, stat in sorted_stats[:10]:
            print(f"\nPattern: {pattern[:100]}...")
            print(f"  Count: {stat['count']}")
            print(f"  Avg time: {stat['avg_time']:.2f}ms")
            print(f"  Total time: {stat['total_time']:.2f}ms")
    
    def analyze_temporal_distribution(self):
        """Analyser distribution temporelle"""
        hourly_counts = defaultdict(int)
        
        for query in self.queries:
            timestamp = datetime.fromisoformat(query['timestamp'].replace('Z', '+00:00'))
            hour = timestamp.hour
            hourly_counts[hour] += 1
        
        print("\nQuery Distribution by Hour:")
        for hour in sorted(hourly_counts.keys()):
            count = hourly_counts[hour]
            bar = '‚ñà' * (count // 100)
            print(f"{hour:02d}h: {count:6d} {bar}")
    
    def analyze_slow_queries(self, threshold_ms=100):
        """Identifier requ√™tes lentes"""
        slow_queries = [q for q in self.queries 
                       if q.get('execution_time_ms', 0) > threshold_ms]
        
        print(f"\nSlow Queries (>{threshold_ms}ms): {len(slow_queries)}")
        print(f"Percentage: {len(slow_queries) / len(self.queries) * 100:.2f}%")
        
        # Top 5 plus lentes
        sorted_slow = sorted(slow_queries, 
                            key=lambda x: x.get('execution_time_ms', 0), 
                            reverse=True)
        
        print("\nTop 5 Slowest Queries:")
        for i, query in enumerate(sorted_slow[:5], 1):
            print(f"\n{i}. {query.get('execution_time_ms', 0)}ms")
            print(f"   {query['query'][:100]}...")
    
    def normalize_query(self, query):
        """Normaliser requ√™te pour pattern matching"""
        import re
        # Remplacer nombres
        normalized = re.sub(r'\b\d+\b', '?', query)
        # Remplacer strings
        normalized = re.sub(r"'[^']*'", '?', normalized)
        normalized = re.sub(r'"[^"]*"', '?', normalized)
        return normalized

# Utilisation
if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print("Usage: analyze-workload-custom.py <workload_file>")
        sys.exit(1)
    
    analyzer = WorkloadAnalyzer(sys.argv[1])
    analyzer.analyze_query_patterns()
    analyzer.analyze_temporal_distribution()
    analyzer.analyze_slow_queries(threshold_ms=100)
```

---

## ‚úÖ Points cl√©s √† retenir

- **Workload Capture** : Nouvelle fonctionnalit√© MaxScale 25.01 pour enregistrer trafic SQL r√©el
- **Cas d'usage** : Validation upgrades, tests configs, benchmarks infra, analyse optimisation
- **Configuration** : `workload_capture=true`, fichier output, format (JSON/binary), dur√©e, sample_rate
- **Filtrage** : Par query type, user, database, dur√©e minimale pour r√©duire volume
- **√âchantillonnage** : Uniforme (10-50%), lent seulement (>100ms), adaptatif selon charge
- **Anonymisation** : Essentiel pour s√©curit√©, hash/mask/fake data pour PII
- **Impact performance** : +2-25% latence selon format/sample_rate, binaire + √©chantillonnage minimal
- **Format JSON** : Lisible, flexible, id√©al analyse, ~500 bytes/requ√™te
- **Format binaire** : Compact, rapide, production haute charge, n√©cessite conversion
- **Stockage** : ~1.8GB/heure @ 1000 QPS (JSON), compression gzip ratio 5:1
- **R√©tention** : 7j local compress√©, 30j archive, upload S3 Standard-IA
- **Analyse** : Outil int√©gr√© + scripts custom, patterns, distribution temporelle, slow queries

---

## üîó Ressources et r√©f√©rences

### Documentation officielle MaxScale
- [üÜï Workload Capture](https://mariadb.com/docs/server/operations/workload-testing/capture/)
- [üìñ MaxScale 25.01 Release Notes](https://mariadb.com/docs/release-notes/maxscale/25-01/)
- [üìñ MaxScale Configuration](https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-configuration-guide/)

### Guides techniques
- [Workload Testing Best Practices](https://mariadb.com/resources/blog/workload-testing-best-practices/)
- [Database Performance Analysis](https://www.percona.com/blog/database-performance-analysis/)

### Outils
- [jq - JSON processor](https://stedolan.github.io/jq/)
- [Faker - Fake data generator](https://faker.readthedocs.io/)
- [GPG - Encryption](https://gnupg.org/)

---

## ‚û°Ô∏è Section suivante

**14.5.2 Workload Replay** : Rejeu des workloads captur√©s, validation upgrades, benchmarking, comparaison de r√©sultats, analyse de diff√©rences, et automatisation des tests de r√©gression avec MaxScale 25.01.

---


‚è≠Ô∏è [Workload Replay](/14-haute-disponibilite/05.2-workload-replay.md)
