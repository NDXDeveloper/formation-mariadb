ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.8 StratÃ©gies de RÃ©cupÃ©ration aprÃ¨s Incident

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : Sections 14.1-14.7, expÃ©rience opÃ©rationnelle en gestion d'incidents

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- **Concevoir** une stratÃ©gie complÃ¨te de disaster recovery (DR)
- **DÃ©finir** des objectifs RPO/RTO rÃ©alistes et mesurables
- **Documenter** des procÃ©dures de rÃ©cupÃ©ration testÃ©es et validÃ©es
- **RÃ©agir** efficacement aux diffÃ©rents types d'incidents
- **Tester** rÃ©guliÃ¨rement vos plans de rÃ©cupÃ©ration
- **Conduire** des post-mortems constructifs
- **AmÃ©liorer** continuellement votre rÃ©silience
- **Communiquer** efficacement durant les crises

---

## Introduction

Le **disaster recovery (DR)** n'est pas une question de "si" mais de "quand". Tout systÃ¨me, mÃªme parfaitement conÃ§u, finira par rencontrer un incident. Ce qui distingue les organisations matures n'est pas l'absence d'incidents, mais leur capacitÃ© Ã  rÃ©cupÃ©rer rapidement et Ã  apprendre de chaque Ã©chec.

**Statistiques industrie** :
```
93% des entreprises sans plan DR qui subissent une perte de donnÃ©es 
    majeure font faillite dans les 5 ans (Source: National Archives)

70% des petites entreprises ferment dans l'annÃ©e suivant un dÃ©sastre majeur
    (Source: FEMA)

CoÃ»t moyen downtime : 5,600$ par minute (Source: Gartner 2024)
    = 336,000$ par heure
    = 8M$ par jour
```

> âš ï¸ **RÃ©alitÃ© Brutale** : "Tout le monde a un plan jusqu'Ã  ce qu'ils se prennent un coup dans la figure." - Mike Tyson (adaptable au DR)

**Principe fondamental** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Un plan DR non testÃ© = Pas de plan DR            â”‚
â”‚                                                   â”‚
â”‚  Tester 1Ã— par an = Insuffisant                   â”‚
â”‚  Tester 1Ã— par trimestre = Minimum                â”‚
â”‚  Tester 1Ã— par mois = RecommandÃ©                  â”‚
â”‚  Chaos engineering continu = Optimal              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Planification RPO/RTO

### 1.1 DÃ©finitions et Calculs

#### **RTO (Recovery Time Objective)**

```
RTO = Temps maximum acceptable de downtime

Calcul du coÃ»t RTO :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Exemple E-commerce                             â”‚
â”‚  Revenue moyen : 10,000â‚¬/heure                  â”‚
â”‚                                                 â”‚
â”‚  RTO 1 heure  â†’ Perte max : 10,000â‚¬             â”‚
â”‚  RTO 4 heures â†’ Perte max : 40,000â‚¬             â”‚
â”‚  RTO 1 jour   â†’ Perte max : 240,000â‚¬            â”‚
â”‚                                                 â”‚
â”‚  + CoÃ»ts indirects :                            â”‚
â”‚    - Perte de confiance clients                 â”‚
â”‚    - PÃ©nalitÃ©s SLA                              â”‚
â”‚    - CoÃ»ts Ã©quipe d'urgence                     â”‚
â”‚    - Image de marque                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composantes du RTO** :
```
RTO Total = DÃ©tection + DÃ©cision + RÃ©cupÃ©ration + Validation

Exemple breakdown :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase                â”‚ DurÃ©e    â”‚ Optimisable â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DÃ©tection incident   â”‚ 5 min    â”‚ âœ… Alerting â”‚
â”‚ Analyse initiale     â”‚ 10 min   â”‚ âœ… Runbooks â”‚
â”‚ DÃ©cision recovery    â”‚ 5 min    â”‚ âœ… Auto     â”‚
â”‚ Execution failover   â”‚ 2 min    â”‚ âœ… Auto     â”‚
â”‚ Validation service   â”‚ 3 min    â”‚ âš ï¸ Manuel   â”‚
â”‚ Communication        â”‚ 5 min    â”‚ âš ï¸ Manuel   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL                â”‚ 30 min   â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Objectif : RTO < 30 minutes
```

#### **RPO (Recovery Point Objective)**

```
RPO = Perte de donnÃ©es maximum acceptable

Calcul du coÃ»t RPO :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Exemple Banking                                â”‚
â”‚  Transactions moyennes : 1,000/minute           â”‚
â”‚  Valeur moyenne transaction : 500â‚¬              â”‚
â”‚                                                 â”‚
â”‚  RPO 1 minute  â†’ Perte : 1,000 tx (500,000â‚¬)    â”‚
â”‚  RPO 5 minutes â†’ Perte : 5,000 tx (2,500,000â‚¬)  â”‚
â”‚  RPO 1 heure   â†’ Perte : 60,000 tx (30Mâ‚¬)       â”‚
â”‚                                                 â”‚
â”‚  + CoÃ»ts de reconstitution manuelle             â”‚
â”‚  + Implications lÃ©gales/rÃ©glementaires          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technologies par RPO** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RPO Objectif â”‚ Solution          â”‚ CoÃ»t Relatif         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0 seconde    â”‚ Galera Cluster    â”‚ â‚¬â‚¬â‚¬â‚¬â‚¬ (trÃ¨s Ã©levÃ©)   â”‚
â”‚              â”‚ Synchronous Rep   â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 1 minute   â”‚ Semi-Sync Rep     â”‚ â‚¬â‚¬â‚¬â‚¬ (Ã©levÃ©)         â”‚
â”‚              â”‚ DRBD              â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 5 minutes  â”‚ Async Rep         â”‚ â‚¬â‚¬â‚¬ (moyen)          â”‚
â”‚              â”‚ + Monitoring      â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 1 heure    â”‚ Async Rep         â”‚ â‚¬â‚¬ (faible)          â”‚
â”‚              â”‚ + Backup incr.    â”‚                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 24 heures  â”‚ Backup quotidien  â”‚ â‚¬ (trÃ¨s faible)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Matrice RPO/RTO par Type d'Application

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Type Application    â”‚ RPO Typique  â”‚ RTO Typique       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Banking (core)      â”‚ 0 sec        â”‚ < 1 min           â”‚
â”‚  Trading             â”‚ 0 sec        â”‚ < 30 sec          â”‚
â”‚  E-commerce          â”‚ < 5 min      â”‚ < 15 min          â”‚
â”‚  SaaS B2B            â”‚ < 15 min     â”‚ < 1 heure         â”‚
â”‚  CMS/Blog            â”‚ < 1 heure    â”‚ < 4 heures        â”‚
â”‚  Internal tools      â”‚ < 4 heures   â”‚ < 1 jour          â”‚
â”‚  Analytics/BI        â”‚ < 24 heures  â”‚ < 2 jours         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Framework de DÃ©cision RPO/RTO

```python
#!/usr/bin/env python3
# rpo_rto_calculator.py

class RPORTOCalculator:
    """Calculateur RPO/RTO basÃ© sur impact business"""
    
    def __init__(self):
        self.revenue_per_hour = 0
        self.transactions_per_minute = 0
        self.avg_transaction_value = 0
        self.reputation_cost_multiplier = 1.5
        
    def calculate_rto_cost(self, rto_hours):
        """Calcule le coÃ»t d'un RTO donnÃ©"""
        direct_cost = self.revenue_per_hour * rto_hours
        indirect_cost = direct_cost * (self.reputation_cost_multiplier - 1)
        total_cost = direct_cost + indirect_cost
        
        return {
            'rto_hours': rto_hours,
            'direct_cost': direct_cost,
            'indirect_cost': indirect_cost,
            'total_cost': total_cost
        }
    
    def calculate_rpo_cost(self, rpo_minutes):
        """Calcule le coÃ»t d'un RPO donnÃ©"""
        lost_transactions = self.transactions_per_minute * rpo_minutes
        data_loss_cost = lost_transactions * self.avg_transaction_value
        reconstitution_cost = lost_transactions * 10  # 10â‚¬ par transaction Ã  recrÃ©er
        
        return {
            'rpo_minutes': rpo_minutes,
            'lost_transactions': lost_transactions,
            'data_loss_cost': data_loss_cost,
            'reconstitution_cost': reconstitution_cost,
            'total_cost': data_loss_cost + reconstitution_cost
        }
    
    def recommend_solution(self, max_budget):
        """Recommande une solution basÃ©e sur le budget"""
        solutions = [
            {
                'name': 'Galera Cluster 5 nodes + MaxScale HA',
                'rpo': 0,
                'rto_minutes': 2,
                'annual_cost': 150000,
                'availability': 99.99
            },
            {
                'name': 'Semi-Sync Replication + Auto-failover',
                'rpo': 5,
                'rto_minutes': 5,
                'annual_cost': 80000,
                'availability': 99.95
            },
            {
                'name': 'Async Replication + Manual failover',
                'rpo': 60,
                'rto_minutes': 30,
                'annual_cost': 40000,
                'availability': 99.9
            },
            {
                'name': 'Daily backups + DR site',
                'rpo': 1440,  # 24 hours
                'rto_minutes': 240,  # 4 hours
                'annual_cost': 15000,
                'availability': 99.5
            }
        ]
        
        affordable_solutions = [s for s in solutions if s['annual_cost'] <= max_budget]
        
        if not affordable_solutions:
            return None
        
        # Trier par meilleur RTO
        return sorted(affordable_solutions, key=lambda x: x['rto_minutes'])[0]

# Exemple d'utilisation
if __name__ == "__main__":
    calc = RPORTOCalculator()
    
    # E-commerce moyen
    calc.revenue_per_hour = 10000
    calc.transactions_per_minute = 50
    calc.avg_transaction_value = 80
    
    # Calculer coÃ»ts pour diffÃ©rents RTO
    for rto in [0.5, 1, 4, 24]:
        cost = calc.calculate_rto_cost(rto)
        print(f"\nRTO {rto}h: CoÃ»t total = {cost['total_cost']:,.0f}â‚¬")
    
    # Recommandation
    budget = 100000
    solution = calc.recommend_solution(budget)
    print(f"\n\nBudget: {budget:,}â‚¬")
    print(f"Solution recommandÃ©e: {solution['name']}")
    print(f"RPO: {solution['rpo']} min, RTO: {solution['rto_minutes']} min")
    print(f"DisponibilitÃ©: {solution['availability']}%")
```

---

## 2. Types d'Incidents et ProcÃ©dures

### 2.1 Classification des Incidents

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SeveritÃ© â”‚ DÃ©finition              â”‚ Exemples         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P0       â”‚ Service complÃ¨tement    â”‚ - Cluster down   â”‚
â”‚  CRITICAL â”‚ down, perte financiÃ¨re  â”‚ - Data loss      â”‚
â”‚           â”‚ importante              â”‚ - Security breachâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P1       â”‚ Service dÃ©gradÃ©,        â”‚ - 1 node down    â”‚
â”‚  HIGH     â”‚ impact utilisateurs     â”‚ - High latency   â”‚
â”‚           â”‚ significatif            â”‚ - Repl lag       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P2       â”‚ Impact limitÃ©,          â”‚ - Slow queries   â”‚
â”‚  MEDIUM   â”‚ workaround possible     â”‚ - Disk 80% full  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P3       â”‚ Impact minime,          â”‚ - Log warnings   â”‚
â”‚  LOW      â”‚ pas d'urgence           â”‚ - Monitoring gap â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temps de rÃ©ponse requis :
P0 : ImmÃ©diat (< 15 minutes)
P1 : < 1 heure
P2 : < 4 heures
P3 : < 1 jour ouvrÃ©
```

### 2.2 ScÃ©nario 1 : Panne MatÃ©rielle (Serveur Down)

#### **Phase 1 : DÃ©tection et Alerte**

```bash
#!/bin/bash
# /usr/local/bin/detect_server_down.sh

SERVER="db-master.example.com"
ALERT_CHANNEL="#database-alerts"

# Tentative ping
if ! ping -c 3 -W 2 $SERVER &>/dev/null; then
    # Serveur inaccessible
    
    # Double-check via monitoring secondaire
    if ! curl -f http://monitoring-backup/check/$SERVER &>/dev/null; then
        # ConfirmÃ© down
        
        # Alert P0
        send_pagerduty_alert "P0: Database server $SERVER down" "critical"
        send_slack_alert "$ALERT_CHANNEL" "ğŸš¨ P0 INCIDENT: $SERVER DOWN"
        
        # Lancer procÃ©dure failover automatique
        if [ "$AUTO_FAILOVER_ENABLED" = "true" ]; then
            /usr/local/bin/trigger_failover.sh $SERVER
        else
            echo "Manual failover required - awaiting ops intervention"
        fi
    fi
fi
```

#### **Phase 2 : ProcÃ©dure de RÃ©cupÃ©ration**

```markdown
# RUNBOOK: Panne Serveur Master

## DÃ©tection
âœ… Alert PagerDuty reÃ§ue
âœ… VÃ©rifier dashboard monitoring

## Validation
[ ] Confirmer serveur rÃ©ellement down (pas faux positif)
    - Ping depuis 2+ sources diffÃ©rentes
    - Check console hyperviseur (si VM)
    - Check IPMI/iLO (si bare metal)

## DÃ©cision Failover
[ ] VÃ©rifier state replicas
    ```bash
    for replica in replica1 replica2 replica3; do
        mysql -h $replica -e "SHOW SLAVE STATUS\G"
    done
    ```

[ ] SÃ©lectionner best candidate
    - Lag minimal
    - GTID position la plus avancÃ©e
    - Ressources disponibles (CPU, RAM, Disk)

## ExÃ©cution Failover (automatique ou manuel)

### Automatique (mariadbmon)
[ ] VÃ©rifier logs MaxScale
    ```bash
    tail -f /var/log/maxscale/maxscale.log | grep -i failover
    ```

[ ] Attendre promotion automatique (~60s)

### Manuel
[ ] Promouvoir replica sÃ©lectionnÃ©e
    ```bash
    REPLICA="replica1.example.com"
    
    # Sur la replica Ã  promouvoir
    mysql -h $REPLICA << EOF
    STOP SLAVE;
    RESET SLAVE ALL;
    SET GLOBAL read_only = OFF;
    EOF
    ```

[ ] Reconfigurer autres replicas
    ```bash
    for other in replica2 replica3; do
        mysql -h $other << EOF
        STOP SLAVE;
        CHANGE MASTER TO 
            MASTER_HOST='$REPLICA',
            MASTER_USER='repl_user',
            MASTER_PASSWORD='ReplicationPassword',
            MASTER_USE_GTID=current_pos;
        START SLAVE;
        EOF
    done
    ```

[ ] Basculer VIP (si keepalived non auto)
    ```bash
    # Sur nouveau master
    systemctl stop keepalived
    systemctl start keepalived
    # VÃ©rifier VIP assignÃ©e
    ip addr show | grep 10.0.1.100
    ```

## Validation
[ ] Tester connexion via VIP
    ```bash
    mysql -h 10.0.1.100 -u app -p -e "SELECT @@hostname, @@read_only"
    # hostname = nouveau master
    # read_only = OFF
    ```

[ ] VÃ©rifier rÃ©plication autres replicas
    ```bash
    for replica in replica2 replica3; do
        mysql -h $replica -e "SHOW SLAVE STATUS\G" | grep Seconds_Behind_Master
    done
    # Doit Ãªtre < 10 secondes
    ```

[ ] Test applicatif
    ```bash
    # Test write
    mysql -h 10.0.1.100 -u app -p << EOF
    CREATE TABLE IF NOT EXISTS test_failover (
        id INT AUTO_INCREMENT PRIMARY KEY,
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    INSERT INTO test_failover VALUES ();
    SELECT * FROM test_failover ORDER BY id DESC LIMIT 1;
    EOF
    ```

## Communication
[ ] Update incident ticket
[ ] Notify stakeholders (Slack, Email)
[ ] Update status page

## Post-Incident (aprÃ¨s stabilisation)
[ ] Investiguer cause panne master original
[ ] Planifier reconstruction master original
[ ] Scheduler post-mortem meeting
[ ] Documenter timeline incident

## RTO VisÃ©: < 5 minutes
## Statut: [ ] RÃ©solu  [ ] En cours  [ ] EscaladÃ©
```

### 2.3 ScÃ©nario 2 : Corruption de DonnÃ©es

#### **DÃ©tection**

```sql
-- SymptÃ´mes typiques
-- 1. Erreurs de cohÃ©rence InnoDB
ERROR 1030 (HY000): Got error 168 from storage engine

-- 2. Inconsistencies dÃ©tectÃ©es
CHECK TABLE orders;
-- Table is marked as crashed

-- 3. RÃ©plication cassÃ©e
SHOW SLAVE STATUS\G
# Last_SQL_Error: Error 'Duplicate entry' on query

-- 4. Queries retournent rÃ©sultats invalides
SELECT COUNT(*) FROM accounts WHERE balance < 0;
-- 52 rows (impossible normalement !)
```

#### **ProcÃ©dure de RÃ©cupÃ©ration**

```bash
#!/bin/bash
# /usr/local/bin/recover_from_corruption.sh

set -e

CORRUPTED_SERVER="db-master.example.com"
BACKUP_LOCATION="/mnt/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

echo "=== Data Corruption Recovery ==="
echo "Server: $CORRUPTED_SERVER"
echo "Timestamp: $TIMESTAMP"

# 1. STOP WRITES IMMEDIATELY
echo "[1/8] Stopping writes..."
mysql -h $CORRUPTED_SERVER -e "SET GLOBAL read_only = ON"

# 2. Isoler serveur corrompu
echo "[2/8] Isolating corrupted server..."
# Retirer du pool MaxScale
maxctrl unlink service Read-Write-Service $CORRUPTED_SERVER
# Ou arrÃªter keepalived
ssh $CORRUPTED_SERVER "systemctl stop keepalived"

# 3. Snapshot Ã©tat actuel (forensics)
echo "[3/8] Creating forensic snapshot..."
mysqldump -h $CORRUPTED_SERVER --all-databases \
    --single-transaction \
    --routines --triggers \
    --events \
    > /tmp/corrupted_snapshot_${TIMESTAMP}.sql

# 4. Identifier dernier backup sain
echo "[4/8] Identifying last good backup..."
LAST_BACKUP=$(ls -t $BACKUP_LOCATION/*.xbstream | head -1)
echo "Using backup: $LAST_BACKUP"

# 5. Restauration sur serveur temporaire
echo "[5/8] Restoring to temporary server..."
TEMP_SERVER="db-temp-recovery.example.com"

ssh $TEMP_SERVER << 'EOF'
    systemctl stop mariadb
    rm -rf /var/lib/mysql/*
    
    # Restore mariabackup
    mariabackup --decompress --remove-original \
        --target-dir=/var/lib/mysql
    mariabackup --prepare --target-dir=/var/lib/mysql
    
    chown -R mysql:mysql /var/lib/mysql
    systemctl start mariadb
EOF

# 6. Replay binlogs depuis backup jusqu'au point de corruption
echo "[6/8] Replaying binary logs..."
BACKUP_BINLOG_POS=$(ssh $TEMP_SERVER "mysql -N -e \"SELECT @@GLOBAL.gtid_binlog_pos\"")
CORRUPTION_TIME="2025-12-13 14:32:00"  # Time corruption detected

mysqlbinlog --start-position=$BACKUP_BINLOG_POS \
            --stop-datetime="$CORRUPTION_TIME" \
            /var/lib/mysql-binlogs/binlog.* | \
    mysql -h $TEMP_SERVER

# 7. Validation donnÃ©es restaurÃ©es
echo "[7/8] Validating restored data..."
ssh $TEMP_SERVER << 'EOF'
    # Check tables
    mysqlcheck --all-databases --check
    
    # Verify critical tables
    mysql -e "SELECT COUNT(*) FROM accounts WHERE balance < 0"
    # Doit retourner 0
    
    # Verify consistency
    mysql -e "
        SELECT 
            (SELECT SUM(amount) FROM transactions WHERE type='debit') as total_debit,
            (SELECT SUM(amount) FROM transactions WHERE type='credit') as total_credit,
            (SELECT SUM(balance) FROM accounts) as total_balance
    "
    # total_credit - total_debit = total_balance (must match)
EOF

# 8. Promouvoir serveur restaurÃ©
echo "[8/8] Promoting restored server as new master..."
# Point de dÃ©cision : restaurer sur serveur original ou promouvoir temp ?

# Option A: Restaurer sur serveur original
# - ArrÃªter serveur original
# - Rsync depuis temp vers original
# - RedÃ©marrer original

# Option B: Promouvoir temp (plus rapide)
# - Reconfigurer replicas vers temp
# - Basculer VIP vers temp
# - DÃ©commissionner original

echo "=== Recovery Complete ==="
echo "Next steps:"
echo "1. Investigate root cause of corruption"
echo "2. Review binlogs for unauthorized changes"
echo "3. Update monitoring to detect earlier"
echo "4. Schedule post-mortem"
```

### 2.4 ScÃ©nario 3 : Disaster Complet (Datacenter Down)

#### **Architecture Multi-DC Required**

```
Primary DC (Paris)          Disaster Recovery DC (Londres)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Galera Node 1    â”‚â”€â”€â”€â”€â”€â”€â”€â”‚  Galera Node 4    â”‚
â”‚  Galera Node 2    â”‚       â”‚  Galera Node 5    â”‚
â”‚  Galera Node 3    â”‚       â”‚                   â”‚
â”‚  MaxScale 1       â”‚       â”‚  MaxScale 2       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€ Garbd (Dublin) â”€â”€â”€â”˜
                   (Arbitrator)
```

#### **ProcÃ©dure DR ComplÃ¨te**

```bash
#!/bin/bash
# /usr/local/bin/dr_failover_complete.sh

# SCENARIO: Primary DC (Paris) complÃ¨tement inaccessible

echo "=== DISASTER RECOVERY PROCEDURE ==="
echo "PRIMARY DC DOWN - Activating DR site"

DR_SITE="london"
DR_NODES=("db-london-1" "db-london-2")
DR_MAXSCALE="maxscale-london"

# 1. Validation DR site opÃ©rationnel
echo "[1/6] Validating DR site..."
for node in "${DR_NODES[@]}"; do
    if ! mysql -h $node -e "SELECT 1" &>/dev/null; then
        echo "ERROR: DR node $node not accessible"
        exit 1
    fi
done

# 2. VÃ©rifier Ã©tat Galera DR
echo "[2/6] Checking Galera cluster state..."
CLUSTER_SIZE=$(mysql -h ${DR_NODES[0]} -N -e \
    "SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
     WHERE VARIABLE_NAME='wsrep_cluster_size'")

if [ "$CLUSTER_SIZE" -lt 2 ]; then
    echo "WARNING: Only $CLUSTER_SIZE nodes in DR cluster"
fi

# 3. Bootstrap DR cluster si nÃ©cessaire
echo "[3/6] Ensuring DR cluster is PRIMARY..."
CLUSTER_STATUS=$(mysql -h ${DR_NODES[0]} -N -e \
    "SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS 
     WHERE VARIABLE_NAME='wsrep_cluster_status'")

if [ "$CLUSTER_STATUS" != "Primary" ]; then
    echo "Bootstrapping DR cluster..."
    # Sur le nÅ“ud le plus Ã  jour
    ssh ${DR_NODES[0]} "
        mysql -e \"SET GLOBAL wsrep_provider_options='pc.bootstrap=true'\"
    "
fi

# 4. Activer Ã©critures sur DR
echo "[4/6] Enabling writes on DR cluster..."
for node in "${DR_NODES[@]}"; do
    mysql -h $node -e "SET GLOBAL read_only = OFF"
done

# 5. Basculer DNS/VIP vers DR
echo "[5/6] Updating DNS to point to DR site..."

# Option A: DNS (propagation lente, 5-60 minutes)
aws route53 change-resource-record-sets \
    --hosted-zone-id Z1234567890ABC \
    --change-batch "{
        \"Changes\": [{
            \"Action\": \"UPSERT\",
            \"ResourceRecordSet\": {
                \"Name\": \"db.example.com\",
                \"Type\": \"CNAME\",
                \"TTL\": 60,
                \"ResourceRecords\": [{
                    \"Value\": \"db-london.example.com\"
                }]
            }
        }]
    }"

# Option B: Global Load Balancer (propagation rapide, ~30s)
# CloudFlare, AWS Global Accelerator, Azure Traffic Manager

# 6. Validation end-to-end
echo "[6/6] Validating DR activation..."

# Test Ã©criture
mysql -h db.example.com -u app -p << EOF
CREATE TABLE IF NOT EXISTS dr_test (
    id INT AUTO_INCREMENT PRIMARY KEY,
    activated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
INSERT INTO dr_test VALUES ();
EOF

# VÃ©rifier rÃ©plication intra-DR
for node in "${DR_NODES[@]}"; do
    COUNT=$(mysql -h $node -N -e "SELECT COUNT(*) FROM dr_test")
    echo "Node $node: $COUNT rows in dr_test"
done

echo "=== DR ACTIVATION COMPLETE ==="
echo ""
echo "CRITICAL NEXT STEPS:"
echo "1. Notify all stakeholders (exec, customers, teams)"
echo "2. Update status page"
echo "3. Monitor DR cluster closely (alerts, dashboards)"
echo "4. Investigate primary DC status"
echo "5. Plan return to primary DC when available"
echo ""
echo "RTO Achieved: Check incident timeline"
echo "Expected service restoration: IMMEDIATE"
```

---

## 3. Tests et Validation

### 3.1 Programme de Tests DR

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FrÃ©quence â”‚ Type de Test             â”‚ DurÃ©e          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Mensuel   â”‚ Failover automatique     â”‚ 1 heure        â”‚
â”‚            â”‚ (master â†’ replica)       â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Trimes.   â”‚ Restauration backup      â”‚ 4 heures       â”‚
â”‚            â”‚ complÃ¨te                 â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Trimes.   â”‚ Chaos engineering        â”‚ 2 heures       â”‚
â”‚            â”‚ (random node kill)       â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Semes.    â”‚ DR complet               â”‚ 8 heures       â”‚
â”‚            â”‚ (datacenter failover)    â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Annuel    â”‚ DR Full Drill            â”‚ 1-2 jours      â”‚
â”‚            â”‚ (simulation complÃ¨te)    â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Checklist Test Failover Mensuel

```markdown
# DR Test Checklist - Failover Master

**Date**: _____________
**Participants**: _____________
**Environnement**: [ ] Production  [ ] Staging

## PrÃ©-Test
- [ ] Notification Ã©quipes (24h avance)
- [ ] Backup complet < 24h
- [ ] Validation monitoring opÃ©rationnel
- [ ] Communication maintenance window (si prod)
- [ ] Rollback plan documentÃ©

## Phase 1: Baseline (T-15min)
- [ ] Capturer mÃ©triques baseline
    - QPS: _______
    - Latency P99: _______
    - Replication lag: _______
- [ ] VÃ©rifier Ã©tat cluster
    ```bash
    for server in master replica1 replica2; do
        echo "=== $server ==="
        mysql -h $server -e "SHOW MASTER STATUS\G"
        mysql -h $server -e "SHOW SLAVE STATUS\G"
    done
    ```
- [ ] Screenshot dashboards

## Phase 2: ExÃ©cution Failover (T+0)
- [ ] Timestamp dÃ©but: _____________
- [ ] MÃ©thode:
    - [ ] ArrÃªt propre master: `systemctl stop mariadb`
    - [ ] Crash simulation: `kill -9 $(pidof mariadbd)`
    - [ ] Network partition: `iptables -A INPUT -p tcp --dport 3306 -j DROP`
    
- [ ] Observer failover automatique
    - Temps dÃ©tection: _______ secondes
    - Temps promotion: _______ secondes
    - Nouveau master: _____________

## Phase 3: Validation (T+5min)
- [ ] VÃ©rifier nouveau master
    ```bash
    mysql -h NEW_MASTER -e "SHOW MASTER STATUS\G"
    mysql -h NEW_MASTER -e "SELECT @@read_only"  # Must be OFF
    ```

- [ ] VÃ©rifier replicas suivent nouveau master
    ```bash
    mysql -h replica1 -e "SHOW SLAVE STATUS\G" | grep Master_Host
    # Doit afficher NEW_MASTER
    ```

- [ ] Test Ã©criture applicative
    ```bash
    mysql -h VIP -u app -p << EOF
    INSERT INTO test_failover (test_id, timestamp) 
    VALUES (UUID(), NOW());
    EOF
    ```

- [ ] VÃ©rifier mÃ©triques post-failover
    - QPS: _______ (vs baseline: ______)
    - Latency P99: _______ (vs baseline: ______)
    - Errors: _______

## Phase 4: RÃ©cupÃ©ration Ancien Master (T+10min)
- [ ] RedÃ©marrer ancien master
    ```bash
    systemctl start mariadb
    ```

- [ ] VÃ©rifier rejoint comme replica
    ```bash
    mysql -h OLD_MASTER -e "SHOW SLAVE STATUS\G"
    ```

- [ ] Attendre synchronisation complÃ¨te
    ```bash
    while true; do
        LAG=$(mysql -h OLD_MASTER -N -e \
          "SELECT Seconds_Behind_Master FROM information_schema.SLAVE_STATUS")
        [ "$LAG" -eq 0 ] && break
        echo "Lag: ${LAG}s..."
        sleep 2
    done
    ```

## Phase 5: Cleanup (T+20min)
- [ ] VÃ©rifier logs erreurs
    ```bash
    tail -100 /var/log/mysql/error.log | grep -i error
    ```

- [ ] Supprimer donnÃ©es test
    ```bash
    mysql -e "DELETE FROM test_failover WHERE test_id IS NOT NULL"
    ```

- [ ] Restaurer Ã©tat initial (optionnel)
    - [ ] Switchover vers master original

## RÃ©sultats
- RTO MesurÃ©: _______ (objectif: < 5 min)
- RPO MesurÃ©: _______ (objectif: 0)
- SuccÃ¨s: [ ] Oui  [ ] Non
- Issues rencontrÃ©es:
    1. _____________
    2. _____________

## Actions Post-Test
- [ ] Documenter lessons learned
- [ ] CrÃ©er tickets pour issues
- [ ] Update runbooks si nÃ©cessaire
- [ ] Partager rapport avec Ã©quipe

**Responsable Test**: _____________
**Signature**: _____________
```

### 3.3 Chaos Engineering

```python
#!/usr/bin/env python3
# chaos_mariadb.py
"""
Chaos Engineering pour MariaDB
Simule pannes alÃ©atoires pour valider rÃ©silience
"""

import random
import time
import subprocess
from datetime import datetime

class MariaDBChaos:
    def __init__(self, nodes):
        self.nodes = nodes
        self.incidents = []
        
    def random_node_kill(self):
        """Tuer un nÅ“ud alÃ©atoire"""
        node = random.choice(self.nodes)
        print(f"[{datetime.now()}] Killing node: {node}")
        
        subprocess.run([
            "ssh", node, "systemctl stop mariadb"
        ])
        
        self.incidents.append({
            'timestamp': datetime.now(),
            'type': 'node_kill',
            'target': node
        })
        
        # Attendre 2 minutes
        time.sleep(120)
        
        # RedÃ©marrer
        print(f"[{datetime.now()}] Restarting node: {node}")
        subprocess.run([
            "ssh", node, "systemctl start mariadb"
        ])
    
    def network_partition(self, duration=60):
        """Simuler partition rÃ©seau"""
        node = random.choice(self.nodes)
        print(f"[{datetime.now()}] Network partition: {node}")
        
        # Bloquer port 3306 et 4567 (Galera)
        subprocess.run([
            "ssh", node, 
            "iptables -A INPUT -p tcp --dport 3306 -j DROP; "
            "iptables -A INPUT -p tcp --dport 4567 -j DROP"
        ])
        
        time.sleep(duration)
        
        # Restaurer
        print(f"[{datetime.now()}] Restoring network: {node}")
        subprocess.run([
            "ssh", node,
            "iptables -D INPUT -p tcp --dport 3306 -j DROP; "
            "iptables -D INPUT -p tcp --dport 4567 -j DROP"
        ])
    
    def disk_full_simulation(self):
        """Simuler disque plein"""
        node = random.choice(self.nodes)
        print(f"[{datetime.now()}] Disk full simulation: {node}")
        
        # CrÃ©er fichier de 10GB
        subprocess.run([
            "ssh", node,
            "dd if=/dev/zero of=/tmp/fillup bs=1G count=10"
        ])
        
        time.sleep(300)  # 5 minutes
        
        # Cleanup
        subprocess.run([
            "ssh", node,
            "rm -f /tmp/fillup"
        ])
    
    def run_chaos_day(self, duration_hours=4):
        """JournÃ©e chaos : incidents alÃ©atoires pendant N heures"""
        end_time = time.time() + (duration_hours * 3600)
        
        while time.time() < end_time:
            # Choisir type d'incident alÃ©atoire
            incident_type = random.choice([
                'node_kill',
                'network_partition',
                'disk_full'
            ])
            
            if incident_type == 'node_kill':
                self.random_node_kill()
            elif incident_type == 'network_partition':
                self.network_partition()
            elif incident_type == 'disk_full':
                self.disk_full_simulation()
            
            # Pause entre incidents (15-45 minutes)
            pause = random.randint(900, 2700)
            print(f"Next incident in {pause/60:.1f} minutes...")
            time.sleep(pause)
        
        print("\n=== Chaos Day Complete ===")
        print(f"Total incidents: {len(self.incidents)}")
        for incident in self.incidents:
            print(f"  {incident['timestamp']}: {incident['type']} on {incident['target']}")

if __name__ == "__main__":
    nodes = ["db1.example.com", "db2.example.com", "db3.example.com"]
    chaos = MariaDBChaos(nodes)
    
    # Lancer 4h de chaos
    chaos.run_chaos_day(duration_hours=4)
```

---

## 4. Documentation et Runbooks

### 4.1 Structure Runbook

```markdown
# Runbook Template

## MÃ©tadonnÃ©es
- **Titre**: [Titre court descriptif]
- **ID**: RUNBOOK-XXX
- **CatÃ©gorie**: [Incident / Maintenance / Routine]
- **SeveritÃ©**: [P0 / P1 / P2 / P3]
- **Auteur**: [Nom]
- **DerniÃ¨re mise Ã  jour**: [Date]
- **Version**: [X.Y]

## RÃ©sumÃ© ExÃ©cutif
[1-2 paragraphes: Quoi, Pourquoi, Quand utiliser ce runbook]

## Signes et SymptÃ´mes
- [ ] SymptÃ´me 1
- [ ] SymptÃ´me 2
- [ ] SymptÃ´me 3

## PrÃ©requis
- AccÃ¨s: [SSH, MySQL, AWS Console, etc.]
- Permissions: [sudo, DBA role, etc.]
- Outils: [Liste]

## RTO/RPO VisÃ©
- RTO: [X minutes]
- RPO: [Y minutes]

## ProcÃ©dure Ã‰tape par Ã‰tape

### Phase 1: Validation
**DurÃ©e estimÃ©e**: [X min]

1. [Ã‰tape 1]
   ```bash
   # Commande
   ```
   **RÃ©sultat attendu**: [...]

2. [Ã‰tape 2]
   ```bash
   # Commande
   ```
   **RÃ©sultat attendu**: [...]

### Phase 2: ExÃ©cution
**DurÃ©e estimÃ©e**: [Y min]

[...]

### Phase 3: Validation
**DurÃ©e estimÃ©e**: [Z min]

[...]

## Points de DÃ©cision
**Si [condition]:**
- [ ] Option A: [...]
- [ ] Option B: [...]

## Rollback
**Si Ã©chec, procÃ©dure de rollback:**

1. [Ã‰tape rollback 1]
2. [Ã‰tape rollback 2]

## Validation SuccÃ¨s
- [ ] CritÃ¨re 1
- [ ] CritÃ¨re 2
- [ ] CritÃ¨re 3

## Communication
**Qui notifier:**
- [ ] Ã‰quipe: [#channel]
- [ ] Management: [emails]
- [ ] Clients: [status page]

**Template message:**
```
[Incident/Maintenance] [Titre]
Status: [In Progress / Resolved]
Impact: [Description]
ETA: [Estimation]
```

## Post-Action
- [ ] Documenter timeline
- [ ] CrÃ©er ticket post-mortem
- [ ] Update KB
- [ ] Update ce runbook

## RÃ©fÃ©rences
- [Doc 1]
- [Doc 2]

## Changelog
| Version | Date | Auteur | Modifications |
|---------|------|--------|---------------|
| 1.0 | 2025-01-01 | John Doe | Version initiale |
```

### 4.2 Runbook Repository

```bash
# Structure repo runbooks
runbooks/
â”œâ”€â”€ README.md
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ incident_runbook.md
â”‚   â””â”€â”€ maintenance_runbook.md
â”œâ”€â”€ incidents/
â”‚   â”œâ”€â”€ RB-001-master-failover.md
â”‚   â”œâ”€â”€ RB-002-data-corruption.md
â”‚   â”œâ”€â”€ RB-003-disk-full.md
â”‚   â”œâ”€â”€ RB-004-replication-lag.md
â”‚   â””â”€â”€ RB-005-split-brain.md
â”œâ”€â”€ maintenance/
â”‚   â”œâ”€â”€ RB-M01-backup-restore.md
â”‚   â”œâ”€â”€ RB-M02-upgrade-mariadb.md
â”‚   â””â”€â”€ RB-M03-add-replica.md
â””â”€â”€ automation/
    â”œâ”€â”€ failover.sh
    â”œâ”€â”€ backup.sh
    â””â”€â”€ health_check.sh
```

---

## 5. Post-Mortem et AmÃ©lioration Continue

### 5.1 Template Post-Mortem

```markdown
# Post-Mortem: [Incident Title]

**Date Incident**: 2025-12-13
**SeveritÃ©**: P0
**DurÃ©e Total Downtime**: 15 minutes
**Participants Post-Mortem**: [Noms]

## RÃ©sumÃ© ExÃ©cutif (pour non-techniques)
[2-3 paragraphes rÃ©sumant l'incident, impact, et rÃ©solution]

## Timeline DÃ©taillÃ©e

| Time (UTC) | Event | Actor | System State |
|------------|-------|-------|--------------|
| 14:32:00 | Master DB crash (OOM) | System | Master DOWN |
| 14:32:15 | Alert PagerDuty | Monitoring | - |
| 14:32:30 | DBA acknowledged | John Doe | - |
| 14:33:00 | Validated failure | John Doe | - |
| 14:34:00 | Initiated failover | John Doe | Failover started |
| 14:35:30 | Replica promoted | mariadbmon | New master UP |
| 14:36:00 | VIP migrated | keepalived | Traffic restored |
| 14:37:00 | Validation complete | John Doe | RESOLVED |
| 14:47:00 | Old master rejoined | John Doe | Cluster healthy |

**Total RTO**: 5 minutes (dÃ©tection + rÃ©solution)
**Total RPO**: 0 seconds (semi-sync)

## Impact

### Utilisateurs
- **ImpactÃ©s**: ~5,000 utilisateurs actifs
- **DurÃ©e**: 5 minutes
- **ExpÃ©rience**: 
  - Queries retournaient erreurs "Lost connection"
  - Certaines transactions ont dÃ» Ãªtre rejouÃ©es (Transaction Replay ON)

### Business
- **Revenue Loss**: ~500â‚¬ (10,000â‚¬/h Ã— 5min/60)
- **Transactions Ã©chouÃ©es**: ~150 (30/min Ã— 5min)
- **SLA Breach**: Non (SLA = 99.9% mensuel, cet incident = 99.988%)

## Root Cause Analysis

### Cause ImmÃ©diate
Master MariaDB OOM (Out Of Memory) kill par kernel

### Cause Racine
Configuration `innodb_buffer_pool_size` trop Ã©levÃ©e (90% RAM) combinÃ©e avec :
- Spike trafic (+200% normal)
- Slow queries accumulant mÃ©moire
- Absence limite `max_connections`

### Pourquoi cascade
1. **Pourquoi OOM?** 
   â†’ Buffer pool 90% RAM + connection overhead + slow queries

2. **Pourquoi buffer pool 90%?** 
   â†’ Configuration basÃ©e sur recommandation gÃ©nÃ©rique, pas adaptÃ©e Ã  notre workload

3. **Pourquoi spike trafic non anticipÃ©?** 
   â†’ Pas de rate limiting applicatif

4. **Pourquoi slow queries?** 
   â†’ Index manquant sur nouvelle feature (dÃ©ployÃ©e J-2)

5. **Pourquoi index manquant dÃ©tectÃ© tard?** 
   â†’ Pas de review performance queries en staging

## Ce Qui a Bien FonctionnÃ© âœ…
1. Alerting rapide (15 secondes)
2. Failover automatique fonctionnel (90 secondes)
3. Transaction Replay MariaDB 11.8 â†’ Aucune perte transaction
4. Communication Ã©quipe efficace (Slack war room)
5. Runbook suivi correctement

## Ce Qui Doit ÃŠtre AmÃ©liorÃ© âŒ
1. Configuration buffer pool non adaptÃ©e
2. Pas de rate limiting applicatif
3. Pas de review performance avant deploy
4. Monitoring RAM insuffisant (alerte Ã  95%, trop tard)
5. Documentation limite `max_connections` manquante

## Actions Correctives

| Action | Responsable | Deadline | Statut | Ticket |
|--------|-------------|----------|--------|--------|
| RÃ©duire buffer pool Ã  70% RAM | DBA Team | 2025-12-15 | âœ… Done | OPS-1234 |
| ImplÃ©menter rate limiting app | Dev Team | 2025-12-20 | ğŸ”„ In Progress | DEV-5678 |
| Ajouter index manquant | DBA Team | 2025-12-14 | âœ… Done | OPS-1235 |
| Alert RAM Ã  80% (early warning) | SRE Team | 2025-12-16 | âœ… Done | OPS-1236 |
| Documenter max_connections tuning | DBA Team | 2025-12-17 | ğŸ”„ In Progress | DOC-789 |
| Review perf queries staging CI | Dev Team | 2025-12-31 | ğŸ“… Scheduled | DEV-5679 |
| Chaos test OOM scenario | SRE Team | 2026-01-15 | ğŸ“… Scheduled | OPS-1237 |

## LeÃ§ons Apprises

### Technique
1. **MariaDB 11.8 Transaction Replay = Game Changer**
   - Aucune transaction perdue malgrÃ© crash brutal
   - Transparence totale pour applications
   - Investment upgrade 11.8 validÃ©

2. **Semi-Sync Replication Essentiel**
   - RPO = 0 grÃ¢ce Ã  semi-sync
   - CoÃ»t performance nÃ©gligeable (<5%)

### Processus
1. **Runbooks Saved Us**
   - ProcÃ©dure suivie sans hÃ©sitation
   - RTO respectÃ© grÃ¢ce Ã  procÃ©dure claire

2. **Communication War Room Efficace**
   - Slack channel dÃ©diÃ©
   - Updates frÃ©quents
   - Pas de confusion

### Organisationnel
1. **Manque Review Performance Pre-Prod**
   - Besoin CI pipeline avec tests charge
   - Staging doit avoir donnÃ©es reprÃ©sentatives

## MÃ©triques

```
RTO Objectif: 5 minutes
RTO RÃ©alisÃ©: 5 minutes âœ…

RPO Objectif: 0
RPO RÃ©alisÃ©: 0 âœ…

MTBF (avant incident): 45 jours
MTTR: 5 minutes
Availability (mensuel): 99.988%
```

## Conclusion
Incident rÃ©solu efficacement grÃ¢ce Ã  automatisation (failover) et procÃ©dures (runbooks). Transaction Replay MariaDB 11.8 a empÃªchÃ© toute perte de donnÃ©es. Actions correctives identifiÃ©es pour Ã©viter rÃ©currence.

**Prochain Post-Mortem Review**: 2025-12-20

**Archivage**: [Link to ticket/wiki]
```

### 5.2 MÃ©triques de QualitÃ© DR

```sql
-- KPIs Disaster Recovery Ã  suivre

-- 1. MTBF (Mean Time Between Failures)
SELECT 
    DATE_FORMAT(incident_date, '%Y-%m') AS month,
    COUNT(*) AS incidents,
    ROUND(30 * 24 / COUNT(*), 1) AS mtbf_hours
FROM incidents
WHERE severity IN ('P0', 'P1')
GROUP BY DATE_FORMAT(incident_date, '%Y-%m')
ORDER BY month DESC;

-- 2. MTTR (Mean Time To Recover)
SELECT 
    DATE_FORMAT(incident_date, '%Y-%m') AS month,
    AVG(TIMESTAMPDIFF(MINUTE, detected_at, resolved_at)) AS avg_mttr_minutes,
    MIN(TIMESTAMPDIFF(MINUTE, detected_at, resolved_at)) AS best_mttr,
    MAX(TIMESTAMPDIFF(MINUTE, detected_at, resolved_at)) AS worst_mttr
FROM incidents
WHERE severity IN ('P0', 'P1')
GROUP BY DATE_FORMAT(incident_date, '%Y-%m');

-- 3. RTO Achievement Rate
SELECT 
    COUNT(*) AS total_incidents,
    SUM(CASE 
        WHEN TIMESTAMPDIFF(MINUTE, detected_at, resolved_at) <= rto_target 
        THEN 1 ELSE 0 
    END) AS within_rto,
    ROUND(100.0 * SUM(CASE 
        WHEN TIMESTAMPDIFF(MINUTE, detected_at, resolved_at) <= rto_target 
        THEN 1 ELSE 0 
    END) / COUNT(*), 2) AS rto_achievement_rate
FROM incidents
WHERE severity IN ('P0', 'P1');

-- 4. DR Test Success Rate
SELECT 
    DATE_FORMAT(test_date, '%Y-%m') AS month,
    COUNT(*) AS total_tests,
    SUM(CASE WHEN result = 'SUCCESS' THEN 1 ELSE 0 END) AS successful,
    ROUND(100.0 * SUM(CASE WHEN result = 'SUCCESS' THEN 1 ELSE 0 END) / COUNT(*), 2) AS success_rate
FROM dr_tests
GROUP BY DATE_FORMAT(test_date, '%Y-%m')
ORDER BY month DESC;
```

---

## âœ… Points ClÃ©s Ã  Retenir

- **RPO/RTO** : DÃ©finir objectifs rÃ©alistes basÃ©s sur impact business
- **Runbooks** : Documentation vivante, testÃ©e rÃ©guliÃ¨rement, pas un PDF poussiÃ©reux
- **Tests DR** : Minimum trimestriel, idÃ©alement mensuel avec chaos engineering
- **Post-Mortems** : Sans blÃ¢me, focus sur amÃ©lioration systÃ¨me
- **Automatisation** : Failover manuel = RTO Ã©levÃ©, automatisation = RTO < 5 min
- **MariaDB 11.8** : Transaction Replay rÃ©volutionne la gestion d'incidents
- **Communication** : War room, updates frÃ©quents, transparence stakeholders
- **AmÃ©lioration continue** : Chaque incident = opportunitÃ© d'apprentissage
- **Mesure** : MTBF, MTTR, RTO achievement â†’ KPIs essentiels
- **Culture** : RÃ©silience est une responsabilitÃ© d'Ã©quipe, pas individuelle

---

## ğŸ”— Ressources et RÃ©fÃ©rences

### Frameworks et Standards
- [ğŸ“– ITIL Incident Management](https://www.axelos.com/certifications/itil-service-management)
- [ğŸ“– ISO 22301 Business Continuity](https://www.iso.org/standard/75106.html)
- [ğŸ“– NIST Disaster Recovery](https://www.nist.gov/publications/contingency-planning-guide-federal-information-systems)

### Outils
- [PagerDuty](https://www.pagerduty.com/) - Incident management
- [Statuspage](https://www.atlassian.com/software/statuspage) - Status communication
- [Chaos Toolkit](https://chaostoolkit.org/) - Chaos engineering
- [Gremlin](https://www.gremlin.com/) - Enterprise chaos engineering

### Livres RecommandÃ©s
- **"Site Reliability Engineering"** - Google (O'Reilly)
- **"The Phoenix Project"** - Gene Kim
- **"Incident Management for Operations"** - Rob Schnepp (O'Reilly)

### Articles et Blogs
- **"How We Do Post-Mortems at Google"** - Google SRE Blog
- **"Runbook Template"** - Atlassian
- **"Chaos Engineering Principles"** - Netflix Tech Blog

---

**La meilleure stratÃ©gie de rÃ©cupÃ©ration est celle qui est testÃ©e rÃ©guliÃ¨rement, documentÃ©e clairement, et amÃ©liorÃ©e continuellement. Un incident bien gÃ©rÃ© renforce la confiance ; un incident mal gÃ©rÃ© dÃ©truit la rÃ©putation.**

â­ï¸ [Alternatives : ProxySQL et HAProxy](/14-haute-disponibilite/09-proxysql-haproxy.md)
