ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 16.11 Alerting et Incident Response

> **Niveau** : Expert  
> **DurÃ©e estimÃ©e** : 7-8 heures  
> **PrÃ©requis** : 
> - Sections 16.9-16.10 Monitoring et ObservabilitÃ© maÃ®trisÃ©es
> - ExpÃ©rience avec gestion d'incidents en production
> - ComprÃ©hension de Prometheus Alertmanager
> - Notions de SRE (Site Reliability Engineering)
> - FamiliaritÃ© avec on-call et PagerDuty

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :

- **Concevoir** une stratÃ©gie d'alerting efficace (Ã©viter alert fatigue)
- **DÃ©finir** SLOs/SLIs pertinents pour MariaDB
- **Configurer** Alertmanager pour routing et escalation intelligents
- **ImplÃ©menter** un processus d'incident response structurÃ©
- **GÃ©rer** on-call rotations et escalations
- **RÃ©diger** post-mortems constructifs
- **CrÃ©er** runbooks actionnables
- **IntÃ©grer** ChatOps (Slack, PagerDuty, MS Teams)
- **Mesurer** et amÃ©liorer MTTR/MTTD

---

## Introduction

### Le problÃ¨me de l'alert fatigue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Alert Fatigue Cycle                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1ï¸âƒ£  Trop d'alertes â†’ ğŸ“§ğŸ“§ğŸ“§ Inbox pleine                    â”‚
â”‚                                                              â”‚
â”‚  2ï¸âƒ£  MajoritÃ© false positives ou bruit                       â”‚
â”‚                                                              â”‚
â”‚  3ï¸âƒ£  Ã‰quipe ignore alertes (alert fatigue)                   â”‚
â”‚                                                              â”‚
â”‚  4ï¸âƒ£  Alerte critique noyÃ©e dans le bruit                     â”‚
â”‚                                                              â”‚
â”‚  5ï¸âƒ£  Incident critique manquÃ© â†’ OUTAGE ğŸ”¥                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Statistiques rÃ©elles** (source: Google SRE Book) :
- ğŸ“Š Ã‰quipe typique : **95% des alertes sont non-actionnables**
- â±ï¸ MTTD (Mean Time To Detect) augmente avec alert fatigue
- ğŸ˜° Burnout on-call corrÃ©lÃ© avec nombre d'alertes

### Alerting efficace : Principes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Good Alerting Principles                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. ACTIONABLE                                               â”‚
â”‚     Alerte = Action immÃ©diate requise                        â”‚
â”‚     âŒ "CPU high" (et alors?)                                â”‚
â”‚     âœ… "Database down, users impacted"                       â”‚
â”‚                                                              â”‚
â”‚  2. SYMPTOM-BASED (pas cause-based)                          â”‚
â”‚     Alerter sur impact utilisateur, pas mÃ©trique technique   â”‚
â”‚     âŒ "Buffer pool 90% full"                                â”‚
â”‚     âœ… "Query latency p95 > 2s (SLO breach)"                 â”‚
â”‚                                                              â”‚
â”‚  3. PROPORTIONAL                                             â”‚
â”‚     SeveritÃ© proportionnelle Ã  l'impact                      â”‚
â”‚     Critical = Revenue loss / Data loss                      â”‚
â”‚     Warning = Degraded but functional                        â”‚
â”‚                                                              â”‚
â”‚  4. CONTEXTUALIZED                                           â”‚
â”‚     Alerte avec context (what, when, who, impact)            â”‚
â”‚     Lien vers runbook, dashboard, logs                       â”‚
â”‚                                                              â”‚
â”‚  5. TUNED                                                    â”‚
â”‚     Faux positifs < 5%                                       â”‚
â”‚     RÃ©vision rÃ©guliÃ¨re (post-mortems)                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Pyramide d'alerting

### ModÃ¨le en pyramide inversÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Alerting Pyramid                              â”‚
â”‚                                                                  â”‚
â”‚                         ğŸ”´ CRITICAL                              â”‚
â”‚                    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²                            â”‚
â”‚                   â•± Page immÃ©diate   â•²                           â”‚
â”‚                  â•±  (PagerDuty)       â•²                          â”‚
â”‚                 â•±   Ex: DB down        â•²                         â”‚
â”‚                â•±    Impact users        â•²                        â”‚
â”‚               â•±                          â•²                       â”‚
â”‚              â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²                     â”‚
â”‚                                                                  â”‚
â”‚                    ğŸŸ  WARNING                                    â”‚
â”‚              â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²                  â”‚
â”‚             â•± Notification Slack/Email         â•²                 â”‚
â”‚            â•±  (pas de page)                     â•²                â”‚
â”‚           â•±   Ex: RÃ©plication lag >60s           â•²               â”‚
â”‚          â•±    Pas d'impact immÃ©diat               â•²              â”‚
â”‚         â•±                                          â•²             â”‚
â”‚        â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²            â”‚
â”‚                                                                  â”‚
â”‚                    ğŸŸ¡ INFO                                       â”‚
â”‚          â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²        â”‚
â”‚         â•± Dashboard seulement (pas de notification)      â•²       â”‚
â”‚        â•±  Ex: Connexions 60% utilisÃ©es                    â•²      â”‚
â”‚       â•±   Monitoring, pas d'action requise                 â•²     â”‚
â”‚      â•±                                                      â•²    â”‚
â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²   â”‚
â”‚                                                                  â”‚
â”‚                    ğŸŸ¢ DEBUG                                      â”‚
â”‚      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²  â”‚
â”‚     â•± Logs seulement (queryable on-demand)                    â•²  â”‚
â”‚    â•±  Ex: Query execution time dÃ©tails                         â•² â”‚
â”‚   â•±                                                             â•±â”‚
â”‚  â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•± â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RÃ¨gle: Plus on monte, MOINS il y a d'alertes
       Critical: < 5 alertes/semaine
       Warning: < 20 alertes/semaine
```

### Classification des alertes MariaDB

| Alerte | SeveritÃ© | Action | Destination |
|--------|----------|--------|-------------|
| **Database down** | ğŸ”´ Critical | Page immÃ©diate | PagerDuty |
| **Replication stopped** | ğŸ”´ Critical | Page immÃ©diate | PagerDuty |
| **Galera cluster < 3 nodes** | ğŸ”´ Critical | Page immÃ©diate | PagerDuty |
| **Disk space < 5%** | ğŸ”´ Critical | Page immÃ©diate | PagerDuty |
| **Connection pool exhausted** | ğŸ”´ Critical | Page immÃ©diate | PagerDuty |
| **Replication lag > 300s** | ğŸŸ  Warning | Slack notification | Slack #dba-team |
| **Slow queries > 20/min** | ğŸŸ  Warning | Slack notification | Slack #dba-team |
| **Buffer pool hit ratio < 95%** | ğŸŸ  Warning | Slack notification | Slack #dba-team |
| **Connections > 80%** | ğŸŸ¡ Info | Dashboard only | Grafana |
| **Query execution time p95** | ğŸŸ¡ Info | Dashboard only | Grafana |

---

## SLOs, SLIs, SLAs

### DÃ©finitions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SLO / SLI / SLA Explained                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SLI (Service Level Indicator)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  MÃ©trique quantitative de niveau de service                  â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - Query latency p95                                         â”‚
â”‚  - Availability (uptime %)                                   â”‚
â”‚  - Error rate                                                â”‚
â”‚                                                              â”‚
â”‚  SLO (Service Level Objective)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  Target pour un SLI                                          â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - Query latency p95 < 100ms (99.9% du temps)                â”‚
â”‚  - Availability > 99.9% (43min downtime/mois max)            â”‚
â”‚  - Error rate < 0.1%                                         â”‚
â”‚                                                              â”‚
â”‚  SLA (Service Level Agreement)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  Contrat business avec consÃ©quences si non respectÃ©          â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - Uptime 99.9% garanti, sinon 10% credit                    â”‚
â”‚  - SLA externe (clients) vs SLO interne (Ã©quipe)             â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  Relation:                                                   â”‚
â”‚  SLI (mesure) â†’ SLO (objectif) â†’ SLA (contrat)               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SLIs pour MariaDB

**4 Golden Signals (Google SRE)** adaptÃ©s Ã  MariaDB :

```yaml
# 1. LATENCY (Latence)
slis:
  query_latency_p50:
    description: "Median query execution time"
    query: "histogram_quantile(0.50, rate(mysql_global_status_queries[5m]))"
    unit: "seconds"
  
  query_latency_p95:
    description: "95th percentile query execution time"
    query: "histogram_quantile(0.95, rate(mysql_global_status_queries[5m]))"
    unit: "seconds"
  
  query_latency_p99:
    description: "99th percentile query execution time"
    query: "histogram_quantile(0.99, rate(mysql_global_status_queries[5m]))"
    unit: "seconds"

# 2. TRAFFIC (Trafic)
slis:
  qps:
    description: "Queries per second"
    query: "rate(mysql_global_status_queries[5m])"
    unit: "queries/second"
  
  connections:
    description: "Active connections"
    query: "mysql_global_status_threads_connected"
    unit: "count"

# 3. ERRORS (Erreurs)
slis:
  error_rate:
    description: "Rate of failed queries"
    query: "rate(mysql_global_status_aborted_connects[5m])"
    unit: "errors/second"
  
  replication_errors:
    description: "Replication errors"
    query: "mysql_slave_status_last_errno"
    unit: "count"

# 4. SATURATION (Saturation)
slis:
  connection_usage:
    description: "Connection pool utilization"
    query: "(mysql_global_status_threads_connected / mysql_global_variables_max_connections) * 100"
    unit: "percentage"
  
  buffer_pool_usage:
    description: "InnoDB buffer pool utilization"
    query: "(mysql_global_status_innodb_buffer_pool_bytes_data / mysql_global_variables_innodb_buffer_pool_size) * 100"
    unit: "percentage"
```

### SLOs pour MariaDB

**Exemple de SLOs rÃ©alistes** :

```yaml
slos:
  # Availability
  - name: "Database Availability"
    sli: "mysql_up"
    target: 99.95  # 99.95% uptime
    window: "30d"
    # = 21.6 minutes downtime max par mois
  
  # Latency
  - name: "Query Latency P95"
    sli: "query_latency_p95"
    target: 100  # < 100ms
    percentile: 99.9  # 99.9% des queries
    window: "30d"
  
  - name: "Query Latency P99"
    sli: "query_latency_p99"
    target: 500  # < 500ms
    percentile: 99.0
    window: "30d"
  
  # Error rate
  - name: "Query Success Rate"
    sli: "1 - (error_rate / qps)"
    target: 99.9  # < 0.1% errors
    window: "30d"
  
  # Replication
  - name: "Replication Lag"
    sli: "mysql_slave_status_seconds_behind_master"
    target: 60  # < 60 seconds
    percentile: 99.0
    window: "30d"
```

### Error Budget

**Concept** : Si SLO = 99.9%, alors error budget = 0.1%

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Error Budget Example                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SLO: 99.9% availability (monthly)                           â”‚
â”‚                                                              â”‚
â”‚  Error budget = 100% - 99.9% = 0.1%                          â”‚
â”‚                = 43.2 minutes de downtime autorisÃ© / mois    â”‚
â”‚                                                              â”‚
â”‚  Utilisation error budget:                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚  Semaine 1: Incident 15 min    â†’  35% budget utilisÃ©         â”‚
â”‚  Semaine 2: Incident 10 min    â†’  58% budget utilisÃ©         â”‚
â”‚  Semaine 3: Incident 20 min    â†’  104% budget dÃ©passÃ© âš ï¸     â”‚
â”‚                                                              â”‚
â”‚  Actions quand budget dÃ©passÃ©:                               â”‚
â”‚  - Freeze deployments (sauf fixes)                           â”‚
â”‚  - Focus 100% sur stability                                  â”‚
â”‚  - Root cause analysis obligatoire                           â”‚
â”‚  - AmÃ©lioration avant nouveaux features                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recording rule pour tracking** :

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mariadb-slo-recording
spec:
  groups:
  - name: mariadb-slo
    interval: 1m
    rules:
    # Availability SLI
    - record: slo:mariadb_availability:ratio_30d
      expr: |
        avg_over_time(mysql_up[30d])
    
    # Error budget (inverse availability)
    - record: slo:mariadb_error_budget:ratio_30d
      expr: |
        1 - slo:mariadb_availability:ratio_30d
    
    # Error budget burn rate (combien vite on consomme budget)
    - record: slo:mariadb_error_budget_burn_rate:1h
      expr: |
        (
          1 - avg_over_time(mysql_up[1h])
        ) / (1 - 0.999)  # 0.999 = SLO target
```

---

## Configuration Alertmanager avancÃ©e

### Routing complexe

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      
      # IntÃ©grations
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
    
    # Templates pour messages
    templates:
    - '/etc/alertmanager/templates/*.tmpl'
    
    # Routing principal
    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'namespace']
      group_wait: 10s        # Attendre 10s avant d'envoyer groupe
      group_interval: 5m     # Intervalle entre groupes
      repeat_interval: 12h   # Re-notifier si toujours actif
      
      routes:
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # CRITICAL: Database Down
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - match:
          alertname: MariaDBDown
          severity: critical
        receiver: 'dba-pagerduty'
        continue: true  # Aussi envoyer vers autres routes
        
        # Escalation aprÃ¨s 15min si pas rÃ©solu
        routes:
        - match:
            alertname: MariaDBDown
          receiver: 'platform-lead-pagerduty'
          group_wait: 15m
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # CRITICAL: Autres alertes critiques
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - match:
          severity: critical
          component: mariadb
        receiver: 'dba-pagerduty'
        continue: true
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # WARNING: Notifications Slack uniquement
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - match:
          severity: warning
          component: mariadb
        receiver: 'dba-slack'
        # Throttling: max 1 notification/5min pour mÃªme alerte
        group_interval: 5m
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # Production vs Non-Production
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - match:
          environment: production
          severity: critical
        receiver: 'dba-pagerduty'
      
      - match:
          environment: staging
          severity: critical
        receiver: 'dba-slack'  # Staging = pas de page, juste Slack
      
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # Heures ouvrables vs On-call
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - match_re:
          severity: warning
        receiver: 'dba-slack'
        # Mute warnings en dehors heures ouvrables
        mute_time_intervals:
        - weekends
        - nights
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Inhibition Rules (suppress alertes dÃ©rivÃ©es)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    inhibit_rules:
    # Si DB down, supprimer toutes les autres alertes MariaDB
    - source_match:
        alertname: MariaDBDown
      target_match_re:
        alertname: MariaDB.*
      equal: ['namespace', 'pod']
    
    # Si cluster Galera en perte de quorum, supprimer node alerts
    - source_match:
        alertname: GaleraClusterSizeReduced
        severity: critical
      target_match:
        alertname: GaleraNodeNotSynced
      equal: ['namespace']
    
    # Si rÃ©plication stopped, supprimer replication lag alerts
    - source_match:
        alertname: MariaDBReplicationStopped
      target_match:
        alertname: MariaDBReplicationLag
      equal: ['namespace', 'pod']
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Receivers (destinations)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    receivers:
    # Default (fallback)
    - name: 'default'
      slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
    
    # DBA PagerDuty (critical alerts)
    - name: 'dba-pagerduty'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: |
          {{ .GroupLabels.alertname }}
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ end }}
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          num_resolved: '{{ .Alerts.Resolved | len }}'
        client: 'Prometheus'
        client_url: 'http://prometheus.monitoring.svc:9090'
    
    # DBA Slack (warnings)
    - name: 'dba-slack'
      slack_configs:
      - channel: '#dba-team'
        send_resolved: true
        title: |
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Namespace:* {{ .Labels.namespace }}
          *Pod:* {{ .Labels.pod }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        actions:
        - type: button
          text: 'View Dashboard :grafana:'
          url: 'http://grafana.monitoring.svc/d/mariadb-overview'
        - type: button
          text: 'Silence :no_bell:'
          url: 'http://alertmanager.monitoring.svc/#/silences/new'
    
    # Platform Lead (escalation)
    - name: 'platform-lead-pagerduty'
      pagerduty_configs:
      - service_key: 'PLATFORM_LEAD_SERVICE_KEY'
        description: |
          ğŸš¨ ESCALATION: {{ .GroupLabels.alertname }}
          Unresolved after 15 minutes
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Mute Time Intervals (silences programmÃ©es)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    mute_time_intervals:
    - name: weekends
      time_intervals:
      - weekdays: ['saturday', 'sunday']
    
    - name: nights
      time_intervals:
      - times:
        - start_time: '22:00'
          end_time: '08:00'
```

### Templates de messages

```yaml
# /etc/alertmanager/templates/mariadb.tmpl
{{ define "slack.mariadb.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] 
ğŸ—„ï¸ MariaDB: {{ .GroupLabels.alertname }}
{{ end }}

{{ define "slack.mariadb.text" }}
{{ range .Alerts }}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
*Summary:* {{ .Annotations.summary }}
*Description:* {{ .Annotations.description }}

ğŸ“Š *Details:*
â€¢ Severity: {{ .Labels.severity }}
â€¢ Namespace: {{ .Labels.namespace }}
â€¢ Pod: {{ .Labels.pod }}
â€¢ Environment: {{ .Labels.environment }}

â° *Timing:*
â€¢ Started: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
{{ if .EndsAt }}â€¢ Ended: {{ .EndsAt.Format "2006-01-02 15:04:05" }}{{ end }}

ğŸ“– *Runbook:* {{ .Annotations.runbook_url }}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{{ end }}
{{ end }}
```

---

## Incident Response Process

### Incident Severity Levels

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Incident Severity Definitions                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ğŸ”´ SEV-1 (Critical)                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  Impact: Service down pour TOUS les utilisateurs             â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - Database complÃ¨tement down                                â”‚
â”‚  - Data corruption                                           â”‚
â”‚  - Security breach                                           â”‚
â”‚                                                              â”‚
â”‚  Response:                                                   â”‚
â”‚  - Page immÃ©diate (< 5 min)                                  â”‚
â”‚  - War room avec Incident Commander                          â”‚
â”‚  - Communication clients toutes les 30 min                   â”‚
â”‚  - Post-mortem obligatoire                                   â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  ğŸŸ  SEV-2 (High)                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚  Impact: Service dÃ©gradÃ© pour majoritÃ© utilisateurs          â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - RÃ©plication stopped (risque data loss)                    â”‚
â”‚  - Performance trÃ¨s dÃ©gradÃ©e (p95 > 5s)                      â”‚
â”‚  - 1 node Galera cluster down                                â”‚
â”‚                                                              â”‚
â”‚  Response:                                                   â”‚
â”‚  - Page (< 15 min)                                           â”‚
â”‚  - Incident Response Team mobilisÃ©e                          â”‚
â”‚  - Communication interne toutes les heures                   â”‚
â”‚  - Post-mortem recommandÃ©                                    â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  ğŸŸ¡ SEV-3 (Medium)                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  Impact: Subset utilisateurs ou degradation mineure          â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - Slow queries spike                                        â”‚
â”‚  - Connection pool 95% utilisÃ©                               â”‚
â”‚  - Replication lag 5 minutes                                 â”‚
â”‚                                                              â”‚
â”‚  Response:                                                   â”‚
â”‚  - Notification (pas de page)                                â”‚
â”‚  - Investigation pendant heures ouvrables                    â”‚
â”‚  - Fix dans next sprint                                      â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  ğŸŸ¢ SEV-4 (Low)                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  Impact: Aucun impact utilisateur                            â”‚
â”‚  Exemples:                                                   â”‚
â”‚  - MÃ©trique tendance prÃ©occupante                            â”‚
â”‚  - Warning dans logs                                         â”‚
â”‚                                                              â”‚
â”‚  Response:                                                   â”‚
â”‚  - Ticket backlog                                            â”‚
â”‚  - Investigation opportuniste                                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Incident Response Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Incident Response Lifecycle                        â”‚
â”‚                                                                 â”‚
â”‚  1ï¸âƒ£  DETECTION                                                  â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ Alerte Prometheus/Alertmanager                            â”‚
â”‚     â€¢ Page PagerDuty                                            â”‚
â”‚     â€¢ User report                                               â”‚
â”‚     â†“                                                           â”‚
â”‚     â±ï¸ MTTD (Mean Time To Detect) starts                        â”‚
â”‚                                                                 â”‚
â”‚  2ï¸âƒ£  RESPONSE                                                   â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ On-call engineer acknowledge (< 5min for SEV-1)           â”‚
â”‚     â€¢ CrÃ©er incident dans systÃ¨me (Jira, PagerDuty)             â”‚
â”‚     â€¢ DÃ©terminer severity                                       â”‚
â”‚     â†“                                                           â”‚
â”‚     â±ï¸ MTTA (Mean Time To Acknowledge)                          â”‚
â”‚                                                                 â”‚
â”‚  3ï¸âƒ£  TRIAGE                                                     â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ Incident Commander assignÃ© (si SEV-1/SEV-2)               â”‚
â”‚     â€¢ CrÃ©er war room (Slack channel, Zoom)                      â”‚
â”‚     â€¢ Mobiliser Ã©quipe (DBA, Platform, App)                     â”‚
â”‚     â€¢ Assess impact (combien d'utilisateurs?)                   â”‚
â”‚     â†“                                                           â”‚
â”‚                                                                 â”‚
â”‚  4ï¸âƒ£  INVESTIGATION                                              â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ Consulter dashboard (Grafana)                             â”‚
â”‚     â€¢ Analyser logs (Loki)                                      â”‚
â”‚     â€¢ Check traces (Jaeger)                                     â”‚
â”‚     â€¢ Runbook consultation                                      â”‚
â”‚     â€¢ HypothÃ¨ses â†’ Tests                                        â”‚
â”‚     â†“                                                           â”‚
â”‚                                                                 â”‚
â”‚  5ï¸âƒ£  MITIGATION                                                 â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ Action immÃ©diate pour restaurer service                   â”‚
â”‚     â€¢ Peut Ãªtre workaround temporaire                           â”‚
â”‚     â€¢ Exemples:                                                 â”‚
â”‚       - Restart MariaDB                                         â”‚
â”‚       - Rollback deployment                                     â”‚
â”‚       - Scale up resources                                      â”‚
â”‚       - Disable feature flag                                    â”‚
â”‚     â†“                                                           â”‚
â”‚     â±ï¸ MTTR (Mean Time To Repair)                               â”‚
â”‚                                                                 â”‚
â”‚  6ï¸âƒ£  RECOVERY                                                   â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ Service restaurÃ©                                          â”‚
â”‚     â€¢ VÃ©rifier SLIs revenus Ã  normal                            â”‚
â”‚     â€¢ Monitoring intensifiÃ© 24-48h                              â”‚
â”‚     â†“                                                           â”‚
â”‚                                                                 â”‚
â”‚  7ï¸âƒ£  POST-INCIDENT                                              â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚     â€¢ Post-mortem rÃ©union (dans 24-48h)                         â”‚
â”‚     â€¢ Documenter timeline                                       â”‚
â”‚     â€¢ Identifier root cause(s)                                  â”‚
â”‚     â€¢ Action items pour prÃ©venir rÃ©currence                     â”‚
â”‚     â€¢ Mettre Ã  jour runbooks                                    â”‚
â”‚     â€¢ AmÃ©liorer alerting si false positive                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Incident Command System

**RÃ´les pendant incident SEV-1/SEV-2** :

```yaml
roles:
  incident_commander:
    responsibilities:
    - Coordonner rÃ©ponse
    - DÃ©cisions go/no-go
    - Communication externe
    - Assign tasks
    person: "DBA Lead ou Platform Lead"
  
  technical_lead:
    responsibilities:
    - Investigation technique
    - HypothÃ¨ses et tests
    - ImplÃ©mentation fix
    person: "Senior DBA"
  
  communications_lead:
    responsibilities:
    - Status updates (Slack, email)
    - Customer communication
    - Documentation timeline
    person: "Product Manager ou Support Lead"
  
  scribe:
    responsibilities:
    - Document timeline (actions, dÃ©cisions)
    - Logs chat war room
    - PrÃ©parer post-mortem doc
    person: "Junior engineer ou PM"
```

---

## Runbooks

### Structure de runbook

**Template runbook MariaDB** :

```markdown
# Runbook: MariaDB Down

## Metadata
- **Alert Name**: MariaDBDown
- **Severity**: SEV-1 (Critical)
- **Owner**: DBA Team
- **Last Updated**: 2025-12-14
- **Reviewed By**: Platform Team

## Summary
MariaDB instance is down and not accepting connections.
**Impact**: Complete service outage for all users.

## Detection
- Prometheus alert: `mysql_up == 0`
- Users cannot access application
- PagerDuty page sent to on-call DBA

## Triage Questions
1. Is this a single pod or all pods?
   ```bash
   kubectl get pods -n databases -l app=mariadb
   ```

2. Are there recent deployments or changes?
   ```bash
   kubectl rollout history statefulset/mariadb -n databases
   ```

3. Are there disk space issues?
   ```bash
   kubectl exec -n databases mariadb-0 -- df -h
   ```

4. Are there OOM kills?
   ```bash
   kubectl describe pod -n databases mariadb-0 | grep -i oom
   ```

## Investigation Steps

### Step 1: Check Pod Status
```bash
kubectl get pods -n databases -l app=mariadb
kubectl describe pod -n databases mariadb-0
kubectl logs -n databases mariadb-0 --tail=100
```

**Expected Output**: Pod status, recent logs, error messages

### Step 2: Check MariaDB Error Log
```bash
kubectl exec -n databases mariadb-0 -- tail -100 /var/log/mysql/error.log
```

**Look for**:
- `InnoDB: Out of memory`
- `Can't create/write to file`
- `Disk is full`
- `Crash recovery`

### Step 3: Check Resources
```bash
# CPU/Memory
kubectl top pod -n databases mariadb-0

# Disk
kubectl exec -n databases mariadb-0 -- df -h /var/lib/mysql
```

### Step 4: Check Prometheus Metrics
Open Grafana dashboard: http://grafana/d/mariadb-overview
- Check QPS before crash
- Check slow queries
- Check connections spike

## Common Causes & Fixes

### Cause 1: Out of Memory (OOM)
**Symptoms**: Pod restarted, dmesg shows OOM kill
**Fix**:
```bash
# Increase memory limit
kubectl patch statefulset mariadb -n databases -p '
{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "mariadb",
          "resources": {
            "limits": {"memory": "16Gi"}
          }
        }]
      }
    }
  }
}'
```

### Cause 2: Disk Full
**Symptoms**: Error log shows "Disk is full"
**Fix**:
```bash
# Emergency: Purge old binlogs
kubectl exec -n databases mariadb-0 -- mysql -uroot -p${ROOT_PASSWORD} \
  -e "PURGE BINARY LOGS BEFORE DATE_SUB(NOW(), INTERVAL 1 DAY);"

# Long-term: Increase PVC size
kubectl patch pvc data-mariadb-0 -n databases -p '
{
  "spec": {
    "resources": {
      "requests": {"storage": "200Gi"}
    }
  }
}'
```

### Cause 3: Crash During Startup
**Symptoms**: Pod CrashLoopBackOff, error log shows "InnoDB: Database corruption"
**Fix**:
```bash
# Try crash recovery
kubectl exec -n databases mariadb-0 -- \
  mysqld --innodb-force-recovery=1

# If still fails, restore from backup (see Backup Runbook)
```

### Cause 4: Configuration Error
**Symptoms**: Error log shows "unknown variable"
**Fix**:
```bash
# Check my.cnf syntax
kubectl exec -n databases mariadb-0 -- \
  mysqld --help --verbose | grep error

# Rollback ConfigMap
kubectl rollout undo configmap/mariadb-config -n databases
kubectl rollout restart statefulset/mariadb -n databases
```

## Escalation Path

1. **Try fixes above** (15 minutes)
2. If not resolved â†’ **Escalate to DBA Lead**
   - Slack: @dba-lead
   - Phone: +33 6 XX XX XX XX

3. If still not resolved (30 minutes) â†’ **Escalate to Platform Lead**
   - Slack: @platform-lead
   - Phone: +33 6 YY YY YY YY

4. If critical (1 hour) â†’ **Page CTO**

## Communication Templates

### Slack Announcement (Initial)
```
ğŸš¨ INCIDENT: MariaDB Down (SEV-1)
Status: Investigating
Impact: Complete service outage
ETA: Unknown (investigating)
War Room: #incident-2025-12-14-mariadb
Updates: Every 15 minutes
```

### Customer Status Page
```
We are currently experiencing database connectivity issues.
Our team is actively working on a resolution.
Next update: 14:30 UTC
```

## Post-Incident Actions

- [ ] Write post-mortem (within 48h)
- [ ] Update this runbook with learnings
- [ ] Improve alerting if false positive
- [ ] Add to knowledge base
- [ ] Schedule preventive actions

## Related Runbooks
- [Runbook: MariaDB Performance Degradation](runbook-performance.md)
- [Runbook: MariaDB Backup & Restore](runbook-backup.md)
- [Runbook: Galera Cluster Recovery](runbook-galera.md)

## References
- [MariaDB Documentation](https://mariadb.com/kb/en/)
- [Grafana Dashboard](http://grafana/d/mariadb-overview)
- [On-call Schedule](https://pagerduty.com/schedules/mariadb)
```

---

## Post-Mortems

### Template de post-mortem

```markdown
# Post-Mortem: MariaDB Outage - 2025-12-14

## Metadata
- **Date**: 2025-12-14
- **Duration**: 45 minutes (14:15 - 15:00 UTC)
- **Severity**: SEV-1
- **Incident Commander**: Alice (DBA Lead)
- **Authors**: Alice, Bob (Platform), Carol (App Team)

## Executive Summary
MariaDB primary database went down due to disk space exhaustion,
causing complete service outage for 45 minutes. Approximately
10,000 users were impacted. Service was restored by purging old
binary logs. Revenue impact estimated at $5,000.

## Impact
- **Users Affected**: ~10,000 (100% of active users)
- **Duration**: 45 minutes
- **Revenue Loss**: ~$5,000
- **SLA Breach**: Yes (99.9% â†’ 99.87% for month)
- **Error Budget**: 32% consumed in single incident

## Timeline

| Time (UTC) | Event |
|------------|-------|
| 14:15:00 | ğŸ”´ Prometheus alert: MariaDBDown fired |
| 14:15:30 | Alice (on-call) paged via PagerDuty |
| 14:16:00 | Alice acknowledges, starts investigation |
| 14:18:00 | War room created (#incident-2025-12-14) |
| 14:20:00 | Bob joins war room |
| 14:22:00 | Identified: Disk 100% full |
| 14:25:00 | Root cause: Binary logs not purged (auto-purge disabled) |
| 14:27:00 | Decision: Manual purge of binlogs >7 days |
| 14:30:00 | Binlogs purged, 50GB freed |
| 14:32:00 | MariaDB restarted successfully |
| 14:35:00 | Service health checks passing |
| 14:40:00 | Traffic restored to 100% |
| 15:00:00 | Incident closed, monitoring continues |

## Root Cause Analysis

### What Happened
1. Binary logging enabled for replication
2. Auto-purge disabled in my.cnf (expire_logs_days = 0)
3. Binlogs accumulated over 3 months â†’ 150GB
4. Disk 200GB â†’ reached 100% capacity
5. MariaDB unable to write, crashed

### Why It Happened
- **Immediate cause**: Disk full
- **Contributing factors**:
  1. Binary log retention not configured properly
  2. No monitoring on disk usage growth rate
  3. No alert for disk >90% (only >95%)
  4. Binlog rotation not tested in staging

### Five Whys
1. **Why did MariaDB crash?**
   â†’ Disk was full

2. **Why was disk full?**
   â†’ Binary logs accumulated to 150GB

3. **Why did binlogs accumulate?**
   â†’ Auto-purge was disabled (expire_logs_days = 0)

4. **Why was auto-purge disabled?**
   â†’ Initial config mistake, never caught in code review

5. **Why wasn't it caught?**
   â†’ No monitoring on binlog disk usage, no tests for this scenario

## What Went Well
- âœ… Alert fired immediately (MTTD < 1 minute)
- âœ… On-call responded quickly (MTTA < 2 minutes)
- âœ… War room established quickly
- âœ… Root cause identified in 7 minutes
- âœ… Mitigation effective (purge)
- âœ… Communication clear and frequent
- âœ… No data loss

## What Went Wrong
- âŒ Config error not caught in review
- âŒ No staging test for binlog accumulation
- âŒ Disk alert threshold too high (95% vs 85%)
- âŒ No growth rate monitoring
- âŒ Runbook didn't cover this scenario

## Action Items

| Priority | Action | Owner | Due Date | Status |
|----------|--------|-------|----------|--------|
| ğŸ”´ P0 | Enable binlog auto-purge (expire_logs_days=7) | Alice | 2025-12-15 | âœ… Done |
| ğŸ”´ P0 | Add disk usage alert at 85% | Bob | 2025-12-15 | âœ… Done |
| ğŸŸ  P1 | Add binlog size monitoring | Alice | 2025-12-18 | ğŸŸ¡ In Progress |
| ğŸŸ  P1 | Update runbook with binlog scenario | Carol | 2025-12-20 | â³ Todo |
| ğŸŸ  P1 | Add staging test for binlog accumulation | Bob | 2025-12-22 | â³ Todo |
| ğŸŸ¡ P2 | Review all my.cnf configs for similar issues | Alice | 2025-12-30 | â³ Todo |
| ğŸŸ¡ P2 | Increase PVC size to 500GB | Bob | 2026-01-15 | â³ Todo |

## Lessons Learned

### Technical
1. **Config reviews must include operational impact**
   - Adding checklist: "How will this config behave after 3 months?"

2. **Disk monitoring insufficient**
   - Need both absolute threshold (85%) AND growth rate

3. **Binlog management critical**
   - Documented best practice: expire_logs_days = 7-14

### Process
1. **Staging must mirror production config**
   - Binlogs disabled in staging â†’ missed this issue

2. **Runbooks need regular review**
   - Last update: 6 months ago (too old)

3. **Error budget tracking needed**
   - 32% consumed in one incident â†’ need visibility

## Follow-up Review
- **1 week**: Review action items progress
- **1 month**: Verify no recurrence
- **3 months**: Incorporate learnings into training

## Attachments
- [Grafana Dashboard Screenshot](link)
- [Error Log Excerpt](link)
- [War Room Chat Log](link)
```

### Post-Mortem Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Blameless Post-Mortem Culture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  âœ… DO:                                                      â”‚
â”‚  - Focus on WHAT happened, not WHO                           â”‚
â”‚  - Assume good intent                                        â”‚
â”‚  - Look for systemic issues                                  â”‚
â”‚  - Learn and improve                                         â”‚
â”‚  - Share publicly (transparency)                             â”‚
â”‚                                                              â”‚
â”‚  âŒ DON'T:                                                   â”‚
â”‚  - Blame individuals                                         â”‚
â”‚  - Punish mistakes                                           â”‚
â”‚  - Hide incidents                                            â”‚
â”‚  - Skip action items                                         â”‚
â”‚                                                              â”‚
â”‚  Mantra: "Bad systems create bad outcomes"                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ChatOps Integration

### Slack Commands

```python
# Slack bot pour incident management
# /incident create <severity> <description>
@slack_app.command("/incident")
def incident_create(ack, command, say):
    ack()
    
    # Parse command
    severity = command['text'].split()[0]  # SEV-1, SEV-2, etc.
    description = ' '.join(command['text'].split()[1:])
    
    # Create incident in PagerDuty
    incident = pagerduty.create_incident(
        title=f"{severity}: {description}",
        service_id="MARIADB_SERVICE_ID",
        urgency="high" if severity in ["SEV-1", "SEV-2"] else "low"
    )
    
    # Create war room channel
    channel_name = f"incident-{incident['id']}"
    channel = slack_client.conversations_create(name=channel_name)
    
    # Post to channel
    say(
        channel=channel['id'],
        text=f"""
ğŸš¨ **INCIDENT CREATED**
Severity: {severity}
Description: {description}
PagerDuty: {incident['html_url']}
War Room: #{channel_name}

@here - Incident Response Team, please join!
        """
    )
    
    # Start incident timeline
    timeline = IncidentTimeline(incident['id'])
    timeline.add_event("Incident created", command['user_name'])

# /incident status
@slack_app.command("/incident-status")
def incident_status(ack, command, say):
    ack()
    
    # Get active incidents from PagerDuty
    incidents = pagerduty.list_incidents(status="triggered")
    
    if not incidents:
        say("âœ… No active incidents")
    else:
        message = "ğŸš¨ **ACTIVE INCIDENTS**\n"
        for inc in incidents:
            message += f"â€¢ {inc['title']} - {inc['status']}\n"
        say(message)

# /runbook <alert-name>
@slack_app.command("/runbook")
def get_runbook(ack, command, say):
    ack()
    
    alert_name = command['text']
    runbook_url = f"https://wiki.company.com/runbooks/{alert_name}"
    
    say(f"ğŸ“– Runbook: {runbook_url}")
```

---

## âœ… Points clÃ©s Ã  retenir

- **Alert fatigue = ennemi #1** : Trop d'alertes â†’ ignorer alertes critiques
- **Pyramide alerting** : Critical (page) < Warning (Slack) < Info (dashboard)
- **SLOs/SLIs** : Mesurer ce qui compte pour users (latency, availability, errors)
- **Error budget** : SLO breach = freeze features, focus stability
- **Symptom-based alerting** : Alerter sur impact user, pas mÃ©trique technique
- **Routing intelligent** : Severity, environment, time-of-day
- **Inhibition rules** : Supprimer alertes dÃ©rivÃ©es (DB down â†’ mute other alerts)
- **Incident severity** : SEV-1 (total outage) vs SEV-4 (monitoring only)
- **Runbooks essentiels** : Investigation steps, fixes, escalation
- **Post-mortems blameless** : Focus on systems, not people
- **ChatOps** : Incident management dans Slack

ğŸ’¡ **Golden rule** : "Hope is not a strategy. Monitor, alert, respond, learn, improve."

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle
- [ğŸ“– Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)
- [ğŸ“– Google SRE Workbook - Alerting](https://sre.google/workbook/alerting-on-slos/)
- [ğŸ“– Prometheus Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/)

### Articles de rÃ©fÃ©rence
- [ğŸ“ The Art of SLOs (Alex Hidalgo)](https://www.alex-hidalgo.com/the-art-of-slos)
- [ğŸ“ Incident Response (PagerDuty)](https://response.pagerduty.com/)

### Outils
- [ğŸ”§ PagerDuty](https://www.pagerduty.com/)
- [ğŸ”§ Opsgenie](https://www.atlassian.com/software/opsgenie)
- [ğŸ”§ OnCall (Grafana)](https://grafana.com/products/oncall/)

---

## â¡ï¸ Prochaine section

**16.12 GitOps pour bases de donnÃ©es** : ArgoCD, FluxCD, declarative database management, drift detection, automated reconciliation.

---

**MariaDB** : Version 11.8 LTS

â­ï¸ [GitOps pour les bases de donnÃ©es](/16-devops-automatisation/12-gitops-bases-donnees.md)
