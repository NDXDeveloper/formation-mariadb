ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 16.5.3 RÃ©plication avec mariadb-operator

> **Niveau** : AvancÃ© Ã  Expert  
> **DurÃ©e estimÃ©e** : 3-4 heures  
> **PrÃ©requis** : Section 16.5.1 (Installation mariadb-operator), Section 13 (RÃ©plication), Kubernetes avancÃ©

## ğŸ¯ Objectifs d'apprentissage

Ã€ l'issue de cette section, vous serez capable de :
- DÃ©ployer une topologie de rÃ©plication Primary-Replica avec mariadb-operator
- Configurer la rÃ©plication GTID dans Kubernetes
- GÃ©rer plusieurs replicas en lecture avec load balancing automatique
- ImplÃ©menter le monitoring de la rÃ©plication via les CRDs
- Orchestrer les failovers et switchovers en production
- Mettre en place une architecture multi-rÃ©gion avec rÃ©plication

---

## Introduction

La rÃ©plication MariaDB dans un contexte Kubernetes prÃ©sente des dÃ©fis spÃ©cifiques liÃ©s Ã  la nature Ã©phÃ©mÃ¨re des pods et Ã  la gestion des identitÃ©s rÃ©seau. Le **mariadb-operator** simplifie considÃ©rablement cette complexitÃ© en fournissant des Custom Resources dÃ©diÃ©es qui automatisent :

- La configuration initiale de la rÃ©plication (GTID, binlog)
- La gestion des connexions Primary â†” Replica
- Le provisionnement automatique des credentials de rÃ©plication
- Le monitoring et le health checking
- Les opÃ©rations de maintenance (failover, switchover)

Cette approche dÃ©clarative permet de dÃ©finir l'Ã©tat dÃ©sirÃ© de l'architecture de rÃ©plication dans des manifestes YAML, tandis que l'operator se charge de maintenir cet Ã©tat.

### DiffÃ©rence avec Galera Cluster

| Aspect | RÃ©plication Primary-Replica | Galera Cluster |
|--------|----------------------------|----------------|
| **ModÃ¨le** | Asynchrone/Semi-synchrone | Synchrone multi-master |
| **Writes** | Primary uniquement | Tous les nÅ“uds |
| **Reads** | Primary + Replicas | Tous les nÅ“uds |
| **Cas d'usage** | Read scaling, DR, Analytics | High availability, write scaling |
| **ComplexitÃ©** | Plus simple | Plus complexe |
| **Latence writes** | Faible | Moyenne (certification) |

ğŸ’¡ **Conseil** : Utilisez la rÃ©plication Primary-Replica pour scaler les lectures et sÃ©parer les charges OLTP/OLAP. PrivilÃ©giez Galera pour la haute disponibilitÃ© avec tolÃ©rance aux pannes.

---

## Architecture de rÃ©plication avec mariadb-operator

### Composants clÃ©s

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Kubernetes Cluster                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MariaDB CRD    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  mariadb-operatorâ”‚  â”‚
â”‚  â”‚  (Primary)      â”‚         â”‚                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                            â”‚           â”‚
â”‚           â–¼                            â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  StatefulSet    â”‚         â”‚   Service        â”‚  â”‚
â”‚  â”‚  mariadb-0      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   (Primary)      â”‚  â”‚
â”‚  â”‚  (Primary)      â”‚         â”‚   ClusterIP      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                        â”‚
â”‚           â”‚ Binlog Replication (GTID)              â”‚
â”‚           â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Replica StatefulSets                â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚mariadb-1 â”‚  â”‚mariadb-2 â”‚  â”‚mariadb-3 â”‚   â”‚   â”‚
â”‚  â”‚  â”‚(Replica) â”‚  â”‚(Replica) â”‚  â”‚(Replica) â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚             â”‚             â”‚            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                         â”‚                          â”‚
â”‚                         â–¼                          â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚           â”‚  Service (Replicas)  â”‚                 â”‚
â”‚           â”‚  Load Balancer       â”‚                 â”‚
â”‚           â”‚  (Read-only)         â”‚                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ressources Kubernetes crÃ©Ã©es

L'operator crÃ©e automatiquement :
- **StatefulSet** pour le Primary (identitÃ© stable)
- **StatefulSet** pour chaque Replica
- **Services** :
  - `<name>-primary` : AccÃ¨s au nÅ“ud Primary
  - `<name>-replica` : Load balancing vers les Replicas
  - `<name>-internal` : Communication inter-pods
- **PersistentVolumeClaims** pour chaque instance
- **Secrets** : Credentials de rÃ©plication, root password
- **ConfigMaps** : Configuration MariaDB personnalisÃ©e

---

## Configuration du Primary pour la rÃ©plication

### Manifest MariaDB Primary avec rÃ©plication activÃ©e

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-primary
  namespace: production
spec:
  # Version et image
  image: mariadb:11.8.2
  imagePullPolicy: IfNotPresent
  
  # Credentials root
  rootPasswordSecretKeyRef:
    name: mariadb-root
    key: password
    generate: false  # Utiliser un secret existant
  
  # Stockage persistant
  storage:
    size: 100Gi
    storageClassName: fast-ssd
    resizeInUseVolumes: true
    waitForVolumeResize: true
  
  # Configuration rÃ©plication Primary
  replication:
    enabled: true
    primary:
      podIndex: 0
      automaticFailover: true
    
    # Configuration GTID (recommandÃ©)
    gtid:
      enabled: true
      domainId: 0
    
    # Utilisateur de rÃ©plication
    replication:
      user: repl_user
      passwordSecretKeyRef:
        name: mariadb-replication
        key: password
        generate: true  # L'operator gÃ©nÃ¨re le mot de passe
  
  # Configuration MariaDB
  myCnf: |
    [mariadb]
    # Identifiant serveur (obligatoire)
    server-id=1
    
    # Binary logs (rÃ©plication)
    log-bin=mariadb-bin
    binlog-format=ROW
    binlog-row-image=FULL
    expire-logs-days=7
    max-binlog-size=1G
    sync-binlog=1
    
    # GTID
    gtid-domain-id=0
    gtid-strict-mode=ON
    log-slave-updates=ON
    
    # Performance
    innodb-buffer-pool-size=8G
    innodb-log-file-size=1G
    innodb-flush-log-at-trx-commit=1
    
    # SÃ©curitÃ©
    bind-address=0.0.0.0
    require-secure-transport=ON
    
    # Charset
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci
  
  # Ressources CPU/Memory
  resources:
    requests:
      cpu: "4"
      memory: "12Gi"
    limits:
      cpu: "8"
      memory: "16Gi"
  
  # Services
  service:
    type: ClusterIP
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-internal: "true"
  
  # Monitoring
  metrics:
    enabled: true
    serviceMonitor:
      prometheusRelease: prometheus-operator
  
  # Affinity pour isolation
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - mariadb
        topologyKey: kubernetes.io/hostname
  
  # TolÃ©rances pour nÅ“uds dÃ©diÃ©s bases de donnÃ©es
  tolerations:
  - key: "database"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
```

### Secret pour le root password

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mariadb-root
  namespace: production
type: Opaque
stringData:
  password: "VotreSuperMotDePasseSecurise123!"
```

âš ï¸ **Attention** : En production, utilisez un gestionnaire de secrets comme **Sealed Secrets**, **External Secrets Operator**, ou **Vault** pour ne jamais stocker les credentials en clair dans Git.

---

## Configuration des Replicas

### Manifest pour dÃ©ployer 3 Replicas

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica
  namespace: production
spec:
  # Image identique au Primary
  image: mariadb:11.8.2
  imagePullPolicy: IfNotPresent
  
  # Nombre de replicas
  replicas: 3
  
  # RÃ©fÃ©rence au Primary
  replication:
    enabled: true
    replica:
      connectionTimeout: 10s
      connectionRetries: 10
      gtid: current_pos
      
      # RÃ©fÃ©rence au Primary
      primaryName: mariadb-primary
      primaryService:
        name: mariadb-primary
        namespace: production
      
      # Credentials de rÃ©plication (partagÃ©s avec Primary)
      replicationUser: repl_user
      replicationPasswordSecretKeyRef:
        name: mariadb-replication
        key: password
  
  # Stockage persistant par replica
  storage:
    size: 100Gi
    storageClassName: fast-ssd
  
  # Configuration MariaDB Replica
  myCnf: |
    [mariadb]
    # Server ID unique par replica (gÃ©rÃ© automatiquement)
    # server-id sera dÃ©fini dynamiquement (101, 102, 103...)
    
    # Relay logs
    relay-log=relay-bin
    relay-log-index=relay-bin.index
    relay-log-purge=ON
    
    # GTID
    gtid-strict-mode=ON
    log-slave-updates=ON
    
    # Read-only (important!)
    read-only=ON
    
    # Performance optimisÃ©e pour lectures
    innodb-buffer-pool-size=8G
    
    # Cache de requÃªtes dÃ©sactivÃ© (dÃ©prÃ©ciÃ©)
    query-cache-type=0
    
    # Optimisations lecture
    innodb-read-io-threads=8
    innodb-thread-concurrency=16
    
    # Charset
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci
  
  # Ressources par replica
  resources:
    requests:
      cpu: "2"
      memory: "10Gi"
    limits:
      cpu: "4"
      memory: "12Gi"
  
  # Service pour les Replicas
  service:
    type: ClusterIP
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-internal: "true"
  
  # Anti-affinity pour distribution des replicas
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/instance
              operator: In
              values:
              - mariadb-replica
          topologyKey: kubernetes.io/hostname
      # Ã‰viter de cohabiter avec le Primary
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/instance
              operator: In
              values:
              - mariadb-primary
          topologyKey: topology.kubernetes.io/zone
  
  # Metrics
  metrics:
    enabled: true
    serviceMonitor:
      prometheusRelease: prometheus-operator
```

### Configuration avancÃ©e : Replica lag acceptable

Pour des workloads tolÃ©rants Ã  un lÃ©ger retard de rÃ©plication :

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica-analytics
  namespace: production
spec:
  # ... configuration de base ...
  
  replication:
    enabled: true
    replica:
      # DÃ©lai de rÃ©plication acceptable (pour analytics)
      maxDelay: "30s"
      
      # Parallel replication
      parallelThreads: 4
      parallelMode: "conservative"  # ou "optimistic" pour MariaDB 11.8+
  
  myCnf: |
    [mariadb]
    # RÃ©plication parallÃ¨le
    slave-parallel-threads=4
    slave-parallel-mode=conservative
    slave-parallel-max-queued=134217728  # 128MB
    
    # Optimisations pour analytics
    read-only=ON
    tmp-table-size=256M
    max-heap-table-size=256M
```

ğŸ†• **NouveautÃ© MariaDB 11.8** : Le mode `optimistic` pour `slave-parallel-mode` amÃ©liore les performances de rÃ©plication parallÃ¨le en rÃ©duisant les conflits de verrouillage.

---

## Services et accÃ¨s

### Service Primary (writes)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mariadb-primary-svc
  namespace: production
  labels:
    app: mariadb
    role: primary
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/instance: mariadb-primary
    statefulset.kubernetes.io/pod-name: mariadb-primary-0
  ports:
  - name: mysql
    port: 3306
    targetPort: 3306
  sessionAffinity: ClientIP  # Important pour les transactions
```

### Service Replicas (reads avec load balancing)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mariadb-replica-svc
  namespace: production
  labels:
    app: mariadb
    role: replica
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: mariadb
    app.kubernetes.io/instance: mariadb-replica
  ports:
  - name: mysql
    port: 3306
    targetPort: 3306
  # Pas de sessionAffinity pour distribuer les lectures
```

### Utilisation depuis une application

```yaml
# ConfigMap avec les endpoints
apiVersion: v1
kind: ConfigMap
metadata:
  name: mariadb-endpoints
  namespace: production
data:
  MARIADB_PRIMARY_HOST: "mariadb-primary-svc.production.svc.cluster.local"
  MARIADB_REPLICA_HOST: "mariadb-replica-svc.production.svc.cluster.local"
  MARIADB_PORT: "3306"
  MARIADB_DATABASE: "myapp"
```

Exemple de connexion applicative (Python avec SQLAlchemy) :

```python
from sqlalchemy import create_engine, event
from sqlalchemy.orm import sessionmaker
import os

# Connexions sÃ©parÃ©es read/write
PRIMARY_URL = f"mysql+pymysql://user:pass@{os.getenv('MARIADB_PRIMARY_HOST')}:3306/myapp"
REPLICA_URL = f"mysql+pymysql://user:pass@{os.getenv('MARIADB_REPLICA_HOST')}:3306/myapp"

# Engine pour les writes
engine_write = create_engine(
    PRIMARY_URL,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True
)

# Engine pour les reads (load balancing DNS round-robin)
engine_read = create_engine(
    REPLICA_URL,
    pool_size=50,
    max_overflow=20,
    pool_pre_ping=True
)

# Sessions
SessionWrite = sessionmaker(bind=engine_write)
SessionRead = sessionmaker(bind=engine_read)

# Utilisation
with SessionRead() as session:
    # Lecture depuis un replica
    users = session.query(User).all()

with SessionWrite() as session:
    # Ã‰criture sur le primary
    new_user = User(name="Alice")
    session.add(new_user)
    session.commit()
```

ğŸ’¡ **Conseil** : Pour une gestion automatique read/write splitting au niveau infrastructure, utilisez **ProxySQL** ou **MaxScale** comme proxy devant MariaDB.

---

## Monitoring de la rÃ©plication

### VÃ©rification du statut via kubectl

```bash
# Statut du Primary
kubectl exec -it mariadb-primary-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW MASTER STATUS\G"

# Output attendu :
# *************************** 1. row ***************************
#              File: mariadb-bin.000003
#          Position: 154892
#      Binlog_Do_DB: 
#  Binlog_Ignore_DB: 
# Executed_Gtid_Set: 0-1-1234

# Statut des Replicas
for i in {0..2}; do
  echo "=== mariadb-replica-$i ==="
  kubectl exec -it mariadb-replica-$i -n production -- \
    mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW SLAVE STATUS\G" | \
    grep -E "Slave_IO_Running|Slave_SQL_Running|Seconds_Behind_Master|Last_Error"
done

# Output attendu pour chaque replica :
#              Slave_IO_Running: Yes
#             Slave_SQL_Running: Yes
#       Seconds_Behind_Master: 0
#                  Last_Error: 
```

### ServiceMonitor Prometheus

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mariadb-replication-metrics
  namespace: production
spec:
  selector:
    matchLabels:
      app: mariadb
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
```

### MÃ©triques clÃ©s Ã  surveiller

**Pour le Primary :**
```promql
# Nombre de replicas connectÃ©s
mysql_slave_status_slave_io_running

# Position du binlog (croissance)
rate(mysql_binlog_size_bytes[5m])

# Taille des binlogs
mysql_binlog_file_number
```

**Pour les Replicas :**
```promql
# Lag de rÃ©plication (critique)
mysql_slave_status_seconds_behind_master > 10

# RÃ©plication en cours
mysql_slave_status_slave_io_running == 1
mysql_slave_status_slave_sql_running == 1

# Erreurs de rÃ©plication
mysql_slave_status_last_errno != 0
```

### Dashboard Grafana pour la rÃ©plication

Configuration d'alertes Prometheus :

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mariadb-replication-alerts
  namespace: production
spec:
  groups:
  - name: mariadb-replication
    interval: 30s
    rules:
    # Alerte si un replica est arrÃªtÃ©
    - alert: MariaDBReplicaStopped
      expr: mysql_slave_status_slave_sql_running == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "MariaDB Replica SQL thread stopped on {{ $labels.pod }}"
        description: "Replica {{ $labels.pod }} has SQL thread stopped. Check replication status."
    
    # Alerte si lag > 30 secondes
    - alert: MariaDBReplicationLag
      expr: mysql_slave_status_seconds_behind_master > 30
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High replication lag on {{ $labels.pod }}"
        description: "Replica {{ $labels.pod }} is {{ $value }}s behind master."
    
    # Alerte si erreur de rÃ©plication
    - alert: MariaDBReplicationError
      expr: mysql_slave_status_last_errno != 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Replication error on {{ $labels.pod }}"
        description: "Error code {{ $value }} - {{ $labels.last_error }}"
```

---

## Health checks et readiness probes

### Configuration des probes dans le CRD

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica
spec:
  # ... configuration ...
  
  # Liveness probe : redÃ©marre le pod si MariaDB ne rÃ©pond pas
  livenessProbe:
    exec:
      command:
      - /bin/sh
      - -c
      - |
        mariadb -u root -p"${MARIADB_ROOT_PASSWORD}" \
          -e "SELECT 1" > /dev/null 2>&1
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  
  # Readiness probe : retire du service si rÃ©plication en retard
  readinessProbe:
    exec:
      command:
      - /bin/sh
      - -c
      - |
        # VÃ©rifie que la rÃ©plication fonctionne ET lag < 60s
        SLAVE_STATUS=$(mariadb -u root -p"${MARIADB_ROOT_PASSWORD}" \
          -e "SHOW SLAVE STATUS\G" 2>/dev/null)
        
        IO_RUNNING=$(echo "$SLAVE_STATUS" | grep "Slave_IO_Running:" | awk '{print $2}')
        SQL_RUNNING=$(echo "$SLAVE_STATUS" | grep "Slave_SQL_Running:" | awk '{print $2}')
        LAG=$(echo "$SLAVE_STATUS" | grep "Seconds_Behind_Master:" | awk '{print $2}')
        
        if [ "$IO_RUNNING" = "Yes" ] && [ "$SQL_RUNNING" = "Yes" ]; then
          if [ "$LAG" != "NULL" ] && [ "$LAG" -lt 60 ]; then
            exit 0
          fi
        fi
        exit 1
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
```

ğŸ’¡ **Conseil** : Ajustez le seuil de lag dans la readiness probe selon vos besoins. Pour des lectures ultra-fraÃ®ches, descendez Ã  5-10s. Pour de l'analytics, tolÃ©rez jusqu'Ã  plusieurs minutes.

---

## Failover et switchover

### Failover automatique du Primary

Configuration dans le CRD :

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-primary
spec:
  replication:
    enabled: true
    primary:
      automaticFailover: true
      podIndex: 0
    
    # Configuration du failover
    failover:
      enabled: true
      # Temps d'attente avant de dÃ©clarer le primary down
      timeout: 30s
      # Nombre de tentatives de reconnexion
      retries: 3
      # Promotion automatique du replica le plus Ã  jour
      autoPromote: true
```

Lorsque le Primary Ã©choue :
1. L'operator dÃ©tecte l'Ã©chec via les health checks
2. Il identifie le replica avec le GTID le plus avancÃ©
3. Il promeut ce replica en nouveau Primary (`STOP SLAVE; RESET MASTER`)
4. Il reconfigure les autres replicas pour pointer vers le nouveau Primary
5. Il met Ã  jour les Services pour rediriger le trafic

### Switchover planifiÃ© (maintenance)

Pour basculer sur un nouveau Primary sans interruption :

```bash
# 1. Identifier le replica cible (avec lag minimal)
kubectl exec -it mariadb-replica-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW SLAVE STATUS\G" | \
  grep "Seconds_Behind_Master"

# 2. CrÃ©er un Job de switchover
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: mariadb-switchover
  namespace: production
spec:
  template:
    spec:
      containers:
      - name: switchover
        image: mariadb:11.8.2
        command:
        - /bin/bash
        - -c
        - |
          # Ã‰tape 1 : Bloquer les Ã©critures sur l'ancien primary
          mariadb -h mariadb-primary-svc -u root -p"\${ROOT_PASSWORD}" \\
            -e "SET GLOBAL read_only=ON; FLUSH TABLES WITH READ LOCK;"
          
          # Ã‰tape 2 : Attendre que le replica soit Ã  jour
          while true; do
            LAG=\$(mariadb -h mariadb-replica-0 -u root -p"\${ROOT_PASSWORD}" \\
              -e "SHOW SLAVE STATUS\G" | grep "Seconds_Behind_Master:" | awk '{print \$2}')
            if [ "\$LAG" = "0" ]; then break; fi
            sleep 1
          done
          
          # Ã‰tape 3 : Promouvoir le replica
          mariadb -h mariadb-replica-0 -u root -p"\${ROOT_PASSWORD}" \\
            -e "STOP SLAVE; RESET MASTER; SET GLOBAL read_only=OFF;"
          
          # Ã‰tape 4 : DÃ©grader l'ancien primary en replica
          GTID=\$(mariadb -h mariadb-replica-0 -u root -p"\${ROOT_PASSWORD}" \\
            -e "SELECT @@gtid_binlog_pos" -s -N)
          
          mariadb -h mariadb-primary-svc -u root -p"\${ROOT_PASSWORD}" \\
            -e "UNLOCK TABLES; SET GLOBAL gtid_slave_pos='\$GTID'; \\
                CHANGE MASTER TO MASTER_HOST='mariadb-replica-0', \\
                MASTER_USER='repl_user', MASTER_PASSWORD='\${REPL_PASSWORD}', \\
                MASTER_USE_GTID=slave_pos; START SLAVE;"
          
          echo "Switchover completed successfully"
        env:
        - name: ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-root
              key: password
        - name: REPL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-replication
              key: password
      restartPolicy: OnFailure
  backoffLimit: 3
EOF
```

âš ï¸ **Attention** : Le switchover manuel nÃ©cessite de mettre Ã  jour les labels et selectors des Services pour pointer vers le nouveau Primary. L'operator automatisÃ© gÃ¨re cela automatiquement.

---

## Architecture multi-rÃ©gion avec rÃ©plication en cascade

### Topologie Primary â†’ Regional Replicas â†’ Local Replicas

```
Region 1 (Primary)          Region 2                Region 3
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Primary   â”‚â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â–¶â”‚  Replica-R2 â”‚â”€â”€â”€â”€â”¬â”€â”€â”€â–¶â”‚  Replica-R3 â”‚
â”‚  mariadb-p  â”‚     â”‚      â”‚  (Regional) â”‚    â”‚    â”‚  (Regional) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚             â”‚            â”‚            â”‚          â”‚
      â–¼             â”‚            â–¼            â”‚          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replica-L1 â”‚     â”‚      â”‚  Replica-L2 â”‚    â”‚    â”‚  Replica-L3 â”‚
â”‚   (Local)   â”‚     â”‚      â”‚   (Local)   â”‚    â”‚    â”‚   (Local)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     Cross-region replication
                      (Binlog over WAN)
```

### Configuration Region 2 (Replica rÃ©gional)

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica-region2
  namespace: production
spec:
  image: mariadb:11.8.2
  
  # RÃ©plication depuis le Primary (autre rÃ©gion)
  replication:
    enabled: true
    replica:
      # Connexion WAN vers Region 1
      primaryName: mariadb-primary
      primaryService:
        name: mariadb-primary-external
        namespace: production
      
      # Credentials de rÃ©plication
      replicationUser: repl_user_global
      replicationPasswordSecretKeyRef:
        name: mariadb-replication-global
        key: password
      
      # Configuration pour WAN
      connectionTimeout: 30s
      connectionRetries: 10
      
      # Semi-synchronous replication dÃ©sactivÃ© (latence WAN)
      semiSync: false
  
  # Configuration pour servir de relay (log-slave-updates)
  myCnf: |
    [mariadb]
    server-id=200
    log-bin=mariadb-bin
    relay-log=relay-bin
    log-slave-updates=ON
    
    # Important : permet aux replicas locaux de se connecter
    read-only=ON
    
    # Optimisation WAN
    slave-net-timeout=120
    slave-compressed-protocol=ON
    
    # Binlog pour replicas locaux
    binlog-format=ROW
    expire-logs-days=3
  
  storage:
    size: 200Gi
    storageClassName: regional-ssd
```

### Replicas locaux en Region 2

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica-local-region2
  namespace: production
spec:
  image: mariadb:11.8.2
  replicas: 2
  
  # RÃ©plication depuis le replica rÃ©gional (cascade)
  replication:
    enabled: true
    replica:
      primaryName: mariadb-replica-region2
      primaryService:
        name: mariadb-replica-region2
        namespace: production
      
      replicationUser: repl_user
      replicationPasswordSecretKeyRef:
        name: mariadb-replication-local
        key: password
  
  myCnf: |
    [mariadb]
    # Server ID unique par replica local
    read-only=ON
    relay-log=relay-bin
```

ğŸ’¡ **Conseil** : Utilisez la rÃ©plication en cascade pour limiter la bande passante WAN et amÃ©liorer la latence locale. Le replica rÃ©gional sert de hub pour sa zone gÃ©ographique.

---

## Best practices de production

### 1. **GTID obligatoire**

Toujours activer GTID pour simplifier les failovers et la gestion de la topologie :

```ini
[mariadb]
gtid-domain-id=0
gtid-strict-mode=ON
log-slave-updates=ON
```

### 2. **Monitoring proactif**

Alerter **avant** que les problÃ¨mes ne deviennent critiques :

```promql
# Alerte prÃ©ventive si lag > 10s pendant 5min
ALERT ReplicationLagWarning
  IF mysql_slave_status_seconds_behind_master > 10
  FOR 5m
  LABELS { severity="warning" }
  ANNOTATIONS {
    summary = "Replication lag increasing",
    description = "Consider investigating load on primary or network issues"
  }
```

### 3. **Isolation rÃ©seau**

Utilisez des **NetworkPolicies** pour sÃ©curiser la communication :

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mariadb-replication-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: mariadb
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Autoriser les replicas Ã  se connecter au primary sur port 3306
  - from:
    - podSelector:
        matchLabels:
          app: mariadb
    ports:
    - protocol: TCP
      port: 3306
  # Autoriser les applications Ã  se connecter
  - from:
    - namespaceSelector:
        matchLabels:
          name: app-namespace
    ports:
    - protocol: TCP
      port: 3306
  egress:
  # Autoriser la rÃ©plication vers le primary
  - to:
    - podSelector:
        matchLabels:
          app: mariadb
          role: primary
    ports:
    - protocol: TCP
      port: 3306
  # DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
```

### 4. **Backup rÃ©gulier des replicas**

Les replicas sont parfaits pour les backups sans impacter le Primary :

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: Backup
metadata:
  name: mariadb-backup-from-replica
  namespace: production
spec:
  mariadbRef:
    name: mariadb-replica
  
  # SÃ©lection d'un replica spÃ©cifique pour le backup
  podSelector:
    matchLabels:
      statefulset.kubernetes.io/pod-name: mariadb-replica-2
  
  storage:
    s3:
      bucket: mariadb-backups
      endpoint: s3.amazonaws.com
      region: eu-west-1
      prefix: replica-backups/
  
  schedule:
    cron: "0 2 * * *"  # 2h du matin tous les jours
  
  # Utiliser mariabackup pour backup physique
  backupType: physical
```

### 5. **Resource quotas et limites**

Ã‰vitez le "noisy neighbor" effect avec des limites appropriÃ©es :

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mariadb-quota
  namespace: production
spec:
  hard:
    requests.cpu: "32"
    requests.memory: "128Gi"
    limits.cpu: "64"
    limits.memory: "256Gi"
    persistentvolumeclaims: "10"
    requests.storage: "2Ti"
```

### 6. **Encryption en transit**

Pour la rÃ©plication inter-rÃ©gions, activez SSL :

```yaml
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica-region2
spec:
  replication:
    replica:
      ssl:
        enabled: true
        caSecretKeyRef:
          name: mariadb-ca-cert
          key: ca.crt
        clientCertSecretKeyRef:
          name: mariadb-client-cert
          key: tls.crt
        clientKeySecretKeyRef:
          name: mariadb-client-cert
          key: tls.key
```

ğŸ†• **NouveautÃ© MariaDB 11.8** : TLS est maintenant activÃ© par dÃ©faut pour toutes les connexions, y compris la rÃ©plication. Assurez-vous d'avoir des certificats valides.

### 7. **Test de failover rÃ©gulier**

Planifiez des **chaos engineering drills** :

```bash
#!/bin/bash
# Script de test de failover (Ã  exÃ©cuter en non-prod d'abord)

echo "=== Test de Failover MariaDB ==="

# 1. VÃ©rifier l'Ã©tat initial
kubectl exec -it mariadb-primary-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW MASTER STATUS\G"

# 2. Simuler une panne du primary (delete pod)
echo "Simulation panne du primary..."
kubectl delete pod mariadb-primary-0 -n production

# 3. Observer la promotion automatique
echo "Attente de la promotion automatique (30s)..."
sleep 30

# 4. VÃ©rifier le nouveau primary
kubectl get pods -n production -l app=mariadb -o wide

# 5. Valider la rÃ©plication
for i in {0..2}; do
  kubectl exec -it mariadb-replica-$i -n production -- \
    mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW SLAVE STATUS\G" | \
    grep -E "Master_Host|Slave_IO_Running|Slave_SQL_Running"
done

# 6. VÃ©rifier l'intÃ©gritÃ© des donnÃ©es
kubectl exec -it mariadb-replica-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SELECT COUNT(*) FROM myapp.users;"

echo "Test de failover terminÃ©"
```

---

## Troubleshooting courant

### ProblÃ¨me 1 : Replica ne dÃ©marre pas la rÃ©plication

**SymptÃ´mes :**
```
Slave_IO_Running: No
Last_IO_Error: Got fatal error 1236 from master when reading data from binary log
```

**Diagnostic :**
```bash
# VÃ©rifier les logs du replica
kubectl logs mariadb-replica-0 -n production | grep -i error

# VÃ©rifier la connectivitÃ© au primary
kubectl exec -it mariadb-replica-0 -n production -- \
  mariadb -h mariadb-primary-svc -u repl_user -p"${REPL_PASSWORD}" -e "SELECT 1"

# VÃ©rifier la position GTID
kubectl exec -it mariadb-primary-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SELECT @@gtid_binlog_pos"
```

**Solution :**
```bash
# Reset la rÃ©plication avec la bonne position GTID
GTID=$(kubectl exec -it mariadb-primary-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SELECT @@gtid_binlog_pos" -s -N)

kubectl exec -it mariadb-replica-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" <<EOF
STOP SLAVE;
SET GLOBAL gtid_slave_pos='${GTID}';
CHANGE MASTER TO MASTER_USE_GTID=slave_pos;
START SLAVE;
SHOW SLAVE STATUS\G
EOF
```

### ProblÃ¨me 2 : Lag de rÃ©plication croissant

**SymptÃ´mes :**
```
Seconds_Behind_Master: 3456  # Augmente continuellement
```

**Diagnostic :**
```bash
# VÃ©rifier la charge du primary
kubectl exec -it mariadb-primary-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW FULL PROCESSLIST"

# VÃ©rifier les slow queries
kubectl exec -it mariadb-primary-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "
    SELECT * FROM mysql.slow_log 
    WHERE query_time > 1 
    ORDER BY start_time DESC 
    LIMIT 10"

# VÃ©rifier l'I/O du replica
kubectl exec -it mariadb-replica-0 -n production -- iostat -x 5
```

**Solutions possibles :**

1. **Activer la rÃ©plication parallÃ¨le :**
```sql
SET GLOBAL slave_parallel_threads=4;
SET GLOBAL slave_parallel_mode='conservative';
```

2. **Augmenter les ressources du replica :**
```yaml
resources:
  requests:
    cpu: "4"      # Doubler
    memory: "16Gi"
```

3. **Optimiser les index sur le replica :**
```sql
-- Identifier les requÃªtes lentes
SELECT * FROM performance_schema.events_statements_summary_by_digest
ORDER BY sum_timer_wait DESC LIMIT 10;
```

### ProblÃ¨me 3 : Split-brain aprÃ¨s une partition rÃ©seau

**SymptÃ´mes :**
- Plusieurs nÅ“uds pensent Ãªtre le Primary
- Erreurs de duplication de clÃ©s

**PrÃ©vention :**
```yaml
# Configuration avec fencing automatique
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-primary
spec:
  replication:
    primary:
      automaticFailover: true
      # Fencing : empÃªche l'ancien primary de revenir
      fencing:
        enabled: true
        method: "STONITH"  # Shoot The Other Node In The Head
```

**RÃ©cupÃ©ration manuelle :**
```bash
# 1. Identifier le nÅ“ud avec le GTID le plus avancÃ©
for pod in mariadb-primary-0 mariadb-replica-{0..2}; do
  echo "=== $pod ==="
  kubectl exec -it $pod -n production -- \
    mariadb -u root -p"${ROOT_PASSWORD}" -e "SELECT @@gtid_binlog_pos"
done

# 2. Choisir ce nÅ“ud comme nouveau primary
# 3. Forcer les autres Ã  se synchroniser depuis lui
kubectl exec -it mariadb-replica-0 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" <<EOF
STOP SLAVE;
RESET SLAVE ALL;
SET GLOBAL gtid_slave_pos='<GTID_DU_NOUVEAU_PRIMARY>';
CHANGE MASTER TO 
  MASTER_HOST='mariadb-primary-svc',
  MASTER_USER='repl_user',
  MASTER_PASSWORD='${REPL_PASSWORD}',
  MASTER_USE_GTID=slave_pos;
START SLAVE;
EOF
```

---

## Migration d'une rÃ©plication existante vers l'operator

### Ã‰tape 1 : PrÃ©parer l'environnement

```bash
# 1. Installer l'operator
kubectl apply -f https://github.com/mariadb-operator/mariadb-operator/releases/latest/download/manifests.yaml

# 2. CrÃ©er les secrets nÃ©cessaires
kubectl create secret generic mariadb-root \
  --from-literal=password="${ROOT_PASSWORD}" \
  -n production

kubectl create secret generic mariadb-replication \
  --from-literal=password="${REPL_PASSWORD}" \
  -n production
```

### Ã‰tape 2 : CrÃ©er les CRDs pour les nÅ“uds existants

```yaml
# Import du Primary existant
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-primary-imported
  namespace: production
spec:
  # Ne pas crÃ©er de nouveau pod, adopter l'existant
  adopt:
    enabled: true
    statefulSet:
      name: mariadb-existing
      namespace: production
  
  # Configuration existante
  image: mariadb:11.8.2
  rootPasswordSecretKeyRef:
    name: mariadb-root
    key: password
  
  storage:
    size: 100Gi
    storageClassName: fast-ssd
  
  replication:
    enabled: true
    primary:
      podIndex: 0
```

### Ã‰tape 3 : Importer progressivement les replicas

```bash
# Pour chaque replica existant
for i in 0 1 2; do
  cat <<EOF | kubectl apply -f -
apiVersion: k8s.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-replica-imported-$i
  namespace: production
spec:
  adopt:
    enabled: true
    statefulSet:
      name: mariadb-replica-$i
      namespace: production
  
  image: mariadb:11.8.2
  replication:
    enabled: true
    replica:
      primaryName: mariadb-primary-imported
EOF
done
```

### Ã‰tape 4 : Valider la gestion par l'operator

```bash
# VÃ©rifier que l'operator gÃ¨re les ressources
kubectl get mariadb -n production -o wide

# Tester une opÃ©ration gÃ©rÃ©e (ex: ajout d'un replica)
kubectl scale mariadb mariadb-replica-imported --replicas=4 -n production

# Valider la rÃ©plication
kubectl exec -it mariadb-replica-imported-3 -n production -- \
  mariadb -u root -p"${ROOT_PASSWORD}" -e "SHOW SLAVE STATUS\G"
```

---

## âœ… Points clÃ©s Ã  retenir

- **Architecture dÃ©clarative** : L'operator gÃ¨re automatiquement la topologie de rÃ©plication via des CRDs Kubernetes
- **GTID obligatoire** : Simplifie les failovers et la gestion de la topologie en production
- **Services sÃ©parÃ©s** : Utiliser des services distincts pour Primary (writes) et Replicas (reads) avec load balancing
- **Monitoring proactif** : Alerter sur le lag de rÃ©plication, les threads SQL/IO arrÃªtÃ©s, et les erreurs avant qu'elles ne soient critiques
- **Health checks adaptÃ©s** : Readiness probes doivent vÃ©rifier non seulement la disponibilitÃ© mais aussi le lag de rÃ©plication
- **Failover automatique** : Configuration `automaticFailover: true` pour la promotion automatique en cas de panne
- **RÃ©plication parallÃ¨le** : Activer `slave-parallel-threads` pour rÃ©duire le lag sur des workloads avec beaucoup d'Ã©critures
- **Cascade pour multi-rÃ©gion** : Utiliser des replicas rÃ©gionaux comme relais pour optimiser la bande passante WAN
- **Encryption en transit** : Toujours activer SSL pour la rÃ©plication inter-rÃ©gions (natif depuis MariaDB 11.8)
- **Tests rÃ©guliers** : Chaos engineering pour valider les procÃ©dures de failover avant les incidents rÃ©els

---

## ğŸ”— Ressources et rÃ©fÃ©rences

### Documentation officielle
- **[MariaDB Replication Overview](https://mariadb.com/kb/en/replication-overview/)** - Guide complet de la rÃ©plication MariaDB
- **[GTID Documentation](https://mariadb.com/kb/en/gtid/)** - Global Transaction ID
- **[mariadb-operator GitHub](https://github.com/mariadb-operator/mariadb-operator)** - Operator Kubernetes officiel
- **[mariadb-operator Replication Guide](https://github.com/mariadb-operator/mariadb-operator/blob/main/docs/REPLICATION.md)** - Documentation spÃ©cifique rÃ©plication

### Articles techniques
- **[Parallel Replication in MariaDB](https://mariadb.com/kb/en/parallel-replication/)** - Optimisation performances rÃ©plication
- **[Replication with SSL/TLS](https://mariadb.com/kb/en/replication-with-secure-connections/)** - SÃ©curisation des connexions
- **[Kubernetes StatefulSets for Databases](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)** - Best practices Kubernetes

### Outils
- **[Prometheus mysqld_exporter](https://github.com/prometheus/mysqld_exporter)** - MÃ©triques MariaDB pour Prometheus
- **[Grafana MariaDB Dashboards](https://grafana.com/grafana/dashboards/?search=mariadb)** - Dashboards communautaires
- **[pt-heartbeat](https://www.percona.com/doc/percona-toolkit/LATEST/pt-heartbeat.html)** - Mesure prÃ©cise du lag de rÃ©plication

### Webinars et confÃ©rences
- **[MariaDB Server Fest 2024](https://mariadb.org/fest/)** - Talks sur Kubernetes et rÃ©plication
- **[Replication Best Practices](https://www.youtube.com/watch?v=...)** - Webinar MariaDB Foundation

---

## â¡ï¸ Section suivante

**16.5.4 Backups automatisÃ©s avec operator** : StratÃ©gies de sauvegarde dans Kubernetes avec le mariadb-operator, incluant les backups vers S3, la restauration automatique, et l'intÃ©gration avec Velero.

â­ï¸ [Backups automatisÃ©s](/16-devops-automatisation/05.4-backups-automatises.md)
